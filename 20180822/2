Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3180) |  Loss2: (0.0000) | Acc: (8.00%) (11/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3123) |  Loss2: (0.0000) | Acc: (8.00%) (116/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.3021) |  Loss2: (0.0000) | Acc: (9.00%) (257/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2945) |  Loss2: (0.0000) | Acc: (11.00%) (449/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2871) |  Loss2: (0.0000) | Acc: (13.00%) (694/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2803) |  Loss2: (0.0000) | Acc: (14.00%) (960/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2729) |  Loss2: (0.0000) | Acc: (15.00%) (1243/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2647) |  Loss2: (0.0000) | Acc: (16.00%) (1544/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2567) |  Loss2: (0.0000) | Acc: (17.00%) (1833/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2497) |  Loss2: (0.0000) | Acc: (18.00%) (2114/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2407) |  Loss2: (0.0000) | Acc: (18.00%) (2410/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2310) |  Loss2: (0.0000) | Acc: (19.00%) (2719/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2216) |  Loss2: (0.0000) | Acc: (19.00%) (3020/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2134) |  Loss2: (0.0000) | Acc: (19.00%) (3314/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2060) |  Loss2: (0.0000) | Acc: (19.00%) (3606/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1973) |  Loss2: (0.0000) | Acc: (20.00%) (3936/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1891) |  Loss2: (0.0000) | Acc: (20.00%) (4238/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1815) |  Loss2: (0.0000) | Acc: (20.00%) (4571/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1745) |  Loss2: (0.0000) | Acc: (21.00%) (4919/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1664) |  Loss2: (0.0000) | Acc: (21.00%) (5265/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1585) |  Loss2: (0.0000) | Acc: (21.00%) (5625/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1502) |  Loss2: (0.0000) | Acc: (22.00%) (5972/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1424) |  Loss2: (0.0000) | Acc: (22.00%) (6325/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1359) |  Loss2: (0.0000) | Acc: (22.00%) (6672/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1288) |  Loss2: (0.0000) | Acc: (22.00%) (7018/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1226) |  Loss2: (0.0000) | Acc: (22.00%) (7353/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1152) |  Loss2: (0.0000) | Acc: (23.00%) (7747/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1086) |  Loss2: (0.0000) | Acc: (23.00%) (8146/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.1020) |  Loss2: (0.0000) | Acc: (23.00%) (8557/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0960) |  Loss2: (0.0000) | Acc: (24.00%) (8951/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0893) |  Loss2: (0.0000) | Acc: (24.00%) (9373/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0822) |  Loss2: (0.0000) | Acc: (24.00%) (9784/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0764) |  Loss2: (0.0000) | Acc: (24.00%) (10195/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0705) |  Loss2: (0.0000) | Acc: (24.00%) (10585/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0649) |  Loss2: (0.0000) | Acc: (25.00%) (10993/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0583) |  Loss2: (0.0000) | Acc: (25.00%) (11426/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0538) |  Loss2: (0.0000) | Acc: (25.00%) (11813/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0479) |  Loss2: (0.0000) | Acc: (25.00%) (12252/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0429) |  Loss2: (0.0000) | Acc: (25.00%) (12677/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0378) |  Loss2: (0.0000) | Acc: (26.00%) (13101/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.7863) | Acc: (34.00%) (3443/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.1901, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(772.8687, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(767.9380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1531.8798, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(520.6130, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.8420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4334.5688, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1453.8347, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6141.6021, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12280.2295, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4097.2881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17365.8711, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8560) |  Loss2: (0.0000) | Acc: (35.00%) (45/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8010) |  Loss2: (0.0000) | Acc: (34.00%) (490/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.7947) |  Loss2: (0.0000) | Acc: (35.00%) (967/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.7924) |  Loss2: (0.0000) | Acc: (35.00%) (1413/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.7903) |  Loss2: (0.0000) | Acc: (35.00%) (1885/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7811) |  Loss2: (0.0000) | Acc: (36.00%) (2367/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7807) |  Loss2: (0.0000) | Acc: (36.00%) (2822/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7801) |  Loss2: (0.0000) | Acc: (35.00%) (3269/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7783) |  Loss2: (0.0000) | Acc: (35.00%) (3721/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7728) |  Loss2: (0.0000) | Acc: (35.00%) (4186/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7679) |  Loss2: (0.0000) | Acc: (36.00%) (4671/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7634) |  Loss2: (0.0000) | Acc: (36.00%) (5175/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7610) |  Loss2: (0.0000) | Acc: (36.00%) (5612/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7589) |  Loss2: (0.0000) | Acc: (36.00%) (6061/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7546) |  Loss2: (0.0000) | Acc: (36.00%) (6524/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7512) |  Loss2: (0.0000) | Acc: (36.00%) (6995/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7477) |  Loss2: (0.0000) | Acc: (36.00%) (7471/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7459) |  Loss2: (0.0000) | Acc: (36.00%) (7926/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7428) |  Loss2: (0.0000) | Acc: (36.00%) (8417/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7390) |  Loss2: (0.0000) | Acc: (36.00%) (8892/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7365) |  Loss2: (0.0000) | Acc: (36.00%) (9383/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7334) |  Loss2: (0.0000) | Acc: (36.00%) (9853/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7305) |  Loss2: (0.0000) | Acc: (36.00%) (10327/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7279) |  Loss2: (0.0000) | Acc: (36.00%) (10817/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7247) |  Loss2: (0.0000) | Acc: (36.00%) (11326/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7225) |  Loss2: (0.0000) | Acc: (36.00%) (11819/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7200) |  Loss2: (0.0000) | Acc: (36.00%) (12302/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7174) |  Loss2: (0.0000) | Acc: (36.00%) (12808/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7143) |  Loss2: (0.0000) | Acc: (37.00%) (13317/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7126) |  Loss2: (0.0000) | Acc: (37.00%) (13832/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7094) |  Loss2: (0.0000) | Acc: (37.00%) (14348/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7064) |  Loss2: (0.0000) | Acc: (37.00%) (14844/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7045) |  Loss2: (0.0000) | Acc: (37.00%) (15338/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7017) |  Loss2: (0.0000) | Acc: (37.00%) (15843/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.6991) |  Loss2: (0.0000) | Acc: (37.00%) (16372/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.6964) |  Loss2: (0.0000) | Acc: (37.00%) (16896/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.6943) |  Loss2: (0.0000) | Acc: (37.00%) (17393/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.6916) |  Loss2: (0.0000) | Acc: (37.00%) (17930/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.6885) |  Loss2: (0.0000) | Acc: (37.00%) (18439/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.6864) |  Loss2: (0.0000) | Acc: (37.00%) (18919/50000)
# TEST : Loss: (1.5386) | Acc: (41.00%) (4168/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.6477) |  Loss2: (0.0000) | Acc: (35.00%) (45/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5670) |  Loss2: (0.0000) | Acc: (41.00%) (578/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5605) |  Loss2: (0.0000) | Acc: (41.00%) (1124/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5672) |  Loss2: (0.0000) | Acc: (42.00%) (1673/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5732) |  Loss2: (0.0000) | Acc: (42.00%) (2206/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5677) |  Loss2: (0.0000) | Acc: (42.00%) (2758/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5632) |  Loss2: (0.0000) | Acc: (42.00%) (3309/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5599) |  Loss2: (0.0000) | Acc: (42.00%) (3884/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5619) |  Loss2: (0.0000) | Acc: (42.00%) (4433/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5602) |  Loss2: (0.0000) | Acc: (42.00%) (4998/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5590) |  Loss2: (0.0000) | Acc: (42.00%) (5547/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5550) |  Loss2: (0.0000) | Acc: (42.00%) (6104/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5520) |  Loss2: (0.0000) | Acc: (42.00%) (6657/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5514) |  Loss2: (0.0000) | Acc: (43.00%) (7225/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5507) |  Loss2: (0.0000) | Acc: (43.00%) (7772/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5497) |  Loss2: (0.0000) | Acc: (43.00%) (8329/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5492) |  Loss2: (0.0000) | Acc: (43.00%) (8895/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5472) |  Loss2: (0.0000) | Acc: (43.00%) (9463/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5456) |  Loss2: (0.0000) | Acc: (43.00%) (10060/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5439) |  Loss2: (0.0000) | Acc: (43.00%) (10629/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5425) |  Loss2: (0.0000) | Acc: (43.00%) (11164/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5400) |  Loss2: (0.0000) | Acc: (43.00%) (11768/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5382) |  Loss2: (0.0000) | Acc: (43.00%) (12344/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5360) |  Loss2: (0.0000) | Acc: (43.00%) (12925/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5329) |  Loss2: (0.0000) | Acc: (43.00%) (13510/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5312) |  Loss2: (0.0000) | Acc: (43.00%) (14110/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5275) |  Loss2: (0.0000) | Acc: (44.00%) (14749/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5253) |  Loss2: (0.0000) | Acc: (44.00%) (15324/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5237) |  Loss2: (0.0000) | Acc: (44.00%) (15898/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5214) |  Loss2: (0.0000) | Acc: (44.00%) (16483/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5192) |  Loss2: (0.0000) | Acc: (44.00%) (17079/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5175) |  Loss2: (0.0000) | Acc: (44.00%) (17688/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5157) |  Loss2: (0.0000) | Acc: (44.00%) (18299/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5135) |  Loss2: (0.0000) | Acc: (44.00%) (18883/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5115) |  Loss2: (0.0000) | Acc: (44.00%) (19455/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5093) |  Loss2: (0.0000) | Acc: (44.00%) (20049/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5080) |  Loss2: (0.0000) | Acc: (44.00%) (20609/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5053) |  Loss2: (0.0000) | Acc: (44.00%) (21228/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5025) |  Loss2: (0.0000) | Acc: (44.00%) (21868/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5007) |  Loss2: (0.0000) | Acc: (44.00%) (22453/50000)
# TEST : Loss: (1.4041) | Acc: (48.00%) (4827/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.3655) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (49.00%) (693/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4019) |  Loss2: (0.0000) | Acc: (49.00%) (1340/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.3983) |  Loss2: (0.0000) | Acc: (49.00%) (1949/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.3950) |  Loss2: (0.0000) | Acc: (48.00%) (2567/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.3928) |  Loss2: (0.0000) | Acc: (49.00%) (3214/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.3989) |  Loss2: (0.0000) | Acc: (48.00%) (3819/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.3993) |  Loss2: (0.0000) | Acc: (48.00%) (4414/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.3998) |  Loss2: (0.0000) | Acc: (48.00%) (5036/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.3966) |  Loss2: (0.0000) | Acc: (48.00%) (5643/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (48.00%) (6267/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.3963) |  Loss2: (0.0000) | Acc: (48.00%) (6896/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.3955) |  Loss2: (0.0000) | Acc: (48.00%) (7528/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.3946) |  Loss2: (0.0000) | Acc: (48.00%) (8143/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.3935) |  Loss2: (0.0000) | Acc: (48.00%) (8799/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.3913) |  Loss2: (0.0000) | Acc: (48.00%) (9455/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.3900) |  Loss2: (0.0000) | Acc: (49.00%) (10101/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.3914) |  Loss2: (0.0000) | Acc: (48.00%) (10703/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.3914) |  Loss2: (0.0000) | Acc: (49.00%) (11357/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.3890) |  Loss2: (0.0000) | Acc: (49.00%) (12025/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.3883) |  Loss2: (0.0000) | Acc: (49.00%) (12663/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.3888) |  Loss2: (0.0000) | Acc: (49.00%) (13293/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.3851) |  Loss2: (0.0000) | Acc: (49.00%) (13981/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.3822) |  Loss2: (0.0000) | Acc: (49.00%) (14620/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.3812) |  Loss2: (0.0000) | Acc: (49.00%) (15273/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.3808) |  Loss2: (0.0000) | Acc: (49.00%) (15915/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3784) |  Loss2: (0.0000) | Acc: (49.00%) (16601/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3778) |  Loss2: (0.0000) | Acc: (49.00%) (17240/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3779) |  Loss2: (0.0000) | Acc: (49.00%) (17871/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3764) |  Loss2: (0.0000) | Acc: (49.00%) (18550/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3743) |  Loss2: (0.0000) | Acc: (49.00%) (19208/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3725) |  Loss2: (0.0000) | Acc: (49.00%) (19888/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3705) |  Loss2: (0.0000) | Acc: (50.00%) (20555/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3693) |  Loss2: (0.0000) | Acc: (50.00%) (21228/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3674) |  Loss2: (0.0000) | Acc: (50.00%) (21899/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3666) |  Loss2: (0.0000) | Acc: (50.00%) (22566/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3644) |  Loss2: (0.0000) | Acc: (50.00%) (23258/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3619) |  Loss2: (0.0000) | Acc: (50.00%) (23973/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3608) |  Loss2: (0.0000) | Acc: (50.00%) (24650/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3602) |  Loss2: (0.0000) | Acc: (50.00%) (25280/50000)
# TEST : Loss: (1.2917) | Acc: (52.00%) (5209/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.4297) |  Loss2: (0.0000) | Acc: (45.00%) (58/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3256) |  Loss2: (0.0000) | Acc: (51.00%) (732/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3220) |  Loss2: (0.0000) | Acc: (52.00%) (1412/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3204) |  Loss2: (0.0000) | Acc: (52.00%) (2075/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3101) |  Loss2: (0.0000) | Acc: (52.00%) (2771/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3043) |  Loss2: (0.0000) | Acc: (52.00%) (3434/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.3074) |  Loss2: (0.0000) | Acc: (52.00%) (4093/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.3082) |  Loss2: (0.0000) | Acc: (52.00%) (4784/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.3021) |  Loss2: (0.0000) | Acc: (53.00%) (5496/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2936) |  Loss2: (0.0000) | Acc: (53.00%) (6224/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2938) |  Loss2: (0.0000) | Acc: (53.00%) (6918/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2934) |  Loss2: (0.0000) | Acc: (53.00%) (7597/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2889) |  Loss2: (0.0000) | Acc: (53.00%) (8301/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2883) |  Loss2: (0.0000) | Acc: (53.00%) (8996/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2879) |  Loss2: (0.0000) | Acc: (53.00%) (9698/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2842) |  Loss2: (0.0000) | Acc: (53.00%) (10410/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11110/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11805/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2786) |  Loss2: (0.0000) | Acc: (54.00%) (12529/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2777) |  Loss2: (0.0000) | Acc: (54.00%) (13221/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2746) |  Loss2: (0.0000) | Acc: (54.00%) (13952/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2733) |  Loss2: (0.0000) | Acc: (54.00%) (14656/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2713) |  Loss2: (0.0000) | Acc: (54.00%) (15385/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2688) |  Loss2: (0.0000) | Acc: (54.00%) (16102/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2668) |  Loss2: (0.0000) | Acc: (54.00%) (16820/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (17519/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2626) |  Loss2: (0.0000) | Acc: (54.00%) (18236/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2620) |  Loss2: (0.0000) | Acc: (54.00%) (18931/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2614) |  Loss2: (0.0000) | Acc: (54.00%) (19669/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2608) |  Loss2: (0.0000) | Acc: (54.00%) (20376/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2578) |  Loss2: (0.0000) | Acc: (54.00%) (21123/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2559) |  Loss2: (0.0000) | Acc: (54.00%) (21869/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2539) |  Loss2: (0.0000) | Acc: (55.00%) (22615/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2519) |  Loss2: (0.0000) | Acc: (55.00%) (23347/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2514) |  Loss2: (0.0000) | Acc: (55.00%) (24060/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2519) |  Loss2: (0.0000) | Acc: (55.00%) (24763/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2497) |  Loss2: (0.0000) | Acc: (55.00%) (25504/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2479) |  Loss2: (0.0000) | Acc: (55.00%) (26238/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2480) |  Loss2: (0.0000) | Acc: (55.00%) (26943/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2466) |  Loss2: (0.0000) | Acc: (55.00%) (27658/50000)
# TEST : Loss: (1.2948) | Acc: (53.00%) (5335/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.0548) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2142) |  Loss2: (0.0000) | Acc: (55.00%) (775/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3016) |  Loss2: (0.0000) | Acc: (52.00%) (1417/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3438) |  Loss2: (0.0000) | Acc: (51.00%) (2062/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.3748) |  Loss2: (0.0000) | Acc: (50.00%) (2672/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.3793) |  Loss2: (0.0000) | Acc: (50.00%) (3318/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.3879) |  Loss2: (0.0000) | Acc: (50.00%) (3927/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.3840) |  Loss2: (0.0000) | Acc: (50.00%) (4595/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.3827) |  Loss2: (0.0000) | Acc: (50.00%) (5228/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.3785) |  Loss2: (0.0000) | Acc: (50.00%) (5885/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.3754) |  Loss2: (0.0000) | Acc: (50.00%) (6524/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3697) |  Loss2: (0.0000) | Acc: (50.00%) (7209/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3634) |  Loss2: (0.0000) | Acc: (51.00%) (7899/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3652) |  Loss2: (0.0000) | Acc: (50.00%) (8533/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3576) |  Loss2: (0.0000) | Acc: (51.00%) (9238/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3527) |  Loss2: (0.0000) | Acc: (51.00%) (9945/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3498) |  Loss2: (0.0000) | Acc: (51.00%) (10613/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3457) |  Loss2: (0.0000) | Acc: (51.00%) (11305/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3439) |  Loss2: (0.0000) | Acc: (51.00%) (11969/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3404) |  Loss2: (0.0000) | Acc: (51.00%) (12662/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3377) |  Loss2: (0.0000) | Acc: (51.00%) (13338/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3362) |  Loss2: (0.0000) | Acc: (51.00%) (14023/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3336) |  Loss2: (0.0000) | Acc: (52.00%) (14722/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3329) |  Loss2: (0.0000) | Acc: (52.00%) (15399/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3303) |  Loss2: (0.0000) | Acc: (52.00%) (16072/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3293) |  Loss2: (0.0000) | Acc: (52.00%) (16742/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3271) |  Loss2: (0.0000) | Acc: (52.00%) (17456/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3262) |  Loss2: (0.0000) | Acc: (52.00%) (18124/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3234) |  Loss2: (0.0000) | Acc: (52.00%) (18826/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3234) |  Loss2: (0.0000) | Acc: (52.00%) (19467/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3221) |  Loss2: (0.0000) | Acc: (52.00%) (20146/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3217) |  Loss2: (0.0000) | Acc: (52.00%) (20827/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3197) |  Loss2: (0.0000) | Acc: (52.00%) (21509/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3179) |  Loss2: (0.0000) | Acc: (52.00%) (22195/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3164) |  Loss2: (0.0000) | Acc: (52.00%) (22877/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3136) |  Loss2: (0.0000) | Acc: (52.00%) (23595/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3121) |  Loss2: (0.0000) | Acc: (52.00%) (24305/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3106) |  Loss2: (0.0000) | Acc: (52.00%) (25000/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3097) |  Loss2: (0.0000) | Acc: (52.00%) (25689/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.3080) |  Loss2: (0.0000) | Acc: (52.00%) (26370/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2201) | Acc: (55.00%) (5503/10000)
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4842, 0.5158], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.4837, 0.5163], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2930) |  Loss2: (0.0000) | Acc: (50.00%) (64/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2423) |  Loss2: (0.0000) | Acc: (55.00%) (775/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2482) |  Loss2: (0.0000) | Acc: (54.00%) (1473/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2469) |  Loss2: (0.0000) | Acc: (55.00%) (2183/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2370) |  Loss2: (0.0000) | Acc: (55.00%) (2910/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2397) |  Loss2: (0.0000) | Acc: (55.00%) (3607/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2381) |  Loss2: (0.0000) | Acc: (55.00%) (4299/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2319) |  Loss2: (0.0000) | Acc: (55.00%) (5016/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2314) |  Loss2: (0.0000) | Acc: (55.00%) (5729/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2301) |  Loss2: (0.0000) | Acc: (55.00%) (6418/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2304) |  Loss2: (0.0000) | Acc: (55.00%) (7112/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2350) |  Loss2: (0.0000) | Acc: (54.00%) (7803/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (55.00%) (8528/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2297) |  Loss2: (0.0000) | Acc: (55.00%) (9242/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2262) |  Loss2: (0.0000) | Acc: (55.00%) (9974/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2277) |  Loss2: (0.0000) | Acc: (55.00%) (10676/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2274) |  Loss2: (0.0000) | Acc: (55.00%) (11381/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2261) |  Loss2: (0.0000) | Acc: (55.00%) (12105/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (12831/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2271) |  Loss2: (0.0000) | Acc: (55.00%) (13541/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2278) |  Loss2: (0.0000) | Acc: (55.00%) (14257/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2273) |  Loss2: (0.0000) | Acc: (55.00%) (14986/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (15675/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2266) |  Loss2: (0.0000) | Acc: (55.00%) (16406/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (17110/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2269) |  Loss2: (0.0000) | Acc: (55.00%) (17844/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2251) |  Loss2: (0.0000) | Acc: (55.00%) (18572/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2255) |  Loss2: (0.0000) | Acc: (55.00%) (19269/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2264) |  Loss2: (0.0000) | Acc: (55.00%) (19975/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2279) |  Loss2: (0.0000) | Acc: (55.00%) (20665/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2267) |  Loss2: (0.0000) | Acc: (55.00%) (21384/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2259) |  Loss2: (0.0000) | Acc: (55.00%) (22119/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2247) |  Loss2: (0.0000) | Acc: (55.00%) (22843/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2241) |  Loss2: (0.0000) | Acc: (55.00%) (23571/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2235) |  Loss2: (0.0000) | Acc: (55.00%) (24282/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2229) |  Loss2: (0.0000) | Acc: (55.00%) (25010/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2212) |  Loss2: (0.0000) | Acc: (55.00%) (25743/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2203) |  Loss2: (0.0000) | Acc: (55.00%) (26476/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2186) |  Loss2: (0.0000) | Acc: (55.00%) (27225/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2179) |  Loss2: (0.0000) | Acc: (55.00%) (27932/50000)
# TEST : Loss: (1.1856) | Acc: (56.00%) (5668/10000)
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.4859, 0.5141], device='cuda:0')
percent tensor([0.4890, 0.5110], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.6943, 0.3057], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1268) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2106) |  Loss2: (0.0000) | Acc: (55.00%) (776/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2078) |  Loss2: (0.0000) | Acc: (56.00%) (1509/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2075) |  Loss2: (0.0000) | Acc: (56.00%) (2229/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2077) |  Loss2: (0.0000) | Acc: (56.00%) (2954/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.2035) |  Loss2: (0.0000) | Acc: (56.00%) (3665/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (4372/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2056) |  Loss2: (0.0000) | Acc: (56.00%) (5109/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.2058) |  Loss2: (0.0000) | Acc: (56.00%) (5829/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.2086) |  Loss2: (0.0000) | Acc: (56.00%) (6549/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.2061) |  Loss2: (0.0000) | Acc: (56.00%) (7263/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.2051) |  Loss2: (0.0000) | Acc: (56.00%) (7975/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.2059) |  Loss2: (0.0000) | Acc: (56.00%) (8689/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.2062) |  Loss2: (0.0000) | Acc: (56.00%) (9401/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.2066) |  Loss2: (0.0000) | Acc: (56.00%) (10120/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.2060) |  Loss2: (0.0000) | Acc: (56.00%) (10832/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.2032) |  Loss2: (0.0000) | Acc: (56.00%) (11570/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.2000) |  Loss2: (0.0000) | Acc: (56.00%) (12324/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (13074/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (56.00%) (13781/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1964) |  Loss2: (0.0000) | Acc: (56.00%) (14501/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1948) |  Loss2: (0.0000) | Acc: (56.00%) (15237/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (15950/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1931) |  Loss2: (0.0000) | Acc: (56.00%) (16692/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (17385/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (18124/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1944) |  Loss2: (0.0000) | Acc: (56.00%) (18851/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1931) |  Loss2: (0.0000) | Acc: (56.00%) (19586/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1915) |  Loss2: (0.0000) | Acc: (56.00%) (20330/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1924) |  Loss2: (0.0000) | Acc: (56.00%) (21033/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1918) |  Loss2: (0.0000) | Acc: (56.00%) (21766/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (22484/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1923) |  Loss2: (0.0000) | Acc: (56.00%) (23198/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1912) |  Loss2: (0.0000) | Acc: (56.00%) (23950/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (24686/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (56.00%) (25451/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1906) |  Loss2: (0.0000) | Acc: (56.00%) (26168/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1920) |  Loss2: (0.0000) | Acc: (56.00%) (26865/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1921) |  Loss2: (0.0000) | Acc: (56.00%) (27563/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1919) |  Loss2: (0.0000) | Acc: (56.00%) (28273/50000)
# TEST : Loss: (1.1739) | Acc: (57.00%) (5763/10000)
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.4902, 0.5098], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1608) |  Loss2: (0.0000) | Acc: (57.00%) (74/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1840) |  Loss2: (0.0000) | Acc: (57.00%) (805/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1713) |  Loss2: (0.0000) | Acc: (57.00%) (1534/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (2230/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (2942/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1899) |  Loss2: (0.0000) | Acc: (56.00%) (3671/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1918) |  Loss2: (0.0000) | Acc: (56.00%) (4398/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (5141/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1896) |  Loss2: (0.0000) | Acc: (56.00%) (5855/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1895) |  Loss2: (0.0000) | Acc: (56.00%) (6576/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1932) |  Loss2: (0.0000) | Acc: (56.00%) (7271/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1922) |  Loss2: (0.0000) | Acc: (56.00%) (7976/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (8730/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1886) |  Loss2: (0.0000) | Acc: (56.00%) (9480/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1850) |  Loss2: (0.0000) | Acc: (56.00%) (10213/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1880) |  Loss2: (0.0000) | Acc: (56.00%) (10927/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (11663/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (12417/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (13131/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (13868/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (14599/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1854) |  Loss2: (0.0000) | Acc: (56.00%) (15335/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1853) |  Loss2: (0.0000) | Acc: (56.00%) (16067/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1842) |  Loss2: (0.0000) | Acc: (56.00%) (16801/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1851) |  Loss2: (0.0000) | Acc: (56.00%) (17517/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1864) |  Loss2: (0.0000) | Acc: (56.00%) (18231/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (18956/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1849) |  Loss2: (0.0000) | Acc: (56.00%) (19689/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1862) |  Loss2: (0.0000) | Acc: (56.00%) (20388/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1862) |  Loss2: (0.0000) | Acc: (56.00%) (21119/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1855) |  Loss2: (0.0000) | Acc: (56.00%) (21868/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1868) |  Loss2: (0.0000) | Acc: (56.00%) (22588/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (23312/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (56.00%) (24033/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1850) |  Loss2: (0.0000) | Acc: (56.00%) (24796/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1830) |  Loss2: (0.0000) | Acc: (56.00%) (25562/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1822) |  Loss2: (0.0000) | Acc: (56.00%) (26298/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1814) |  Loss2: (0.0000) | Acc: (56.00%) (27045/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1799) |  Loss2: (0.0000) | Acc: (56.00%) (27797/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1793) |  Loss2: (0.0000) | Acc: (57.00%) (28502/50000)
# TEST : Loss: (1.1619) | Acc: (57.00%) (5792/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.4965, 0.5035], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.7711, 0.2289], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1314) |  Loss2: (0.0000) | Acc: (58.00%) (822/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1520) |  Loss2: (0.0000) | Acc: (58.00%) (1573/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1590) |  Loss2: (0.0000) | Acc: (58.00%) (2323/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1562) |  Loss2: (0.0000) | Acc: (58.00%) (3058/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1545) |  Loss2: (0.0000) | Acc: (58.00%) (3821/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1538) |  Loss2: (0.0000) | Acc: (58.00%) (4562/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1542) |  Loss2: (0.0000) | Acc: (58.00%) (5311/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1597) |  Loss2: (0.0000) | Acc: (58.00%) (6035/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1619) |  Loss2: (0.0000) | Acc: (57.00%) (6754/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1669) |  Loss2: (0.0000) | Acc: (57.00%) (7455/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1691) |  Loss2: (0.0000) | Acc: (57.00%) (8186/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1756) |  Loss2: (0.0000) | Acc: (57.00%) (8855/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1752) |  Loss2: (0.0000) | Acc: (57.00%) (9607/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (57.00%) (10360/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1722) |  Loss2: (0.0000) | Acc: (57.00%) (11108/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1704) |  Loss2: (0.0000) | Acc: (57.00%) (11830/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1696) |  Loss2: (0.0000) | Acc: (57.00%) (12567/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1696) |  Loss2: (0.0000) | Acc: (57.00%) (13317/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1712) |  Loss2: (0.0000) | Acc: (57.00%) (14020/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1714) |  Loss2: (0.0000) | Acc: (57.00%) (14769/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1713) |  Loss2: (0.0000) | Acc: (57.00%) (15500/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1705) |  Loss2: (0.0000) | Acc: (57.00%) (16236/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1719) |  Loss2: (0.0000) | Acc: (57.00%) (16932/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1715) |  Loss2: (0.0000) | Acc: (57.00%) (17662/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1718) |  Loss2: (0.0000) | Acc: (57.00%) (18404/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (57.00%) (19127/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1729) |  Loss2: (0.0000) | Acc: (57.00%) (19849/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1729) |  Loss2: (0.0000) | Acc: (57.00%) (20564/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1719) |  Loss2: (0.0000) | Acc: (57.00%) (21322/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1720) |  Loss2: (0.0000) | Acc: (57.00%) (22061/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1712) |  Loss2: (0.0000) | Acc: (57.00%) (22801/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1723) |  Loss2: (0.0000) | Acc: (57.00%) (23517/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1706) |  Loss2: (0.0000) | Acc: (57.00%) (24289/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (25049/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (25778/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (26529/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1683) |  Loss2: (0.0000) | Acc: (57.00%) (27265/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1693) |  Loss2: (0.0000) | Acc: (57.00%) (28001/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1697) |  Loss2: (0.0000) | Acc: (57.00%) (28691/50000)
# TEST : Loss: (1.1592) | Acc: (58.00%) (5813/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4829, 0.5171], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.7864, 0.2136], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.2573) |  Loss2: (0.0000) | Acc: (56.00%) (72/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.2284) |  Loss2: (0.0000) | Acc: (57.00%) (803/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.2202) |  Loss2: (0.0000) | Acc: (56.00%) (1522/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.2198) |  Loss2: (0.0000) | Acc: (56.00%) (2238/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1971) |  Loss2: (0.0000) | Acc: (56.00%) (2983/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1969) |  Loss2: (0.0000) | Acc: (56.00%) (3712/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (57.00%) (4463/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1806) |  Loss2: (0.0000) | Acc: (57.00%) (5212/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1736) |  Loss2: (0.0000) | Acc: (57.00%) (5981/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (58.00%) (6760/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1625) |  Loss2: (0.0000) | Acc: (58.00%) (7530/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1598) |  Loss2: (0.0000) | Acc: (58.00%) (8289/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1577) |  Loss2: (0.0000) | Acc: (58.00%) (9041/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1527) |  Loss2: (0.0000) | Acc: (58.00%) (9814/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1541) |  Loss2: (0.0000) | Acc: (58.00%) (10544/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1530) |  Loss2: (0.0000) | Acc: (58.00%) (11300/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1544) |  Loss2: (0.0000) | Acc: (58.00%) (12033/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1536) |  Loss2: (0.0000) | Acc: (58.00%) (12789/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1514) |  Loss2: (0.0000) | Acc: (58.00%) (13571/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1512) |  Loss2: (0.0000) | Acc: (58.00%) (14324/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1482) |  Loss2: (0.0000) | Acc: (58.00%) (15106/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1460) |  Loss2: (0.0000) | Acc: (58.00%) (15893/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1454) |  Loss2: (0.0000) | Acc: (58.00%) (16650/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (17419/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (59.00%) (18202/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1386) |  Loss2: (0.0000) | Acc: (59.00%) (18982/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1361) |  Loss2: (0.0000) | Acc: (59.00%) (19785/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (59.00%) (20534/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1340) |  Loss2: (0.0000) | Acc: (59.00%) (21319/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1336) |  Loss2: (0.0000) | Acc: (59.00%) (22081/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1328) |  Loss2: (0.0000) | Acc: (59.00%) (22839/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1312) |  Loss2: (0.0000) | Acc: (59.00%) (23612/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1287) |  Loss2: (0.0000) | Acc: (59.00%) (24418/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1261) |  Loss2: (0.0000) | Acc: (59.00%) (25231/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1241) |  Loss2: (0.0000) | Acc: (59.00%) (26039/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1235) |  Loss2: (0.0000) | Acc: (59.00%) (26787/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1223) |  Loss2: (0.0000) | Acc: (59.00%) (27564/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1210) |  Loss2: (0.0000) | Acc: (59.00%) (28351/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1205) |  Loss2: (0.0000) | Acc: (59.00%) (29138/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1188) |  Loss2: (0.0000) | Acc: (59.00%) (29901/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.0776) | Acc: (60.00%) (6068/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4834, 0.5166], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.7918, 0.2082], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.7254, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(778.6592, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(771.5042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7139, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(518.8765, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2167.2141, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4317.1187, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1448.1422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6111.4082, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12226.0586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4080.1460, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17279.5762, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (0.9706) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0712) |  Loss2: (0.0000) | Acc: (62.00%) (880/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0550) |  Loss2: (0.0000) | Acc: (62.00%) (1684/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (2479/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0652) |  Loss2: (0.0000) | Acc: (62.00%) (3275/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0726) |  Loss2: (0.0000) | Acc: (61.00%) (4045/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (62.00%) (4901/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0616) |  Loss2: (0.0000) | Acc: (62.00%) (5694/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0620) |  Loss2: (0.0000) | Acc: (62.00%) (6505/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0601) |  Loss2: (0.0000) | Acc: (62.00%) (7285/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0596) |  Loss2: (0.0000) | Acc: (62.00%) (8068/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0577) |  Loss2: (0.0000) | Acc: (62.00%) (8878/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0591) |  Loss2: (0.0000) | Acc: (62.00%) (9663/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0592) |  Loss2: (0.0000) | Acc: (62.00%) (10447/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (11261/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0549) |  Loss2: (0.0000) | Acc: (62.00%) (12069/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0519) |  Loss2: (0.0000) | Acc: (62.00%) (12892/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0499) |  Loss2: (0.0000) | Acc: (62.00%) (13707/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0471) |  Loss2: (0.0000) | Acc: (62.00%) (14548/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0473) |  Loss2: (0.0000) | Acc: (62.00%) (15349/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0467) |  Loss2: (0.0000) | Acc: (62.00%) (16150/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0468) |  Loss2: (0.0000) | Acc: (62.00%) (16949/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0452) |  Loss2: (0.0000) | Acc: (62.00%) (17783/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0427) |  Loss2: (0.0000) | Acc: (62.00%) (18612/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0414) |  Loss2: (0.0000) | Acc: (63.00%) (19438/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0398) |  Loss2: (0.0000) | Acc: (63.00%) (20248/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0358) |  Loss2: (0.0000) | Acc: (63.00%) (21105/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0347) |  Loss2: (0.0000) | Acc: (63.00%) (21922/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0326) |  Loss2: (0.0000) | Acc: (63.00%) (22754/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0304) |  Loss2: (0.0000) | Acc: (63.00%) (23581/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0283) |  Loss2: (0.0000) | Acc: (63.00%) (24417/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0279) |  Loss2: (0.0000) | Acc: (63.00%) (25248/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0273) |  Loss2: (0.0000) | Acc: (63.00%) (26057/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0263) |  Loss2: (0.0000) | Acc: (63.00%) (26889/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0252) |  Loss2: (0.0000) | Acc: (63.00%) (27707/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0235) |  Loss2: (0.0000) | Acc: (63.00%) (28551/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0220) |  Loss2: (0.0000) | Acc: (63.00%) (29365/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0209) |  Loss2: (0.0000) | Acc: (63.00%) (30198/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0200) |  Loss2: (0.0000) | Acc: (63.00%) (31019/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0191) |  Loss2: (0.0000) | Acc: (63.00%) (31802/50000)
# TEST : Loss: (1.1754) | Acc: (58.00%) (5832/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4829, 0.5171], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.8073, 0.1927], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9802) |  Loss2: (0.0000) | Acc: (66.00%) (85/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (1.0186) |  Loss2: (0.0000) | Acc: (62.00%) (876/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (1.0100) |  Loss2: (0.0000) | Acc: (63.00%) (1697/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9892) |  Loss2: (0.0000) | Acc: (64.00%) (2545/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9964) |  Loss2: (0.0000) | Acc: (64.00%) (3370/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9957) |  Loss2: (0.0000) | Acc: (64.00%) (4189/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9846) |  Loss2: (0.0000) | Acc: (64.00%) (5058/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9820) |  Loss2: (0.0000) | Acc: (64.00%) (5884/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9813) |  Loss2: (0.0000) | Acc: (64.00%) (6719/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9748) |  Loss2: (0.0000) | Acc: (65.00%) (7598/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9748) |  Loss2: (0.0000) | Acc: (65.00%) (8431/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9690) |  Loss2: (0.0000) | Acc: (65.00%) (9275/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9704) |  Loss2: (0.0000) | Acc: (65.00%) (10107/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9708) |  Loss2: (0.0000) | Acc: (65.00%) (10937/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (11792/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9683) |  Loss2: (0.0000) | Acc: (65.00%) (12639/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9637) |  Loss2: (0.0000) | Acc: (65.00%) (13515/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9619) |  Loss2: (0.0000) | Acc: (65.00%) (14384/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9627) |  Loss2: (0.0000) | Acc: (65.00%) (15221/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9589) |  Loss2: (0.0000) | Acc: (65.00%) (16095/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (16929/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9611) |  Loss2: (0.0000) | Acc: (65.00%) (17758/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9605) |  Loss2: (0.0000) | Acc: (65.00%) (18625/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9575) |  Loss2: (0.0000) | Acc: (65.00%) (19490/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9577) |  Loss2: (0.0000) | Acc: (65.00%) (20323/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9565) |  Loss2: (0.0000) | Acc: (65.00%) (21174/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9553) |  Loss2: (0.0000) | Acc: (65.00%) (22037/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9545) |  Loss2: (0.0000) | Acc: (66.00%) (22895/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9555) |  Loss2: (0.0000) | Acc: (65.00%) (23724/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (65.00%) (24580/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (25438/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (26279/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9536) |  Loss2: (0.0000) | Acc: (66.00%) (27125/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9531) |  Loss2: (0.0000) | Acc: (66.00%) (27979/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9528) |  Loss2: (0.0000) | Acc: (66.00%) (28844/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (66.00%) (29707/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9522) |  Loss2: (0.0000) | Acc: (66.00%) (30547/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9514) |  Loss2: (0.0000) | Acc: (66.00%) (31412/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9497) |  Loss2: (0.0000) | Acc: (66.00%) (32288/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9476) |  Loss2: (0.0000) | Acc: (66.00%) (33142/50000)
# TEST : Loss: (1.0360) | Acc: (63.00%) (6339/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4972, 0.5028], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.8005, 0.1995], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.7668) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9448) |  Loss2: (0.0000) | Acc: (66.00%) (934/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9423) |  Loss2: (0.0000) | Acc: (66.00%) (1789/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9363) |  Loss2: (0.0000) | Acc: (66.00%) (2653/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9271) |  Loss2: (0.0000) | Acc: (67.00%) (3518/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9176) |  Loss2: (0.0000) | Acc: (67.00%) (4407/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9094) |  Loss2: (0.0000) | Acc: (67.00%) (5300/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9036) |  Loss2: (0.0000) | Acc: (68.00%) (6191/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9042) |  Loss2: (0.0000) | Acc: (67.00%) (7047/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9005) |  Loss2: (0.0000) | Acc: (68.00%) (7932/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8999) |  Loss2: (0.0000) | Acc: (68.00%) (8805/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (9696/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8924) |  Loss2: (0.0000) | Acc: (68.00%) (10594/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (11467/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8913) |  Loss2: (0.0000) | Acc: (68.00%) (12335/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8887) |  Loss2: (0.0000) | Acc: (68.00%) (13228/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8885) |  Loss2: (0.0000) | Acc: (68.00%) (14104/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.8892) |  Loss2: (0.0000) | Acc: (68.00%) (14969/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.8876) |  Loss2: (0.0000) | Acc: (68.00%) (15871/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.8874) |  Loss2: (0.0000) | Acc: (68.00%) (16739/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.8882) |  Loss2: (0.0000) | Acc: (68.00%) (17598/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8872) |  Loss2: (0.0000) | Acc: (68.00%) (18487/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8861) |  Loss2: (0.0000) | Acc: (68.00%) (19389/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8871) |  Loss2: (0.0000) | Acc: (68.00%) (20267/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8848) |  Loss2: (0.0000) | Acc: (68.00%) (21165/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8858) |  Loss2: (0.0000) | Acc: (68.00%) (22036/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8860) |  Loss2: (0.0000) | Acc: (68.00%) (22898/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8862) |  Loss2: (0.0000) | Acc: (68.00%) (23761/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8859) |  Loss2: (0.0000) | Acc: (68.00%) (24652/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8857) |  Loss2: (0.0000) | Acc: (68.00%) (25546/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8840) |  Loss2: (0.0000) | Acc: (68.00%) (26463/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8829) |  Loss2: (0.0000) | Acc: (68.00%) (27371/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8844) |  Loss2: (0.0000) | Acc: (68.00%) (28222/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8834) |  Loss2: (0.0000) | Acc: (68.00%) (29128/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8835) |  Loss2: (0.0000) | Acc: (68.00%) (30028/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8827) |  Loss2: (0.0000) | Acc: (68.00%) (30916/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8830) |  Loss2: (0.0000) | Acc: (68.00%) (31784/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8831) |  Loss2: (0.0000) | Acc: (68.00%) (32651/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8835) |  Loss2: (0.0000) | Acc: (68.00%) (33515/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8842) |  Loss2: (0.0000) | Acc: (68.00%) (34341/50000)
# TEST : Loss: (0.9239) | Acc: (67.00%) (6742/10000)
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.8002, 0.1998], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8217) |  Loss2: (0.0000) | Acc: (70.00%) (995/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8317) |  Loss2: (0.0000) | Acc: (70.00%) (1894/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8581) |  Loss2: (0.0000) | Acc: (69.00%) (2750/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8550) |  Loss2: (0.0000) | Acc: (69.00%) (3650/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (4530/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8590) |  Loss2: (0.0000) | Acc: (69.00%) (5408/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8577) |  Loss2: (0.0000) | Acc: (69.00%) (6301/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8566) |  Loss2: (0.0000) | Acc: (69.00%) (7194/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8559) |  Loss2: (0.0000) | Acc: (69.00%) (8104/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8571) |  Loss2: (0.0000) | Acc: (69.00%) (8990/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8551) |  Loss2: (0.0000) | Acc: (69.00%) (9903/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8526) |  Loss2: (0.0000) | Acc: (69.00%) (10815/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8527) |  Loss2: (0.0000) | Acc: (69.00%) (11719/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8545) |  Loss2: (0.0000) | Acc: (69.00%) (12594/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8505) |  Loss2: (0.0000) | Acc: (69.00%) (13508/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8502) |  Loss2: (0.0000) | Acc: (69.00%) (14406/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (69.00%) (15297/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8483) |  Loss2: (0.0000) | Acc: (69.00%) (16205/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8504) |  Loss2: (0.0000) | Acc: (69.00%) (17093/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (70.00%) (18015/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8472) |  Loss2: (0.0000) | Acc: (70.00%) (18944/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8464) |  Loss2: (0.0000) | Acc: (70.00%) (19840/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8459) |  Loss2: (0.0000) | Acc: (70.00%) (20745/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8433) |  Loss2: (0.0000) | Acc: (70.00%) (21666/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (22592/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8416) |  Loss2: (0.0000) | Acc: (70.00%) (23503/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8409) |  Loss2: (0.0000) | Acc: (70.00%) (24415/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8388) |  Loss2: (0.0000) | Acc: (70.00%) (25330/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8377) |  Loss2: (0.0000) | Acc: (70.00%) (26235/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8364) |  Loss2: (0.0000) | Acc: (70.00%) (27150/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8359) |  Loss2: (0.0000) | Acc: (70.00%) (28050/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (28955/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (29889/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8351) |  Loss2: (0.0000) | Acc: (70.00%) (30781/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8358) |  Loss2: (0.0000) | Acc: (70.00%) (31675/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8354) |  Loss2: (0.0000) | Acc: (70.00%) (32585/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (33519/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8342) |  Loss2: (0.0000) | Acc: (70.00%) (34430/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8338) |  Loss2: (0.0000) | Acc: (70.00%) (35288/50000)
# TEST : Loss: (0.8658) | Acc: (69.00%) (6915/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.8030, 0.1970], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.8983) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.9020) |  Loss2: (0.0000) | Acc: (68.00%) (958/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (66.00%) (1788/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9451) |  Loss2: (0.0000) | Acc: (66.00%) (2636/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9647) |  Loss2: (0.0000) | Acc: (65.00%) (3457/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9775) |  Loss2: (0.0000) | Acc: (65.00%) (4275/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9785) |  Loss2: (0.0000) | Acc: (65.00%) (5088/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9747) |  Loss2: (0.0000) | Acc: (65.00%) (5956/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9745) |  Loss2: (0.0000) | Acc: (65.00%) (6791/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9690) |  Loss2: (0.0000) | Acc: (65.00%) (7657/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9674) |  Loss2: (0.0000) | Acc: (65.00%) (8493/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (10213/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9568) |  Loss2: (0.0000) | Acc: (65.00%) (11040/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9491) |  Loss2: (0.0000) | Acc: (66.00%) (11941/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9483) |  Loss2: (0.0000) | Acc: (66.00%) (12799/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9457) |  Loss2: (0.0000) | Acc: (66.00%) (13661/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9411) |  Loss2: (0.0000) | Acc: (66.00%) (14563/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9415) |  Loss2: (0.0000) | Acc: (66.00%) (15411/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9385) |  Loss2: (0.0000) | Acc: (66.00%) (16295/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9365) |  Loss2: (0.0000) | Acc: (66.00%) (17155/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9336) |  Loss2: (0.0000) | Acc: (66.00%) (18021/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9319) |  Loss2: (0.0000) | Acc: (66.00%) (18901/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9292) |  Loss2: (0.0000) | Acc: (66.00%) (19790/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9261) |  Loss2: (0.0000) | Acc: (67.00%) (20680/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9222) |  Loss2: (0.0000) | Acc: (67.00%) (21568/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9193) |  Loss2: (0.0000) | Acc: (67.00%) (22478/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (23356/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (24211/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9182) |  Loss2: (0.0000) | Acc: (67.00%) (25072/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9169) |  Loss2: (0.0000) | Acc: (67.00%) (25944/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.9155) |  Loss2: (0.0000) | Acc: (67.00%) (26826/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.9127) |  Loss2: (0.0000) | Acc: (67.00%) (27735/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.9099) |  Loss2: (0.0000) | Acc: (67.00%) (28627/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.9082) |  Loss2: (0.0000) | Acc: (67.00%) (29537/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.9066) |  Loss2: (0.0000) | Acc: (67.00%) (30418/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.9053) |  Loss2: (0.0000) | Acc: (67.00%) (31303/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.9032) |  Loss2: (0.0000) | Acc: (67.00%) (32231/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.9023) |  Loss2: (0.0000) | Acc: (67.00%) (33106/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.9009) |  Loss2: (0.0000) | Acc: (67.00%) (33971/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.8844) | Acc: (68.00%) (6895/10000)
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.4738, 0.5262], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.8512, 0.1488], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8686) |  Loss2: (0.0000) | Acc: (69.00%) (978/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8764) |  Loss2: (0.0000) | Acc: (68.00%) (1853/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8755) |  Loss2: (0.0000) | Acc: (68.00%) (2729/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8742) |  Loss2: (0.0000) | Acc: (68.00%) (3595/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8598) |  Loss2: (0.0000) | Acc: (68.00%) (4499/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8573) |  Loss2: (0.0000) | Acc: (68.00%) (5385/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8552) |  Loss2: (0.0000) | Acc: (69.00%) (6280/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (7165/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (8051/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (8937/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8578) |  Loss2: (0.0000) | Acc: (69.00%) (9833/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8573) |  Loss2: (0.0000) | Acc: (69.00%) (10729/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8520) |  Loss2: (0.0000) | Acc: (69.00%) (11664/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8501) |  Loss2: (0.0000) | Acc: (69.00%) (12567/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (13450/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8484) |  Loss2: (0.0000) | Acc: (69.00%) (14348/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8458) |  Loss2: (0.0000) | Acc: (69.00%) (15272/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8438) |  Loss2: (0.0000) | Acc: (69.00%) (16179/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8458) |  Loss2: (0.0000) | Acc: (69.00%) (17061/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8448) |  Loss2: (0.0000) | Acc: (69.00%) (17987/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (69.00%) (18886/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8413) |  Loss2: (0.0000) | Acc: (70.00%) (19810/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (20708/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8415) |  Loss2: (0.0000) | Acc: (70.00%) (21622/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8387) |  Loss2: (0.0000) | Acc: (70.00%) (22545/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8402) |  Loss2: (0.0000) | Acc: (70.00%) (23444/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8403) |  Loss2: (0.0000) | Acc: (70.00%) (24335/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (25239/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8386) |  Loss2: (0.0000) | Acc: (70.00%) (26157/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (27074/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8367) |  Loss2: (0.0000) | Acc: (70.00%) (27973/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (28865/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8357) |  Loss2: (0.0000) | Acc: (70.00%) (29779/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (30671/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8351) |  Loss2: (0.0000) | Acc: (70.00%) (31566/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8336) |  Loss2: (0.0000) | Acc: (70.00%) (32488/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8347) |  Loss2: (0.0000) | Acc: (70.00%) (33371/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8344) |  Loss2: (0.0000) | Acc: (70.00%) (34293/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (35175/50000)
# TEST : Loss: (0.8596) | Acc: (70.00%) (7013/10000)
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.4784, 0.5216], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.4832, 0.5168], device='cuda:0')
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.8990, 0.1010], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.7031) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.7507) |  Loss2: (0.0000) | Acc: (73.00%) (1029/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.7872) |  Loss2: (0.0000) | Acc: (72.00%) (1937/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (2841/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (71.00%) (3727/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8142) |  Loss2: (0.0000) | Acc: (71.00%) (4655/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8152) |  Loss2: (0.0000) | Acc: (71.00%) (5544/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (6461/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8106) |  Loss2: (0.0000) | Acc: (71.00%) (7387/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8125) |  Loss2: (0.0000) | Acc: (71.00%) (8273/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (9184/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8144) |  Loss2: (0.0000) | Acc: (70.00%) (10085/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (11010/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8118) |  Loss2: (0.0000) | Acc: (70.00%) (11901/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8117) |  Loss2: (0.0000) | Acc: (71.00%) (12822/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8128) |  Loss2: (0.0000) | Acc: (71.00%) (13726/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8128) |  Loss2: (0.0000) | Acc: (71.00%) (14636/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (70.00%) (15524/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (70.00%) (16425/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8160) |  Loss2: (0.0000) | Acc: (70.00%) (17344/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (18249/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (70.00%) (19158/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8151) |  Loss2: (0.0000) | Acc: (70.00%) (20067/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (20955/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (21849/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (70.00%) (22762/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (23671/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (24585/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8187) |  Loss2: (0.0000) | Acc: (70.00%) (25491/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (26415/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (70.00%) (27338/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (28239/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (29127/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (30019/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8184) |  Loss2: (0.0000) | Acc: (70.00%) (30922/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (31819/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (32750/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8171) |  Loss2: (0.0000) | Acc: (70.00%) (33668/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8185) |  Loss2: (0.0000) | Acc: (70.00%) (34552/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (35439/50000)
# TEST : Loss: (0.8463) | Acc: (70.00%) (7052/10000)
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.4838, 0.5162], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.9140, 0.0860], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (68.00%) (88/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7682) |  Loss2: (0.0000) | Acc: (71.00%) (1012/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.7778) |  Loss2: (0.0000) | Acc: (72.00%) (1946/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (2850/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (71.00%) (3762/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7934) |  Loss2: (0.0000) | Acc: (71.00%) (4676/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (5590/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8062) |  Loss2: (0.0000) | Acc: (71.00%) (6480/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.7974) |  Loss2: (0.0000) | Acc: (71.00%) (7417/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (8327/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.7993) |  Loss2: (0.0000) | Acc: (71.00%) (9245/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8022) |  Loss2: (0.0000) | Acc: (71.00%) (10161/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (11078/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8029) |  Loss2: (0.0000) | Acc: (71.00%) (12000/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (12900/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (13799/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8036) |  Loss2: (0.0000) | Acc: (71.00%) (14713/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (15621/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8018) |  Loss2: (0.0000) | Acc: (71.00%) (16552/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (17450/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (71.00%) (18371/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (19287/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8033) |  Loss2: (0.0000) | Acc: (71.00%) (20220/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (21120/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8045) |  Loss2: (0.0000) | Acc: (71.00%) (22027/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (22957/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (23862/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (24773/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (25677/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (26592/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (71.00%) (27481/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (28408/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8022) |  Loss2: (0.0000) | Acc: (71.00%) (29328/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8034) |  Loss2: (0.0000) | Acc: (71.00%) (30196/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8042) |  Loss2: (0.0000) | Acc: (71.00%) (31111/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8037) |  Loss2: (0.0000) | Acc: (71.00%) (32029/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (32954/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (33865/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (34763/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8051) |  Loss2: (0.0000) | Acc: (71.00%) (35620/50000)
# TEST : Loss: (0.8359) | Acc: (70.00%) (7081/10000)
percent tensor([0.5149, 0.4851], device='cuda:0')
percent tensor([0.4855, 0.5145], device='cuda:0')
percent tensor([0.4817, 0.5183], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.4873, 0.5127], device='cuda:0')
percent tensor([0.5894, 0.4106], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.9279, 0.0721], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.7422) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (992/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8056) |  Loss2: (0.0000) | Acc: (70.00%) (1900/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7957) |  Loss2: (0.0000) | Acc: (71.00%) (2835/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7842) |  Loss2: (0.0000) | Acc: (71.00%) (3762/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7857) |  Loss2: (0.0000) | Acc: (71.00%) (4682/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (5606/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (71.00%) (6517/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7894) |  Loss2: (0.0000) | Acc: (71.00%) (7443/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7859) |  Loss2: (0.0000) | Acc: (71.00%) (8377/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (9290/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (71.00%) (10227/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7909) |  Loss2: (0.0000) | Acc: (72.00%) (11162/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7950) |  Loss2: (0.0000) | Acc: (71.00%) (12061/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7961) |  Loss2: (0.0000) | Acc: (71.00%) (12973/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8005) |  Loss2: (0.0000) | Acc: (71.00%) (13860/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8015) |  Loss2: (0.0000) | Acc: (71.00%) (14767/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (15697/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (16599/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (17524/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (18470/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7988) |  Loss2: (0.0000) | Acc: (71.00%) (19398/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (71.00%) (20337/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7999) |  Loss2: (0.0000) | Acc: (71.00%) (21233/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (22133/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (71.00%) (23078/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (23969/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (24873/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8000) |  Loss2: (0.0000) | Acc: (71.00%) (25789/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (26700/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8011) |  Loss2: (0.0000) | Acc: (71.00%) (27624/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8018) |  Loss2: (0.0000) | Acc: (71.00%) (28521/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8023) |  Loss2: (0.0000) | Acc: (71.00%) (29424/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8026) |  Loss2: (0.0000) | Acc: (71.00%) (30344/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (31285/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8007) |  Loss2: (0.0000) | Acc: (71.00%) (32205/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.7997) |  Loss2: (0.0000) | Acc: (71.00%) (33139/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7993) |  Loss2: (0.0000) | Acc: (71.00%) (34086/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (35032/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (35924/50000)
# TEST : Loss: (0.8294) | Acc: (71.00%) (7109/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.4895, 0.5105], device='cuda:0')
percent tensor([0.6003, 0.3997], device='cuda:0')
percent tensor([0.5767, 0.4233], device='cuda:0')
percent tensor([0.9370, 0.0630], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7470) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8168) |  Loss2: (0.0000) | Acc: (70.00%) (990/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8151) |  Loss2: (0.0000) | Acc: (71.00%) (1920/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8209) |  Loss2: (0.0000) | Acc: (71.00%) (2824/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8227) |  Loss2: (0.0000) | Acc: (70.00%) (3716/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8223) |  Loss2: (0.0000) | Acc: (70.00%) (4614/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (5539/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8194) |  Loss2: (0.0000) | Acc: (70.00%) (6437/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8195) |  Loss2: (0.0000) | Acc: (70.00%) (7340/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8205) |  Loss2: (0.0000) | Acc: (70.00%) (8249/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (9145/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (70.00%) (10045/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8195) |  Loss2: (0.0000) | Acc: (70.00%) (10967/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8162) |  Loss2: (0.0000) | Acc: (70.00%) (11898/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8112) |  Loss2: (0.0000) | Acc: (71.00%) (12858/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8075) |  Loss2: (0.0000) | Acc: (71.00%) (13799/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8071) |  Loss2: (0.0000) | Acc: (71.00%) (14725/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8062) |  Loss2: (0.0000) | Acc: (71.00%) (15637/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (16571/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (17490/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8020) |  Loss2: (0.0000) | Acc: (71.00%) (18441/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (19374/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (20308/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7985) |  Loss2: (0.0000) | Acc: (71.00%) (21231/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (22171/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (23092/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7963) |  Loss2: (0.0000) | Acc: (71.00%) (24024/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7973) |  Loss2: (0.0000) | Acc: (71.00%) (24939/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (25878/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7942) |  Loss2: (0.0000) | Acc: (71.00%) (26810/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7924) |  Loss2: (0.0000) | Acc: (72.00%) (27762/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7912) |  Loss2: (0.0000) | Acc: (72.00%) (28710/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (29631/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7913) |  Loss2: (0.0000) | Acc: (72.00%) (30545/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7903) |  Loss2: (0.0000) | Acc: (72.00%) (31487/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (32365/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7908) |  Loss2: (0.0000) | Acc: (72.00%) (33297/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7900) |  Loss2: (0.0000) | Acc: (72.00%) (34238/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7902) |  Loss2: (0.0000) | Acc: (72.00%) (35154/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (72.00%) (36052/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.8517) | Acc: (71.00%) (7114/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4894, 0.5106], device='cuda:0')
percent tensor([0.6013, 0.3987], device='cuda:0')
percent tensor([0.5752, 0.4248], device='cuda:0')
percent tensor([0.9364, 0.0636], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.9641, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.3159, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(778.2573, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.5526, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(517.2247, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.2354, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4304.9199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1442.8206, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6095.0156, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12176.5576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4064.1052, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17199.0586, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.6720) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7189) |  Loss2: (0.0000) | Acc: (74.00%) (1053/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (74.00%) (2007/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7138) |  Loss2: (0.0000) | Acc: (74.00%) (2975/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7096) |  Loss2: (0.0000) | Acc: (75.00%) (3955/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7132) |  Loss2: (0.0000) | Acc: (75.00%) (4899/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7202) |  Loss2: (0.0000) | Acc: (74.00%) (5841/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7210) |  Loss2: (0.0000) | Acc: (74.00%) (6799/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7259) |  Loss2: (0.0000) | Acc: (74.00%) (7725/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7260) |  Loss2: (0.0000) | Acc: (74.00%) (8680/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7310) |  Loss2: (0.0000) | Acc: (74.00%) (9608/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7304) |  Loss2: (0.0000) | Acc: (74.00%) (10554/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7345) |  Loss2: (0.0000) | Acc: (74.00%) (11486/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7312) |  Loss2: (0.0000) | Acc: (74.00%) (12451/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7293) |  Loss2: (0.0000) | Acc: (74.00%) (13423/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7299) |  Loss2: (0.0000) | Acc: (74.00%) (14375/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7337) |  Loss2: (0.0000) | Acc: (74.00%) (15306/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7357) |  Loss2: (0.0000) | Acc: (74.00%) (16234/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7344) |  Loss2: (0.0000) | Acc: (74.00%) (17186/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7323) |  Loss2: (0.0000) | Acc: (74.00%) (18153/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7325) |  Loss2: (0.0000) | Acc: (74.00%) (19107/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7323) |  Loss2: (0.0000) | Acc: (74.00%) (20069/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7354) |  Loss2: (0.0000) | Acc: (74.00%) (20977/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7370) |  Loss2: (0.0000) | Acc: (74.00%) (21901/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7364) |  Loss2: (0.0000) | Acc: (74.00%) (22849/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7361) |  Loss2: (0.0000) | Acc: (74.00%) (23811/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (24754/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7377) |  Loss2: (0.0000) | Acc: (74.00%) (25683/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7362) |  Loss2: (0.0000) | Acc: (74.00%) (26665/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7363) |  Loss2: (0.0000) | Acc: (74.00%) (27617/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (28584/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7358) |  Loss2: (0.0000) | Acc: (74.00%) (29514/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7347) |  Loss2: (0.0000) | Acc: (74.00%) (30475/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7362) |  Loss2: (0.0000) | Acc: (74.00%) (31417/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7355) |  Loss2: (0.0000) | Acc: (74.00%) (32373/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7356) |  Loss2: (0.0000) | Acc: (74.00%) (33316/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7360) |  Loss2: (0.0000) | Acc: (74.00%) (34262/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7361) |  Loss2: (0.0000) | Acc: (74.00%) (35209/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7367) |  Loss2: (0.0000) | Acc: (74.00%) (36139/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (37054/50000)
# TEST : Loss: (0.7980) | Acc: (72.00%) (7237/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4820, 0.5180], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.4893, 0.5107], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.9444, 0.0556], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.7335) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7312) |  Loss2: (0.0000) | Acc: (74.00%) (1053/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.6843) |  Loss2: (0.0000) | Acc: (76.00%) (2057/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6915) |  Loss2: (0.0000) | Acc: (75.00%) (3011/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6922) |  Loss2: (0.0000) | Acc: (75.00%) (3980/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6932) |  Loss2: (0.0000) | Acc: (75.00%) (4948/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7002) |  Loss2: (0.0000) | Acc: (75.00%) (5897/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7024) |  Loss2: (0.0000) | Acc: (75.00%) (6871/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (7867/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6958) |  Loss2: (0.0000) | Acc: (75.00%) (8831/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (9799/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (10752/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6984) |  Loss2: (0.0000) | Acc: (75.00%) (11716/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7001) |  Loss2: (0.0000) | Acc: (75.00%) (12679/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.6977) |  Loss2: (0.0000) | Acc: (75.00%) (13669/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (14652/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (15609/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (16568/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (17533/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.6974) |  Loss2: (0.0000) | Acc: (75.00%) (18519/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (19476/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (75.00%) (20444/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.6948) |  Loss2: (0.0000) | Acc: (75.00%) (21431/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.6961) |  Loss2: (0.0000) | Acc: (75.00%) (22401/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (23372/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (24340/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (25295/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (26257/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (27228/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (28190/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (29164/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (30119/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (75.00%) (31111/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6962) |  Loss2: (0.0000) | Acc: (75.00%) (32065/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (33058/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (34025/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6937) |  Loss2: (0.0000) | Acc: (75.00%) (35032/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6925) |  Loss2: (0.0000) | Acc: (75.00%) (36018/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6912) |  Loss2: (0.0000) | Acc: (75.00%) (37002/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6903) |  Loss2: (0.0000) | Acc: (75.00%) (37949/50000)
# TEST : Loss: (0.7912) | Acc: (73.00%) (7318/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4891, 0.5109], device='cuda:0')
percent tensor([0.6018, 0.3982], device='cuda:0')
percent tensor([0.5792, 0.4208], device='cuda:0')
percent tensor([0.9422, 0.0578], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (1071/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (2087/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (3071/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6606) |  Loss2: (0.0000) | Acc: (77.00%) (4045/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6582) |  Loss2: (0.0000) | Acc: (77.00%) (5038/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (6031/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (7034/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (8059/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6495) |  Loss2: (0.0000) | Acc: (77.00%) (9039/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (9996/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (10982/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6555) |  Loss2: (0.0000) | Acc: (77.00%) (11951/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (77.00%) (12971/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (13956/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (14941/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6560) |  Loss2: (0.0000) | Acc: (77.00%) (15927/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6598) |  Loss2: (0.0000) | Acc: (77.00%) (16882/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (77.00%) (17880/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6593) |  Loss2: (0.0000) | Acc: (77.00%) (18877/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6570) |  Loss2: (0.0000) | Acc: (77.00%) (19876/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6558) |  Loss2: (0.0000) | Acc: (77.00%) (20882/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6558) |  Loss2: (0.0000) | Acc: (77.00%) (21868/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (77.00%) (22862/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (23851/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (24842/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (25846/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (26826/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (27837/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (28849/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6493) |  Loss2: (0.0000) | Acc: (77.00%) (29830/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6505) |  Loss2: (0.0000) | Acc: (77.00%) (30801/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6513) |  Loss2: (0.0000) | Acc: (77.00%) (31778/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (32766/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (77.00%) (33747/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (34719/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (35696/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6533) |  Loss2: (0.0000) | Acc: (77.00%) (36688/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6525) |  Loss2: (0.0000) | Acc: (77.00%) (37683/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (38635/50000)
# TEST : Loss: (0.6967) | Acc: (75.00%) (7561/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.4827, 0.5173], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4893, 0.5107], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.5772, 0.4228], device='cuda:0')
percent tensor([0.9363, 0.0637], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6117) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (1090/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6340) |  Loss2: (0.0000) | Acc: (77.00%) (2082/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6247) |  Loss2: (0.0000) | Acc: (78.00%) (3115/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6378) |  Loss2: (0.0000) | Acc: (77.00%) (4092/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6394) |  Loss2: (0.0000) | Acc: (77.00%) (5086/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (78.00%) (6099/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (78.00%) (7109/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (78.00%) (8109/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6344) |  Loss2: (0.0000) | Acc: (78.00%) (9100/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (78.00%) (10103/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (11109/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (78.00%) (12111/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (13119/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6293) |  Loss2: (0.0000) | Acc: (78.00%) (14106/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (15124/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6275) |  Loss2: (0.0000) | Acc: (78.00%) (16106/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (78.00%) (17089/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6273) |  Loss2: (0.0000) | Acc: (78.00%) (18083/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (77.00%) (19066/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6290) |  Loss2: (0.0000) | Acc: (78.00%) (20070/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (78.00%) (21086/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6269) |  Loss2: (0.0000) | Acc: (78.00%) (22109/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6266) |  Loss2: (0.0000) | Acc: (78.00%) (23125/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (24146/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (25128/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (26130/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6277) |  Loss2: (0.0000) | Acc: (78.00%) (27129/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (28107/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (29116/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (78.00%) (30114/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6276) |  Loss2: (0.0000) | Acc: (78.00%) (31143/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6266) |  Loss2: (0.0000) | Acc: (78.00%) (32151/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6257) |  Loss2: (0.0000) | Acc: (78.00%) (33174/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (34187/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (35187/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (36190/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (37197/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (38206/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (39169/50000)
# TEST : Loss: (0.6404) | Acc: (77.00%) (7794/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.4892, 0.5108], device='cuda:0')
percent tensor([0.6014, 0.3986], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.9367, 0.0633], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6104) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6709) |  Loss2: (0.0000) | Acc: (75.00%) (1067/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.7366) |  Loss2: (0.0000) | Acc: (73.00%) (1976/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.7820) |  Loss2: (0.0000) | Acc: (72.00%) (2875/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.7851) |  Loss2: (0.0000) | Acc: (72.00%) (3807/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.7886) |  Loss2: (0.0000) | Acc: (72.00%) (4727/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (72.00%) (5657/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.7812) |  Loss2: (0.0000) | Acc: (72.00%) (6613/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.7769) |  Loss2: (0.0000) | Acc: (72.00%) (7545/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (8498/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7773) |  Loss2: (0.0000) | Acc: (72.00%) (9419/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (10364/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7705) |  Loss2: (0.0000) | Acc: (73.00%) (11316/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7702) |  Loss2: (0.0000) | Acc: (73.00%) (12253/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7709) |  Loss2: (0.0000) | Acc: (73.00%) (13187/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7684) |  Loss2: (0.0000) | Acc: (73.00%) (14146/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7686) |  Loss2: (0.0000) | Acc: (73.00%) (15079/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7672) |  Loss2: (0.0000) | Acc: (73.00%) (16039/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (73.00%) (16990/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (17930/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7629) |  Loss2: (0.0000) | Acc: (73.00%) (18866/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7623) |  Loss2: (0.0000) | Acc: (73.00%) (19813/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7619) |  Loss2: (0.0000) | Acc: (73.00%) (20757/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.7608) |  Loss2: (0.0000) | Acc: (73.00%) (21716/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.7573) |  Loss2: (0.0000) | Acc: (73.00%) (22698/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.7531) |  Loss2: (0.0000) | Acc: (73.00%) (23698/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.7521) |  Loss2: (0.0000) | Acc: (73.00%) (24659/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.7505) |  Loss2: (0.0000) | Acc: (73.00%) (25619/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.7472) |  Loss2: (0.0000) | Acc: (74.00%) (26619/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (74.00%) (27598/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (74.00%) (28555/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.7436) |  Loss2: (0.0000) | Acc: (74.00%) (29530/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.7414) |  Loss2: (0.0000) | Acc: (74.00%) (30534/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.7402) |  Loss2: (0.0000) | Acc: (74.00%) (31488/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.7385) |  Loss2: (0.0000) | Acc: (74.00%) (32465/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.7359) |  Loss2: (0.0000) | Acc: (74.00%) (33452/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.7351) |  Loss2: (0.0000) | Acc: (74.00%) (34435/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.7351) |  Loss2: (0.0000) | Acc: (74.00%) (35391/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.7330) |  Loss2: (0.0000) | Acc: (74.00%) (36387/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.7318) |  Loss2: (0.0000) | Acc: (74.00%) (37329/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.7136) | Acc: (75.00%) (7583/10000)
percent tensor([0.5278, 0.4722], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.4661, 0.5339], device='cuda:0')
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.5075, 0.4925], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.9492, 0.0508], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.6829) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.7277) |  Loss2: (0.0000) | Acc: (74.00%) (1046/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.7040) |  Loss2: (0.0000) | Acc: (75.00%) (2025/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6927) |  Loss2: (0.0000) | Acc: (75.00%) (3003/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6830) |  Loss2: (0.0000) | Acc: (76.00%) (4002/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6883) |  Loss2: (0.0000) | Acc: (76.00%) (4968/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6813) |  Loss2: (0.0000) | Acc: (76.00%) (5958/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6795) |  Loss2: (0.0000) | Acc: (76.00%) (6936/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (7916/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6723) |  Loss2: (0.0000) | Acc: (76.00%) (8919/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6706) |  Loss2: (0.0000) | Acc: (76.00%) (9901/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6706) |  Loss2: (0.0000) | Acc: (76.00%) (10898/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6708) |  Loss2: (0.0000) | Acc: (76.00%) (11875/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6696) |  Loss2: (0.0000) | Acc: (76.00%) (12881/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6681) |  Loss2: (0.0000) | Acc: (76.00%) (13870/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6688) |  Loss2: (0.0000) | Acc: (76.00%) (14851/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (15844/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6696) |  Loss2: (0.0000) | Acc: (76.00%) (16802/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6676) |  Loss2: (0.0000) | Acc: (76.00%) (17797/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6686) |  Loss2: (0.0000) | Acc: (76.00%) (18788/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6691) |  Loss2: (0.0000) | Acc: (76.00%) (19762/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (20753/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6653) |  Loss2: (0.0000) | Acc: (76.00%) (21747/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6634) |  Loss2: (0.0000) | Acc: (76.00%) (22742/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6636) |  Loss2: (0.0000) | Acc: (76.00%) (23721/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6619) |  Loss2: (0.0000) | Acc: (76.00%) (24734/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (77.00%) (25743/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (77.00%) (26746/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (77.00%) (27729/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (77.00%) (28722/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6578) |  Loss2: (0.0000) | Acc: (77.00%) (29738/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6588) |  Loss2: (0.0000) | Acc: (77.00%) (30693/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6594) |  Loss2: (0.0000) | Acc: (77.00%) (31684/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6585) |  Loss2: (0.0000) | Acc: (77.00%) (32689/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6575) |  Loss2: (0.0000) | Acc: (77.00%) (33674/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (34687/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (35692/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (36685/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (37677/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (38624/50000)
# TEST : Loss: (0.6752) | Acc: (77.00%) (7718/10000)
percent tensor([0.5262, 0.4738], device='cuda:0')
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4640, 0.5360], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.9669, 0.0331], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6381) |  Loss2: (0.0000) | Acc: (77.00%) (1091/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (2072/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6392) |  Loss2: (0.0000) | Acc: (77.00%) (3060/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (76.00%) (4025/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (76.00%) (5015/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6505) |  Loss2: (0.0000) | Acc: (77.00%) (6015/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6490) |  Loss2: (0.0000) | Acc: (77.00%) (7005/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (7989/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (8984/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (9972/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6529) |  Loss2: (0.0000) | Acc: (77.00%) (10979/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6478) |  Loss2: (0.0000) | Acc: (77.00%) (12000/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (13021/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (14035/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6456) |  Loss2: (0.0000) | Acc: (77.00%) (14993/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (15977/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6484) |  Loss2: (0.0000) | Acc: (77.00%) (16968/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (17936/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6507) |  Loss2: (0.0000) | Acc: (77.00%) (18920/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6473) |  Loss2: (0.0000) | Acc: (77.00%) (19944/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6455) |  Loss2: (0.0000) | Acc: (77.00%) (20955/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6463) |  Loss2: (0.0000) | Acc: (77.00%) (21952/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (22972/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (23981/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (24958/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (25985/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (26981/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6408) |  Loss2: (0.0000) | Acc: (77.00%) (27979/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (28978/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (29966/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6393) |  Loss2: (0.0000) | Acc: (77.00%) (30996/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6388) |  Loss2: (0.0000) | Acc: (77.00%) (32019/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6381) |  Loss2: (0.0000) | Acc: (77.00%) (33040/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6367) |  Loss2: (0.0000) | Acc: (78.00%) (34065/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (35074/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (78.00%) (36090/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (78.00%) (37051/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6361) |  Loss2: (0.0000) | Acc: (78.00%) (38057/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (78.00%) (39029/50000)
# TEST : Loss: (0.6562) | Acc: (77.00%) (7762/10000)
percent tensor([0.5270, 0.4730], device='cuda:0')
percent tensor([0.4942, 0.5058], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5236, 0.4764], device='cuda:0')
percent tensor([0.6350, 0.3650], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.9758, 0.0242], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6063) |  Loss2: (0.0000) | Acc: (79.00%) (1116/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6014) |  Loss2: (0.0000) | Acc: (78.00%) (2121/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6127) |  Loss2: (0.0000) | Acc: (78.00%) (3117/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6109) |  Loss2: (0.0000) | Acc: (78.00%) (4115/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6046) |  Loss2: (0.0000) | Acc: (78.00%) (5138/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (78.00%) (6148/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6057) |  Loss2: (0.0000) | Acc: (78.00%) (7161/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6089) |  Loss2: (0.0000) | Acc: (78.00%) (8160/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6077) |  Loss2: (0.0000) | Acc: (78.00%) (9182/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6103) |  Loss2: (0.0000) | Acc: (78.00%) (10178/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6114) |  Loss2: (0.0000) | Acc: (78.00%) (11172/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (78.00%) (12169/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (78.00%) (13151/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (14149/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (15145/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (16137/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (17150/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (18155/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6186) |  Loss2: (0.0000) | Acc: (78.00%) (19151/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6202) |  Loss2: (0.0000) | Acc: (78.00%) (20139/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (21130/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6212) |  Loss2: (0.0000) | Acc: (78.00%) (22160/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (23153/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6193) |  Loss2: (0.0000) | Acc: (78.00%) (24170/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (25187/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (26166/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (27158/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (28153/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (29141/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (30172/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (31145/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (32162/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (33169/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (34172/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (35183/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (36197/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (78.00%) (37232/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (38254/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (78.00%) (39198/50000)
# TEST : Loss: (0.6476) | Acc: (77.00%) (7793/10000)
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.6338, 0.3662], device='cuda:0')
percent tensor([0.6079, 0.3921], device='cuda:0')
percent tensor([0.9820, 0.0180], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (77.00%) (1089/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (77.00%) (2081/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6299) |  Loss2: (0.0000) | Acc: (77.00%) (3086/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (78.00%) (4101/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (78.00%) (5113/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6250) |  Loss2: (0.0000) | Acc: (78.00%) (6131/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (7133/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (8153/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (9131/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (10135/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (11130/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (12128/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6226) |  Loss2: (0.0000) | Acc: (78.00%) (13127/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6218) |  Loss2: (0.0000) | Acc: (78.00%) (14136/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6221) |  Loss2: (0.0000) | Acc: (78.00%) (15133/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (78.00%) (16121/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6235) |  Loss2: (0.0000) | Acc: (78.00%) (17130/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (18156/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (19160/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6220) |  Loss2: (0.0000) | Acc: (78.00%) (20150/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (21177/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (22187/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6216) |  Loss2: (0.0000) | Acc: (78.00%) (23189/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (24219/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (25250/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (26246/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (27259/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (28280/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (29295/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (30325/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (31320/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (78.00%) (32335/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (33322/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6172) |  Loss2: (0.0000) | Acc: (78.00%) (34332/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (35321/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (36332/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6165) |  Loss2: (0.0000) | Acc: (78.00%) (37349/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (38345/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (39321/50000)
# TEST : Loss: (0.6430) | Acc: (78.00%) (7807/10000)
percent tensor([0.5276, 0.4724], device='cuda:0')
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4654, 0.5346], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.6136, 0.3864], device='cuda:0')
percent tensor([0.9855, 0.0145], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.6319) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5996) |  Loss2: (0.0000) | Acc: (79.00%) (1122/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6078) |  Loss2: (0.0000) | Acc: (78.00%) (2117/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (3112/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5999) |  Loss2: (0.0000) | Acc: (78.00%) (4141/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6067) |  Loss2: (0.0000) | Acc: (78.00%) (5155/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6045) |  Loss2: (0.0000) | Acc: (79.00%) (6188/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6094) |  Loss2: (0.0000) | Acc: (79.00%) (7188/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6143) |  Loss2: (0.0000) | Acc: (78.00%) (8168/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6166) |  Loss2: (0.0000) | Acc: (78.00%) (9166/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (10189/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6131) |  Loss2: (0.0000) | Acc: (78.00%) (11204/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6096) |  Loss2: (0.0000) | Acc: (78.00%) (12218/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6091) |  Loss2: (0.0000) | Acc: (78.00%) (13228/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6108) |  Loss2: (0.0000) | Acc: (78.00%) (14234/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (78.00%) (15218/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6154) |  Loss2: (0.0000) | Acc: (78.00%) (16210/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (17231/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (18267/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6094) |  Loss2: (0.0000) | Acc: (78.00%) (19295/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6070) |  Loss2: (0.0000) | Acc: (79.00%) (20326/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6063) |  Loss2: (0.0000) | Acc: (78.00%) (21336/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (78.00%) (22333/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6035) |  Loss2: (0.0000) | Acc: (78.00%) (23352/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6040) |  Loss2: (0.0000) | Acc: (78.00%) (24362/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6028) |  Loss2: (0.0000) | Acc: (78.00%) (25379/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (26401/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6022) |  Loss2: (0.0000) | Acc: (79.00%) (27419/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (79.00%) (28442/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6006) |  Loss2: (0.0000) | Acc: (79.00%) (29459/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6000) |  Loss2: (0.0000) | Acc: (79.00%) (30490/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6008) |  Loss2: (0.0000) | Acc: (79.00%) (31494/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (32498/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6014) |  Loss2: (0.0000) | Acc: (79.00%) (33517/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6024) |  Loss2: (0.0000) | Acc: (79.00%) (34507/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (35524/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6010) |  Loss2: (0.0000) | Acc: (79.00%) (36567/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6005) |  Loss2: (0.0000) | Acc: (79.00%) (37575/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6003) |  Loss2: (0.0000) | Acc: (79.00%) (38586/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (79.00%) (39570/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.7046) | Acc: (75.00%) (7594/10000)
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4660, 0.5340], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.6101, 0.3899], device='cuda:0')
percent tensor([0.9851, 0.0149], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(172.8340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.4263, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(785.2326, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.5641, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(515.6425, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2181.0874, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.8647, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1437.6057, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6092.9346, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12132.3027, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4048.1936, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17123.6445, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5464) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (79.00%) (2145/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5784) |  Loss2: (0.0000) | Acc: (79.00%) (3153/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (79.00%) (4164/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (5189/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5822) |  Loss2: (0.0000) | Acc: (79.00%) (6212/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5759) |  Loss2: (0.0000) | Acc: (80.00%) (7276/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5779) |  Loss2: (0.0000) | Acc: (80.00%) (8303/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (80.00%) (9348/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (80.00%) (10359/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5767) |  Loss2: (0.0000) | Acc: (80.00%) (11375/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (12426/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5708) |  Loss2: (0.0000) | Acc: (80.00%) (13459/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (14484/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5705) |  Loss2: (0.0000) | Acc: (80.00%) (15505/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5683) |  Loss2: (0.0000) | Acc: (80.00%) (16540/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (80.00%) (17566/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (18624/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (19661/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5676) |  Loss2: (0.0000) | Acc: (80.00%) (20684/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (21710/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5685) |  Loss2: (0.0000) | Acc: (80.00%) (22742/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5681) |  Loss2: (0.0000) | Acc: (80.00%) (23768/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (24808/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5700) |  Loss2: (0.0000) | Acc: (80.00%) (25801/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5689) |  Loss2: (0.0000) | Acc: (80.00%) (26846/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (27864/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (28884/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5713) |  Loss2: (0.0000) | Acc: (80.00%) (29909/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (30926/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (31968/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5716) |  Loss2: (0.0000) | Acc: (80.00%) (32981/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5736) |  Loss2: (0.0000) | Acc: (80.00%) (33988/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5721) |  Loss2: (0.0000) | Acc: (80.00%) (35037/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5715) |  Loss2: (0.0000) | Acc: (80.00%) (36072/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (80.00%) (37120/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (38181/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (39210/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5682) |  Loss2: (0.0000) | Acc: (80.00%) (40207/50000)
# TEST : Loss: (0.7063) | Acc: (76.00%) (7617/10000)
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4669, 0.5331], device='cuda:0')
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.9869, 0.0131], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.5135) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5327) |  Loss2: (0.0000) | Acc: (81.00%) (1142/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (2197/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5305) |  Loss2: (0.0000) | Acc: (81.00%) (3233/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5414) |  Loss2: (0.0000) | Acc: (81.00%) (4256/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5296) |  Loss2: (0.0000) | Acc: (81.00%) (5327/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (6350/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5335) |  Loss2: (0.0000) | Acc: (81.00%) (7395/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5284) |  Loss2: (0.0000) | Acc: (81.00%) (8460/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5317) |  Loss2: (0.0000) | Acc: (81.00%) (9506/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5308) |  Loss2: (0.0000) | Acc: (81.00%) (10557/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5367) |  Loss2: (0.0000) | Acc: (81.00%) (11580/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (12633/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5392) |  Loss2: (0.0000) | Acc: (81.00%) (13652/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (14658/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (15716/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (81.00%) (16740/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5454) |  Loss2: (0.0000) | Acc: (81.00%) (17780/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (18833/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (19876/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (20904/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (21968/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (23015/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5403) |  Loss2: (0.0000) | Acc: (81.00%) (24077/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5400) |  Loss2: (0.0000) | Acc: (81.00%) (25116/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (26154/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (27178/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (28230/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (29218/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (30289/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (31332/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (32359/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (33384/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (34436/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (35488/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (81.00%) (36526/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (37564/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (38627/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5418) |  Loss2: (0.0000) | Acc: (81.00%) (39661/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5406) |  Loss2: (0.0000) | Acc: (81.00%) (40675/50000)
# TEST : Loss: (0.6897) | Acc: (76.00%) (7643/10000)
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4656, 0.5344], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.6089, 0.3911], device='cuda:0')
percent tensor([0.9832, 0.0168], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (1158/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (2203/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (82.00%) (3254/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (81.00%) (4296/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (5350/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (6384/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5266) |  Loss2: (0.0000) | Acc: (81.00%) (7437/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5214) |  Loss2: (0.0000) | Acc: (82.00%) (8506/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (9538/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (10581/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5242) |  Loss2: (0.0000) | Acc: (81.00%) (11627/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (12679/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5275) |  Loss2: (0.0000) | Acc: (81.00%) (13722/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5277) |  Loss2: (0.0000) | Acc: (81.00%) (14767/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5265) |  Loss2: (0.0000) | Acc: (81.00%) (15807/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5276) |  Loss2: (0.0000) | Acc: (81.00%) (16851/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5305) |  Loss2: (0.0000) | Acc: (81.00%) (17888/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5304) |  Loss2: (0.0000) | Acc: (81.00%) (18938/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5308) |  Loss2: (0.0000) | Acc: (81.00%) (19969/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5289) |  Loss2: (0.0000) | Acc: (81.00%) (21043/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5272) |  Loss2: (0.0000) | Acc: (81.00%) (22111/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5262) |  Loss2: (0.0000) | Acc: (81.00%) (23169/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (24214/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (81.00%) (25283/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (26327/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (27377/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (82.00%) (28456/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (29518/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (30563/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (31622/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (32657/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (82.00%) (33695/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (34755/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (35781/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (36847/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (37901/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (38962/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (40014/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (41023/50000)
# TEST : Loss: (0.6170) | Acc: (79.00%) (7932/10000)
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5366, 0.4634], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.9845, 0.0155], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (81.00%) (1152/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5151) |  Loss2: (0.0000) | Acc: (82.00%) (2210/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (3280/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (4351/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (5427/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.4925) |  Loss2: (0.0000) | Acc: (83.00%) (6495/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.4898) |  Loss2: (0.0000) | Acc: (83.00%) (7576/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (8634/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (83.00%) (9691/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (83.00%) (10746/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.4983) |  Loss2: (0.0000) | Acc: (83.00%) (11800/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (12854/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (13905/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.4971) |  Loss2: (0.0000) | Acc: (83.00%) (14988/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (83.00%) (16050/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (83.00%) (17115/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (18158/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (83.00%) (19233/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (83.00%) (20292/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (21324/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (22351/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5061) |  Loss2: (0.0000) | Acc: (82.00%) (23408/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (24466/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (25527/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (26602/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (27671/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (28749/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (29790/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (30841/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (31907/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (32989/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (34044/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (35086/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (36152/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (37215/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (38283/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (39343/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (40384/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (41405/50000)
# TEST : Loss: (0.6709) | Acc: (77.00%) (7789/10000)
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6375, 0.3625], device='cuda:0')
percent tensor([0.6189, 0.3811], device='cuda:0')
percent tensor([0.9870, 0.0130], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.5847) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.6391) |  Loss2: (0.0000) | Acc: (77.00%) (2081/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.6791) |  Loss2: (0.0000) | Acc: (76.00%) (3026/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6876) |  Loss2: (0.0000) | Acc: (76.00%) (3990/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (76.00%) (4964/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6912) |  Loss2: (0.0000) | Acc: (76.00%) (5945/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6894) |  Loss2: (0.0000) | Acc: (76.00%) (6942/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6949) |  Loss2: (0.0000) | Acc: (76.00%) (7894/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6940) |  Loss2: (0.0000) | Acc: (76.00%) (8864/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (76.00%) (9865/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6851) |  Loss2: (0.0000) | Acc: (76.00%) (10862/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (11895/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6715) |  Loss2: (0.0000) | Acc: (76.00%) (12883/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6665) |  Loss2: (0.0000) | Acc: (77.00%) (13904/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6624) |  Loss2: (0.0000) | Acc: (77.00%) (14926/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6582) |  Loss2: (0.0000) | Acc: (77.00%) (15950/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (16971/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6526) |  Loss2: (0.0000) | Acc: (77.00%) (17964/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (18962/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6500) |  Loss2: (0.0000) | Acc: (77.00%) (19964/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6468) |  Loss2: (0.0000) | Acc: (77.00%) (20985/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6457) |  Loss2: (0.0000) | Acc: (77.00%) (21975/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (23010/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (24053/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6365) |  Loss2: (0.0000) | Acc: (78.00%) (25078/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (78.00%) (26083/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (78.00%) (27107/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (78.00%) (28121/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6333) |  Loss2: (0.0000) | Acc: (78.00%) (29108/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (78.00%) (30132/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (31155/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6274) |  Loss2: (0.0000) | Acc: (78.00%) (32184/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6264) |  Loss2: (0.0000) | Acc: (78.00%) (33195/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6249) |  Loss2: (0.0000) | Acc: (78.00%) (34225/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (35253/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (36299/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (37333/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (38334/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (39315/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6273) | Acc: (78.00%) (7875/10000)
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.4889, 0.5111], device='cuda:0')
percent tensor([0.4663, 0.5337], device='cuda:0')
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5900, 0.4100], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.9844, 0.0156], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.4838) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5606) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5425) |  Loss2: (0.0000) | Acc: (80.00%) (2160/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (80.00%) (3195/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (79.00%) (4198/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5705) |  Loss2: (0.0000) | Acc: (79.00%) (5189/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5685) |  Loss2: (0.0000) | Acc: (79.00%) (6222/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (79.00%) (7261/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (8307/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (9293/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5683) |  Loss2: (0.0000) | Acc: (79.00%) (10296/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (79.00%) (11332/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (12373/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5647) |  Loss2: (0.0000) | Acc: (79.00%) (13401/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5641) |  Loss2: (0.0000) | Acc: (80.00%) (14441/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (15473/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5624) |  Loss2: (0.0000) | Acc: (80.00%) (16526/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5646) |  Loss2: (0.0000) | Acc: (80.00%) (17542/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (18573/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (19604/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5639) |  Loss2: (0.0000) | Acc: (80.00%) (20626/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5652) |  Loss2: (0.0000) | Acc: (80.00%) (21659/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (22694/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (23730/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (24777/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (25803/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5621) |  Loss2: (0.0000) | Acc: (80.00%) (26831/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (27842/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5609) |  Loss2: (0.0000) | Acc: (80.00%) (28891/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (29941/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (30970/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (32022/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (33078/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (34147/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (35173/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5576) |  Loss2: (0.0000) | Acc: (80.00%) (36194/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (80.00%) (37227/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5573) |  Loss2: (0.0000) | Acc: (80.00%) (38272/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (39316/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (40317/50000)
# TEST : Loss: (0.5957) | Acc: (79.00%) (7970/10000)
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5646, 0.4354], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5883, 0.4117], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.9860, 0.0140], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (80.00%) (1139/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (2172/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (81.00%) (3223/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (4298/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5337) |  Loss2: (0.0000) | Acc: (81.00%) (5351/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5378) |  Loss2: (0.0000) | Acc: (81.00%) (6368/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (7401/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (81.00%) (8417/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5489) |  Loss2: (0.0000) | Acc: (81.00%) (9447/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (10495/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (81.00%) (11526/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (81.00%) (12569/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (13614/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5460) |  Loss2: (0.0000) | Acc: (81.00%) (14675/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5463) |  Loss2: (0.0000) | Acc: (81.00%) (15710/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (16773/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (17818/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (18842/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (19891/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5425) |  Loss2: (0.0000) | Acc: (81.00%) (20938/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (21960/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5421) |  Loss2: (0.0000) | Acc: (81.00%) (23009/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5407) |  Loss2: (0.0000) | Acc: (81.00%) (24060/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (25092/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5408) |  Loss2: (0.0000) | Acc: (81.00%) (26129/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5411) |  Loss2: (0.0000) | Acc: (81.00%) (27152/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (28220/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (29299/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (30352/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (31410/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (32437/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (33492/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (34525/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (35578/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5361) |  Loss2: (0.0000) | Acc: (81.00%) (36631/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (81.00%) (37681/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5354) |  Loss2: (0.0000) | Acc: (81.00%) (38732/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (39790/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (40777/50000)
# TEST : Loss: (0.5843) | Acc: (80.00%) (8011/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4856, 0.5144], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.9882, 0.0118], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (2214/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (3250/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (4326/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (5370/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (6437/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5122) |  Loss2: (0.0000) | Acc: (82.00%) (7456/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (8514/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5141) |  Loss2: (0.0000) | Acc: (82.00%) (9555/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5164) |  Loss2: (0.0000) | Acc: (82.00%) (10605/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (11631/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (12684/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (13721/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (14759/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (15797/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (16859/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (81.00%) (17915/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (81.00%) (18948/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (19989/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (21031/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (22075/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (23119/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5241) |  Loss2: (0.0000) | Acc: (81.00%) (24158/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (25219/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (26270/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (27317/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (28360/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (29415/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (30459/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (31512/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (32563/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (33604/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (81.00%) (34671/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (35717/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (36774/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (37815/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (38848/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (39907/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (40898/50000)
# TEST : Loss: (0.5743) | Acc: (80.00%) (8068/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6011, 0.3989], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.9903, 0.0097], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (83.00%) (2250/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (83.00%) (3302/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (4341/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (5399/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (6438/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5138) |  Loss2: (0.0000) | Acc: (82.00%) (7478/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (8550/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (9598/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (10641/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5067) |  Loss2: (0.0000) | Acc: (82.00%) (11691/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (12716/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (13788/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (14858/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (15887/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (16965/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5060) |  Loss2: (0.0000) | Acc: (82.00%) (18022/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (19075/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5067) |  Loss2: (0.0000) | Acc: (82.00%) (20147/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (21200/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (22233/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (23293/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (24350/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (25413/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (26488/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5060) |  Loss2: (0.0000) | Acc: (82.00%) (27559/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (28574/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (29638/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (30689/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5094) |  Loss2: (0.0000) | Acc: (82.00%) (31722/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (32794/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (33846/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5106) |  Loss2: (0.0000) | Acc: (82.00%) (34866/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5110) |  Loss2: (0.0000) | Acc: (82.00%) (35911/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (36949/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (37994/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5120) |  Loss2: (0.0000) | Acc: (82.00%) (39048/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (40112/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (41120/50000)
# TEST : Loss: (0.5648) | Acc: (80.00%) (8081/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4728, 0.5272], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5534, 0.4466], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.9916, 0.0084], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4788) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (1173/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.4821) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4802) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (4366/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (82.00%) (5416/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (83.00%) (6487/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (83.00%) (7544/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (8608/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (83.00%) (9671/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (10735/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4920) |  Loss2: (0.0000) | Acc: (83.00%) (11811/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (13953/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4883) |  Loss2: (0.0000) | Acc: (83.00%) (15013/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (16057/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (17143/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (18211/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (83.00%) (19267/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4882) |  Loss2: (0.0000) | Acc: (83.00%) (20334/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (21386/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (22444/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (83.00%) (23488/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (83.00%) (24554/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (25618/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (82.00%) (26658/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (82.00%) (27721/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (82.00%) (28788/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4937) |  Loss2: (0.0000) | Acc: (82.00%) (29844/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (82.00%) (30910/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (31958/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (33006/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (34070/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (35125/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (36177/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (37264/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (38309/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4960) |  Loss2: (0.0000) | Acc: (82.00%) (39363/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (40428/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (41460/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.6512) | Acc: (78.00%) (7842/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4859, 0.5141], device='cuda:0')
percent tensor([0.4729, 0.5271], device='cuda:0')
percent tensor([0.5656, 0.4344], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6047, 0.3953], device='cuda:0')
percent tensor([0.6698, 0.3302], device='cuda:0')
percent tensor([0.9922, 0.0078], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.4092, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.4547, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(790.2896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.2834, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(513.8759, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2189.9766, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4291.5176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1432.3967, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.1411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12091.4668, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4032.4648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17051.7754, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (84.00%) (4417/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (84.00%) (5486/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (6554/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (84.00%) (7641/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (8704/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (9765/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4716) |  Loss2: (0.0000) | Acc: (83.00%) (10841/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (11907/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4725) |  Loss2: (0.0000) | Acc: (83.00%) (12978/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (14048/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (15109/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (16183/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (17260/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (18316/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (19405/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4746) |  Loss2: (0.0000) | Acc: (83.00%) (20466/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4750) |  Loss2: (0.0000) | Acc: (83.00%) (21529/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (22594/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (23670/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4732) |  Loss2: (0.0000) | Acc: (83.00%) (24756/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (25825/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4737) |  Loss2: (0.0000) | Acc: (83.00%) (26898/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (27959/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (29023/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (30072/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (31148/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4774) |  Loss2: (0.0000) | Acc: (83.00%) (32205/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4772) |  Loss2: (0.0000) | Acc: (83.00%) (33280/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (34360/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4763) |  Loss2: (0.0000) | Acc: (83.00%) (35434/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (83.00%) (36504/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (37563/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (38629/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (39713/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4755) |  Loss2: (0.0000) | Acc: (83.00%) (40787/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4743) |  Loss2: (0.0000) | Acc: (83.00%) (41838/50000)
# TEST : Loss: (0.6159) | Acc: (79.00%) (7978/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4861, 0.5139], device='cuda:0')
percent tensor([0.4732, 0.5268], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (1214/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (2295/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (3360/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (4433/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4488) |  Loss2: (0.0000) | Acc: (84.00%) (5525/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (6601/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (7683/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (8783/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (9838/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4587) |  Loss2: (0.0000) | Acc: (84.00%) (10904/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (11984/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4599) |  Loss2: (0.0000) | Acc: (84.00%) (13045/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (14128/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4572) |  Loss2: (0.0000) | Acc: (84.00%) (15202/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (16285/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (17357/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (18412/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (19510/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (20593/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (21705/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (22774/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4555) |  Loss2: (0.0000) | Acc: (84.00%) (23852/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (24920/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (25998/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (27088/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (28161/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (29230/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (30281/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (31372/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (32460/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (33551/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (34634/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (35712/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (36797/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (37853/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (38924/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4561) |  Loss2: (0.0000) | Acc: (84.00%) (40018/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (41098/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (42155/50000)
# TEST : Loss: (0.6649) | Acc: (79.00%) (7917/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.4727, 0.5273], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.5551, 0.4449], device='cuda:0')
percent tensor([0.6026, 0.3974], device='cuda:0')
percent tensor([0.6663, 0.3337], device='cuda:0')
percent tensor([0.9923, 0.0077], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (3319/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (83.00%) (4407/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4452) |  Loss2: (0.0000) | Acc: (84.00%) (5507/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (6592/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (7690/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (8776/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (9854/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (10940/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (12042/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (13132/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (14218/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (15281/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (16358/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (17437/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (18514/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (19602/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (20715/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (21815/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (22898/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (23958/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (25063/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (26149/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (27240/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (28330/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (29418/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (30515/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (84.00%) (31608/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (32676/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (33768/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (34862/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (35932/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (37035/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (38134/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (39203/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (40309/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (41403/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (42435/50000)
# TEST : Loss: (0.5353) | Acc: (81.00%) (8195/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4861, 0.5139], device='cuda:0')
percent tensor([0.4727, 0.5273], device='cuda:0')
percent tensor([0.5663, 0.4337], device='cuda:0')
percent tensor([0.5536, 0.4464], device='cuda:0')
percent tensor([0.6021, 0.3979], device='cuda:0')
percent tensor([0.6644, 0.3356], device='cuda:0')
percent tensor([0.9914, 0.0086], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (84.00%) (2279/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (4472/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (5549/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (6644/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (7730/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (8814/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (9916/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (10994/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4202) |  Loss2: (0.0000) | Acc: (85.00%) (12106/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (13181/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4243) |  Loss2: (0.0000) | Acc: (85.00%) (14284/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (15375/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (16473/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (17547/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (85.00%) (18630/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (19725/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (20830/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (21939/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (23027/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (24121/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (25207/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (26298/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (85.00%) (27384/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (28492/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (29580/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (30679/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (31763/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4252) |  Loss2: (0.0000) | Acc: (85.00%) (32850/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (33951/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (35044/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (36135/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (37216/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (38334/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (39435/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (40535/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4242) |  Loss2: (0.0000) | Acc: (85.00%) (41619/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (42676/50000)
# TEST : Loss: (0.5286) | Acc: (82.00%) (8239/10000)
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.4864, 0.5136], device='cuda:0')
percent tensor([0.4728, 0.5272], device='cuda:0')
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.6740, 0.3260], device='cuda:0')
percent tensor([0.9923, 0.0077], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (84.00%) (1194/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4648) |  Loss2: (0.0000) | Acc: (83.00%) (2234/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (82.00%) (3283/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (82.00%) (4333/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (5371/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (6418/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (7475/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.4997) |  Loss2: (0.0000) | Acc: (82.00%) (8538/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (9580/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5113) |  Loss2: (0.0000) | Acc: (82.00%) (10607/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (11663/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (12729/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (13783/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5073) |  Loss2: (0.0000) | Acc: (82.00%) (14839/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (15903/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (16960/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (18023/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (19081/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (20146/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4980) |  Loss2: (0.0000) | Acc: (82.00%) (21221/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4948) |  Loss2: (0.0000) | Acc: (82.00%) (22307/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (23388/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (24458/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (82.00%) (25529/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (26600/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (27662/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (82.00%) (28725/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (82.00%) (29802/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (82.00%) (30849/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4885) |  Loss2: (0.0000) | Acc: (82.00%) (31926/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (82.00%) (32988/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (82.00%) (34059/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4878) |  Loss2: (0.0000) | Acc: (82.00%) (35120/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (82.00%) (36203/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (82.00%) (37277/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4849) |  Loss2: (0.0000) | Acc: (83.00%) (38356/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (83.00%) (39435/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (40502/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4826) |  Loss2: (0.0000) | Acc: (83.00%) (41547/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.5539) | Acc: (81.00%) (8156/10000)
percent tensor([0.5063, 0.4937], device='cuda:0')
percent tensor([0.4925, 0.5075], device='cuda:0')
percent tensor([0.4732, 0.5268], device='cuda:0')
percent tensor([0.5708, 0.4292], device='cuda:0')
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.6822, 0.3178], device='cuda:0')
percent tensor([0.9919, 0.0081], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (1205/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (84.00%) (2279/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (3349/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (4410/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (83.00%) (5468/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (83.00%) (6544/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (83.00%) (7611/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (83.00%) (8671/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4605) |  Loss2: (0.0000) | Acc: (83.00%) (9763/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (83.00%) (10855/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (83.00%) (11923/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (83.00%) (13000/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (83.00%) (14076/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (83.00%) (15158/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (83.00%) (16219/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (83.00%) (17286/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (83.00%) (18377/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (83.00%) (19454/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4588) |  Loss2: (0.0000) | Acc: (83.00%) (20509/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (83.00%) (21593/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (22694/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (23776/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (24869/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (25964/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (27038/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4524) |  Loss2: (0.0000) | Acc: (84.00%) (28124/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (29197/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4519) |  Loss2: (0.0000) | Acc: (84.00%) (30279/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (31378/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (32434/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (33518/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4504) |  Loss2: (0.0000) | Acc: (84.00%) (34602/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (35688/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (36772/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4510) |  Loss2: (0.0000) | Acc: (84.00%) (37849/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (38942/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (40021/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (41099/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (84.00%) (42159/50000)
# TEST : Loss: (0.5257) | Acc: (82.00%) (8240/10000)
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4749, 0.5251], device='cuda:0')
percent tensor([0.5727, 0.4273], device='cuda:0')
percent tensor([0.5588, 0.4412], device='cuda:0')
percent tensor([0.6042, 0.3958], device='cuda:0')
percent tensor([0.6952, 0.3048], device='cuda:0')
percent tensor([0.9925, 0.0075], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (86.00%) (1211/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (2299/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (3377/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (4457/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4257) |  Loss2: (0.0000) | Acc: (85.00%) (5568/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4233) |  Loss2: (0.0000) | Acc: (85.00%) (6658/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (7741/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (8818/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (9921/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4277) |  Loss2: (0.0000) | Acc: (85.00%) (11008/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (12103/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (13187/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (14275/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4259) |  Loss2: (0.0000) | Acc: (85.00%) (15356/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (16445/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4238) |  Loss2: (0.0000) | Acc: (85.00%) (17553/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4248) |  Loss2: (0.0000) | Acc: (85.00%) (18643/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (19748/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (20845/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (21931/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (23016/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4227) |  Loss2: (0.0000) | Acc: (85.00%) (24111/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4244) |  Loss2: (0.0000) | Acc: (85.00%) (25200/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (26264/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4259) |  Loss2: (0.0000) | Acc: (85.00%) (27357/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (28462/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4267) |  Loss2: (0.0000) | Acc: (85.00%) (29545/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (30606/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (31688/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (32776/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4287) |  Loss2: (0.0000) | Acc: (85.00%) (33874/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (34961/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (85.00%) (36057/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (37136/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (85.00%) (38200/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (39278/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4310) |  Loss2: (0.0000) | Acc: (85.00%) (40377/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4305) |  Loss2: (0.0000) | Acc: (85.00%) (41489/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (42524/50000)
# TEST : Loss: (0.5137) | Acc: (82.00%) (8270/10000)
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.4759, 0.5241], device='cuda:0')
percent tensor([0.5754, 0.4246], device='cuda:0')
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.6088, 0.3912], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9934, 0.0066], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (1198/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (2288/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (84.00%) (3362/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (4469/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4220) |  Loss2: (0.0000) | Acc: (85.00%) (5561/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (6663/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (7763/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (8852/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (9939/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4231) |  Loss2: (0.0000) | Acc: (85.00%) (11030/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (12116/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (13222/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4236) |  Loss2: (0.0000) | Acc: (85.00%) (14304/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (15402/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4218) |  Loss2: (0.0000) | Acc: (85.00%) (16503/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (17598/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (18672/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (85.00%) (19747/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4256) |  Loss2: (0.0000) | Acc: (85.00%) (20854/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (21950/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (23011/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (24105/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (25199/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (26294/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (27415/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (28535/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (29639/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (30726/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (31815/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (32910/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (34000/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4236) |  Loss2: (0.0000) | Acc: (85.00%) (35107/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (36205/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (37317/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (38402/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (39511/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (40602/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (41700/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (42740/50000)
# TEST : Loss: (0.5045) | Acc: (83.00%) (8302/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.4787, 0.5213], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.6077, 0.3923], device='cuda:0')
percent tensor([0.7002, 0.2998], device='cuda:0')
percent tensor([0.9943, 0.0057], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4056) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (2308/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4319) |  Loss2: (0.0000) | Acc: (85.00%) (3386/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (4499/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (5600/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (6710/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (7796/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (8901/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (10012/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (86.00%) (11120/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (12211/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (13300/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (14395/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (15491/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (16576/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (17679/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (18782/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (19888/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (20993/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (22081/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (23186/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (24291/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (25390/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (26493/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (27579/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (28682/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (29779/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (30887/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (31959/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (33055/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (34156/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (35254/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (36340/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (37442/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (38555/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (39658/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (40739/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (41829/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (42879/50000)
# TEST : Loss: (0.5002) | Acc: (83.00%) (8329/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.6077, 0.3923], device='cuda:0')
percent tensor([0.7054, 0.2946], device='cuda:0')
percent tensor([0.9951, 0.0049], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4349) |  Loss2: (0.0000) | Acc: (84.00%) (1192/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (86.00%) (2321/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (3428/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (86.00%) (4517/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (86.00%) (5617/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (86.00%) (6725/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (7805/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (8895/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (9985/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (11083/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (12187/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (13287/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (14410/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (15485/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (16597/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (17716/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (18817/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (19910/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (21000/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (22115/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (86.00%) (23240/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (24323/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (25404/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (26500/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (27581/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (28659/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (29745/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (30827/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (31932/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (33020/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (34116/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (35196/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (36299/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (37404/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (38498/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (39594/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (40703/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (41773/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (42837/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.4943) | Acc: (83.00%) (8346/10000)
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4802, 0.5198], device='cuda:0')
percent tensor([0.5804, 0.4196], device='cuda:0')
percent tensor([0.5657, 0.4343], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(175.7307, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(803.8704, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.6786, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.3600, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(512.0760, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2198.8411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4286.9521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1427.3369, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6103.7212, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12052.9170, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4016.8042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16981.1602, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (1219/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (2329/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (3433/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (4563/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (5659/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (6753/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (7865/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (8964/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (10076/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (11170/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (12252/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.3978) |  Loss2: (0.0000) | Acc: (86.00%) (13356/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (14484/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (15590/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (16695/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (17819/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.3941) |  Loss2: (0.0000) | Acc: (86.00%) (18933/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (20048/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (21154/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (22275/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.3941) |  Loss2: (0.0000) | Acc: (86.00%) (23368/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (24453/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (25561/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (26660/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (27779/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (28882/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (29960/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (31060/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (32167/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.3984) |  Loss2: (0.0000) | Acc: (86.00%) (33251/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (34386/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (35480/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (86.00%) (36593/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (86.00%) (37694/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (38781/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (39855/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (40948/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4002) |  Loss2: (0.0000) | Acc: (86.00%) (42047/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (43097/50000)
# TEST : Loss: (0.6194) | Acc: (80.00%) (8005/10000)
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4804, 0.5196], device='cuda:0')
percent tensor([0.5798, 0.4202], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.6074, 0.3926], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9957, 0.0043], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (2355/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (3470/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (87.00%) (5699/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (87.00%) (6810/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (87.00%) (7926/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (9030/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (87.00%) (10142/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (11252/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3760) |  Loss2: (0.0000) | Acc: (87.00%) (12384/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (13496/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3766) |  Loss2: (0.0000) | Acc: (87.00%) (14612/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (87.00%) (15716/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (16821/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (17932/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (19024/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (20141/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (21240/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3809) |  Loss2: (0.0000) | Acc: (86.00%) (22335/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (23460/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (24586/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (25699/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (26787/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (27871/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (28980/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (30089/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (31212/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (32321/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (33414/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (34501/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (35604/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (86.00%) (36694/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3869) |  Loss2: (0.0000) | Acc: (86.00%) (37803/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (38900/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (39984/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (41097/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (42216/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (43275/50000)
# TEST : Loss: (0.5894) | Acc: (81.00%) (8147/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4907, 0.5093], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.5809, 0.4191], device='cuda:0')
percent tensor([0.5670, 0.4330], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.7029, 0.2971], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (85.00%) (1204/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (2342/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (86.00%) (3452/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (4578/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (5691/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (6809/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (7929/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3731) |  Loss2: (0.0000) | Acc: (87.00%) (9045/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (10173/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (87.00%) (11265/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (86.00%) (12360/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (87.00%) (13493/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (87.00%) (14609/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3741) |  Loss2: (0.0000) | Acc: (87.00%) (15720/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3755) |  Loss2: (0.0000) | Acc: (87.00%) (16831/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (86.00%) (17925/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3762) |  Loss2: (0.0000) | Acc: (86.00%) (19036/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (87.00%) (20158/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (21268/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (22391/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (87.00%) (23508/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (24620/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (25737/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3754) |  Loss2: (0.0000) | Acc: (87.00%) (26844/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (87.00%) (27961/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (29060/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (30165/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (31270/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (86.00%) (32389/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (86.00%) (33505/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (34608/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (35743/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (86.00%) (36853/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (37952/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (39068/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (40188/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (41319/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (42426/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (43512/50000)
# TEST : Loss: (0.4622) | Acc: (84.00%) (8444/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.4809, 0.5191], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.6075, 0.3925], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9952, 0.0048], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3753) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (3483/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (4606/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (5717/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (6826/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (7949/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (9072/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (10193/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (11295/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3616) |  Loss2: (0.0000) | Acc: (87.00%) (12412/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (13545/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (14648/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (15762/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (16876/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (17991/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (19110/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (20223/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (21351/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (22481/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (23599/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (24726/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (25824/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (26963/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (28095/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (29209/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (30321/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (31439/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (32545/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (33674/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (34803/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (35935/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3653) |  Loss2: (0.0000) | Acc: (87.00%) (37044/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (38158/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (39280/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (40403/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (41539/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (42651/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (43732/50000)
# TEST : Loss: (0.5503) | Acc: (82.00%) (8236/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4908, 0.5092], device='cuda:0')
percent tensor([0.4803, 0.5197], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.7011, 0.2989], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (86.00%) (2312/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4226) |  Loss2: (0.0000) | Acc: (85.00%) (3391/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (4451/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (5504/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (6588/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (7648/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (8731/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4563) |  Loss2: (0.0000) | Acc: (84.00%) (9809/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (10895/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4503) |  Loss2: (0.0000) | Acc: (84.00%) (11988/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (13045/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (14129/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (15222/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (84.00%) (16323/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4470) |  Loss2: (0.0000) | Acc: (84.00%) (17416/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4463) |  Loss2: (0.0000) | Acc: (84.00%) (18508/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (19610/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (20688/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (21781/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (22895/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (23994/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (25086/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (26156/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (27244/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (28340/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (29435/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (30525/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (31608/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (84.00%) (32708/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (84.00%) (33797/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4346) |  Loss2: (0.0000) | Acc: (84.00%) (34900/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (84.00%) (35991/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (84.00%) (37099/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (85.00%) (38205/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4310) |  Loss2: (0.0000) | Acc: (85.00%) (39299/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (40380/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (41484/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (42569/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.5126) | Acc: (82.00%) (8291/10000)
percent tensor([0.5063, 0.4937], device='cuda:0')
percent tensor([0.4835, 0.5165], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5806, 0.4194], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (2295/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (3408/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (85.00%) (4512/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (5628/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (6731/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (7831/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (86.00%) (8937/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (10047/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (11150/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (12253/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4030) |  Loss2: (0.0000) | Acc: (86.00%) (13340/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (14452/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (15544/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (16642/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (17767/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (18865/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (19981/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (21084/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (22197/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (23311/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (86.00%) (24408/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (25512/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (26622/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (27726/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (28832/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (29927/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (31033/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (32141/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (33232/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (34348/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (35452/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (36566/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (37682/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3921) |  Loss2: (0.0000) | Acc: (86.00%) (38805/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (39902/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (41022/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (42144/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3903) |  Loss2: (0.0000) | Acc: (86.00%) (43212/50000)
# TEST : Loss: (0.4854) | Acc: (83.00%) (8370/10000)
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.4827, 0.5173], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.6215, 0.3785], device='cuda:0')
percent tensor([0.5904, 0.4096], device='cuda:0')
percent tensor([0.6476, 0.3524], device='cuda:0')
percent tensor([0.9956, 0.0044], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (2320/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (3435/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (4546/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (5650/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (6758/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (7878/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (9003/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (10117/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3735) |  Loss2: (0.0000) | Acc: (87.00%) (11253/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (12353/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (86.00%) (13473/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (14567/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (15668/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (16777/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3813) |  Loss2: (0.0000) | Acc: (86.00%) (17881/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (18988/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (20116/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (21238/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (86.00%) (22355/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (86.00%) (23472/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (86.00%) (24590/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (25700/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3768) |  Loss2: (0.0000) | Acc: (86.00%) (26827/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (27936/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (29056/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (30175/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (86.00%) (31292/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (32389/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (86.00%) (33499/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (34617/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (35746/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (86.00%) (36859/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (86.00%) (37968/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (39060/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (40168/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (86.00%) (41284/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (86.00%) (42416/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3756) |  Loss2: (0.0000) | Acc: (86.00%) (43496/50000)
# TEST : Loss: (0.4735) | Acc: (84.00%) (8401/10000)
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.4839, 0.5161], device='cuda:0')
percent tensor([0.4843, 0.5157], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.6174, 0.3826], device='cuda:0')
percent tensor([0.5992, 0.4008], device='cuda:0')
percent tensor([0.6587, 0.3413], device='cuda:0')
percent tensor([0.9961, 0.0039], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (4555/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (6788/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (7916/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (9025/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3738) |  Loss2: (0.0000) | Acc: (87.00%) (10144/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (11275/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (12397/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (13512/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (14650/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (15781/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (16884/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (18004/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (19127/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (20247/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (21374/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (22485/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (23605/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (24715/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (25834/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (26948/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (28059/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (29196/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (30327/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (31451/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (32570/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (33704/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (34814/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (35923/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (37047/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (38178/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (39287/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (40401/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (41534/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (42659/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (43742/50000)
# TEST : Loss: (0.4650) | Acc: (84.00%) (8442/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4856, 0.5144], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.5676, 0.4324], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6076, 0.3924], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4850) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (2359/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (88.00%) (3496/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (4612/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (5728/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (6857/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (88.00%) (7999/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (88.00%) (9139/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (88.00%) (10252/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (88.00%) (11386/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (88.00%) (12534/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (88.00%) (13661/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (88.00%) (14797/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (88.00%) (15896/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (88.00%) (17023/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (88.00%) (18143/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (88.00%) (19264/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (88.00%) (20397/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (21511/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (22629/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (23746/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (24877/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (26001/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (27130/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (28230/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (29324/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (30459/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (31590/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (32712/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (33845/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (34954/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (36075/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (37203/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (38342/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (39474/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (40586/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (41693/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (42800/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (43878/50000)
# TEST : Loss: (0.4585) | Acc: (84.00%) (8455/10000)
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.4887, 0.5113], device='cuda:0')
percent tensor([0.5710, 0.4290], device='cuda:0')
percent tensor([0.6146, 0.3854], device='cuda:0')
percent tensor([0.6122, 0.3878], device='cuda:0')
percent tensor([0.6634, 0.3366], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (2377/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (3504/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (4628/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (5702/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (6824/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (7949/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (9057/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (10211/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (11339/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (12458/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (13568/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (14676/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (15799/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (16926/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (18065/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (19191/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (20293/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (21406/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (22510/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (23623/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (24761/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (25901/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (27026/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (28153/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (29273/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (30387/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (31510/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (32619/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (33735/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (34854/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (35955/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (37072/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (38209/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (39337/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (40445/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (41572/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (42693/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (43774/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.5515) | Acc: (82.00%) (8251/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.4882, 0.5118], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.6137, 0.3863], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.9794, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.7946, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.6042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.4009, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(510.3359, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2207.1226, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4283.6123, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1422.1923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6112.9932, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12015.9512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4001.2571, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16911.5840, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (88.00%) (1243/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (2381/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (3493/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (88.00%) (4621/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (5765/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (6925/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (8059/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (9186/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (10302/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (11410/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (13664/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (14789/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (15915/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (17047/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (18171/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (19304/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (88.00%) (20420/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (21504/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (22631/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (87.00%) (23763/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (24899/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (26023/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (27152/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (28271/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (29408/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (30530/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (87.00%) (31649/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (32775/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (33910/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (35049/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (36186/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (37300/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (38440/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (39552/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (87.00%) (40662/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (41772/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (42900/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (87.00%) (43978/50000)
# TEST : Loss: (0.5025) | Acc: (83.00%) (8399/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6138, 0.3862], device='cuda:0')
percent tensor([0.6654, 0.3346], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (2416/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (3577/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (4698/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (5856/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (6984/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (8091/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (9208/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (10351/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (11461/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (12582/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (13715/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (14855/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (15982/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (17109/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (18237/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (19364/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (20488/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (21601/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (22707/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (23816/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (24941/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (27209/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (28343/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (29483/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (30613/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (31721/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (32840/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (33946/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (35073/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (36196/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (37327/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (38459/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (39592/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (40734/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (41885/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (43021/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (44106/50000)
# TEST : Loss: (0.4854) | Acc: (84.00%) (8451/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.4888, 0.5112], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.6145, 0.3855], device='cuda:0')
percent tensor([0.6145, 0.3855], device='cuda:0')
percent tensor([0.6621, 0.3379], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (1249/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (2381/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (4685/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (5826/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (89.00%) (6963/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (89.00%) (8092/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (9233/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (10368/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (89.00%) (11517/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (89.00%) (12660/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (89.00%) (13785/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (89.00%) (14924/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (89.00%) (16063/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (17178/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (18305/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (19440/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (20580/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (21724/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (22853/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (24001/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (25128/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (26264/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (27408/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (28539/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (29677/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (30833/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (31964/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (33099/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (34240/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (35372/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (36521/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (37657/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (38797/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (39931/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (41050/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (42165/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (43289/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (44372/50000)
# TEST : Loss: (0.4680) | Acc: (84.00%) (8479/10000)
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4883, 0.5117], device='cuda:0')
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.6643, 0.3357], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (90.00%) (2429/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (5870/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (7000/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (8106/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (89.00%) (9244/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (10396/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (11549/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (12706/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (13859/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (15000/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (16122/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (17270/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (18420/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (19568/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (20706/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (21841/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (22988/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (24129/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (25263/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (26402/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (27532/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (28637/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (89.00%) (29761/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (30908/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (32037/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (33178/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (89.00%) (34308/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (35437/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (36562/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (37678/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (38839/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (39970/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (41118/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (42261/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (43402/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (44478/50000)
# TEST : Loss: (0.4971) | Acc: (83.00%) (8359/10000)
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.6114, 0.3886], device='cuda:0')
percent tensor([0.6123, 0.3877], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (3444/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (4538/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (5632/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (6756/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (7859/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (8949/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (10030/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (85.00%) (11112/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (12225/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (85.00%) (13311/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (85.00%) (14417/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (85.00%) (15508/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (16627/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (17728/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (85.00%) (18798/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (85.00%) (19902/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (21026/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (22127/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (23231/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (24330/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (25446/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (26558/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (27672/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (28768/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3904) |  Loss2: (0.0000) | Acc: (86.00%) (29891/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (30979/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (32088/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (33183/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3903) |  Loss2: (0.0000) | Acc: (86.00%) (34275/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (35403/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (36489/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (37617/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (38747/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (39855/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (40995/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (42120/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (43191/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.4795) | Acc: (83.00%) (8392/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4961, 0.5039], device='cuda:0')
percent tensor([0.4918, 0.5082], device='cuda:0')
percent tensor([0.5751, 0.4249], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6116, 0.3884], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (86.00%) (2336/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (3454/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (5710/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (87.00%) (6831/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (7962/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (9079/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (10200/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (11311/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (12436/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (13566/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (14697/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (15828/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (16950/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (18061/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (19194/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (20293/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (21422/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (22538/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (23653/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (24768/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (25878/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (27006/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (28132/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29249/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (30381/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (31487/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (32605/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (33734/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (34849/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (35939/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (87.00%) (37078/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (38210/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (39364/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (40484/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (41620/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (42737/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (43815/50000)
# TEST : Loss: (0.4560) | Acc: (84.00%) (8477/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.5968, 0.4032], device='cuda:0')
percent tensor([0.6125, 0.3875], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (4631/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (5780/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (6924/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (8050/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (9185/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (10302/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (11429/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (13690/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (14815/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (15935/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (17073/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (18187/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (19316/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (20416/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (21536/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (22663/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (23805/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (24938/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (27221/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (28350/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (29490/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (30607/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (31730/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (32862/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (33983/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (35107/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (36254/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (37395/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (38521/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (39646/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (40770/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (41905/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (43039/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (44133/50000)
# TEST : Loss: (0.4404) | Acc: (85.00%) (8537/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.6157, 0.3843], device='cuda:0')
percent tensor([0.9972, 0.0028], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (3501/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (6895/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (8034/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (9146/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (10291/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (11434/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (12574/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (13707/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (14822/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (15947/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (17066/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (18194/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (19342/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (20483/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (21597/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (22717/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (23844/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (24972/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (26096/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (27234/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (28368/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (29508/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (30634/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (31763/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (32878/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (34016/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (35141/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (36285/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (37427/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (38560/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (39699/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (40866/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (42003/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (43145/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (44234/50000)
# TEST : Loss: (0.4340) | Acc: (85.00%) (8564/10000)
percent tensor([0.5158, 0.4842], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.6275, 0.3725], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (2387/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (89.00%) (3550/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (4681/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (5806/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (89.00%) (6957/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (89.00%) (8099/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (89.00%) (9243/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (10373/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (89.00%) (11525/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (89.00%) (12657/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (89.00%) (13790/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (89.00%) (14944/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (89.00%) (16074/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (89.00%) (17226/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (89.00%) (18384/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (89.00%) (19517/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (20668/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (89.00%) (21796/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (89.00%) (22921/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (89.00%) (24071/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (89.00%) (25212/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (89.00%) (26367/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (89.00%) (27493/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (28617/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (89.00%) (29738/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (89.00%) (30875/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (32009/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (89.00%) (33173/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (34301/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (89.00%) (35442/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (36585/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (89.00%) (37735/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (89.00%) (38877/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (89.00%) (40020/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (89.00%) (41163/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (42301/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (89.00%) (43430/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (89.00%) (44536/50000)
# TEST : Loss: (0.4274) | Acc: (85.00%) (8579/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5859, 0.4141], device='cuda:0')
percent tensor([0.6049, 0.3951], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (3534/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (4664/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (5823/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (6956/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (88.00%) (8087/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (9194/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (10330/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (11481/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (12620/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (13766/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (14926/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (16072/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (17221/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (18353/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (19503/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (20656/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (21804/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (22944/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (24093/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (25241/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (26383/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (27521/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (28656/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (29816/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (30941/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (32078/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (33219/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (34359/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (35497/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (36625/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (37766/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (38904/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (40047/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (41179/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (42334/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (43484/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (44576/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4441) | Acc: (85.00%) (8567/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.5867, 0.4133], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6125, 0.3875], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.8922, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.7232, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.1703, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.5442, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.6483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2215.1282, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.5200, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1417.1688, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6122.4697, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11978.6436, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3985.7937, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16842.7617, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (88.00%) (2389/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (3532/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (4685/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (5827/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (6998/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (8135/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (89.00%) (9277/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (10425/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (11585/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (12718/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (13861/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (14984/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (16124/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (17272/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (18412/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (19566/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (20702/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (21834/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (22964/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (24091/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (25243/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (26391/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (27529/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (28673/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (29812/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (30962/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (32105/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (33245/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (34380/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (35528/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (36675/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (37821/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (38957/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (40117/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (41257/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (42392/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (43545/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (44646/50000)
# TEST : Loss: (0.4756) | Acc: (84.00%) (8457/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6377, 0.3623], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (3547/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (4695/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (5847/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (6986/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (8143/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (9302/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (10438/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (11580/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (12730/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (13878/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (15041/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (16185/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (17340/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (18483/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (19633/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (20781/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (21929/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (23089/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (24260/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (25399/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (26553/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (27696/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (28836/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (29979/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (31104/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (32252/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (33409/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (34563/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (35717/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (36858/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (38016/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (39170/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (40315/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (41483/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (42635/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (43798/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (44910/50000)
# TEST : Loss: (0.4207) | Acc: (85.00%) (8576/10000)
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5855, 0.4145], device='cuda:0')
percent tensor([0.6044, 0.3956], device='cuda:0')
percent tensor([0.6092, 0.3908], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (2423/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (3562/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (4726/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (5877/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (7021/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (8159/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (9304/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (10468/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (89.00%) (11627/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (12798/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (13948/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (15088/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (16233/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (17376/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (18521/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (19682/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (20849/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (21990/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (23161/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (24300/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (25444/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (26601/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (27742/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (89.00%) (28898/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (30049/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (31186/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (32329/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (33454/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (34608/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (35768/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (36912/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (38065/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (39220/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (40369/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (41516/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (42645/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (43790/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (44907/50000)
# TEST : Loss: (0.5268) | Acc: (83.00%) (8337/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5848, 0.4152], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.6121, 0.3879], device='cuda:0')
percent tensor([0.6420, 0.3580], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (91.00%) (3619/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (7090/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (8231/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (9389/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (10546/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (11683/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (12860/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (14035/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (15194/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (16345/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (17483/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (18631/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (19781/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (20941/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (22087/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (23246/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (24411/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (25574/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (26728/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (27873/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2831) |  Loss2: (0.0000) | Acc: (90.00%) (29030/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (30189/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (31347/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (32488/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (33629/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (34778/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (35929/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (37084/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (38242/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (39409/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (40546/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (41711/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (42862/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (44023/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (45126/50000)
# TEST : Loss: (0.4170) | Acc: (86.00%) (8639/10000)
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.5864, 0.4136], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6105, 0.3895], device='cuda:0')
percent tensor([0.6424, 0.3576], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (2372/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (3498/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (87.00%) (4616/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (87.00%) (5735/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (87.00%) (6860/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (87.00%) (7986/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (87.00%) (9117/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (10255/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (87.00%) (11370/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (12511/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (13632/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (14774/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (87.00%) (15878/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (17014/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (87.00%) (18128/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (87.00%) (19257/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (20398/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (21541/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (22673/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (23810/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (24972/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3333) |  Loss2: (0.0000) | Acc: (88.00%) (26125/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (27268/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (28414/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (29562/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (30700/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (31826/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (32958/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (34077/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (35204/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (36358/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (37477/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (38614/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (39756/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (40899/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (42037/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (43178/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (44297/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4617) | Acc: (84.00%) (8476/10000)
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.6022, 0.3978], device='cuda:0')
percent tensor([0.5875, 0.4125], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6731, 0.3269], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (2410/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (3533/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (88.00%) (4665/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (5818/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (6959/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (8122/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (9275/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (10412/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (11583/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (12720/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (13871/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (15026/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (16182/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (17318/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (18453/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (19578/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (20718/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (21857/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (23011/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (24163/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (25313/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (26455/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (27605/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (28746/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (29894/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (31028/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (32188/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (33343/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (34502/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (35654/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (36801/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (37953/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (39116/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (40277/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (41422/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (42590/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (43749/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (44865/50000)
# TEST : Loss: (0.4411) | Acc: (85.00%) (8554/10000)
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5959, 0.4041], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (88.00%) (2386/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (4696/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (5838/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (6984/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (8141/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (9295/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (10448/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (11601/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (12750/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (13891/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (15043/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (16223/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (17375/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (18513/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (19675/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (20834/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (21987/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (23132/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (24297/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (25459/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (26617/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (27791/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (28953/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (30121/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (31281/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (32439/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (33607/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (34757/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (35906/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (90.00%) (37066/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (38234/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (39387/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (40553/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (41709/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (42868/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (44020/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (45139/50000)
# TEST : Loss: (0.4282) | Acc: (85.00%) (8597/10000)
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.5918, 0.4082], device='cuda:0')
percent tensor([0.6572, 0.3428], device='cuda:0')
percent tensor([0.6888, 0.3112], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (2428/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (3580/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (5930/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (7071/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (8228/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (9407/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (10566/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (11717/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (12872/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (14042/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (15204/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (16372/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (17535/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (18677/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (19826/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (20981/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (22137/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (24464/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (25624/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (26768/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (27939/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (29087/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (30241/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (31397/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (32533/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (33690/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (34826/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (35980/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (37150/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (38299/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (39435/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (40602/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (41769/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (42932/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (44084/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (45216/50000)
# TEST : Loss: (0.4196) | Acc: (86.00%) (8626/10000)
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5972, 0.4028], device='cuda:0')
percent tensor([0.6015, 0.3985], device='cuda:0')
percent tensor([0.6592, 0.3408], device='cuda:0')
percent tensor([0.6930, 0.3070], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (89.00%) (2417/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (4732/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (5886/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (7037/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (8198/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (90.00%) (9361/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (90.00%) (10520/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (11689/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (12850/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (13997/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (15162/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (16301/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (17440/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (18604/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (19779/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (20934/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (22077/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (23223/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (24381/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (25547/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (26706/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (27861/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (29032/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (30201/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (31355/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (32514/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (33664/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (34837/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (35991/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (37150/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (38306/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (39453/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (40621/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (41785/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (42944/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (44105/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (45237/50000)
# TEST : Loss: (0.4184) | Acc: (86.00%) (8622/10000)
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5934, 0.4066], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6494, 0.3506], device='cuda:0')
percent tensor([0.6950, 0.3050], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (3613/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (4749/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (5916/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (7070/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (8237/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (9399/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (10572/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (11735/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (12897/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (14050/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (15202/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (16365/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (17514/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (18693/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (19850/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (21010/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (22154/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (23312/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (24436/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (25582/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (26754/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (27916/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (29074/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (30230/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (31396/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (32570/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (33712/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (34886/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (36040/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (37192/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (38331/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (39484/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (40637/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (41781/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (42912/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (44046/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (45152/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4622) | Acc: (85.00%) (8533/10000)
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.5949, 0.4051], device='cuda:0')
percent tensor([0.6471, 0.3529], device='cuda:0')
percent tensor([0.6862, 0.3138], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.7941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.7054, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.0360, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1514.1942, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(507.0573, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2223.0957, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4278.9409, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1412.0759, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6134.6948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11941.9648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3970.3564, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16774.4355, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (2438/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (3591/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (4775/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (5936/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (7124/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (8288/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (9436/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (90.00%) (10598/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (11745/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (12889/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (14052/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (15216/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (16382/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (17549/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (18704/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (19870/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (21027/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (22194/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (23360/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (24534/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (26851/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (27997/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (29158/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (30307/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (31467/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (32617/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (33783/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (34935/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (36071/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (37230/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (38371/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (39525/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (40681/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (41817/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (42973/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (44121/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (45228/50000)
# TEST : Loss: (0.5378) | Acc: (83.00%) (8391/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5938, 0.4062], device='cuda:0')
percent tensor([0.5987, 0.4013], device='cuda:0')
percent tensor([0.6492, 0.3508], device='cuda:0')
percent tensor([0.6963, 0.3037], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (3587/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (4762/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (5911/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (7071/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (8237/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (9375/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (10537/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (11685/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (12861/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (14020/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (15189/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (16358/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17535/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (18693/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (19862/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (21037/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (22210/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (23362/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (24529/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (25702/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (26856/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (28020/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (29178/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (30339/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (31511/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (32672/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (33845/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (35001/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (36162/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37315/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (38451/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (39614/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (40782/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (41970/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (43143/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (44299/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (45421/50000)
# TEST : Loss: (0.4606) | Acc: (85.00%) (8535/10000)
percent tensor([0.5300, 0.4700], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.5983, 0.4017], device='cuda:0')
percent tensor([0.6497, 0.3503], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (91.00%) (3615/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (4771/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (7099/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (8261/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (9439/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (90.00%) (10598/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (11763/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (12934/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (14109/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (15263/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (16428/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (17589/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (18742/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (19886/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (21054/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (90.00%) (22221/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (23389/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (24569/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (25729/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (26886/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (90.00%) (28070/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (29245/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (30408/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (31585/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (32746/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (33896/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (90.00%) (35055/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (36203/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (90.00%) (37355/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (38511/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (39678/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (40827/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (41981/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (43130/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (44277/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (45388/50000)
# TEST : Loss: (0.4373) | Acc: (86.00%) (8637/10000)
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.6481, 0.3519], device='cuda:0')
percent tensor([0.6906, 0.3094], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (4808/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (5976/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (7144/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (8312/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (9494/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (10665/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (12981/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (14147/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (15312/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (16495/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (17670/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (18811/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (19976/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (21127/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (22291/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (23474/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (24655/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (25805/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (26987/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (28162/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (29338/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (30504/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (31663/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (32823/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (33985/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (35166/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (36330/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (37502/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (38674/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (39842/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (41012/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (42167/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (43338/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (44528/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (45648/50000)
# TEST : Loss: (0.4373) | Acc: (86.00%) (8622/10000)
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.6498, 0.3502], device='cuda:0')
percent tensor([0.6905, 0.3095], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (3598/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (4755/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (5906/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (7081/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (8232/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (9393/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (10567/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (11714/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (12882/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (14035/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (15208/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (16351/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (17498/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (18638/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (19783/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (20955/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (22135/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (23282/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (24463/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (25641/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (26791/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (27948/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (29107/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (30270/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (31445/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (32621/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (33782/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (34953/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (36121/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (37291/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (38448/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (39620/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (40789/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (41976/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (43156/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (44316/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (45443/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4279) | Acc: (86.00%) (8608/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6161, 0.3839], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.6543, 0.3457], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (3607/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (4773/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (7102/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (8257/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (9439/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (10623/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (11774/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (12942/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (14119/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (15286/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (16453/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (17598/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (18782/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (19951/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (21112/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (22269/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (23420/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (24585/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (25758/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (26930/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (28097/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (29257/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (30423/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (31599/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (32762/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (33937/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (35113/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (36279/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (37471/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (38631/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (39790/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (40970/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (42141/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (43314/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (44490/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (45630/50000)
# TEST : Loss: (0.4159) | Acc: (86.00%) (8640/10000)
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.6026, 0.3974], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.6461, 0.3539], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (3638/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (4819/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (5999/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (7185/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (8356/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (9532/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (10695/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (13040/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (14219/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (15405/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (16591/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (17756/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (18924/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (20106/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (21269/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (22447/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (23605/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (24779/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (25959/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (27135/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (28331/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (29504/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (30687/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (31848/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (33020/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (34206/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (35392/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (36555/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (37734/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (38910/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (40089/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (41276/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (42467/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (43648/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (44807/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (45936/50000)
# TEST : Loss: (0.4064) | Acc: (86.00%) (8692/10000)
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.4928, 0.5072], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.6157, 0.3843], device='cuda:0')
percent tensor([0.6485, 0.3515], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (3623/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (4810/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (6002/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (7176/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (8353/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (9529/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (10702/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (11884/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (13049/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (14226/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (16577/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (17761/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (18948/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (20127/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (21304/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (22483/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (23653/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (24811/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (25995/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (27178/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (28364/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (29538/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (30707/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (31891/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (33054/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (34249/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (35420/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (36597/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (37768/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (38936/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (40112/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (41303/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (42483/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (43656/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (44828/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (45963/50000)
# TEST : Loss: (0.3988) | Acc: (87.00%) (8706/10000)
percent tensor([0.5317, 0.4683], device='cuda:0')
percent tensor([0.4924, 0.5076], device='cuda:0')
percent tensor([0.5064, 0.4936], device='cuda:0')
percent tensor([0.6047, 0.3953], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6535, 0.3465], device='cuda:0')
percent tensor([0.6626, 0.3374], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (3663/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (4839/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (6028/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (7210/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (8394/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (9564/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (10760/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (11958/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (13127/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (14307/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (15450/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (16615/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (17795/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (18956/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (20134/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (21300/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (22467/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (23623/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (24803/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (25972/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (27164/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (28336/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (29513/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (30676/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (31870/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (33065/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (34239/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (35425/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (36601/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (37766/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (38944/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (40118/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (41303/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (42476/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (43682/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (44842/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (45969/50000)
# TEST : Loss: (0.3974) | Acc: (87.00%) (8729/10000)
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6165, 0.3835], device='cuda:0')
percent tensor([0.6553, 0.3447], device='cuda:0')
percent tensor([0.6637, 0.3363], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (3639/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (6027/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (92.00%) (7190/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (92.00%) (8375/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (92.00%) (9547/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (92.00%) (10733/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (92.00%) (11896/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (13075/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (92.00%) (14262/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (92.00%) (15432/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (92.00%) (16624/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (92.00%) (17788/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (18948/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (20104/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (21275/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (22456/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (23614/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (24786/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (25954/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (27108/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (28269/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (29450/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (30618/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (31773/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (32949/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (34120/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (35295/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (36458/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (37629/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (38781/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (39944/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (41114/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (42292/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (43451/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (44614/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (45723/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4266) | Acc: (86.00%) (8639/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6173, 0.3827], device='cuda:0')
percent tensor([0.6528, 0.3472], device='cuda:0')
percent tensor([0.6655, 0.3345], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.5372, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.2371, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.1080, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1512.6659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(505.3882, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2230.3831, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.8203, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1406.9672, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6148.8403, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11906.6514, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3954.8784, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16707.0371, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (2487/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (3670/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (4845/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (6030/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (8380/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (9545/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (10712/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (91.00%) (11889/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (13064/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (14224/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (15379/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (16557/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (17739/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (18932/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (20109/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (21296/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (22472/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (23645/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (24821/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (26001/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (27167/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (28329/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (29500/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (30675/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (31843/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (33033/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (34212/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (35399/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (36565/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (37721/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (38902/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (40062/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (41225/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (42390/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (43548/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (44727/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (45841/50000)
# TEST : Loss: (0.4379) | Acc: (86.00%) (8618/10000)
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.6054, 0.3946], device='cuda:0')
percent tensor([0.6159, 0.3841], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6615, 0.3385], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (3641/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (5980/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (7164/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (8337/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (91.00%) (9527/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (91.00%) (10715/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (11910/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (13101/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (15468/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (16658/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (17816/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (18999/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (20179/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (21362/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (22534/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (23698/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (24884/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (26074/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27268/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (28421/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (29602/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (30775/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (31938/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (33129/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (34297/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (35482/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (36662/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (37852/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (39022/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (40196/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (41351/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (42519/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (43679/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (44843/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (45960/50000)
# TEST : Loss: (0.4432) | Acc: (86.00%) (8628/10000)
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.6494, 0.3506], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (92.00%) (2476/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (8378/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (9558/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (10728/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (11902/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (13080/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (14246/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (16610/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (17788/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (18985/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (20151/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (21335/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (22527/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (23697/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (24883/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (26079/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27256/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (28442/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (29601/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (30777/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (31946/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (33130/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (34310/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (35478/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (36649/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (37824/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (39013/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (40185/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (41366/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (42548/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (43732/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (44916/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (46053/50000)
# TEST : Loss: (0.4718) | Acc: (85.00%) (8534/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6179, 0.3821], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6648, 0.3352], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (4826/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (6004/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (92.00%) (8365/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (9565/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (10742/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (11936/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (13112/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (15458/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (16639/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (17839/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (19015/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (20210/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (21396/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (22578/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (23762/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (24938/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (26129/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (27322/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (28495/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (29681/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (30853/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (32038/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (33225/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (34402/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (35586/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (36769/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (37941/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (39121/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (40313/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (41484/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (42682/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (43844/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (45014/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (46146/50000)
# TEST : Loss: (0.4376) | Acc: (86.00%) (8675/10000)
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.6167, 0.3833], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (3584/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (5840/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (6953/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (88.00%) (8079/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (88.00%) (9204/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (88.00%) (10342/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (88.00%) (11477/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (88.00%) (12608/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (88.00%) (13748/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (88.00%) (14889/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (88.00%) (16030/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (88.00%) (17171/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (88.00%) (18320/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (88.00%) (19455/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (88.00%) (20585/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (88.00%) (21746/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (88.00%) (22877/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (88.00%) (24028/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (25192/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (26333/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (27474/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (28602/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (29759/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (30916/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (32061/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (33221/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (34370/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (35519/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (36672/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (37842/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (38987/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (40125/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (41278/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (42428/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (43575/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (44689/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4799) | Acc: (85.00%) (8543/10000)
percent tensor([0.5352, 0.4648], device='cuda:0')
percent tensor([0.4954, 0.5046], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6269, 0.3731], device='cuda:0')
percent tensor([0.5955, 0.4045], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (2397/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (4738/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (5900/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (7047/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (8188/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (9357/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (10515/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (11687/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (12855/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (14017/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (15158/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (16316/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (17483/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (18644/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (19785/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (20969/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (22143/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (24451/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (25631/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (26801/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (27959/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (29118/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (30261/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (31420/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (32596/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (33766/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (34939/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (36121/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (37289/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (38433/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (39590/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (40755/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (41910/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (43086/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (44263/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (45394/50000)
# TEST : Loss: (0.4537) | Acc: (86.00%) (8605/10000)
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.6097, 0.3903], device='cuda:0')
percent tensor([0.6298, 0.3702], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.6768, 0.3232], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (4782/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (7146/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (8321/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (9474/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (10636/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (11810/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (12960/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (14132/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (15306/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (16480/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (17655/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (18815/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (19968/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (21137/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (22323/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (23497/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (24667/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (25832/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (27011/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (28190/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (29379/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (30557/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (31723/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (32888/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (34055/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (35237/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (36419/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (37582/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (38755/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (39932/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41113/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (42293/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (43476/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (44641/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (45762/50000)
# TEST : Loss: (0.4382) | Acc: (86.00%) (8662/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6237, 0.3763], device='cuda:0')
percent tensor([0.6029, 0.3971], device='cuda:0')
percent tensor([0.6849, 0.3151], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (4797/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (5968/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (7138/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (8308/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (9486/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (10675/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (11848/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (13032/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (14223/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (15393/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (16576/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (17749/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (18915/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (20087/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (21254/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (22431/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (23585/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (24753/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (25921/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (27105/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (28280/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (29460/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (30635/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (31808/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (32987/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (34154/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (35327/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (36508/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (37702/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (38874/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (40052/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (41237/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (42410/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (43581/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (44755/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (45885/50000)
# TEST : Loss: (0.4252) | Acc: (86.00%) (8675/10000)
percent tensor([0.5321, 0.4679], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6869, 0.3131], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (4848/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (92.00%) (6007/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (8371/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (9549/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (10734/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (11918/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (13106/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (14270/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (15451/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (16640/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (17815/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (18975/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (20146/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (21341/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (22519/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (23705/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (24887/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (26054/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (27226/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (28412/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (29593/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (30757/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (31933/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (33108/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (34269/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (35459/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (36622/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (37801/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (38973/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (40152/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (41340/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (42512/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (43683/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (44860/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (45973/50000)
# TEST : Loss: (0.4196) | Acc: (86.00%) (8684/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6239, 0.3761], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.6864, 0.3136], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (6045/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (7224/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (8410/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (9579/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (10762/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (11953/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13118/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (14322/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (15509/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (16676/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (17860/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (19028/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (20218/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (21407/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (22582/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (23770/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (24955/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (26134/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (27319/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (28506/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (29693/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (30874/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (32040/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (33206/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (34384/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (35552/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (36727/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (37900/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (39096/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (40290/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (41452/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (42624/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (43797/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (44951/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (46067/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.4634) | Acc: (86.00%) (8606/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6866, 0.3134], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.2119, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.5329, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.5601, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.0315, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.7317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2237.6687, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4274.8271, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1401.9159, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6161.6938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11871.4756, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3939.6179, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16639.9512, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (3664/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (4854/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (6050/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (7230/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (8430/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (9628/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (10809/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (11993/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (13193/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (14389/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (15576/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (16774/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (17952/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (19131/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (20314/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (21499/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (22679/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (23868/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (25053/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (26233/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (27415/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (28596/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (29766/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (30958/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (32126/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (33326/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (34512/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (35700/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (36885/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (38051/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (39232/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (40403/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (41576/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (42773/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (43968/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (45137/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (46263/50000)
# TEST : Loss: (0.5036) | Acc: (84.00%) (8498/10000)
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.6209, 0.3791], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6847, 0.3153], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (93.00%) (3693/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (93.00%) (6077/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (7276/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (8461/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (93.00%) (9648/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (10841/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (12014/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (13213/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (14402/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (15585/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (16768/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (17953/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (19145/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (20328/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (21515/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (22712/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (23903/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (25083/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (26269/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (27455/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (28631/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (29810/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (31003/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (32184/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (33360/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (34543/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (35746/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (36910/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (38087/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (39264/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (40448/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (41632/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (42822/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (43999/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (45178/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (46322/50000)
# TEST : Loss: (0.4139) | Acc: (87.00%) (8735/10000)
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6127, 0.3873], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6852, 0.3148], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (6074/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (7270/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (8466/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (9671/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (10862/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (12060/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (13258/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (14440/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (15624/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (16805/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (18000/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (19187/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (20381/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (21567/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (22752/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23948/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (93.00%) (25124/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (26305/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (27494/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (28699/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (29883/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (31060/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (32237/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (33426/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (92.00%) (34624/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (35826/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (37015/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (38202/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (39385/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (40579/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (41756/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (42953/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (44131/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (45296/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (46439/50000)
# TEST : Loss: (0.4728) | Acc: (85.00%) (8529/10000)
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6187, 0.3813], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (2485/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (3662/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (6028/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (7200/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (8378/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (9567/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (10762/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (11939/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (13132/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (14326/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (15525/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (16706/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (17895/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (19098/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (20284/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (21475/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (22678/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (23874/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25071/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (26269/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (27469/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (28668/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (29842/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (31051/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (32236/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (33438/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (34630/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (35815/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (37027/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (38207/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (39397/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (40591/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (41769/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (42958/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (44137/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (45321/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (46451/50000)
# TEST : Loss: (0.4587) | Acc: (86.00%) (8637/10000)
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.6131, 0.3869], device='cuda:0')
percent tensor([0.6241, 0.3759], device='cuda:0')
percent tensor([0.6178, 0.3822], device='cuda:0')
percent tensor([0.6878, 0.3122], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (5883/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (7033/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (8194/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (9347/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (10495/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (11653/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (12810/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (13974/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (15127/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (16277/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (17427/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (18562/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (19703/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (20875/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (22042/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (23183/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (24332/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (25507/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (26671/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (27828/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (28997/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (30153/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (31333/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (32507/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (33686/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (34842/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (36000/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (37160/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (38321/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (39478/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (40625/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (41800/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (42959/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (44124/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (45254/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4563) | Acc: (86.00%) (8621/10000)
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5157, 0.4843], device='cuda:0')
percent tensor([0.6168, 0.3832], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.6072, 0.3928], device='cuda:0')
percent tensor([0.6987, 0.3013], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (3620/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (4787/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (5967/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (7133/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (8297/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (9478/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (10653/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (13008/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (14189/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (15374/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (16539/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (17731/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (18921/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (20102/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (21290/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (22492/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (23678/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (24863/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (26044/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (27215/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (28385/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (29578/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (30763/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (31930/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (33103/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (34282/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (35458/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (36636/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (37838/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (39024/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (40213/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (41380/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (42544/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (43720/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (44891/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (46023/50000)
# TEST : Loss: (0.4347) | Acc: (86.00%) (8670/10000)
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.4992, 0.5008], device='cuda:0')
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.6153, 0.3847], device='cuda:0')
percent tensor([0.6201, 0.3799], device='cuda:0')
percent tensor([0.6130, 0.3870], device='cuda:0')
percent tensor([0.7074, 0.2926], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (3669/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (6014/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (91.00%) (7179/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (8366/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (9560/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (10749/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (11942/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (14315/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (15503/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (16681/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (17859/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (19042/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (20228/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (21407/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (22599/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (23779/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (24967/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (26170/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (27357/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (28563/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (29753/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (30952/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (32135/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (33318/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (34498/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (35683/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (36869/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (38046/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (39235/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (40415/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (41599/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (42786/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (43986/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (45163/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (46295/50000)
# TEST : Loss: (0.4230) | Acc: (86.00%) (8687/10000)
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5206, 0.4794], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.6197, 0.3803], device='cuda:0')
percent tensor([0.6191, 0.3809], device='cuda:0')
percent tensor([0.7183, 0.2817], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (3678/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (6051/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (7242/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (9629/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (10812/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (12013/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (13202/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (14393/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (15574/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (16762/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (17950/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (19135/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (20319/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (21504/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (22706/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (23888/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25077/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (26248/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (27427/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (28617/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (29804/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (30991/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (32182/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (33370/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (34563/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (35747/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (36928/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (38109/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (39312/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (40498/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (41696/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (42895/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (44083/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (45253/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (46398/50000)
# TEST : Loss: (0.4182) | Acc: (87.00%) (8705/10000)
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.6149, 0.3851], device='cuda:0')
percent tensor([0.6191, 0.3809], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.7177, 0.2823], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (4873/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (92.00%) (6064/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (7254/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (8458/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (9627/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (92.00%) (10829/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (12031/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (13207/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (14399/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (92.00%) (15592/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (16780/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (17963/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (19152/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (20349/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (21553/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (22744/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23930/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (25103/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (26292/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (27480/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (28669/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (29859/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (31050/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (32246/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (33437/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (34636/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (35829/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (37022/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (38206/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (39397/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (40606/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (41787/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (42993/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (44172/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (93.00%) (45356/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (46505/50000)
# TEST : Loss: (0.4104) | Acc: (87.00%) (8722/10000)
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.6200, 0.3800], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.7240, 0.2760], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (3677/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (4874/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (8459/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (9655/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (10854/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (12035/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (13239/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (15606/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (16817/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (17996/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (19190/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (20396/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (21589/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (22785/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (23980/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (25164/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (26367/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (27554/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (28759/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (29937/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (31133/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (32317/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (33514/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (34692/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (35873/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (37073/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (38280/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (39488/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (40657/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (41847/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (43040/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (44214/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (45409/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (46569/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4707) | Acc: (85.00%) (8571/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.6203, 0.3797], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.7196, 0.2804], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.8369, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.6205, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(812.8804, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.0594, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.1016, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.9351, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.7163, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1396.8903, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6175.6221, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11837.7373, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3924.3916, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16573.3262, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (8515/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (9719/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (10916/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (12101/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (13289/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (14492/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (15678/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (16879/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (18083/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (19286/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (20484/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (21671/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (22873/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (24069/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (25263/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (26453/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (27631/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (28824/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (30014/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (31210/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (32405/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (33594/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (34802/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (35991/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (37170/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (38377/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (39578/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (40774/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (41957/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (43142/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (44347/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (45537/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (46676/50000)
# TEST : Loss: (0.5432) | Acc: (84.00%) (8433/10000)
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6200, 0.3800], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.7286, 0.2714], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (2517/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (4900/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (6100/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (7292/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (8475/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (9692/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (10896/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (12095/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (13286/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (14488/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (15683/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (16879/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (18073/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (19285/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (20476/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (21666/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (22858/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (24044/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (25227/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (26401/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (27592/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (28771/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (29947/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (31134/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (32333/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (33531/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (34716/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (35920/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (37115/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (38280/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (39478/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (40667/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (41865/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (43056/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (44248/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (45438/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (46576/50000)
# TEST : Loss: (0.4023) | Acc: (87.00%) (8729/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.6210, 0.3790], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.7190, 0.2810], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (95.00%) (4991/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (6187/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (9804/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (13420/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (15833/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (17019/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (18216/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (19425/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (20611/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (21797/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (22980/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (94.00%) (24192/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (94.00%) (25392/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (94.00%) (26602/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (27793/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28980/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (30184/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (31377/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (32573/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (33757/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (34958/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (36137/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (37312/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (38508/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (39702/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (40882/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (42060/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (43242/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (44456/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (45659/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (46809/50000)
# TEST : Loss: (0.4462) | Acc: (86.00%) (8637/10000)
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5247, 0.4753], device='cuda:0')
percent tensor([0.6209, 0.3791], device='cuda:0')
percent tensor([0.6316, 0.3684], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.7292, 0.2708], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (3726/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (4940/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (6150/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (7360/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (8582/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (9785/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (10977/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (12174/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (14580/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (15796/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (16988/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (18195/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (19393/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (20605/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (21808/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (23006/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (24208/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (25413/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (26604/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (27809/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (29020/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (30215/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (31380/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (32569/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (33784/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (34973/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (36177/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (37357/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (38558/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (39759/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (40950/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (42141/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (43338/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (44547/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (45748/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (46899/50000)
# TEST : Loss: (0.4194) | Acc: (87.00%) (8734/10000)
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6205, 0.3795], device='cuda:0')
percent tensor([0.6268, 0.3732], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.7239, 0.2761], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (4887/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (6068/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (7251/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (8429/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (10768/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (11963/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13132/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (14330/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (15526/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (16693/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (17876/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (19053/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (20227/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (21408/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (22574/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (23768/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (24941/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (26128/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (27298/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (28479/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (29678/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (30862/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (32050/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (33248/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (34434/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (35631/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (36831/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (38016/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (39206/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (40400/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (41608/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (42788/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (43989/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (45191/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (46346/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4259) | Acc: (86.00%) (8697/10000)
percent tensor([0.5275, 0.4725], device='cuda:0')
percent tensor([0.4979, 0.5021], device='cuda:0')
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.5946, 0.4054], device='cuda:0')
percent tensor([0.6150, 0.3850], device='cuda:0')
percent tensor([0.6196, 0.3804], device='cuda:0')
percent tensor([0.7350, 0.2650], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (2508/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (6053/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (92.00%) (7248/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (8435/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (9639/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (10833/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (12028/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (13222/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (15623/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (16821/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (18008/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (19205/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (20411/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (21616/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (22808/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (23991/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (25176/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (26381/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (27558/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (28745/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (29947/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (31149/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (32355/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (33544/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (34749/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (35954/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (37154/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (38347/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (39546/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (40749/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (41955/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (43153/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (44356/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (45555/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (46709/50000)
# TEST : Loss: (0.4156) | Acc: (87.00%) (8734/10000)
percent tensor([0.5270, 0.4730], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.5981, 0.4019], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.7423, 0.2577], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (3721/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (6124/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (7315/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (8499/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (9704/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (10907/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (12112/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (13310/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (14506/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (15706/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (16901/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (18115/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (19315/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (20517/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (21723/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (22932/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (24116/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (25340/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (26536/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (27732/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (28941/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (30133/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (31326/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (32537/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (33737/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (34936/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (36140/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (37350/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (38569/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (39778/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (40973/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (42166/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (43364/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (44571/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (45766/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (46927/50000)
# TEST : Loss: (0.4039) | Acc: (87.00%) (8762/10000)
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.6210, 0.3790], device='cuda:0')
percent tensor([0.6316, 0.3684], device='cuda:0')
percent tensor([0.7404, 0.2596], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (4912/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (6116/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (7309/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (9721/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (12136/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (13337/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (14549/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (15754/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (16964/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (18156/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (19356/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (20568/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (21783/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (22969/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (24177/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (25381/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (26563/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (27743/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (28939/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (30131/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (31333/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (32540/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (33738/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (34938/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (36130/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (37331/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (38540/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (39752/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (40951/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (42150/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (43335/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (44534/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (45740/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (46905/50000)
# TEST : Loss: (0.3996) | Acc: (87.00%) (8778/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.6223, 0.3777], device='cuda:0')
percent tensor([0.6334, 0.3666], device='cuda:0')
percent tensor([0.7411, 0.2589], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (7385/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (8592/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (9802/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (11013/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (12215/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (13428/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (14630/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (15819/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (17013/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (18219/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (19436/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (20653/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (21862/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (23065/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (24261/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (25471/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (26678/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (27893/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (29095/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (30311/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (31520/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (32728/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (33939/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (35135/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (36343/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (37549/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (38752/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (39961/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (41173/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (42382/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (43582/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (44781/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (45989/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (47149/50000)
# TEST : Loss: (0.3943) | Acc: (87.00%) (8788/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.6232, 0.3768], device='cuda:0')
percent tensor([0.6379, 0.3621], device='cuda:0')
percent tensor([0.7387, 0.2613], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (4944/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (6143/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (7324/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (8526/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (9735/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (10939/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (12133/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (13331/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (14535/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (15747/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (16948/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (18148/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (19340/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (20546/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (21745/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (22961/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (24158/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (25371/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (26567/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (27755/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (28972/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (30161/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (31360/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (32553/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (33759/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (34949/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (36131/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (37316/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (38531/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (39729/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (40933/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (42132/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (43317/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (44507/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (45689/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (46837/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4833) | Acc: (85.00%) (8528/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6385, 0.3615], device='cuda:0')
percent tensor([0.7443, 0.2557], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.4363, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.6320, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.9678, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.9069, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.4231, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2251.5298, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4271.4790, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1391.7866, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6190.3657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11804.3945, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3909.2258, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16507.0039, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (3738/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (4927/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (7341/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (8558/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (9748/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (10951/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (12178/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (13389/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (14576/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (15765/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (16967/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (18164/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (19370/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (20586/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (21794/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (23003/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (24177/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (25385/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (26594/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (27788/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (28996/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (30209/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (31411/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (32595/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (33782/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (34973/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (36183/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (37385/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (38580/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (39769/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (40957/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (42158/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (43361/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (44564/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (45771/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (46922/50000)
# TEST : Loss: (0.4709) | Acc: (85.00%) (8583/10000)
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.6226, 0.3774], device='cuda:0')
percent tensor([0.6400, 0.3600], device='cuda:0')
percent tensor([0.7353, 0.2647], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (4974/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (6181/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (8593/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (9807/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (11009/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (12219/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (13420/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (14624/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (15838/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (17042/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (18262/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (19449/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (20641/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (21858/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (23053/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (24273/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (25460/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (26652/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (27852/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (29063/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (30272/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (31479/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (32673/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (33872/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (35069/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (36260/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (37462/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (38661/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (39869/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (41062/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (42254/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (43461/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (44653/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (45846/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (47007/50000)
# TEST : Loss: (0.4134) | Acc: (87.00%) (8727/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.6062, 0.3938], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.6386, 0.3614], device='cuda:0')
percent tensor([0.7388, 0.2612], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (6199/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (7397/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (8594/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (9789/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (10994/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (12197/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (13414/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (14621/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (15812/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (17034/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (18243/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (19459/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (20666/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (21864/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (23075/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (24274/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (25470/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (26679/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (27895/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (29088/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (30286/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (31490/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (32702/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (33904/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (35102/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (36303/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (37516/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (38713/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (39922/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (41122/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (42324/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (43533/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (44748/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (45938/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (47102/50000)
# TEST : Loss: (0.4207) | Acc: (87.00%) (8765/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.6070, 0.3930], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.7427, 0.2573], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (3758/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (4966/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (6175/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7389/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (8604/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (11025/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (12240/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (13447/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (14671/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (15891/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (17101/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (18307/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (19518/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (20725/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (21925/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (23125/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (24326/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (25537/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (26734/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (27932/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (29154/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (30353/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (31547/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (32750/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (33961/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (35167/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (36371/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (37562/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (38766/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (39973/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (41180/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (42394/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (43590/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (44803/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (46007/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (47157/50000)
# TEST : Loss: (0.4403) | Acc: (87.00%) (8702/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6232, 0.3768], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.7384, 0.2616], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (7320/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (9725/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (10935/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (12128/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (13327/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (14520/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (15711/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (16915/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (18115/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (19317/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (20518/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (21714/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (22914/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (24118/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (25305/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (26513/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (27708/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (28902/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (30112/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (31293/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (32483/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (33680/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (34870/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (36069/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (37263/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (38462/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (39681/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (40880/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (42098/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (43296/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (44494/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45694/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (46865/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4090) | Acc: (87.00%) (8797/10000)
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.6242, 0.3758], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (2540/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (3752/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (4957/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (6160/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (7365/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (8580/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (9772/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (10971/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (12188/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (13394/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (14606/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (15817/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (17034/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (18248/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (19466/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (20669/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (21869/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (23073/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (24281/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (25478/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (26689/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (27882/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (29086/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (30306/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (31504/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (32713/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (33932/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (35144/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (36342/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (37553/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (38757/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (39970/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (41183/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (42401/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (43607/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (44819/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (46024/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (47191/50000)
# TEST : Loss: (0.3933) | Acc: (88.00%) (8842/10000)
percent tensor([0.5252, 0.4748], device='cuda:0')
percent tensor([0.5024, 0.4976], device='cuda:0')
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.6123, 0.3877], device='cuda:0')
percent tensor([0.6226, 0.3774], device='cuda:0')
percent tensor([0.6383, 0.3617], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (3745/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (7376/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (8590/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (12221/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (13429/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (14640/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (15869/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (17075/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (18275/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (19474/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (20673/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (21884/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (23106/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (24322/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (25546/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (26761/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (27973/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (29192/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (30400/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (31612/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (32829/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (34054/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (35255/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (36467/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (37664/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (38883/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (40079/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (41286/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (42506/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (43729/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (44936/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (46142/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (47322/50000)
# TEST : Loss: (0.3863) | Acc: (88.00%) (8846/10000)
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5031, 0.4969], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.6126, 0.3874], device='cuda:0')
percent tensor([0.6216, 0.3784], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (3759/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (4965/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (6181/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (8605/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (9812/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (12238/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (13448/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (14659/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (15869/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (18306/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (19521/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (20741/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (21951/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (23162/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (24371/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (25581/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (26799/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (28018/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (29238/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (30445/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (31658/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (32870/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (34082/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (35304/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (36511/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (37708/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (38914/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (40115/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (41318/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (42527/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (43744/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (44954/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (46166/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (47325/50000)
# TEST : Loss: (0.3836) | Acc: (88.00%) (8854/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5236, 0.4764], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.6934, 0.3066], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (3747/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (6161/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (11032/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (12246/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (13465/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (15905/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (17114/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (18335/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (19544/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (20753/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (21976/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (23187/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (24392/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (25597/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (26823/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (28039/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (29263/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (30478/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (31703/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (32901/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (34110/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (35318/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (36526/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (37735/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (38948/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (40172/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (41406/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (42614/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (43827/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (45050/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (46265/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (47428/50000)
# TEST : Loss: (0.3832) | Acc: (88.00%) (8859/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5230, 0.4770], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6372, 0.3628], device='cuda:0')
percent tensor([0.7023, 0.2977], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (3743/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (4946/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (7373/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (9795/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (11011/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (12225/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (15872/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (17070/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (18265/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (19476/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (20689/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (21899/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (23114/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (24331/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (25525/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (26739/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (27951/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (29160/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (30362/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (31569/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (32774/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (33977/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (35181/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (36386/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (37589/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (38786/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (39983/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (41188/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (42392/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (43602/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (44828/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (46043/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (47216/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4151) | Acc: (87.00%) (8727/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.6079, 0.3921], device='cuda:0')
percent tensor([0.6253, 0.3747], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.9299, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.2408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.7097, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1507.3893, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.9632, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2257.5488, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.5879, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1386.8275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6204.1255, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11771.6562, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3894.0447, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16440.9160, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (6165/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (7367/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (11028/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (12245/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (13460/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (14667/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (15873/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (18297/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (19496/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (20702/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (21909/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (24321/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (25512/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (26712/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (27928/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (29138/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (30363/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (31589/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (32783/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (33990/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (35202/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (36403/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (37607/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38817/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (40012/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (41225/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (42438/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (43637/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (44858/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (46066/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (47230/50000)
# TEST : Loss: (0.4438) | Acc: (87.00%) (8710/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6384, 0.3616], device='cuda:0')
percent tensor([0.7036, 0.2964], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (6220/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (8653/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (9867/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (11087/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (12297/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (14700/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (15916/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (17128/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (18319/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (19534/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (20735/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (21943/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (23154/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (24364/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (25568/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (26782/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (28000/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (29219/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (30430/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (31653/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (32867/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (34099/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (35303/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (36500/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (37718/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (38921/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (40123/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (41344/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (42553/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (43760/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (44973/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (46190/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (47357/50000)
# TEST : Loss: (0.4431) | Acc: (86.00%) (8694/10000)
percent tensor([0.5245, 0.4755], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.6109, 0.3891], device='cuda:0')
percent tensor([0.6277, 0.3723], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.6988, 0.3012], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (3790/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (7436/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (8662/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (9882/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (11103/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (12317/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (13524/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (14745/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (15951/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (17172/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (18395/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (19615/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (20834/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (22053/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (23263/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (24482/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (28118/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (29336/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (30545/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (31760/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (32972/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (34181/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (35399/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (36602/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (37812/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (39028/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (40247/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (41458/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (42656/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (43863/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (45074/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (46291/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (47443/50000)
# TEST : Loss: (0.4805) | Acc: (86.00%) (8625/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5220, 0.4780], device='cuda:0')
percent tensor([0.6090, 0.3910], device='cuda:0')
percent tensor([0.6264, 0.3736], device='cuda:0')
percent tensor([0.6354, 0.3646], device='cuda:0')
percent tensor([0.6993, 0.3007], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (4981/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (7421/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (8636/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (9850/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (11066/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (12263/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (13487/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (14706/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (15904/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (17115/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (18327/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (19535/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (20758/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (21970/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (23186/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (24383/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (25595/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (26805/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (28023/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (29253/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (30460/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (31679/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (32883/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (34107/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (35316/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (36513/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (37720/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (38933/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (40160/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (41374/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (42578/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (43790/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (45004/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (46210/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (47382/50000)
# TEST : Loss: (0.4174) | Acc: (87.00%) (8779/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.6092, 0.3908], device='cuda:0')
percent tensor([0.6261, 0.3739], device='cuda:0')
percent tensor([0.6347, 0.3653], device='cuda:0')
percent tensor([0.7009, 0.2991], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (93.00%) (3719/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (4914/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (7315/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (8515/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (9732/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (10916/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (12120/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (13325/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (14530/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (15736/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (16927/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (18138/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (19343/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (21756/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (22962/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (24175/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (25372/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (93.00%) (26590/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (27805/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (29005/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (30208/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (31416/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (32614/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (33826/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (35031/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (36256/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (37455/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (38669/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (39884/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (41089/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (42301/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (43507/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (44727/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (45941/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (47120/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4193) | Acc: (87.00%) (8773/10000)
percent tensor([0.5281, 0.4719], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.6142, 0.3858], device='cuda:0')
percent tensor([0.6174, 0.3826], device='cuda:0')
percent tensor([0.6459, 0.3541], device='cuda:0')
percent tensor([0.6849, 0.3151], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (3764/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (6194/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (7413/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (8615/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (11019/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (14671/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (15875/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (17082/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (18290/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (19510/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (20725/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (21941/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (23157/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (24366/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (25586/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (26810/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (28013/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (29229/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (30438/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (31669/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (32875/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (34093/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (35306/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (36526/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (37747/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (38960/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (40177/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (41386/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (42595/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (43822/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (45033/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (46250/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (47412/50000)
# TEST : Loss: (0.4058) | Acc: (87.00%) (8796/10000)
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6161, 0.3839], device='cuda:0')
percent tensor([0.6238, 0.3762], device='cuda:0')
percent tensor([0.6521, 0.3479], device='cuda:0')
percent tensor([0.6902, 0.3098], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (2556/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (4996/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (6206/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (7420/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8628/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (9835/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (11052/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (12272/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (14725/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (15935/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (17163/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (18378/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (19597/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (20812/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (22027/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (23249/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (24462/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (25669/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (26875/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (28097/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (29312/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (30529/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (31753/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (32981/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (34183/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (35398/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (36623/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (37845/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (39066/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (40287/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (41505/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (42726/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (43939/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (45155/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (46373/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (47553/50000)
# TEST : Loss: (0.4023) | Acc: (88.00%) (8821/10000)
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.6175, 0.3825], device='cuda:0')
percent tensor([0.6261, 0.3739], device='cuda:0')
percent tensor([0.6492, 0.3508], device='cuda:0')
percent tensor([0.6890, 0.3110], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (3784/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (5001/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (6207/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (8648/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (9872/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (11086/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (12318/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (13532/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (14749/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (15961/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (17187/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (18402/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (19627/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (20843/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (22065/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (23288/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (24500/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (25706/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (26924/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (29359/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (30579/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (31803/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (33001/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (34216/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (35443/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (36668/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (37883/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (39111/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (40346/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (41571/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (42786/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (44002/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (45229/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (47624/50000)
# TEST : Loss: (0.3934) | Acc: (88.00%) (8839/10000)
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6861, 0.3139], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (2547/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (3772/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (4990/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (6224/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (7446/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (8667/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (9886/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (11101/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (12306/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (13526/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (14753/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (15971/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (17191/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (18420/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (19635/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (20858/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (22083/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (23309/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (24529/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (25748/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (26976/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (28197/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (29421/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (30651/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (31872/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (33089/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (34310/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (35541/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (36768/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (37982/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (39208/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (40423/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (41637/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (42863/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (44077/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (45304/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (46521/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (47688/50000)
# TEST : Loss: (0.3933) | Acc: (88.00%) (8817/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.6219, 0.3781], device='cuda:0')
percent tensor([0.6308, 0.3692], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (5008/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (6223/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (7449/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (8670/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (9892/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (11098/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (12316/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (13541/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (14769/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (15980/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (17191/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (18418/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (19642/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (20849/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (22074/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (23267/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (24486/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (25704/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (26923/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (28141/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (29356/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (30571/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31790/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (32992/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (34202/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (35409/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (36617/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (37829/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (39035/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (40242/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (41462/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (42668/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (43894/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (45116/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (46330/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (47471/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4256) | Acc: (87.00%) (8770/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.6225, 0.3775], device='cuda:0')
percent tensor([0.6345, 0.3655], device='cuda:0')
percent tensor([0.6600, 0.3400], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.3289, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.6276, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.6632, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1506.1842, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.4200, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2263.5176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.4556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1381.8256, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6217.7856, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11737.7314, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3879.0850, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16375.1260, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (2559/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (3784/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (6227/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (7451/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (8648/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (9862/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (11079/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (94.00%) (12280/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (13504/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (14720/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (94.00%) (15929/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (17152/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (18370/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (19590/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (20808/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (22019/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (23259/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (24477/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (26906/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (28115/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (29342/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (30551/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31760/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (32991/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34204/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (35439/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (36659/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (37873/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (39090/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (40305/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (41524/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (42742/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (43964/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (45187/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (46405/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (47575/50000)
# TEST : Loss: (0.4088) | Acc: (87.00%) (8796/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5073, 0.4927], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6224, 0.3776], device='cuda:0')
percent tensor([0.6312, 0.3688], device='cuda:0')
percent tensor([0.6577, 0.3423], device='cuda:0')
percent tensor([0.6987, 0.3013], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (2572/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (5012/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (6221/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (7445/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (8673/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (9882/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (11096/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (12312/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (13528/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (14744/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (15973/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (17197/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (18407/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (19625/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (20834/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (22056/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (23261/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (24479/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (25700/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (26926/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (29364/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (30578/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (31798/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (33033/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34253/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (35472/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (36671/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (37896/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (39117/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (40342/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (41568/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (42795/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (44014/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (45225/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (46435/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (47594/50000)
# TEST : Loss: (0.4815) | Acc: (86.00%) (8660/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.6217, 0.3783], device='cuda:0')
percent tensor([0.6294, 0.3706], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (2556/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (6231/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (7461/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (8673/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (9888/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (11114/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (12335/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (13551/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (14782/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (16006/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (17219/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (18450/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (19674/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (20899/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (22127/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (23343/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (24563/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (25794/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (27010/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (28233/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (29455/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (30678/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (31897/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (33109/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (34326/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (35541/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (36766/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (37965/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (39188/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (40401/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (41626/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (42846/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (44070/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (45292/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (46513/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (47676/50000)
# TEST : Loss: (0.4331) | Acc: (87.00%) (8773/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.6199, 0.3801], device='cuda:0')
percent tensor([0.6321, 0.3679], device='cuda:0')
percent tensor([0.6588, 0.3412], device='cuda:0')
percent tensor([0.7043, 0.2957], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (5026/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (6232/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (7444/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (8663/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (9883/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (11119/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (12339/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (13570/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (14800/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (16019/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (17242/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (18472/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (19690/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (20917/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (22131/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (23337/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (24567/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (25796/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (27034/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (28250/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (29472/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (30686/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (31906/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (33121/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (34332/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (35543/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (36767/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (38000/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (39214/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (40433/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (41651/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (42881/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (44085/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (45302/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (46509/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (47694/50000)
# TEST : Loss: (0.5002) | Acc: (86.00%) (8622/10000)
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.6220, 0.3780], device='cuda:0')
percent tensor([0.6317, 0.3683], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.7033, 0.2967], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (6214/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (8634/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (9833/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (11056/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (12278/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (13500/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (14712/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (15932/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (17143/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (18357/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (19580/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (20799/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (22007/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (23217/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (24431/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (25649/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (26854/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (28066/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (29301/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (30527/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (31747/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (32966/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (34185/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (35404/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (36626/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (37837/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (39054/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (40277/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (41497/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (42726/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (43947/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (45168/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (46386/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (47565/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4086) | Acc: (88.00%) (8837/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.6660, 0.3340], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')


Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (3791/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (5011/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (6235/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (7449/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (8672/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (9895/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (11108/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (12334/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (13546/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (14761/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (15990/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (17220/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (18441/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (19667/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (20891/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (22123/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (23349/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (24579/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (25801/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (27024/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (28254/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (29489/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (30710/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (31943/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (33169/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (34410/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (35627/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (36841/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (38063/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (39298/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (40514/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (41733/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (42956/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (44183/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (45408/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (46624/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (47794/50000)
# TEST : Loss: (0.3941) | Acc: (88.00%) (8877/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.6175, 0.3825], device='cuda:0')
percent tensor([0.6455, 0.3545], device='cuda:0')
percent tensor([0.6754, 0.3246], device='cuda:0')
percent tensor([0.6835, 0.3165], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (3811/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (96.00%) (5042/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (7518/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (96.00%) (8732/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (96.00%) (9959/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (96.00%) (11187/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (96.00%) (12416/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (13632/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (14860/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (16093/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (17319/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (18530/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (19765/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (20992/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (22213/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (23438/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (24653/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (25881/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (27108/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (28342/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (29567/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (30796/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (32029/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (33238/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (34470/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (35700/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (36927/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (38159/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (39383/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (40600/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (41836/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (43056/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (44289/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (45510/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (46741/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (47920/50000)
# TEST : Loss: (0.3882) | Acc: (88.00%) (8881/10000)
percent tensor([0.5360, 0.4640], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.6454, 0.3546], device='cuda:0')
percent tensor([0.6759, 0.3241], device='cuda:0')
percent tensor([0.6865, 0.3135], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (96.00%) (3815/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (96.00%) (5050/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (96.00%) (6279/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (96.00%) (7501/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (96.00%) (8738/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (9984/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (11228/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (12460/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (13689/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (14921/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (16155/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (17387/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (18607/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (19829/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (21052/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (22289/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (23518/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (24739/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (25952/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (27186/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (28402/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (29630/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (30866/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (32100/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (33337/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (34567/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (35794/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (37023/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (38268/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (39497/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (40736/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (41957/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (43188/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (44416/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (45659/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (46896/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (48076/50000)
# TEST : Loss: (0.3853) | Acc: (88.00%) (8877/10000)
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5344, 0.4656], device='cuda:0')
percent tensor([0.6164, 0.3836], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.6699, 0.3301], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (97.00%) (2608/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (5066/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (6301/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (7537/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (8772/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (10006/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (11234/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (12453/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (13688/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (14928/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (16154/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (17381/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (18617/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (19842/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (21052/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (22268/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (23502/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (96.00%) (24724/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (25958/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (27186/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (28425/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (29654/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (30891/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (32117/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (33356/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (34591/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (35825/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (37061/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (38290/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (39523/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (40746/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (41980/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (43211/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (44455/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (45672/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (46890/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (48081/50000)
# TEST : Loss: (0.3870) | Acc: (88.00%) (8892/10000)
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6199, 0.3801], device='cuda:0')
percent tensor([0.6446, 0.3554], device='cuda:0')
percent tensor([0.6707, 0.3293], device='cuda:0')
percent tensor([0.6989, 0.3011], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (6284/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (7509/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (8737/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (96.00%) (9957/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (11179/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (12397/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (13626/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (14847/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (16071/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (17280/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (18511/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (19724/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (20954/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (22188/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (23415/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (24646/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (25860/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (27084/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (28294/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (29521/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (30741/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (31968/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (33196/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (34421/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (35641/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (36864/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (38077/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (39292/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (40513/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (41744/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (42965/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (44203/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (45429/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (46647/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (47813/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
# TEST : Loss: (0.4697) | Acc: (87.00%) (8722/10000)
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.6197, 0.3803], device='cuda:0')
percent tensor([0.6449, 0.3551], device='cuda:0')
percent tensor([0.6707, 0.3293], device='cuda:0')
percent tensor([0.6985, 0.3015], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.9437, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.2482, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.8285, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1507.3489, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.4892, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2272.2815, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.2363, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.1700, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6240.8208, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11722.8027, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3870.1646, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16335.0791, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 151 | Batch_idx: 0 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (2569/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (8694/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (9913/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (11131/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (12360/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (13585/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (14817/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (16024/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (17250/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (18477/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (19689/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (20911/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (22142/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (23369/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (24593/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (25810/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (27039/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (28259/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (29487/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (30716/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (31933/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (33146/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (34362/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (35591/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (36822/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (38039/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (39262/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (40487/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (41705/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (42938/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (44176/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (45392/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (46615/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (47790/50000)
# TEST : Loss: (0.4774) | Acc: (86.00%) (8658/10000)
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6205, 0.3795], device='cuda:0')
percent tensor([0.6451, 0.3549], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.7006, 0.2994], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (5045/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (6273/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (7506/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (8733/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (9963/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (11191/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (12420/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (13654/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (14876/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (16093/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (17330/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (18567/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (19800/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (21025/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (22233/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (23452/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (24665/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (25895/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (27128/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (28355/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (29589/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (30813/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (32050/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (33286/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (34505/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (35737/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (36953/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (38170/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (39394/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (40619/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (41835/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (43064/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (44290/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (45508/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (46738/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (47913/50000)
# TEST : Loss: (0.4499) | Acc: (87.00%) (8733/10000)
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5341, 0.4659], device='cuda:0')
percent tensor([0.6196, 0.3804], device='cuda:0')
percent tensor([0.6481, 0.3519], device='cuda:0')
percent tensor([0.6720, 0.3280], device='cuda:0')
percent tensor([0.7043, 0.2957], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (6259/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (8720/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (95.00%) (9948/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (11168/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (12406/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (13647/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (14883/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (16121/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (17337/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (18569/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (19810/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (21043/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (22262/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (23485/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (24719/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (25948/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (27190/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (28425/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (29649/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (30877/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (32102/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (33324/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (34554/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (35770/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (36994/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (38223/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (39449/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (40675/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (41897/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (43112/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (44340/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (45553/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (46779/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (47960/50000)
# TEST : Loss: (0.4779) | Acc: (87.00%) (8704/10000)
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.6177, 0.3823], device='cuda:0')
percent tensor([0.6433, 0.3567], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6978, 0.3022], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (3808/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (7497/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (8725/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (9935/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (11162/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (12380/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (13610/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (14845/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (16075/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (17300/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (18527/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (19760/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (20979/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (22209/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23442/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (24662/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (25888/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (27114/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (28349/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (29560/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (30783/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (32011/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (33237/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (34465/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (35690/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (36924/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (38164/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (39391/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (40613/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (41839/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (43063/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (44295/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (45521/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (46748/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (47937/50000)
# TEST : Loss: (0.4511) | Acc: (87.00%) (8786/10000)
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.6194, 0.3806], device='cuda:0')
percent tensor([0.6436, 0.3564], device='cuda:0')
percent tensor([0.6710, 0.3290], device='cuda:0')
percent tensor([0.7000, 0.3000], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (2565/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (4985/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (6195/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (7403/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (8609/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (9825/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (11038/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (12257/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (13480/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (14685/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (15895/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (17103/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (18302/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (19496/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (20708/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (21925/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (23139/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (24353/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (25573/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (26794/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (28016/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (29238/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (30468/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (31686/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (32910/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (34125/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (35339/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (36565/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (37786/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (39007/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (40224/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (41445/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (42661/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (43888/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (45102/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (94.00%) (46328/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (47497/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_155.pth.tar'
# TEST : Loss: (0.4408) | Acc: (87.00%) (8771/10000)
percent tensor([0.5401, 0.4599], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5217, 0.4783], device='cuda:0')
percent tensor([0.6016, 0.3984], device='cuda:0')
percent tensor([0.6417, 0.3583], device='cuda:0')
percent tensor([0.6805, 0.3195], device='cuda:0')
percent tensor([0.6929, 0.3071], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (6218/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (7439/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (8674/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (9893/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (11112/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (12338/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (13555/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (14777/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (16003/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (17240/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (18459/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (19682/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (20900/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (22118/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (23348/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (24571/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (25781/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (27002/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (28217/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (29444/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (30668/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (31883/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (33112/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (34337/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (35573/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (36797/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (38016/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (39236/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (40470/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (41689/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (42918/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (44148/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (45375/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (46597/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (47775/50000)
# TEST : Loss: (0.4247) | Acc: (88.00%) (8807/10000)
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.6727, 0.3273], device='cuda:0')
percent tensor([0.6944, 0.3056], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (3788/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (6240/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (7476/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (8706/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (9926/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (11148/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (12376/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (13605/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (14825/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (16045/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (17271/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (18494/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (19718/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (20953/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (22190/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (23408/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (24640/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (25860/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (27098/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (28328/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (29564/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (30796/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (32040/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (33272/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (34509/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (35744/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (36970/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (38192/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (39410/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (40641/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (41859/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (43093/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (44327/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (45569/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (46798/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (47976/50000)
# TEST : Loss: (0.4111) | Acc: (88.00%) (8844/10000)
percent tensor([0.5403, 0.4597], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5208, 0.4792], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.6460, 0.3540], device='cuda:0')
percent tensor([0.6789, 0.3211], device='cuda:0')
percent tensor([0.6993, 0.3007], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (6277/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (7498/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (9982/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (11211/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (12443/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (13672/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (14897/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (16122/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (17351/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (18584/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (19814/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (21039/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (22269/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (23495/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (24713/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (25954/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (27189/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (28416/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (29632/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (30871/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (32097/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (33335/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (34566/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (35803/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (37041/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (38275/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (39515/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (40735/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (41971/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (43210/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (44454/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (45682/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (46919/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (48104/50000)
# TEST : Loss: (0.4089) | Acc: (88.00%) (8856/10000)
percent tensor([0.5384, 0.4616], device='cuda:0')
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.6070, 0.3930], device='cuda:0')
percent tensor([0.6503, 0.3497], device='cuda:0')
percent tensor([0.6815, 0.3185], device='cuda:0')
percent tensor([0.7076, 0.2924], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (5032/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (6265/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (95.00%) (7493/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (8726/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (9964/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (11189/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (12423/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (13660/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (14898/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (16128/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (17357/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (18575/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (19818/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (21036/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (22257/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (23485/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (24708/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (25934/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (27158/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (28386/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (29622/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (30838/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (32073/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (95.00%) (33288/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (95.00%) (34516/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (35752/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (36984/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (38207/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (39445/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (40678/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (41917/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (43156/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (44385/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (45625/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (46852/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (48042/50000)
# TEST : Loss: (0.4078) | Acc: (88.00%) (8844/10000)
percent tensor([0.5382, 0.4618], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5212, 0.4788], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.6497, 0.3503], device='cuda:0')
percent tensor([0.6807, 0.3193], device='cuda:0')
percent tensor([0.7073, 0.2927], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (2600/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (5086/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (6315/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (7540/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (8773/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (10013/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (11245/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (12475/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (13714/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (14948/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (16165/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (17386/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (18620/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (19837/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (21071/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (22308/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (23539/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (24779/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (26002/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (27221/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (28443/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (29683/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (30907/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (32137/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (33362/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (34594/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (35827/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (37055/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (38282/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (39512/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (40731/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (41953/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (43175/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (44398/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (45622/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (46847/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (48016/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_160.pth.tar'
# TEST : Loss: (0.5013) | Acc: (86.00%) (8653/10000)
percent tensor([0.5384, 0.4616], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.6084, 0.3916], device='cuda:0')
percent tensor([0.6514, 0.3486], device='cuda:0')
percent tensor([0.6823, 0.3177], device='cuda:0')
percent tensor([0.7073, 0.2927], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.2725, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.3480, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.1721, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1505.9257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(494.8849, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2277.1812, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4270.5234, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.1659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6254.0176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11689.2119, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3855.3003, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16269.9883, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (3825/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (5053/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (7519/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (8765/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (10001/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (11232/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (12467/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (13705/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (14933/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (16171/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (17399/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (18633/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (19859/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (21085/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (22315/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (23552/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (24776/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (26003/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (27232/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (28460/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (29681/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (30901/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (32124/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (33348/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (34569/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (35795/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (37031/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (38255/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (39486/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (40724/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (41942/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (43167/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (44400/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (45630/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (46847/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (48015/50000)
# TEST : Loss: (0.4232) | Acc: (88.00%) (8834/10000)
percent tensor([0.5383, 0.4617], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.6507, 0.3493], device='cuda:0')
percent tensor([0.6803, 0.3197], device='cuda:0')
percent tensor([0.7078, 0.2922], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (2602/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (3840/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (5064/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (6287/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (7526/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (8752/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (9980/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (11219/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (12460/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (13709/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (14931/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (16168/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (17405/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (18625/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (19849/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (21080/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (22316/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (23542/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (24761/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (25991/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (27214/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (28440/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (29663/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (30889/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (32115/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (33344/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (34574/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (35800/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (37028/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (38252/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (39484/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (40709/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (41950/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (43184/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (44406/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (45637/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (46859/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (48051/50000)
# TEST : Loss: (0.4206) | Acc: (88.00%) (8826/10000)
percent tensor([0.5387, 0.4613], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5217, 0.4783], device='cuda:0')
percent tensor([0.6085, 0.3915], device='cuda:0')
percent tensor([0.6473, 0.3527], device='cuda:0')
percent tensor([0.6816, 0.3184], device='cuda:0')
percent tensor([0.7030, 0.2970], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (5084/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (6327/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (97.00%) (7575/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (8810/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (10054/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (11293/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (12527/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (13750/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (14977/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (16213/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (17455/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (18678/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (19905/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (21133/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (22356/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (23583/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (24820/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (26047/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (27266/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (28489/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (29713/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (30937/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (32161/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (33387/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (34621/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (35863/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (37094/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (38325/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (39559/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (40795/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (42022/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (43246/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (44479/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (45720/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (46944/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (48141/50000)
# TEST : Loss: (0.4453) | Acc: (87.00%) (8786/10000)
percent tensor([0.5385, 0.4615], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.6084, 0.3916], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.6787, 0.3213], device='cuda:0')
percent tensor([0.7047, 0.2953], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (5047/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (6286/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (7521/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (8749/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (9984/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (11221/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (12457/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (13701/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (14940/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (16183/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (17413/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (18642/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (19869/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (21106/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (22342/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (23579/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (24800/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (26032/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (27257/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (28476/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (29706/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (30941/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (32172/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (33405/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (34627/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (35876/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (37094/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (38327/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (39563/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (40792/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (42030/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (43266/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (44499/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (45734/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (46955/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (48150/50000)
# TEST : Loss: (0.4939) | Acc: (86.00%) (8670/10000)
percent tensor([0.5383, 0.4617], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5215, 0.4785], device='cuda:0')
percent tensor([0.6086, 0.3914], device='cuda:0')
percent tensor([0.6499, 0.3501], device='cuda:0')
percent tensor([0.6789, 0.3211], device='cuda:0')
percent tensor([0.7064, 0.2936], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (2574/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (3794/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (6225/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (7437/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (8657/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (9884/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (11102/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (12330/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (13559/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (14770/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (15989/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (17217/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (18439/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (19669/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (20897/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (22119/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (23346/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (24588/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (25810/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (27036/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (28258/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (29488/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (30733/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (31958/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (33188/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (34422/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (35656/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (36883/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (38124/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (39352/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (40581/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (41809/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (43034/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (44256/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (45485/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (46718/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (47903/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_165.pth.tar'
# TEST : Loss: (0.4419) | Acc: (87.00%) (8787/10000)
percent tensor([0.5397, 0.4603], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5328, 0.4672], device='cuda:0')
percent tensor([0.6188, 0.3812], device='cuda:0')
percent tensor([0.6555, 0.3445], device='cuda:0')
percent tensor([0.6536, 0.3464], device='cuda:0')
percent tensor([0.6719, 0.3281], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 166 | Batch_idx: 0 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (7511/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (9972/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (11201/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (12431/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (13665/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (14906/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (16135/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (17356/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (18590/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (19818/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (21052/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (22281/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (23503/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (24736/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (25975/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (27209/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (28458/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (29686/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (30915/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (32159/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (33396/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (34629/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (35867/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (37097/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (38339/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (39577/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (40814/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (42058/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (43292/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (44520/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (45743/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (46980/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (48167/50000)
# TEST : Loss: (0.4282) | Acc: (88.00%) (8819/10000)
percent tensor([0.5406, 0.4594], device='cuda:0')
percent tensor([0.5114, 0.4886], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.6632, 0.3368], device='cuda:0')
percent tensor([0.6546, 0.3454], device='cuda:0')
percent tensor([0.6703, 0.3297], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (2598/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (5059/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (6289/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (7518/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (95.00%) (8723/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (95.00%) (9953/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (11188/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (12421/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (13641/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (14879/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (16120/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (17356/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (18587/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (19823/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (21053/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (22293/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (23538/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (24788/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (26029/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (27267/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (28500/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (29744/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (30976/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (32216/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (33468/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (34694/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (35933/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (37159/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (38387/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (39623/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (40862/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (42085/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (43314/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (44547/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (45789/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (47043/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (48228/50000)
# TEST : Loss: (0.4143) | Acc: (88.00%) (8847/10000)
percent tensor([0.5405, 0.4595], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.6806, 0.3194], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 168 | Batch_idx: 0 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (3839/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (5065/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (6308/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (7543/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (8785/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (10013/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (11254/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (12483/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (13720/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (14972/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (16214/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (17452/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (18679/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (19906/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (21152/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (22387/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (23618/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (24851/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (26088/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (27325/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (28560/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (29799/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (31029/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (32266/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (33510/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (34752/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (35992/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (37211/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (38448/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (39682/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (40925/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (42162/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (43405/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (44642/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (45876/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (47133/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (48319/50000)
# TEST : Loss: (0.4135) | Acc: (88.00%) (8851/10000)
percent tensor([0.5406, 0.4594], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.5380, 0.4620], device='cuda:0')
percent tensor([0.6116, 0.3884], device='cuda:0')
percent tensor([0.6590, 0.3410], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 169 | Batch_idx: 0 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (3829/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (5062/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (6295/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (8779/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (10016/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (11249/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (12490/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (13733/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (14965/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (16211/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (17455/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (18695/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (19934/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (21170/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (22408/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (23650/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (24885/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (26118/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (27352/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (28582/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (29827/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (31067/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (32307/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (33537/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (34766/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (36007/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (37249/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (38486/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (39715/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (40951/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (42179/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (43417/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (44652/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (45896/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (47132/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (48336/50000)
# TEST : Loss: (0.4093) | Acc: (88.00%) (8870/10000)
percent tensor([0.5402, 0.4598], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6152, 0.3848], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.6608, 0.3392], device='cuda:0')
percent tensor([0.7036, 0.2964], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (3815/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (5061/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (6299/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (7527/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (8762/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (9974/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (11208/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (12434/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (13679/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (14901/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (16148/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (17392/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (18628/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (19862/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (21099/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (22338/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (23577/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (24804/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (26038/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (27269/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (28501/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (29742/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (30960/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (32200/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (33431/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (34653/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (35886/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (37115/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (38345/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (39575/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (40797/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (42024/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (43261/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (44491/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (45716/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (46949/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (48136/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_170.pth.tar'
# TEST : Loss: (0.4228) | Acc: (88.00%) (8814/10000)
percent tensor([0.5397, 0.4603], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.6152, 0.3848], device='cuda:0')
percent tensor([0.6595, 0.3405], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.7047, 0.2953], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.4857, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.5349, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.5480, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.2578, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.1853, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2281.9106, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4268.4941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1369.0101, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6266.3755, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11656.0244, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3840.4050, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16205.1768, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 171 | Batch_idx: 0 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (3830/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (5073/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (6303/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (8766/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (10004/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (11240/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (12475/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (13699/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (14941/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (16182/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (17411/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (18647/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (19883/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (21113/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (22352/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (23589/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (24825/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (26058/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (27298/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (28539/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (29779/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (31027/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (32266/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (33505/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (34743/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (35977/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (37206/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (38433/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (39660/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (40907/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (42134/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (43370/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (44603/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (45844/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (47071/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (48254/50000)
# TEST : Loss: (0.5323) | Acc: (86.00%) (8616/10000)
percent tensor([0.5399, 0.4601], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.5360, 0.4640], device='cuda:0')
percent tensor([0.6150, 0.3850], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.7025, 0.2975], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 172 | Batch_idx: 0 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (97.00%) (5097/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (97.00%) (7580/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (8812/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (10049/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (11287/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (12518/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (13756/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (14994/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (16235/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (17482/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (18722/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (19959/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (21196/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (22437/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (23670/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (24896/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (26115/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (27348/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (28583/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (29816/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (31046/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (32277/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (33514/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (34760/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (35997/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (37227/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (38461/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (39689/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (40926/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (42162/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (43388/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (44619/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (45849/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (47082/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (48274/50000)
# TEST : Loss: (0.4159) | Acc: (88.00%) (8866/10000)
percent tensor([0.5400, 0.4600], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6560, 0.3440], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.6970, 0.3030], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 173 | Batch_idx: 0 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (5086/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (6319/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (7569/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (8816/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (10053/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (11294/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (12534/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (13766/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (15008/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (16251/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (17493/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (18732/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (19971/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (21209/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (22450/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (23694/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (24932/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (26177/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (27424/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (28668/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (29906/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (31136/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (32374/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (33604/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (34827/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (36057/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (37299/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (38532/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (39757/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (40999/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (42228/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (43475/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (44706/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (45949/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (47193/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (48385/50000)
# TEST : Loss: (0.4479) | Acc: (88.00%) (8816/10000)
percent tensor([0.5397, 0.4603], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6149, 0.3851], device='cuda:0')
percent tensor([0.6585, 0.3415], device='cuda:0')
percent tensor([0.6594, 0.3406], device='cuda:0')
percent tensor([0.7022, 0.2978], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 174 | Batch_idx: 0 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (5085/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (6320/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (7560/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (8799/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (10039/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (11278/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (12519/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (13768/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (15008/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (16253/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (17481/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (18717/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (19962/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (21207/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (22452/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (23683/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (24928/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (26158/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (27408/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (28638/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (29877/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (31109/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (32342/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (33577/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (34803/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (36032/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (37262/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (38489/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (39731/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (40960/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (42193/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (43429/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (44662/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (45890/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (47142/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (48331/50000)
# TEST : Loss: (0.5227) | Acc: (86.00%) (8617/10000)
percent tensor([0.5397, 0.4603], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.5360, 0.4640], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6576, 0.3424], device='cuda:0')
percent tensor([0.6564, 0.3436], device='cuda:0')
percent tensor([0.7022, 0.2978], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (3825/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (7506/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (8734/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (9961/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (11187/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (12408/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (13642/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (14869/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (16110/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (17336/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (18565/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (19786/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (21022/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (22254/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (23485/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (24716/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (25940/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (27172/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (28402/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (29631/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (30853/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (32080/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (33325/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (34564/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (35806/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (37039/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (38275/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (39504/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (40742/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (41972/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (43192/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (44434/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (45664/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (46909/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (48100/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_175.pth.tar'
# TEST : Loss: (0.4256) | Acc: (88.00%) (8840/10000)
percent tensor([0.5381, 0.4619], device='cuda:0')
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6616, 0.3384], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.6788, 0.3212], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 176 | Batch_idx: 0 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (2602/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (3836/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (6313/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (7549/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (8785/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (10016/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (11253/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (12489/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (13723/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (14970/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (16216/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (17458/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (18700/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (19943/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (21168/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (22408/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (23653/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (24889/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (26117/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (27360/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (28611/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (29854/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (31099/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (32342/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (33575/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (34814/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (36050/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (37288/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (38528/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (39762/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (41004/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (42246/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (43486/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (44717/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (45958/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (47210/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (48402/50000)
# TEST : Loss: (0.4159) | Acc: (88.00%) (8865/10000)
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.6154, 0.3846], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.6594, 0.3406], device='cuda:0')
percent tensor([0.6836, 0.3164], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 177 | Batch_idx: 0 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (5084/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (6316/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (7571/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (8812/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (97.00%) (10059/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (11295/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (12532/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (13778/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (15014/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (16265/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (17497/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (18736/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (19972/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (21217/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (22461/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (23697/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (24937/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (26178/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (27424/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (28661/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (29902/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (31148/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (32394/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (33634/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (34877/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (36125/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (37370/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (38613/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (39852/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (41090/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (97.00%) (42342/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (43586/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (44831/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (46072/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (47317/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (48513/50000)
# TEST : Loss: (0.4070) | Acc: (88.00%) (8867/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5043, 0.4957], device='cuda:0')
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.6198, 0.3802], device='cuda:0')
percent tensor([0.6638, 0.3362], device='cuda:0')
percent tensor([0.6634, 0.3366], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 178 | Batch_idx: 0 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (3855/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (5099/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (6345/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (7575/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (8814/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (10056/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (11300/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (12549/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (13800/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (15043/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (16288/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (17527/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (18766/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (20011/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (21261/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (22499/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (23746/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (24985/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (26234/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (27482/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (28721/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (29970/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (31220/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (32455/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (33700/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (34949/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (36190/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (37446/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (38685/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (39933/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (41182/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (42429/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (43674/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (44924/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (46166/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (47420/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (48621/50000)
# TEST : Loss: (0.4077) | Acc: (88.00%) (8869/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.6585, 0.3415], device='cuda:0')
percent tensor([0.6614, 0.3386], device='cuda:0')
percent tensor([0.6939, 0.3061], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 179 | Batch_idx: 0 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (3862/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (6341/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (7583/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (8823/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (10077/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (11321/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (12574/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (13817/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (15059/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (16296/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (17534/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (18767/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (19998/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (21244/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (22485/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (23725/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (24972/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (26212/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (27469/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (28719/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (29964/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (31209/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (32457/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (33706/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (34961/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (36200/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (37439/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (38690/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (39942/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (41186/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (42437/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (43680/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (44928/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (46173/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (47413/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (48613/50000)
# TEST : Loss: (0.4020) | Acc: (88.00%) (8876/10000)
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.6615, 0.3385], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.6954, 0.3046], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (5095/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (6332/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (7580/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (8827/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (10072/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (11309/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (12546/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (13790/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (15033/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (16285/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (17531/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (18768/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (20017/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (21269/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (22510/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (23754/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (24993/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (26230/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (27467/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (97.00%) (28704/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (29943/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (31173/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (97.00%) (32408/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (97.00%) (33652/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (97.00%) (34899/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (36136/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (97.00%) (37377/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (38620/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (39854/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (41094/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (42347/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (43589/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (97.00%) (44833/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (46076/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (97.00%) (47309/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (97.00%) (48501/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_180.pth.tar'
# TEST : Loss: (0.4413) | Acc: (87.00%) (8783/10000)
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.6579, 0.3421], device='cuda:0')
percent tensor([0.6933, 0.3067], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.5670, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.1498, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.4923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1502.2971, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(491.5113, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2285.3706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4266.0659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1364.0852, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6276.7827, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11621.8672, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3825.5708, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16140.4180, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (5096/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (6340/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (7585/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (8820/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (10062/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (11300/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (12544/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (13789/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (15023/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (16273/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (17517/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (18757/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (19999/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (21249/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (22496/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (23738/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (24979/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (26216/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (27454/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (28690/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (29928/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (31164/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (32403/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (33633/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (34870/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (36113/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (37349/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (38580/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (39821/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (41060/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (42291/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (43528/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (44774/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (46014/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (47239/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (48426/50000)
# TEST : Loss: (0.4696) | Acc: (87.00%) (8750/10000)
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.6264, 0.3736], device='cuda:0')
percent tensor([0.6601, 0.3399], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 182 | Batch_idx: 0 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (3858/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (5105/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (6345/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (7588/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (97.00%) (8822/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (97.00%) (10059/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (97.00%) (11302/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (12549/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (13787/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (15031/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (16283/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (17521/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (18762/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (20007/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (21245/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (22485/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (23730/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (24977/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (26223/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (27464/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (28707/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (29944/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (31188/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (32429/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (33676/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (34923/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (36166/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (37411/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (38649/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (39884/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (41120/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (42366/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (43602/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (44827/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (46078/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (47318/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (48505/50000)
# TEST : Loss: (0.5157) | Acc: (87.00%) (8722/10000)
percent tensor([0.5311, 0.4689], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.6255, 0.3745], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 183 | Batch_idx: 0 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (3869/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (5121/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (6368/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (7608/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (8832/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (10072/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (11313/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (12544/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (13792/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (15044/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (16285/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (17520/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (18760/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (20007/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (21249/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (22495/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (23736/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (24980/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (26223/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (27463/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (28686/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (29928/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (31165/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (32415/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (33656/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (34894/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (36140/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (37383/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (38627/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (39853/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (41099/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (42342/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (43577/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (44816/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (46047/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (47292/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (48475/50000)
# TEST : Loss: (0.5940) | Acc: (85.00%) (8539/10000)
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.6255, 0.3745], device='cuda:0')
percent tensor([0.6594, 0.3406], device='cuda:0')
percent tensor([0.6605, 0.3395], device='cuda:0')
percent tensor([0.6954, 0.3046], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0269) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (2608/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (6335/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (7581/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (96.00%) (8810/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (10048/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (11287/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (12526/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (13765/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (15011/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (16246/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (17499/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (18744/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (19986/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (21234/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (22475/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (23711/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (24959/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (96.00%) (26197/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (27440/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (28691/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (96.00%) (29921/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (31150/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (32380/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (33620/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (34861/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (36102/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (37341/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (38585/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (39830/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (41064/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (42304/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (43547/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (44785/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (46026/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (47274/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (48463/50000)
# TEST : Loss: (0.4562) | Acc: (88.00%) (8829/10000)
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.6258, 0.3742], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.6586, 0.3414], device='cuda:0')
percent tensor([0.6954, 0.3046], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (5048/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (7501/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (8731/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (9959/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (11193/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (12419/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (95.00%) (13633/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (95.00%) (14862/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (95.00%) (16082/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (95.00%) (17326/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (95.00%) (18550/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (95.00%) (19775/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (95.00%) (21010/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (22245/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (95.00%) (23468/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (95.00%) (24692/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (95.00%) (25914/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (27155/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (28399/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (29635/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (30873/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (32096/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (33326/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (34557/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (35789/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (37022/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (38259/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (39483/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (40728/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (41959/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (43196/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (44441/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (45671/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (46911/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (48099/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_185.pth.tar'
# TEST : Loss: (0.4611) | Acc: (87.00%) (8780/10000)
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.6329, 0.3671], device='cuda:0')
percent tensor([0.6467, 0.3533], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.6944, 0.3056], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 186 | Batch_idx: 0 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (2600/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (3829/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (6312/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (7559/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (8788/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (10028/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (11259/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (12509/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (13747/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (14984/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (16218/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (17455/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (18688/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (19926/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (21160/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (22404/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (23636/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (24873/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (26110/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (27345/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (28591/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (29819/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (31062/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (32307/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (33552/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (34787/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (36021/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (37258/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (38499/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (39736/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (40977/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (42206/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (43445/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (44689/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (45929/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (47169/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (48366/50000)
# TEST : Loss: (0.4451) | Acc: (88.00%) (8830/10000)
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.6510, 0.3490], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (6339/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (7586/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (8824/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (10059/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (11294/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (12531/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (13770/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (15020/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (16261/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (17501/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (18742/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (19972/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (21207/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (22453/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (23700/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (24944/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (26182/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (27420/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (28673/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (29914/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (31150/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (32401/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (33647/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (34885/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (36128/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (37380/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (38628/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (39873/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (41116/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (42351/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (43587/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (44824/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (46071/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (47313/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (48513/50000)
# TEST : Loss: (0.4365) | Acc: (88.00%) (8847/10000)
percent tensor([0.5321, 0.4679], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5285, 0.4715], device='cuda:0')
percent tensor([0.6328, 0.3672], device='cuda:0')
percent tensor([0.6510, 0.3490], device='cuda:0')
percent tensor([0.6540, 0.3460], device='cuda:0')
percent tensor([0.7017, 0.2983], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (5085/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (6334/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (7577/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (8819/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (10065/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (11312/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (12550/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (13788/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (15042/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (16292/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (17531/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (18779/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (20021/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (21268/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (22515/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (23758/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (25007/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (26254/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (27505/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (28747/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (29984/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (31227/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (32469/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (33706/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (34957/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (36196/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (37433/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (38681/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (39936/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (41182/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (42432/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (43689/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (44936/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (46177/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (47423/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (48621/50000)
# TEST : Loss: (0.4303) | Acc: (88.00%) (8859/10000)
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.6302, 0.3698], device='cuda:0')
percent tensor([0.6530, 0.3470], device='cuda:0')
percent tensor([0.6605, 0.3395], device='cuda:0')
percent tensor([0.7077, 0.2923], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 189 | Batch_idx: 0 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (3855/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (6350/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (7591/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (8833/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (10072/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (11324/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (12566/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (13809/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (15062/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (16306/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (17550/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (18788/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (20038/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (21274/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (22525/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (23775/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (25019/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (26264/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (27509/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (28760/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (30003/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (31251/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (32501/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (33757/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (35003/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (36259/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (37504/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (38747/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (39993/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (41236/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (42481/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (43725/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (44969/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (46220/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (47464/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (48664/50000)
# TEST : Loss: (0.4240) | Acc: (88.00%) (8875/10000)
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6344, 0.3656], device='cuda:0')
percent tensor([0.6512, 0.3488], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.7146, 0.2854], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (3858/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (6351/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (7598/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (8843/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (10089/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (11336/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (12573/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (13820/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (15065/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (16300/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (17536/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (18781/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (20027/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (21263/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (22508/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (23749/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (24985/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (26232/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (27473/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (28730/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (29970/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (31212/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (32449/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (33696/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (34940/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (36188/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (37433/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (38671/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (39915/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (41162/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (42410/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (43654/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (44888/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (46139/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (47392/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (48593/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_190.pth.tar'
# TEST : Loss: (0.5058) | Acc: (87.00%) (8737/10000)
percent tensor([0.5369, 0.4631], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.6496, 0.3504], device='cuda:0')
percent tensor([0.6628, 0.3372], device='cuda:0')
percent tensor([0.7129, 0.2871], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.6833, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.6866, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.4263, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1500.5653, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.9714, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2289.4385, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4263.3726, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1359.1952, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6287.0190, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11588.2002, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3810.8008, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16076.0234, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (6354/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (7594/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (8841/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (10091/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (11343/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (12591/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (13842/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (15086/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (16326/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (17561/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (18806/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (20045/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (21302/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (22539/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (23780/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (25018/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (26265/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (27507/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (28744/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (29980/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (31219/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (32463/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (33705/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (34945/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (36185/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (37426/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (38667/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (39900/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (41147/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (42391/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (43636/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (44875/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (46108/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (47350/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (48538/50000)
# TEST : Loss: (0.4937) | Acc: (87.00%) (8763/10000)
percent tensor([0.5369, 0.4631], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5311, 0.4689], device='cuda:0')
percent tensor([0.6369, 0.3631], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.6637, 0.3363], device='cuda:0')
percent tensor([0.7137, 0.2863], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 192 | Batch_idx: 0 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (2624/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (3867/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (5118/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (6372/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (7613/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (8850/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (10104/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (11348/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (12592/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (13837/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (15092/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (16345/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (17592/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (18840/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (20086/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (21334/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (22579/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (23831/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (25079/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (26319/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (27565/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (28807/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (30054/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (31285/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (32536/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (33784/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (35034/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (36287/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (37534/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (38778/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (40032/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (41281/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (42523/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (43764/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (45019/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (46263/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (47495/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (48680/50000)
# TEST : Loss: (0.4712) | Acc: (87.00%) (8797/10000)
percent tensor([0.5373, 0.4627], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.6500, 0.3500], device='cuda:0')
percent tensor([0.6614, 0.3386], device='cuda:0')
percent tensor([0.7149, 0.2851], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 193 | Batch_idx: 0 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (2608/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (5109/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (6362/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (7611/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (8862/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (10116/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (11361/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (12615/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (13857/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (15098/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (16343/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (17589/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (18832/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (20083/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (21328/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (22570/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (23807/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (25055/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (26305/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (27544/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (28783/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (30022/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (31265/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (32499/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (33734/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (34979/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (36213/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (37459/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (38688/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (39942/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (41175/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (42422/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (43662/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (44904/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (46148/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (47400/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (48599/50000)
# TEST : Loss: (0.4470) | Acc: (88.00%) (8847/10000)
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.6350, 0.3650], device='cuda:0')
percent tensor([0.6505, 0.3495], device='cuda:0')
percent tensor([0.6633, 0.3367], device='cuda:0')
percent tensor([0.7152, 0.2848], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 194 | Batch_idx: 0 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (3862/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (7601/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (8845/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (10085/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (11318/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (12564/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (13805/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (15046/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (16293/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (17534/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (18776/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (20025/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (21267/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (22513/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (23766/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (25008/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (26249/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (27491/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (28731/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (29976/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (31212/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (32464/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (33709/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (34945/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (36194/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (37432/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (38680/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (39925/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (41175/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (42428/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (43679/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (44919/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (46151/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (47405/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (48601/50000)
# TEST : Loss: (0.4453) | Acc: (88.00%) (8858/10000)
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.6498, 0.3502], device='cuda:0')
percent tensor([0.6613, 0.3387], device='cuda:0')
percent tensor([0.7107, 0.2893], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (6325/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (7565/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (8805/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (10033/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (11268/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (12500/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (13727/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (14951/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (16182/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (17425/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (18663/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (19897/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (21128/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (22371/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (23607/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (24842/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (26074/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (27307/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (28547/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (29785/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (31025/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (32268/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (33512/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (34753/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (35980/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (37227/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (38473/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (39721/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (40957/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (42204/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (43439/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (44691/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (45931/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (47174/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (48365/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_195.pth.tar'
# TEST : Loss: (0.4482) | Acc: (88.00%) (8852/10000)
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5286, 0.4714], device='cuda:0')
percent tensor([0.6342, 0.3658], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.6853, 0.3147], device='cuda:0')
percent tensor([0.7012, 0.2988], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (3851/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (5095/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (6345/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (7596/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (8836/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (10090/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (11339/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (12590/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (13840/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (15084/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (16325/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (17569/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (18811/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (20055/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (21293/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (22549/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (23792/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (25033/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (26279/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (27530/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (28764/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (30010/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (31263/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (32510/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (33753/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (35002/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (36245/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (37488/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (38736/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (39982/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (41235/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (42481/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (43731/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (44983/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (46237/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (47487/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (48699/50000)
# TEST : Loss: (0.4343) | Acc: (88.00%) (8884/10000)
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.5018, 0.4982], device='cuda:0')
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.6628, 0.3372], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.7075, 0.2925], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (3858/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (5098/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (6344/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (7587/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (8840/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (10091/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (11338/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (12584/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (13836/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (15087/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (16337/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (17588/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (18837/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (20079/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (21334/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (22576/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (23820/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (25069/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (26325/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (27573/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (28831/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (30083/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (31328/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (32572/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (33825/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (35078/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (36322/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (37577/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (38827/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (40083/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (41329/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (42582/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (43831/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (45076/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (46331/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (47572/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (48775/50000)
# TEST : Loss: (0.4253) | Acc: (88.00%) (8893/10000)
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5267, 0.4733], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.6917, 0.3083], device='cuda:0')
percent tensor([0.7153, 0.2847], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (98.00%) (5151/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (6390/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (7644/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (8905/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (10152/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (11406/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (12649/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (13896/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (15146/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (16396/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (17644/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (18903/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (20160/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (21409/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (22656/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (23915/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (25166/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (26417/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (27668/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (28916/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (30172/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (31421/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (32675/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (33927/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (35180/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (36428/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (37686/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (38923/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (40176/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (41425/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (42678/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (43919/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (45160/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (46403/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (47645/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (48848/50000)
# TEST : Loss: (0.4205) | Acc: (88.00%) (8898/10000)
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5266, 0.4734], device='cuda:0')
percent tensor([0.6373, 0.3627], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.6922, 0.3078], device='cuda:0')
percent tensor([0.7202, 0.2798], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 199 | Batch_idx: 0 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (3878/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (6374/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (7626/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (8874/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (10125/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (11373/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (12630/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (13888/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (15140/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (16382/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (17636/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (18894/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (20136/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (21383/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (22634/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (23883/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (25132/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (26388/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (27641/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (28896/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (30148/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (31399/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (32663/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (33916/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (35174/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (36422/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (37671/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (38922/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (40175/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (41421/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (42670/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (43929/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (45182/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (46427/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (47678/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (48885/50000)
# TEST : Loss: (0.4163) | Acc: (89.00%) (8913/10000)
percent tensor([0.5340, 0.4660], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5276, 0.4724], device='cuda:0')
percent tensor([0.6365, 0.3635], device='cuda:0')
percent tensor([0.6617, 0.3383], device='cuda:0')
percent tensor([0.6842, 0.3158], device='cuda:0')
percent tensor([0.7251, 0.2749], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (5133/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (6385/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (7623/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (8882/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (10134/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (11381/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (12624/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (13865/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (15109/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (16349/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (17589/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (18833/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (20084/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (21347/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (22591/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (23839/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (25100/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (26343/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (27596/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (28847/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (30082/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (31339/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (32587/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (33830/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (35077/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (36307/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (37544/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (38784/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (40031/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (41263/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (42504/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (43744/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (44983/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (46220/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (47463/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (48659/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_200.pth.tar'
# TEST : Loss: (0.4677) | Acc: (88.00%) (8803/10000)
percent tensor([0.5368, 0.4632], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.6595, 0.3405], device='cuda:0')
percent tensor([0.6838, 0.3162], device='cuda:0')
percent tensor([0.7195, 0.2805], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.7535, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.9044, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.3077, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1498.5118, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.3058, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2292.4814, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4260.0791, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1354.1888, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6295.8545, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11554.2607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3796.0947, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16012.0596, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (3877/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (6372/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (7623/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (10113/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (11353/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (12604/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (13839/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (15084/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (16326/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (17574/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (18820/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (20060/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (21300/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (22554/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (23809/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (25052/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (26297/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (27540/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (28784/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (30018/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (31259/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (32497/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (33749/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (34992/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (36244/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (37501/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (38741/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (39983/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (41227/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (42473/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (43716/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (44952/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (46194/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (47439/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (48647/50000)
# TEST : Loss: (0.4477) | Acc: (88.00%) (8824/10000)
percent tensor([0.5390, 0.4610], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5327, 0.4673], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.6555, 0.3445], device='cuda:0')
percent tensor([0.6868, 0.3132], device='cuda:0')
percent tensor([0.7230, 0.2770], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (6392/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (7650/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (8897/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (10140/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (11388/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (12641/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (13881/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (15129/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (16369/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (17621/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (18868/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (20114/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (21366/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (22622/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (23872/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (25122/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (26365/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (27608/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (28860/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (30098/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (31351/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (32597/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (33846/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (35083/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (36333/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (37585/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (38838/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (40086/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (41342/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (42599/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (43857/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (45103/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (46355/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (47598/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (48803/50000)
# TEST : Loss: (0.4568) | Acc: (88.00%) (8853/10000)
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.6372, 0.3628], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.6949, 0.3051], device='cuda:0')
percent tensor([0.7260, 0.2740], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 203 | Batch_idx: 0 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (3887/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (5133/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (7627/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (8884/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (10130/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (11378/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (12623/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (13878/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (15127/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (16375/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (17629/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (18886/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (20136/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (21384/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (22636/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (23890/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (25134/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (26383/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (27630/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (28878/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (30127/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (31370/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (32617/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (33865/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (35101/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (36348/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (37593/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (38834/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (40091/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (41336/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (42584/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (43834/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (45089/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (46336/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (47583/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (48784/50000)
# TEST : Loss: (0.5054) | Acc: (87.00%) (8726/10000)
percent tensor([0.5407, 0.4593], device='cuda:0')
percent tensor([0.5023, 0.4977], device='cuda:0')
percent tensor([0.5362, 0.4638], device='cuda:0')
percent tensor([0.6405, 0.3595], device='cuda:0')
percent tensor([0.6660, 0.3340], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.7234, 0.2766], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 204 | Batch_idx: 0 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (2632/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (6360/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (7601/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (10096/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (11344/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (12591/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (13846/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (15104/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (16353/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (17604/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (18856/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (20095/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (21330/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (22576/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (23825/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (25078/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (26318/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (27570/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (28832/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (30082/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (31319/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (32557/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (33798/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (35037/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (36289/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (37534/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (38783/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (40039/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (41284/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (42527/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (43772/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (45021/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (46269/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (47519/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (48715/50000)
# TEST : Loss: (0.5204) | Acc: (87.00%) (8705/10000)
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.6458, 0.3542], device='cuda:0')
percent tensor([0.6643, 0.3357], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.7199, 0.2801], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 205 | Batch_idx: 0 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (5126/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (6374/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (7628/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (8867/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (10125/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (11372/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (12633/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (13881/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (15138/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (16396/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (17651/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (18897/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (20159/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (21415/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (22670/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (23932/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (25188/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (26441/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (27691/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (28942/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (30192/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (31442/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (32689/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (33939/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (35190/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (36445/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (37698/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (38953/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (40210/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (41452/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (42712/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (43957/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (45208/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (46450/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (47697/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (48894/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_205.pth.tar'
# TEST : Loss: (0.4724) | Acc: (88.00%) (8863/10000)
percent tensor([0.5427, 0.4573], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5369, 0.4631], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.6614, 0.3386], device='cuda:0')
percent tensor([0.6967, 0.3033], device='cuda:0')
percent tensor([0.7207, 0.2793], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (2632/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (3876/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (97.00%) (5141/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (6396/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (7646/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (8905/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (10149/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (11404/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (12656/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (13915/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (15174/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (16427/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (17683/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (18941/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (20192/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (21441/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (22695/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (23948/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (25197/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (26445/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (27696/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (28953/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (30207/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (31455/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (32710/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (33952/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (35199/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (36446/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (37696/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (38938/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (40183/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (41432/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (42678/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (43934/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (45190/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (46441/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (47681/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (48887/50000)
# TEST : Loss: (0.4525) | Acc: (88.00%) (8841/10000)
percent tensor([0.5424, 0.4576], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5381, 0.4619], device='cuda:0')
percent tensor([0.6427, 0.3573], device='cuda:0')
percent tensor([0.6660, 0.3340], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.7322, 0.2678], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 207 | Batch_idx: 0 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (3898/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (5151/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (7649/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (8903/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (10154/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (11397/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (12645/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (13892/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (15142/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (16383/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (17631/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (18886/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (20139/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (21390/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (22643/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (23888/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (25142/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (26393/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (27638/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (28891/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (30150/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (31408/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (32666/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (33922/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (35179/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36432/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (37673/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (38926/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (40177/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (41426/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (42683/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (43929/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (45174/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (46424/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (47669/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (48878/50000)
# TEST : Loss: (0.4730) | Acc: (87.00%) (8784/10000)
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5398, 0.4602], device='cuda:0')
percent tensor([0.6479, 0.3521], device='cuda:0')
percent tensor([0.6627, 0.3373], device='cuda:0')
percent tensor([0.7060, 0.2940], device='cuda:0')
percent tensor([0.7291, 0.2709], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (5155/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (6410/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (7649/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (8895/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (10151/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (11399/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (12651/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (13911/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (15163/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (16417/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (17660/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (18915/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (20167/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (21421/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (22676/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (23934/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (25187/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (26431/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (27689/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (28942/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (30193/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (31447/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (32700/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (33952/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (35202/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36455/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (37703/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (38953/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (40205/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (41452/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (42712/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (43963/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (45215/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (46473/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (47727/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (48934/50000)
# TEST : Loss: (0.4906) | Acc: (87.00%) (8758/10000)
percent tensor([0.5440, 0.4560], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.6477, 0.3523], device='cuda:0')
percent tensor([0.6687, 0.3313], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.7370, 0.2630], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (3879/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (6391/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (7647/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (8898/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (10149/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (11408/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (12663/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (97.00%) (13917/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (15158/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (16405/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (17652/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (18909/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (20162/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (21424/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (22678/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (23933/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (25193/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (26447/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (27698/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (28950/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (30200/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (31458/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (32714/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (33967/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (35215/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (36464/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (37705/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (38943/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (40197/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (41441/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (42696/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (43943/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (45198/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (46448/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (47703/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (48914/50000)
# TEST : Loss: (0.4400) | Acc: (89.00%) (8901/10000)
percent tensor([0.5443, 0.4557], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5420, 0.4580], device='cuda:0')
percent tensor([0.6514, 0.3486], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.7077, 0.2923], device='cuda:0')
percent tensor([0.7255, 0.2745], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 210 | Batch_idx: 0 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (5163/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (6403/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (97.00%) (7650/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (97.00%) (8901/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (97.00%) (10157/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (11425/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (12687/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (13941/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (15201/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (16460/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (17712/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (18969/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (20222/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (21475/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (22721/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (23977/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (25234/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (26491/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (27744/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (28999/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (30246/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (31499/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (32750/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (34008/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (35261/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (36509/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (37769/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (39024/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (40276/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (41528/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (42778/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (44023/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (97.00%) (45277/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (97.00%) (46533/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (47787/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (48992/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_210.pth.tar'
# TEST : Loss: (0.4626) | Acc: (88.00%) (8829/10000)
percent tensor([0.5472, 0.4528], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.5434, 0.4566], device='cuda:0')
percent tensor([0.6534, 0.3466], device='cuda:0')
percent tensor([0.6680, 0.3320], device='cuda:0')
percent tensor([0.7136, 0.2864], device='cuda:0')
percent tensor([0.7297, 0.2703], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.5513, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(835.3696, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.9424, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1499.9014, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(486.8487, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2306.2595, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4268.1763, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1349.6436, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6334.7627, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11530.3027, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3781.4941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15947.1973, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (97.00%) (2632/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (3879/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (5133/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (6389/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (7642/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (8896/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (10147/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (11403/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (12649/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (13903/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (15163/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (16414/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (17666/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (18919/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (20178/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (21426/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (22674/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (23939/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (25192/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (26449/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (27702/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (28960/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (30218/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (31481/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (32741/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (33990/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (35242/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (36488/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (37743/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (39000/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (40255/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (41508/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (42752/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (43997/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (45244/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (46490/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (47734/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (48931/50000)
# TEST : Loss: (0.4783) | Acc: (87.00%) (8779/10000)
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.5424, 0.4576], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.6679, 0.3321], device='cuda:0')
percent tensor([0.7181, 0.2819], device='cuda:0')
percent tensor([0.7248, 0.2752], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (2648/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (5166/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (6414/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (7670/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (8928/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (11441/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (12699/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (13955/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (15217/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (16483/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (17744/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (18999/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (20255/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (21517/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (22771/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (24035/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (25292/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (26548/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (27805/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (29055/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (30305/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (31561/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (32825/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (34078/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (35339/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (36593/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (37845/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (39098/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (40352/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (41605/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (42850/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (44107/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (45353/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (46601/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (47853/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (49055/50000)
# TEST : Loss: (0.4827) | Acc: (88.00%) (8814/10000)
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5449, 0.4551], device='cuda:0')
percent tensor([0.6548, 0.3452], device='cuda:0')
percent tensor([0.6688, 0.3312], device='cuda:0')
percent tensor([0.7186, 0.2814], device='cuda:0')
percent tensor([0.7332, 0.2668], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0169) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (3908/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (5166/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (6430/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (7682/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (8938/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (10192/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (11446/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (12695/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (13959/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (15212/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (16466/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (17729/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (18987/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (20243/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (21509/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (22769/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (24014/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (25268/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (26522/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (27779/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (29040/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (30289/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (31549/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (32810/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (34055/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (35305/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (36560/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (37812/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (39068/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (40315/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (41572/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (42822/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (44073/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (45324/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (46577/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (47828/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (49036/50000)
# TEST : Loss: (0.4863) | Acc: (88.00%) (8834/10000)
percent tensor([0.5479, 0.4521], device='cuda:0')
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.5460, 0.4540], device='cuda:0')
percent tensor([0.6557, 0.3443], device='cuda:0')
percent tensor([0.6746, 0.3254], device='cuda:0')
percent tensor([0.7166, 0.2834], device='cuda:0')
percent tensor([0.7338, 0.2662], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (6408/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (7667/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (8922/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (10175/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (11425/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (12674/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (13930/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (15185/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (97.00%) (16432/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (17685/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (18940/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (97.00%) (20191/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (21451/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (22715/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (23976/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (25239/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (26498/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (27759/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (29016/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (30273/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (31535/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (32795/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (34050/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (35302/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (36558/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (37815/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (39065/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (40315/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (41570/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (42828/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (44080/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (45333/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (46582/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (47834/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (49038/50000)
# TEST : Loss: (0.4812) | Acc: (88.00%) (8858/10000)
percent tensor([0.5480, 0.4520], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.6515, 0.3485], device='cuda:0')
percent tensor([0.6713, 0.3287], device='cuda:0')
percent tensor([0.7176, 0.2824], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (5162/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (7690/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (8951/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (10207/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (11460/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (12725/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (13984/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (15244/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (16504/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (17762/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (19021/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (20276/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (21531/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (22789/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (24046/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (25309/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (26558/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (27808/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (29060/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (30313/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (31565/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (32827/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (34085/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (35341/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (36585/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (37842/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (39099/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (40356/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (41607/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (42859/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (44116/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (45364/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (46616/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (47862/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (49064/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_215.pth.tar'
# TEST : Loss: (0.5365) | Acc: (87.00%) (8767/10000)
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5489, 0.4511], device='cuda:0')
percent tensor([0.6536, 0.3464], device='cuda:0')
percent tensor([0.6683, 0.3317], device='cuda:0')
percent tensor([0.7186, 0.2814], device='cuda:0')
percent tensor([0.7345, 0.2655], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (3898/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (5153/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (6411/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (7669/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (8932/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (10195/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (11454/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (12714/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (13970/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (15234/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (16494/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (17752/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (19003/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (20261/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (21525/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (22780/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (24040/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (25295/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (26559/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (27822/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (29084/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (30337/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (31583/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (32845/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (34107/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (35363/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (36617/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (37872/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (39128/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (40380/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (41635/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (42894/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (44147/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (45410/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (46664/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (47922/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (49127/50000)
# TEST : Loss: (0.4747) | Acc: (88.00%) (8869/10000)
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.6719, 0.3281], device='cuda:0')
percent tensor([0.7178, 0.2822], device='cuda:0')
percent tensor([0.7334, 0.2666], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (5155/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (6416/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (7669/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (8923/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (10182/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (11443/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (12702/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (13966/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (15225/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (16478/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (17729/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (18983/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (20235/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (21501/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (22768/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (24030/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (25286/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (26543/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (27798/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (29050/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (30310/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (31570/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (32831/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (34089/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (35339/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (36598/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (37857/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (39117/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (40369/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (41627/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (42885/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (44139/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (45392/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (46641/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (47892/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (49099/50000)
# TEST : Loss: (0.4940) | Acc: (88.00%) (8846/10000)
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.6567, 0.3433], device='cuda:0')
percent tensor([0.6768, 0.3232], device='cuda:0')
percent tensor([0.7219, 0.2781], device='cuda:0')
percent tensor([0.7284, 0.2716], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 218 | Batch_idx: 0 |  Loss: (0.0293) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (5167/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (6422/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (7682/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (8939/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (11450/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (12714/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (13972/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (15228/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (16494/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (17757/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (19015/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (20271/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (21537/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (22801/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (24052/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (25314/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (26570/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (27826/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (29088/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (30346/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (31591/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (32849/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (34108/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (35363/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (36621/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (37877/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (39124/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (40372/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (41629/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (42880/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (44137/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (45399/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (46660/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (47921/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (49129/50000)
# TEST : Loss: (0.4921) | Acc: (88.00%) (8823/10000)
percent tensor([0.5514, 0.4486], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.6627, 0.3373], device='cuda:0')
percent tensor([0.6709, 0.3291], device='cuda:0')
percent tensor([0.7222, 0.2778], device='cuda:0')
percent tensor([0.7317, 0.2683], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (3899/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (5153/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (6406/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (7664/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (8924/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (10181/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (11438/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (12704/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (13965/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (15219/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (16488/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (17748/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (19010/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (20270/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (21537/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (22796/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (24063/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (25320/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (26577/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (27838/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (29105/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (30364/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (31627/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (32891/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (34157/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (35416/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (36673/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (37931/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (39193/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (40457/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (41717/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (42973/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (44233/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (45494/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (46750/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (48006/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (49217/50000)
# TEST : Loss: (0.4704) | Acc: (88.00%) (8849/10000)
percent tensor([0.5509, 0.4491], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5535, 0.4465], device='cuda:0')
percent tensor([0.6558, 0.3442], device='cuda:0')
percent tensor([0.6705, 0.3295], device='cuda:0')
percent tensor([0.7223, 0.2777], device='cuda:0')
percent tensor([0.7247, 0.2753], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0288) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (5174/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (6440/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (7699/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (8957/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (10220/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (11481/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (12743/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (14007/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (15264/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (16526/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (17787/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (19047/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (20301/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (21561/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (22814/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (24073/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (25336/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (26597/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (27857/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (29114/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (30378/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (31637/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (32901/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (34162/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (35424/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (36674/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (37929/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (39184/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (40441/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (41702/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (42962/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (44225/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (45486/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (46751/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (48006/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (49214/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_220.pth.tar'
# TEST : Loss: (0.4959) | Acc: (88.00%) (8816/10000)
percent tensor([0.5505, 0.4495], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5533, 0.4467], device='cuda:0')
percent tensor([0.6575, 0.3425], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.7294, 0.2706], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.0692, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(838.0437, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(834.0120, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1500.2694, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.3758, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2317.1714, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4273.2388, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1344.8616, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6367.2319, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11504.0742, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3767.0071, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15882.5977, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (6422/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (7682/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (8939/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (10200/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (11463/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (12730/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (13986/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (15245/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (16506/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (17764/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (19024/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (20289/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (21549/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (22809/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (24061/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (25325/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (26585/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (27849/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (29104/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (30357/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (31615/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (32874/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (34139/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (35396/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (36657/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (37917/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (39178/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (40441/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (41710/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (42970/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (44230/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (45493/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (46740/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (48002/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (49206/50000)
# TEST : Loss: (0.4620) | Acc: (88.00%) (8877/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.7296, 0.2704], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (5178/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (6443/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (7702/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (8963/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (10221/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (11487/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (12750/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (14009/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (15269/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (16530/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (17795/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (19057/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (20323/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (21590/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (22846/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (24102/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (25364/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (26621/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (27866/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (29121/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (30380/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (31635/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (32891/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (34149/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (35410/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (36663/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (37922/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (39184/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (40440/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (41701/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (42957/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (44220/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (45479/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (46729/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (47983/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (49189/50000)
# TEST : Loss: (0.5295) | Acc: (87.00%) (8786/10000)
percent tensor([0.5503, 0.4497], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.5564, 0.4436], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.6751, 0.3249], device='cuda:0')
percent tensor([0.7266, 0.2734], device='cuda:0')
percent tensor([0.7341, 0.2659], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (3912/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (5176/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (6443/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (7695/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (8942/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (10194/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (11455/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (12711/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (13972/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (15232/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (16494/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (17756/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (19018/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (20268/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (21536/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (22801/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (24062/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (25319/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (26573/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (27831/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (29088/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (30347/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (31608/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (32866/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (34126/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (35390/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (36654/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (37916/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (39176/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (40438/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (41697/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (42957/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (44211/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (45462/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (46718/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (47970/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (49188/50000)
# TEST : Loss: (0.4746) | Acc: (88.00%) (8851/10000)
percent tensor([0.5503, 0.4497], device='cuda:0')
percent tensor([0.5059, 0.4941], device='cuda:0')
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.6791, 0.3209], device='cuda:0')
percent tensor([0.7303, 0.2697], device='cuda:0')
percent tensor([0.7334, 0.2666], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (2660/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (3922/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (5192/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (6452/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (7713/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (8971/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (10235/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (11497/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (12759/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (14026/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (15291/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (16550/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (17812/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (19070/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (20337/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (21601/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (22857/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (24123/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (25383/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (26646/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (27912/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (29168/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (30432/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (31693/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (32954/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (34222/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (35489/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (36745/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (38010/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (39262/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (40527/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (41789/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (43048/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (44301/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (45558/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (46805/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (48068/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (49280/50000)
# TEST : Loss: (0.4814) | Acc: (88.00%) (8888/10000)
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5572, 0.4428], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.6749, 0.3251], device='cuda:0')
percent tensor([0.7288, 0.2712], device='cuda:0')
percent tensor([0.7346, 0.2654], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (3927/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (5185/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (7718/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (8978/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (10240/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (11501/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (12762/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (14023/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (16539/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (17799/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (19060/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (20315/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (21577/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (22828/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (24081/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (25340/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (26603/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (27866/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (29123/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (30378/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (31640/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (32903/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (34166/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (35424/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (36692/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (37953/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (39215/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (40482/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (41746/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (43013/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (44268/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (45529/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (46781/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (48043/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (49251/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_225.pth.tar'
# TEST : Loss: (0.4529) | Acc: (89.00%) (8932/10000)
percent tensor([0.5513, 0.4487], device='cuda:0')
percent tensor([0.5065, 0.4935], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.6765, 0.3235], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.7361, 0.2639], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (5170/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (6430/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (7689/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (8947/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (10212/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (11475/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (12736/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (15268/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (16531/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (17792/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (19054/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (20317/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (21570/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (22832/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (24093/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (25359/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (26616/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (27871/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (29131/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (30397/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (31662/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (32917/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (34175/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (35435/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (36691/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (37946/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (39210/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (40468/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (41724/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (42986/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (44241/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (45497/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (46766/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (48019/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (49229/50000)
# TEST : Loss: (0.4900) | Acc: (88.00%) (8860/10000)
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5565, 0.4435], device='cuda:0')
percent tensor([0.6621, 0.3379], device='cuda:0')
percent tensor([0.6791, 0.3209], device='cuda:0')
percent tensor([0.7367, 0.2633], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (3910/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (5174/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (6435/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (7698/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (8956/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (10215/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (11483/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (12748/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (14010/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (15275/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (16539/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (17801/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (19065/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (20319/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (21578/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (22843/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (24105/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (25371/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (26630/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (27900/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (29155/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (30417/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (31683/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (32945/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (34203/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (35466/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (36719/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (37985/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (39242/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (40502/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (41752/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (43015/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (44280/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (45541/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (46806/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (48065/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (49280/50000)
# TEST : Loss: (0.4588) | Acc: (89.00%) (8900/10000)
percent tensor([0.5528, 0.4472], device='cuda:0')
percent tensor([0.5070, 0.4930], device='cuda:0')
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.6615, 0.3385], device='cuda:0')
percent tensor([0.6812, 0.3188], device='cuda:0')
percent tensor([0.7394, 0.2606], device='cuda:0')
percent tensor([0.7398, 0.2602], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (2657/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (5178/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (6449/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (8977/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (10234/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (11502/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (12767/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (14022/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (16550/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (17805/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (19071/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (20334/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (21593/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (22851/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (24117/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (25385/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (26651/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (27911/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (29177/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (30437/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (31701/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (32954/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (34213/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (35468/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (36732/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (37995/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (39259/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (40514/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (41775/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (43040/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (44305/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (45570/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (46829/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (48095/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (49314/50000)
# TEST : Loss: (0.4701) | Acc: (88.00%) (8884/10000)
percent tensor([0.5539, 0.4461], device='cuda:0')
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.6656, 0.3344], device='cuda:0')
percent tensor([0.6812, 0.3188], device='cuda:0')
percent tensor([0.7382, 0.2618], device='cuda:0')
percent tensor([0.7401, 0.2599], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (3912/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (5180/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (8987/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (10250/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (11508/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (12766/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (14029/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (15293/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (16554/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (17805/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (19068/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (20314/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (21579/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (22842/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (24106/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (25367/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (26630/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (27890/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (29154/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (30410/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (31665/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (32927/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (34185/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (35442/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (36707/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (37969/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (39228/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (40487/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (41750/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (43009/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (44262/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (45525/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (46776/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (48032/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (49242/50000)
# TEST : Loss: (0.5065) | Acc: (88.00%) (8845/10000)
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5572, 0.4428], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.6798, 0.3202], device='cuda:0')
percent tensor([0.7373, 0.2627], device='cuda:0')
percent tensor([0.7399, 0.2601], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0211) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (3911/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (6440/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (7697/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (8963/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (10235/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (11503/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (12769/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (14027/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (15293/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (16559/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (17826/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (19089/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (20351/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (21614/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (22880/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (24147/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (25413/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (26675/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (27940/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (29205/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (30478/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (31740/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (33008/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (34271/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (35534/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (36796/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (38060/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (39323/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (40584/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (41853/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (43116/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (44385/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (45650/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (46916/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (48175/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (49386/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_230.pth.tar'
# TEST : Loss: (0.4821) | Acc: (88.00%) (8889/10000)
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5587, 0.4413], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.6820, 0.3180], device='cuda:0')
percent tensor([0.7403, 0.2597], device='cuda:0')
percent tensor([0.7427, 0.2573], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.4788, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(840.3597, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(836.3757, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1500.1350, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(483.8292, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2325.7168, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.8804, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1340.1858, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6394.2378, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11476.7871, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3752.5493, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15818.6875, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (5176/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (6448/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (7706/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (8975/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (10244/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (11510/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (12772/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (14039/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (15300/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (16566/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (17838/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (19097/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (20351/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (21607/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (22867/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (24122/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (25379/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (26638/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (27902/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (29163/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (30434/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (31703/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (32962/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (34229/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (35493/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (36762/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (38017/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (39278/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (40538/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (41796/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (43050/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (44302/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (45560/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (46820/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (48082/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (49291/50000)
# TEST : Loss: (0.4639) | Acc: (89.00%) (8907/10000)
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5595, 0.4405], device='cuda:0')
percent tensor([0.6636, 0.3364], device='cuda:0')
percent tensor([0.6771, 0.3229], device='cuda:0')
percent tensor([0.7357, 0.2643], device='cuda:0')
percent tensor([0.7377, 0.2623], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (5171/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (6438/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (7703/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (8971/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (10236/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (11504/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (12764/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (14018/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (15291/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (16554/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (17812/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (19079/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (20341/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (21605/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (22865/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (24128/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (25392/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (26661/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (27929/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (29189/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (30456/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (31722/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (32981/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (34248/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (35504/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (36766/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (38031/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (39296/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (40562/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (41833/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (43095/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (44363/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (45628/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (46899/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (48163/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (49384/50000)
# TEST : Loss: (0.4796) | Acc: (88.00%) (8897/10000)
percent tensor([0.5547, 0.4453], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5598, 0.4402], device='cuda:0')
percent tensor([0.6633, 0.3367], device='cuda:0')
percent tensor([0.6790, 0.3210], device='cuda:0')
percent tensor([0.7385, 0.2615], device='cuda:0')
percent tensor([0.7426, 0.2574], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (2654/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (3922/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (6450/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (7714/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (8977/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (10242/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (11509/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (12769/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (14031/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (15294/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (16559/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (17820/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (19080/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (20341/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (21604/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (22870/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (24136/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (25400/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (26658/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (27920/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (29184/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (30438/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (31701/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (32962/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (34227/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (35493/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (36747/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (38003/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (39260/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (40522/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (41777/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (43037/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (44301/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (45565/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (46829/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (48092/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (49312/50000)
# TEST : Loss: (0.5003) | Acc: (88.00%) (8857/10000)
percent tensor([0.5543, 0.4457], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.6621, 0.3379], device='cuda:0')
percent tensor([0.6815, 0.3185], device='cuda:0')
percent tensor([0.7410, 0.2590], device='cuda:0')
percent tensor([0.7356, 0.2644], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (2652/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (3915/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (5186/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (6454/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (7715/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (8981/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (10245/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (11507/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (12772/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (14037/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (15297/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (16568/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (17834/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (19099/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (20368/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (21634/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (22904/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (24169/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (25439/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (26704/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (27961/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (29218/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (30486/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (31752/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (33009/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (34271/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (35528/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (36781/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (38039/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (39301/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (40563/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (41815/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (43072/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (44339/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (45602/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (46860/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (48128/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (49341/50000)
# TEST : Loss: (0.4699) | Acc: (89.00%) (8908/10000)
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.6638, 0.3362], device='cuda:0')
percent tensor([0.6789, 0.3211], device='cuda:0')
percent tensor([0.7398, 0.2602], device='cuda:0')
percent tensor([0.7392, 0.2608], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (2654/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (5189/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (6459/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (8976/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (10235/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (11498/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (12760/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (14020/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (15284/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (16545/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (17810/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (19075/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (20339/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (21606/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (22873/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (24130/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (25390/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (26648/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (27911/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (29170/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (30437/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (31698/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (32959/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (34225/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (35484/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (36752/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (38017/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (39280/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (40549/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (41811/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (43064/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (44327/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (45588/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (46844/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (48113/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (49328/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_235.pth.tar'
# TEST : Loss: (0.4829) | Acc: (88.00%) (8887/10000)
percent tensor([0.5547, 0.4453], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.6664, 0.3336], device='cuda:0')
percent tensor([0.6832, 0.3168], device='cuda:0')
percent tensor([0.7428, 0.2572], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0306) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0284) |  Loss2: (0.0000) | Acc: (99.00%) (1400/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (2665/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (3925/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (5184/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (6453/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (7721/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (8989/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (11516/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (12780/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (14039/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (15299/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (16555/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (17818/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (19083/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (20346/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (21612/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (22873/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (24136/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (25401/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (26664/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (27930/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (29199/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (30467/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (31729/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (32993/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (34260/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (35528/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (36792/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (38060/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (39323/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (40585/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (41852/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (43114/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (44375/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (45637/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (46906/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (48171/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (49387/50000)
# TEST : Loss: (0.4698) | Acc: (89.00%) (8917/10000)
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.6672, 0.3328], device='cuda:0')
percent tensor([0.6793, 0.3207], device='cuda:0')
percent tensor([0.7451, 0.2549], device='cuda:0')
percent tensor([0.7378, 0.2622], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (99.00%) (3931/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (99.00%) (5198/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (6470/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (7731/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (99.00%) (8999/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0329) |  Loss2: (0.0000) | Acc: (99.00%) (10268/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (98.00%) (11529/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (12796/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (14064/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (15326/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (16591/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (17858/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (19117/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (20382/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (21650/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (22910/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (24172/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (25435/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (26700/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (27960/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (29225/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (30481/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (31745/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (33011/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (34280/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (35551/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (36817/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (38081/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (39344/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (40602/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (41869/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (43131/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (44396/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (45661/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (46911/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (48176/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (49390/50000)
# TEST : Loss: (0.4630) | Acc: (89.00%) (8958/10000)
percent tensor([0.5571, 0.4429], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5606, 0.4394], device='cuda:0')
percent tensor([0.6696, 0.3304], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.7436, 0.2564], device='cuda:0')
percent tensor([0.7360, 0.2640], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (2654/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (3924/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (5189/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (7721/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (8991/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (10258/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (11526/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (12797/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (14063/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (15330/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (16594/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (17862/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (19126/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (20391/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (21657/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (22924/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (24189/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (25459/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (26721/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (27979/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (29242/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (30499/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (31768/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (33031/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (34301/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (35570/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (36836/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (38096/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (39361/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (40618/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (41874/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (43143/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (44401/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (45654/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (46911/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (48177/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (49391/50000)
# TEST : Loss: (0.5026) | Acc: (88.00%) (8860/10000)
percent tensor([0.5577, 0.4423], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5596, 0.4404], device='cuda:0')
percent tensor([0.6685, 0.3315], device='cuda:0')
percent tensor([0.6818, 0.3182], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
percent tensor([0.7372, 0.2628], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0102) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (2657/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (3923/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (99.00%) (6463/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (99.00%) (7734/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (8992/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (10244/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (11510/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (12778/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (14046/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (15312/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (16584/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (17844/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (19111/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (20376/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (21641/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (22905/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (24169/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (25439/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (26701/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (27962/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (29232/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (30496/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (31768/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (33033/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (34297/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (35556/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (36817/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (38084/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (39341/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (40597/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (41865/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (43132/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (44398/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (45664/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (46919/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (48191/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (49410/50000)
# TEST : Loss: (0.4889) | Acc: (89.00%) (8903/10000)
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.6668, 0.3332], device='cuda:0')
percent tensor([0.6835, 0.3165], device='cuda:0')
percent tensor([0.7444, 0.2556], device='cuda:0')
percent tensor([0.7363, 0.2637], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.8277, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(841.7508, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(838.0622, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1499.8058, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(482.4336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2332.3833, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4278.3198, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1335.8317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6415.4209, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11451.0576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3739.5154, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15761.7451, device='cuda:0', grad_fn=<NormBackward0>)
2 hours 31 mins 7 secs for training