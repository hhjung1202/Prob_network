Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3144) |  Loss2: (0.0000) | Acc: (10.00%) (14/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3042) |  Loss2: (0.0000) | Acc: (10.00%) (154/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.3009) |  Loss2: (0.0000) | Acc: (11.00%) (300/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2941) |  Loss2: (0.0000) | Acc: (12.00%) (492/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2883) |  Loss2: (0.0000) | Acc: (13.00%) (730/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2817) |  Loss2: (0.0000) | Acc: (15.00%) (986/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2753) |  Loss2: (0.0000) | Acc: (15.00%) (1243/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2679) |  Loss2: (0.0000) | Acc: (16.00%) (1522/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2604) |  Loss2: (0.0000) | Acc: (17.00%) (1794/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2521) |  Loss2: (0.0000) | Acc: (17.00%) (2065/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2425) |  Loss2: (0.0000) | Acc: (18.00%) (2361/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2324) |  Loss2: (0.0000) | Acc: (18.00%) (2652/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2217) |  Loss2: (0.0000) | Acc: (19.00%) (2967/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2113) |  Loss2: (0.0000) | Acc: (19.00%) (3254/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2029) |  Loss2: (0.0000) | Acc: (19.00%) (3562/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1940) |  Loss2: (0.0000) | Acc: (19.00%) (3864/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1850) |  Loss2: (0.0000) | Acc: (20.00%) (4200/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1774) |  Loss2: (0.0000) | Acc: (20.00%) (4527/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1685) |  Loss2: (0.0000) | Acc: (21.00%) (4893/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1606) |  Loss2: (0.0000) | Acc: (21.00%) (5233/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1526) |  Loss2: (0.0000) | Acc: (21.00%) (5568/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1451) |  Loss2: (0.0000) | Acc: (21.00%) (5888/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1367) |  Loss2: (0.0000) | Acc: (22.00%) (6237/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1289) |  Loss2: (0.0000) | Acc: (22.00%) (6605/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1227) |  Loss2: (0.0000) | Acc: (22.00%) (6961/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1150) |  Loss2: (0.0000) | Acc: (22.00%) (7326/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1089) |  Loss2: (0.0000) | Acc: (23.00%) (7688/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1029) |  Loss2: (0.0000) | Acc: (23.00%) (8033/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0965) |  Loss2: (0.0000) | Acc: (23.00%) (8420/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0893) |  Loss2: (0.0000) | Acc: (23.00%) (8814/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (23.00%) (9234/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0768) |  Loss2: (0.0000) | Acc: (24.00%) (9592/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0717) |  Loss2: (0.0000) | Acc: (24.00%) (9965/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0657) |  Loss2: (0.0000) | Acc: (24.00%) (10366/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0600) |  Loss2: (0.0000) | Acc: (24.00%) (10766/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0546) |  Loss2: (0.0000) | Acc: (24.00%) (11170/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0493) |  Loss2: (0.0000) | Acc: (25.00%) (11562/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0444) |  Loss2: (0.0000) | Acc: (25.00%) (11971/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0388) |  Loss2: (0.0000) | Acc: (25.00%) (12411/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0342) |  Loss2: (0.0000) | Acc: (25.00%) (12790/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.8129) | Acc: (32.00%) (3253/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(166.9456, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(770.1288, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.8491, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.5629, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(520.1837, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2168.2239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4339.7964, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.2157, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6142.4233, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12287.7715, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4086.3015, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17370.7070, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.7972) |  Loss2: (0.0000) | Acc: (33.00%) (43/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8168) |  Loss2: (0.0000) | Acc: (32.00%) (462/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8185) |  Loss2: (0.0000) | Acc: (32.00%) (882/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8196) |  Loss2: (0.0000) | Acc: (33.00%) (1318/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8134) |  Loss2: (0.0000) | Acc: (33.00%) (1738/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8094) |  Loss2: (0.0000) | Acc: (33.00%) (2195/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8085) |  Loss2: (0.0000) | Acc: (33.00%) (2624/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.8035) |  Loss2: (0.0000) | Acc: (33.00%) (3064/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.8002) |  Loss2: (0.0000) | Acc: (33.00%) (3522/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7962) |  Loss2: (0.0000) | Acc: (34.00%) (4002/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7951) |  Loss2: (0.0000) | Acc: (34.00%) (4449/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7916) |  Loss2: (0.0000) | Acc: (34.00%) (4878/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7908) |  Loss2: (0.0000) | Acc: (34.00%) (5303/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7870) |  Loss2: (0.0000) | Acc: (34.00%) (5787/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7839) |  Loss2: (0.0000) | Acc: (34.00%) (6241/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7805) |  Loss2: (0.0000) | Acc: (34.00%) (6692/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7780) |  Loss2: (0.0000) | Acc: (34.00%) (7151/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7763) |  Loss2: (0.0000) | Acc: (34.00%) (7610/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7727) |  Loss2: (0.0000) | Acc: (34.00%) (8091/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7700) |  Loss2: (0.0000) | Acc: (35.00%) (8562/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7663) |  Loss2: (0.0000) | Acc: (35.00%) (9028/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7632) |  Loss2: (0.0000) | Acc: (35.00%) (9501/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7611) |  Loss2: (0.0000) | Acc: (35.00%) (9982/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7576) |  Loss2: (0.0000) | Acc: (35.00%) (10456/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7549) |  Loss2: (0.0000) | Acc: (35.00%) (10905/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7539) |  Loss2: (0.0000) | Acc: (35.00%) (11375/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7528) |  Loss2: (0.0000) | Acc: (35.00%) (11804/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7509) |  Loss2: (0.0000) | Acc: (35.00%) (12276/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7478) |  Loss2: (0.0000) | Acc: (35.00%) (12748/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7451) |  Loss2: (0.0000) | Acc: (35.00%) (13228/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7423) |  Loss2: (0.0000) | Acc: (35.00%) (13706/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7390) |  Loss2: (0.0000) | Acc: (35.00%) (14210/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7365) |  Loss2: (0.0000) | Acc: (35.00%) (14715/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7339) |  Loss2: (0.0000) | Acc: (35.00%) (15198/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7319) |  Loss2: (0.0000) | Acc: (35.00%) (15668/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7296) |  Loss2: (0.0000) | Acc: (35.00%) (16163/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7259) |  Loss2: (0.0000) | Acc: (36.00%) (16696/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7238) |  Loss2: (0.0000) | Acc: (36.00%) (17189/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7213) |  Loss2: (0.0000) | Acc: (36.00%) (17687/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7193) |  Loss2: (0.0000) | Acc: (36.00%) (18168/50000)
# TEST : Loss: (1.6587) | Acc: (37.00%) (3741/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5636) |  Loss2: (0.0000) | Acc: (40.00%) (52/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.6122) |  Loss2: (0.0000) | Acc: (39.00%) (557/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.6212) |  Loss2: (0.0000) | Acc: (38.00%) (1026/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.6207) |  Loss2: (0.0000) | Acc: (38.00%) (1511/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.6151) |  Loss2: (0.0000) | Acc: (38.00%) (2038/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.6171) |  Loss2: (0.0000) | Acc: (38.00%) (2544/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.6164) |  Loss2: (0.0000) | Acc: (39.00%) (3050/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.6105) |  Loss2: (0.0000) | Acc: (39.00%) (3588/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.6065) |  Loss2: (0.0000) | Acc: (39.00%) (4111/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.6057) |  Loss2: (0.0000) | Acc: (39.00%) (4629/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.6062) |  Loss2: (0.0000) | Acc: (39.00%) (5138/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.6027) |  Loss2: (0.0000) | Acc: (40.00%) (5693/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.6004) |  Loss2: (0.0000) | Acc: (40.00%) (6251/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5995) |  Loss2: (0.0000) | Acc: (40.00%) (6781/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5982) |  Loss2: (0.0000) | Acc: (40.00%) (7296/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5961) |  Loss2: (0.0000) | Acc: (40.00%) (7829/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5944) |  Loss2: (0.0000) | Acc: (40.00%) (8389/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5930) |  Loss2: (0.0000) | Acc: (40.00%) (8934/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5888) |  Loss2: (0.0000) | Acc: (40.00%) (9496/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5856) |  Loss2: (0.0000) | Acc: (41.00%) (10051/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5826) |  Loss2: (0.0000) | Acc: (41.00%) (10629/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5814) |  Loss2: (0.0000) | Acc: (41.00%) (11187/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5802) |  Loss2: (0.0000) | Acc: (41.00%) (11720/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5785) |  Loss2: (0.0000) | Acc: (41.00%) (12289/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5770) |  Loss2: (0.0000) | Acc: (41.00%) (12815/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5741) |  Loss2: (0.0000) | Acc: (41.00%) (13393/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5706) |  Loss2: (0.0000) | Acc: (41.00%) (14003/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5671) |  Loss2: (0.0000) | Acc: (42.00%) (14583/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5645) |  Loss2: (0.0000) | Acc: (42.00%) (15161/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5626) |  Loss2: (0.0000) | Acc: (42.00%) (15723/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5622) |  Loss2: (0.0000) | Acc: (42.00%) (16267/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5604) |  Loss2: (0.0000) | Acc: (42.00%) (16833/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5567) |  Loss2: (0.0000) | Acc: (42.00%) (17455/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5545) |  Loss2: (0.0000) | Acc: (42.00%) (18055/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5518) |  Loss2: (0.0000) | Acc: (42.00%) (18646/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5489) |  Loss2: (0.0000) | Acc: (42.00%) (19246/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5465) |  Loss2: (0.0000) | Acc: (42.00%) (19848/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5441) |  Loss2: (0.0000) | Acc: (43.00%) (20465/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5416) |  Loss2: (0.0000) | Acc: (43.00%) (21044/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5393) |  Loss2: (0.0000) | Acc: (43.00%) (21613/50000)
# TEST : Loss: (1.5044) | Acc: (43.00%) (4324/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.4334) |  Loss2: (0.0000) | Acc: (43.00%) (56/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4589) |  Loss2: (0.0000) | Acc: (48.00%) (680/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4486) |  Loss2: (0.0000) | Acc: (48.00%) (1302/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4337) |  Loss2: (0.0000) | Acc: (48.00%) (1915/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4303) |  Loss2: (0.0000) | Acc: (48.00%) (2521/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4347) |  Loss2: (0.0000) | Acc: (47.00%) (3123/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4372) |  Loss2: (0.0000) | Acc: (47.00%) (3738/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4429) |  Loss2: (0.0000) | Acc: (47.00%) (4334/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4400) |  Loss2: (0.0000) | Acc: (47.00%) (4951/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4354) |  Loss2: (0.0000) | Acc: (47.00%) (5565/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4323) |  Loss2: (0.0000) | Acc: (47.00%) (6195/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4324) |  Loss2: (0.0000) | Acc: (47.00%) (6808/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4302) |  Loss2: (0.0000) | Acc: (47.00%) (7414/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4278) |  Loss2: (0.0000) | Acc: (47.00%) (8045/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4261) |  Loss2: (0.0000) | Acc: (48.00%) (8687/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4197) |  Loss2: (0.0000) | Acc: (48.00%) (9330/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.4181) |  Loss2: (0.0000) | Acc: (48.00%) (9957/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.4170) |  Loss2: (0.0000) | Acc: (48.00%) (10572/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.4156) |  Loss2: (0.0000) | Acc: (48.00%) (11229/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.4156) |  Loss2: (0.0000) | Acc: (48.00%) (11850/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.4130) |  Loss2: (0.0000) | Acc: (48.00%) (12474/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.4105) |  Loss2: (0.0000) | Acc: (48.00%) (13111/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.4066) |  Loss2: (0.0000) | Acc: (48.00%) (13810/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.4045) |  Loss2: (0.0000) | Acc: (48.00%) (14471/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.4047) |  Loss2: (0.0000) | Acc: (48.00%) (15091/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.4025) |  Loss2: (0.0000) | Acc: (48.00%) (15736/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3998) |  Loss2: (0.0000) | Acc: (49.00%) (16423/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3978) |  Loss2: (0.0000) | Acc: (49.00%) (17060/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3973) |  Loss2: (0.0000) | Acc: (49.00%) (17697/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3953) |  Loss2: (0.0000) | Acc: (49.00%) (18348/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (49.00%) (19000/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3934) |  Loss2: (0.0000) | Acc: (49.00%) (19625/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3919) |  Loss2: (0.0000) | Acc: (49.00%) (20284/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3916) |  Loss2: (0.0000) | Acc: (49.00%) (20926/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3900) |  Loss2: (0.0000) | Acc: (49.00%) (21576/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3892) |  Loss2: (0.0000) | Acc: (49.00%) (22214/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3876) |  Loss2: (0.0000) | Acc: (49.00%) (22875/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3876) |  Loss2: (0.0000) | Acc: (49.00%) (23516/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3853) |  Loss2: (0.0000) | Acc: (49.00%) (24184/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3828) |  Loss2: (0.0000) | Acc: (49.00%) (24837/50000)
# TEST : Loss: (1.3552) | Acc: (50.00%) (5016/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.4460) |  Loss2: (0.0000) | Acc: (49.00%) (63/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3107) |  Loss2: (0.0000) | Acc: (52.00%) (736/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3122) |  Loss2: (0.0000) | Acc: (52.00%) (1404/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3083) |  Loss2: (0.0000) | Acc: (52.00%) (2086/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3015) |  Loss2: (0.0000) | Acc: (53.00%) (2814/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3038) |  Loss2: (0.0000) | Acc: (53.00%) (3481/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2993) |  Loss2: (0.0000) | Acc: (53.00%) (4164/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2957) |  Loss2: (0.0000) | Acc: (53.00%) (4866/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2953) |  Loss2: (0.0000) | Acc: (53.00%) (5560/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.3014) |  Loss2: (0.0000) | Acc: (53.00%) (6218/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2974) |  Loss2: (0.0000) | Acc: (53.00%) (6913/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2973) |  Loss2: (0.0000) | Acc: (53.00%) (7583/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2970) |  Loss2: (0.0000) | Acc: (53.00%) (8255/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2961) |  Loss2: (0.0000) | Acc: (53.00%) (8949/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2929) |  Loss2: (0.0000) | Acc: (53.00%) (9647/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2929) |  Loss2: (0.0000) | Acc: (53.00%) (10352/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2902) |  Loss2: (0.0000) | Acc: (53.00%) (11051/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2885) |  Loss2: (0.0000) | Acc: (53.00%) (11753/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2865) |  Loss2: (0.0000) | Acc: (53.00%) (12460/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (13171/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (53.00%) (13832/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2847) |  Loss2: (0.0000) | Acc: (53.00%) (14536/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2859) |  Loss2: (0.0000) | Acc: (53.00%) (15196/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2872) |  Loss2: (0.0000) | Acc: (53.00%) (15860/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2835) |  Loss2: (0.0000) | Acc: (53.00%) (16603/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2820) |  Loss2: (0.0000) | Acc: (53.00%) (17302/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2796) |  Loss2: (0.0000) | Acc: (53.00%) (18029/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2773) |  Loss2: (0.0000) | Acc: (53.00%) (18731/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2763) |  Loss2: (0.0000) | Acc: (54.00%) (19444/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2740) |  Loss2: (0.0000) | Acc: (54.00%) (20158/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2738) |  Loss2: (0.0000) | Acc: (54.00%) (20865/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2722) |  Loss2: (0.0000) | Acc: (54.00%) (21587/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2720) |  Loss2: (0.0000) | Acc: (54.00%) (22304/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2717) |  Loss2: (0.0000) | Acc: (54.00%) (23019/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2712) |  Loss2: (0.0000) | Acc: (54.00%) (23704/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2696) |  Loss2: (0.0000) | Acc: (54.00%) (24422/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2691) |  Loss2: (0.0000) | Acc: (54.00%) (25110/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2690) |  Loss2: (0.0000) | Acc: (54.00%) (25817/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2680) |  Loss2: (0.0000) | Acc: (54.00%) (26522/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2664) |  Loss2: (0.0000) | Acc: (54.00%) (27226/50000)
# TEST : Loss: (1.2478) | Acc: (54.00%) (5444/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.1674) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.3283) |  Loss2: (0.0000) | Acc: (51.00%) (728/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3692) |  Loss2: (0.0000) | Acc: (50.00%) (1362/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.4170) |  Loss2: (0.0000) | Acc: (48.00%) (1933/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.4353) |  Loss2: (0.0000) | Acc: (47.00%) (2506/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.4355) |  Loss2: (0.0000) | Acc: (47.00%) (3119/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (48.00%) (3752/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.4215) |  Loss2: (0.0000) | Acc: (48.00%) (4395/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.4151) |  Loss2: (0.0000) | Acc: (48.00%) (5041/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.4089) |  Loss2: (0.0000) | Acc: (48.00%) (5665/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.4052) |  Loss2: (0.0000) | Acc: (48.00%) (6312/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3984) |  Loss2: (0.0000) | Acc: (49.00%) (6981/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3956) |  Loss2: (0.0000) | Acc: (49.00%) (7630/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3884) |  Loss2: (0.0000) | Acc: (49.00%) (8319/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3802) |  Loss2: (0.0000) | Acc: (49.00%) (9009/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3741) |  Loss2: (0.0000) | Acc: (50.00%) (9703/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3669) |  Loss2: (0.0000) | Acc: (50.00%) (10418/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3613) |  Loss2: (0.0000) | Acc: (50.00%) (11112/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3556) |  Loss2: (0.0000) | Acc: (51.00%) (11848/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3529) |  Loss2: (0.0000) | Acc: (51.00%) (12537/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3494) |  Loss2: (0.0000) | Acc: (51.00%) (13210/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3429) |  Loss2: (0.0000) | Acc: (51.00%) (13912/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3388) |  Loss2: (0.0000) | Acc: (51.00%) (14619/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3350) |  Loss2: (0.0000) | Acc: (51.00%) (15309/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3311) |  Loss2: (0.0000) | Acc: (51.00%) (16005/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3289) |  Loss2: (0.0000) | Acc: (51.00%) (16685/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3247) |  Loss2: (0.0000) | Acc: (52.00%) (17393/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3211) |  Loss2: (0.0000) | Acc: (52.00%) (18104/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3187) |  Loss2: (0.0000) | Acc: (52.00%) (18793/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3155) |  Loss2: (0.0000) | Acc: (52.00%) (19488/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3137) |  Loss2: (0.0000) | Acc: (52.00%) (20171/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3127) |  Loss2: (0.0000) | Acc: (52.00%) (20859/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3111) |  Loss2: (0.0000) | Acc: (52.00%) (21558/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3092) |  Loss2: (0.0000) | Acc: (52.00%) (22251/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3062) |  Loss2: (0.0000) | Acc: (52.00%) (22973/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3044) |  Loss2: (0.0000) | Acc: (52.00%) (23667/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3035) |  Loss2: (0.0000) | Acc: (52.00%) (24359/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3019) |  Loss2: (0.0000) | Acc: (52.00%) (25048/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3004) |  Loss2: (0.0000) | Acc: (52.00%) (25752/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2987) |  Loss2: (0.0000) | Acc: (52.00%) (26436/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2225) | Acc: (55.00%) (5542/10000)
percent tensor([0.4825, 0.5175], device='cuda:0')
percent tensor([0.4308, 0.5692], device='cuda:0')
percent tensor([0.4876, 0.5124], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6260, 0.3740], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.1494) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2698) |  Loss2: (0.0000) | Acc: (54.00%) (761/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (56.00%) (1508/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2266) |  Loss2: (0.0000) | Acc: (56.00%) (2231/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (55.00%) (2926/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2318) |  Loss2: (0.0000) | Acc: (56.00%) (3657/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2357) |  Loss2: (0.0000) | Acc: (55.00%) (4372/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2326) |  Loss2: (0.0000) | Acc: (55.00%) (5086/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2353) |  Loss2: (0.0000) | Acc: (55.00%) (5781/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2367) |  Loss2: (0.0000) | Acc: (55.00%) (6463/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2355) |  Loss2: (0.0000) | Acc: (55.00%) (7157/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2361) |  Loss2: (0.0000) | Acc: (55.00%) (7841/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2317) |  Loss2: (0.0000) | Acc: (55.00%) (8574/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2298) |  Loss2: (0.0000) | Acc: (55.00%) (9289/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2304) |  Loss2: (0.0000) | Acc: (55.00%) (9986/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2259) |  Loss2: (0.0000) | Acc: (55.00%) (10719/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2256) |  Loss2: (0.0000) | Acc: (55.00%) (11402/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2241) |  Loss2: (0.0000) | Acc: (55.00%) (12109/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2225) |  Loss2: (0.0000) | Acc: (55.00%) (12831/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2212) |  Loss2: (0.0000) | Acc: (55.00%) (13543/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2198) |  Loss2: (0.0000) | Acc: (55.00%) (14280/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2179) |  Loss2: (0.0000) | Acc: (55.00%) (15021/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (55.00%) (15741/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2170) |  Loss2: (0.0000) | Acc: (55.00%) (16461/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (55.00%) (17170/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2149) |  Loss2: (0.0000) | Acc: (55.00%) (17894/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2148) |  Loss2: (0.0000) | Acc: (55.00%) (18612/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2131) |  Loss2: (0.0000) | Acc: (55.00%) (19343/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2142) |  Loss2: (0.0000) | Acc: (55.00%) (20072/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2151) |  Loss2: (0.0000) | Acc: (55.00%) (20769/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2153) |  Loss2: (0.0000) | Acc: (55.00%) (21483/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2154) |  Loss2: (0.0000) | Acc: (55.00%) (22202/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2150) |  Loss2: (0.0000) | Acc: (55.00%) (22923/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2137) |  Loss2: (0.0000) | Acc: (55.00%) (23655/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2125) |  Loss2: (0.0000) | Acc: (55.00%) (24385/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2110) |  Loss2: (0.0000) | Acc: (55.00%) (25113/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2098) |  Loss2: (0.0000) | Acc: (55.00%) (25835/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2097) |  Loss2: (0.0000) | Acc: (55.00%) (26551/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (27273/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2115) |  Loss2: (0.0000) | Acc: (55.00%) (27959/50000)
# TEST : Loss: (1.2104) | Acc: (56.00%) (5663/10000)
percent tensor([0.4842, 0.5158], device='cuda:0')
percent tensor([0.4345, 0.5655], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.4927, 0.5073], device='cuda:0')
percent tensor([0.5190, 0.4810], device='cuda:0')
percent tensor([0.5263, 0.4737], device='cuda:0')
percent tensor([0.5378, 0.4622], device='cuda:0')
percent tensor([0.7548, 0.2452], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1469) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2092) |  Loss2: (0.0000) | Acc: (55.00%) (777/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2277) |  Loss2: (0.0000) | Acc: (54.00%) (1469/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2087) |  Loss2: (0.0000) | Acc: (55.00%) (2194/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2085) |  Loss2: (0.0000) | Acc: (55.00%) (2907/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1998) |  Loss2: (0.0000) | Acc: (55.00%) (3641/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.1970) |  Loss2: (0.0000) | Acc: (55.00%) (4367/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2013) |  Loss2: (0.0000) | Acc: (55.00%) (5063/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (55.00%) (5778/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (55.00%) (6516/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (7268/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1943) |  Loss2: (0.0000) | Acc: (56.00%) (7978/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1951) |  Loss2: (0.0000) | Acc: (56.00%) (8681/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.1967) |  Loss2: (0.0000) | Acc: (55.00%) (9381/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.1977) |  Loss2: (0.0000) | Acc: (55.00%) (10099/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (10850/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.1952) |  Loss2: (0.0000) | Acc: (56.00%) (11569/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.1954) |  Loss2: (0.0000) | Acc: (56.00%) (12302/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1956) |  Loss2: (0.0000) | Acc: (56.00%) (13020/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (13752/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1962) |  Loss2: (0.0000) | Acc: (56.00%) (14463/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1942) |  Loss2: (0.0000) | Acc: (56.00%) (15196/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1934) |  Loss2: (0.0000) | Acc: (56.00%) (15946/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (16655/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1933) |  Loss2: (0.0000) | Acc: (56.00%) (17387/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1932) |  Loss2: (0.0000) | Acc: (56.00%) (18125/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (18844/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (19577/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1938) |  Loss2: (0.0000) | Acc: (56.00%) (20302/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (21025/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1952) |  Loss2: (0.0000) | Acc: (56.00%) (21739/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1953) |  Loss2: (0.0000) | Acc: (56.00%) (22453/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (23198/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1942) |  Loss2: (0.0000) | Acc: (56.00%) (23901/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1946) |  Loss2: (0.0000) | Acc: (56.00%) (24593/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1946) |  Loss2: (0.0000) | Acc: (56.00%) (25307/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1939) |  Loss2: (0.0000) | Acc: (56.00%) (26035/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (56.00%) (26768/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1930) |  Loss2: (0.0000) | Acc: (56.00%) (27517/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1925) |  Loss2: (0.0000) | Acc: (56.00%) (28210/50000)
# TEST : Loss: (1.1995) | Acc: (56.00%) (5669/10000)
percent tensor([0.4857, 0.5143], device='cuda:0')
percent tensor([0.4407, 0.5593], device='cuda:0')
percent tensor([0.4807, 0.5193], device='cuda:0')
percent tensor([0.4934, 0.5066], device='cuda:0')
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.7981, 0.2019], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1600) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.2240) |  Loss2: (0.0000) | Acc: (55.00%) (786/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.2177) |  Loss2: (0.0000) | Acc: (55.00%) (1503/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.2221) |  Loss2: (0.0000) | Acc: (55.00%) (2218/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.2172) |  Loss2: (0.0000) | Acc: (56.00%) (2944/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.2184) |  Loss2: (0.0000) | Acc: (56.00%) (3663/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.2113) |  Loss2: (0.0000) | Acc: (56.00%) (4387/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (56.00%) (5118/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.2015) |  Loss2: (0.0000) | Acc: (56.00%) (5839/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1988) |  Loss2: (0.0000) | Acc: (56.00%) (6579/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.2016) |  Loss2: (0.0000) | Acc: (56.00%) (7281/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.2002) |  Loss2: (0.0000) | Acc: (56.00%) (8030/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.2001) |  Loss2: (0.0000) | Acc: (56.00%) (8756/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1984) |  Loss2: (0.0000) | Acc: (56.00%) (9495/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (56.00%) (10239/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1913) |  Loss2: (0.0000) | Acc: (56.00%) (10988/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1910) |  Loss2: (0.0000) | Acc: (56.00%) (11688/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1902) |  Loss2: (0.0000) | Acc: (56.00%) (12427/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1908) |  Loss2: (0.0000) | Acc: (56.00%) (13139/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1902) |  Loss2: (0.0000) | Acc: (56.00%) (13879/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1908) |  Loss2: (0.0000) | Acc: (56.00%) (14613/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1929) |  Loss2: (0.0000) | Acc: (56.00%) (15309/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1909) |  Loss2: (0.0000) | Acc: (56.00%) (16052/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1883) |  Loss2: (0.0000) | Acc: (56.00%) (16803/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (17496/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1901) |  Loss2: (0.0000) | Acc: (56.00%) (18241/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (56.00%) (18966/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1901) |  Loss2: (0.0000) | Acc: (56.00%) (19674/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1886) |  Loss2: (0.0000) | Acc: (56.00%) (20424/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (21154/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1877) |  Loss2: (0.0000) | Acc: (56.00%) (21857/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1889) |  Loss2: (0.0000) | Acc: (56.00%) (22561/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1885) |  Loss2: (0.0000) | Acc: (56.00%) (23290/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1868) |  Loss2: (0.0000) | Acc: (56.00%) (24049/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (24788/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1859) |  Loss2: (0.0000) | Acc: (56.00%) (25524/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (56.00%) (26241/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1855) |  Loss2: (0.0000) | Acc: (56.00%) (26976/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1857) |  Loss2: (0.0000) | Acc: (56.00%) (27684/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1846) |  Loss2: (0.0000) | Acc: (56.00%) (28397/50000)
# TEST : Loss: (1.1968) | Acc: (56.00%) (5685/10000)
percent tensor([0.4872, 0.5128], device='cuda:0')
percent tensor([0.4431, 0.5569], device='cuda:0')
percent tensor([0.4815, 0.5185], device='cuda:0')
percent tensor([0.4951, 0.5049], device='cuda:0')
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.8216, 0.1784], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1472) |  Loss2: (0.0000) | Acc: (55.00%) (71/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.2019) |  Loss2: (0.0000) | Acc: (55.00%) (783/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1835) |  Loss2: (0.0000) | Acc: (56.00%) (1524/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1833) |  Loss2: (0.0000) | Acc: (57.00%) (2263/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1861) |  Loss2: (0.0000) | Acc: (56.00%) (2984/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1930) |  Loss2: (0.0000) | Acc: (56.00%) (3691/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1945) |  Loss2: (0.0000) | Acc: (56.00%) (4402/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (5111/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (5850/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1910) |  Loss2: (0.0000) | Acc: (56.00%) (6548/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1919) |  Loss2: (0.0000) | Acc: (56.00%) (7265/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1937) |  Loss2: (0.0000) | Acc: (56.00%) (7965/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1899) |  Loss2: (0.0000) | Acc: (56.00%) (8715/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1911) |  Loss2: (0.0000) | Acc: (56.00%) (9435/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (10173/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1906) |  Loss2: (0.0000) | Acc: (56.00%) (10880/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (56.00%) (11614/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1885) |  Loss2: (0.0000) | Acc: (56.00%) (12337/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (56.00%) (13090/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1872) |  Loss2: (0.0000) | Acc: (56.00%) (13797/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (14505/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1869) |  Loss2: (0.0000) | Acc: (56.00%) (15231/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1876) |  Loss2: (0.0000) | Acc: (56.00%) (15960/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (16692/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1881) |  Loss2: (0.0000) | Acc: (56.00%) (17405/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1873) |  Loss2: (0.0000) | Acc: (56.00%) (18155/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1878) |  Loss2: (0.0000) | Acc: (56.00%) (18862/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1866) |  Loss2: (0.0000) | Acc: (56.00%) (19618/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1877) |  Loss2: (0.0000) | Acc: (56.00%) (20327/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1883) |  Loss2: (0.0000) | Acc: (56.00%) (21039/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1867) |  Loss2: (0.0000) | Acc: (56.00%) (21778/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1857) |  Loss2: (0.0000) | Acc: (56.00%) (22525/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1845) |  Loss2: (0.0000) | Acc: (56.00%) (23251/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1838) |  Loss2: (0.0000) | Acc: (56.00%) (24005/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1821) |  Loss2: (0.0000) | Acc: (56.00%) (24755/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1817) |  Loss2: (0.0000) | Acc: (56.00%) (25499/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1807) |  Loss2: (0.0000) | Acc: (56.00%) (26227/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1814) |  Loss2: (0.0000) | Acc: (56.00%) (26945/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1803) |  Loss2: (0.0000) | Acc: (56.00%) (27685/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1797) |  Loss2: (0.0000) | Acc: (56.00%) (28392/50000)
# TEST : Loss: (1.1907) | Acc: (57.00%) (5726/10000)
percent tensor([0.4881, 0.5119], device='cuda:0')
percent tensor([0.4475, 0.5525], device='cuda:0')
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4974, 0.5026], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.5315, 0.4685], device='cuda:0')
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.8368, 0.1632], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1070) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.2061) |  Loss2: (0.0000) | Acc: (56.00%) (790/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1900) |  Loss2: (0.0000) | Acc: (55.00%) (1505/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1843) |  Loss2: (0.0000) | Acc: (56.00%) (2250/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1819) |  Loss2: (0.0000) | Acc: (56.00%) (2984/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1915) |  Loss2: (0.0000) | Acc: (56.00%) (3688/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1828) |  Loss2: (0.0000) | Acc: (56.00%) (4433/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1841) |  Loss2: (0.0000) | Acc: (56.00%) (5130/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (56.00%) (5858/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1817) |  Loss2: (0.0000) | Acc: (56.00%) (6583/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1802) |  Loss2: (0.0000) | Acc: (56.00%) (7284/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1796) |  Loss2: (0.0000) | Acc: (56.00%) (8030/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1779) |  Loss2: (0.0000) | Acc: (56.00%) (8754/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1750) |  Loss2: (0.0000) | Acc: (56.00%) (9493/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1770) |  Loss2: (0.0000) | Acc: (56.00%) (10227/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1766) |  Loss2: (0.0000) | Acc: (56.00%) (10971/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1755) |  Loss2: (0.0000) | Acc: (56.00%) (11714/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1738) |  Loss2: (0.0000) | Acc: (56.00%) (12459/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1727) |  Loss2: (0.0000) | Acc: (56.00%) (13195/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1740) |  Loss2: (0.0000) | Acc: (56.00%) (13927/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1740) |  Loss2: (0.0000) | Acc: (57.00%) (14672/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1730) |  Loss2: (0.0000) | Acc: (57.00%) (15415/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1718) |  Loss2: (0.0000) | Acc: (57.00%) (16162/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1686) |  Loss2: (0.0000) | Acc: (57.00%) (16937/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1667) |  Loss2: (0.0000) | Acc: (57.00%) (17693/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1639) |  Loss2: (0.0000) | Acc: (57.00%) (18466/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1599) |  Loss2: (0.0000) | Acc: (57.00%) (19258/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1584) |  Loss2: (0.0000) | Acc: (57.00%) (20021/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1575) |  Loss2: (0.0000) | Acc: (57.00%) (20775/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1553) |  Loss2: (0.0000) | Acc: (57.00%) (21553/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1528) |  Loss2: (0.0000) | Acc: (57.00%) (22322/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1511) |  Loss2: (0.0000) | Acc: (57.00%) (23083/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1499) |  Loss2: (0.0000) | Acc: (58.00%) (23852/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1493) |  Loss2: (0.0000) | Acc: (58.00%) (24619/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1464) |  Loss2: (0.0000) | Acc: (58.00%) (25401/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (58.00%) (26174/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (58.00%) (26950/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1423) |  Loss2: (0.0000) | Acc: (58.00%) (27733/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1406) |  Loss2: (0.0000) | Acc: (58.00%) (28525/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1376) |  Loss2: (0.0000) | Acc: (58.00%) (29303/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.1644) | Acc: (58.00%) (5803/10000)
percent tensor([0.4882, 0.5118], device='cuda:0')
percent tensor([0.4477, 0.5523], device='cuda:0')
percent tensor([0.4812, 0.5188], device='cuda:0')
percent tensor([0.4974, 0.5026], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5531, 0.4469], device='cuda:0')
percent tensor([0.8431, 0.1569], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.5112, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(776.6439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(770.4869, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.2926, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(518.1730, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2162.7375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4322.4346, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1439.3588, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6113.6055, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12233.5244, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4069.5781, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17284.2344, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (1.1319) |  Loss2: (0.0000) | Acc: (60.00%) (77/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.1100) |  Loss2: (0.0000) | Acc: (59.00%) (843/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0824) |  Loss2: (0.0000) | Acc: (60.00%) (1630/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0926) |  Loss2: (0.0000) | Acc: (59.00%) (2377/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0860) |  Loss2: (0.0000) | Acc: (60.00%) (3198/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0821) |  Loss2: (0.0000) | Acc: (60.00%) (3981/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0838) |  Loss2: (0.0000) | Acc: (60.00%) (4753/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0750) |  Loss2: (0.0000) | Acc: (61.00%) (5573/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0740) |  Loss2: (0.0000) | Acc: (61.00%) (6366/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0737) |  Loss2: (0.0000) | Acc: (61.00%) (7162/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0690) |  Loss2: (0.0000) | Acc: (61.00%) (7973/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0676) |  Loss2: (0.0000) | Acc: (61.00%) (8771/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0705) |  Loss2: (0.0000) | Acc: (61.00%) (9549/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0657) |  Loss2: (0.0000) | Acc: (61.00%) (10374/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0620) |  Loss2: (0.0000) | Acc: (62.00%) (11195/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0581) |  Loss2: (0.0000) | Acc: (62.00%) (12026/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0545) |  Loss2: (0.0000) | Acc: (62.00%) (12849/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0519) |  Loss2: (0.0000) | Acc: (62.00%) (13679/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0506) |  Loss2: (0.0000) | Acc: (62.00%) (14477/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0501) |  Loss2: (0.0000) | Acc: (62.00%) (15272/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0493) |  Loss2: (0.0000) | Acc: (62.00%) (16057/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0498) |  Loss2: (0.0000) | Acc: (62.00%) (16844/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0472) |  Loss2: (0.0000) | Acc: (62.00%) (17674/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0454) |  Loss2: (0.0000) | Acc: (62.00%) (18504/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0454) |  Loss2: (0.0000) | Acc: (62.00%) (19326/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0460) |  Loss2: (0.0000) | Acc: (62.00%) (20146/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0460) |  Loss2: (0.0000) | Acc: (62.00%) (20956/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0467) |  Loss2: (0.0000) | Acc: (62.00%) (21736/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0442) |  Loss2: (0.0000) | Acc: (62.00%) (22576/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0433) |  Loss2: (0.0000) | Acc: (62.00%) (23390/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0427) |  Loss2: (0.0000) | Acc: (62.00%) (24203/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0423) |  Loss2: (0.0000) | Acc: (62.00%) (25014/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0400) |  Loss2: (0.0000) | Acc: (62.00%) (25859/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0387) |  Loss2: (0.0000) | Acc: (62.00%) (26678/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0389) |  Loss2: (0.0000) | Acc: (62.00%) (27463/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0383) |  Loss2: (0.0000) | Acc: (62.00%) (28291/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0375) |  Loss2: (0.0000) | Acc: (62.00%) (29109/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0374) |  Loss2: (0.0000) | Acc: (62.00%) (29916/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0358) |  Loss2: (0.0000) | Acc: (63.00%) (30747/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0345) |  Loss2: (0.0000) | Acc: (63.00%) (31535/50000)
# TEST : Loss: (1.1825) | Acc: (58.00%) (5826/10000)
percent tensor([0.4887, 0.5113], device='cuda:0')
percent tensor([0.4468, 0.5532], device='cuda:0')
percent tensor([0.4835, 0.5165], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.8504, 0.1496], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.8855) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9495) |  Loss2: (0.0000) | Acc: (65.00%) (924/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9560) |  Loss2: (0.0000) | Acc: (65.00%) (1769/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9564) |  Loss2: (0.0000) | Acc: (65.00%) (2611/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9603) |  Loss2: (0.0000) | Acc: (65.00%) (3447/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9610) |  Loss2: (0.0000) | Acc: (65.00%) (4286/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (5122/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (5958/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9668) |  Loss2: (0.0000) | Acc: (65.00%) (6781/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9679) |  Loss2: (0.0000) | Acc: (65.00%) (7617/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9682) |  Loss2: (0.0000) | Acc: (65.00%) (8468/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (9308/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9738) |  Loss2: (0.0000) | Acc: (65.00%) (10097/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9767) |  Loss2: (0.0000) | Acc: (65.00%) (10901/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9735) |  Loss2: (0.0000) | Acc: (65.00%) (11754/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9712) |  Loss2: (0.0000) | Acc: (65.00%) (12622/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9710) |  Loss2: (0.0000) | Acc: (65.00%) (13466/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9733) |  Loss2: (0.0000) | Acc: (65.00%) (14284/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9733) |  Loss2: (0.0000) | Acc: (65.00%) (15130/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9707) |  Loss2: (0.0000) | Acc: (65.00%) (15997/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9683) |  Loss2: (0.0000) | Acc: (65.00%) (16840/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9675) |  Loss2: (0.0000) | Acc: (65.00%) (17682/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9671) |  Loss2: (0.0000) | Acc: (65.00%) (18522/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9680) |  Loss2: (0.0000) | Acc: (65.00%) (19355/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9674) |  Loss2: (0.0000) | Acc: (65.00%) (20198/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9680) |  Loss2: (0.0000) | Acc: (65.00%) (21021/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9659) |  Loss2: (0.0000) | Acc: (65.00%) (21898/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9662) |  Loss2: (0.0000) | Acc: (65.00%) (22729/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9666) |  Loss2: (0.0000) | Acc: (65.00%) (23560/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9650) |  Loss2: (0.0000) | Acc: (65.00%) (24437/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9640) |  Loss2: (0.0000) | Acc: (65.00%) (25329/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9649) |  Loss2: (0.0000) | Acc: (65.00%) (26160/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9643) |  Loss2: (0.0000) | Acc: (65.00%) (27000/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9632) |  Loss2: (0.0000) | Acc: (65.00%) (27873/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9614) |  Loss2: (0.0000) | Acc: (65.00%) (28753/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9598) |  Loss2: (0.0000) | Acc: (65.00%) (29604/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9584) |  Loss2: (0.0000) | Acc: (65.00%) (30473/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (65.00%) (31281/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9588) |  Loss2: (0.0000) | Acc: (65.00%) (32146/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (32940/50000)
# TEST : Loss: (0.9929) | Acc: (64.00%) (6458/10000)
percent tensor([0.4884, 0.5116], device='cuda:0')
percent tensor([0.4495, 0.5505], device='cuda:0')
percent tensor([0.4811, 0.5189], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.5314, 0.4686], device='cuda:0')
percent tensor([0.5492, 0.4508], device='cuda:0')
percent tensor([0.8409, 0.1591], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (1.1924) |  Loss2: (0.0000) | Acc: (51.00%) (66/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9291) |  Loss2: (0.0000) | Acc: (65.00%) (928/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (66.00%) (1788/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9203) |  Loss2: (0.0000) | Acc: (66.00%) (2627/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9100) |  Loss2: (0.0000) | Acc: (66.00%) (3514/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9082) |  Loss2: (0.0000) | Acc: (67.00%) (4386/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9131) |  Loss2: (0.0000) | Acc: (67.00%) (5247/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (6136/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (67.00%) (6974/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9112) |  Loss2: (0.0000) | Acc: (67.00%) (7843/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.9135) |  Loss2: (0.0000) | Acc: (67.00%) (8681/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.9110) |  Loss2: (0.0000) | Acc: (67.00%) (9585/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.9123) |  Loss2: (0.0000) | Acc: (67.00%) (10424/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.9129) |  Loss2: (0.0000) | Acc: (67.00%) (11286/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.9116) |  Loss2: (0.0000) | Acc: (67.00%) (12168/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.9133) |  Loss2: (0.0000) | Acc: (67.00%) (13015/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.9125) |  Loss2: (0.0000) | Acc: (67.00%) (13880/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9139) |  Loss2: (0.0000) | Acc: (67.00%) (14725/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9161) |  Loss2: (0.0000) | Acc: (67.00%) (15597/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9168) |  Loss2: (0.0000) | Acc: (67.00%) (16460/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9153) |  Loss2: (0.0000) | Acc: (67.00%) (17315/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.9141) |  Loss2: (0.0000) | Acc: (67.00%) (18184/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.9121) |  Loss2: (0.0000) | Acc: (67.00%) (19074/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.9093) |  Loss2: (0.0000) | Acc: (67.00%) (19987/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.9096) |  Loss2: (0.0000) | Acc: (67.00%) (20839/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.9081) |  Loss2: (0.0000) | Acc: (67.00%) (21712/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.9069) |  Loss2: (0.0000) | Acc: (67.00%) (22604/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.9057) |  Loss2: (0.0000) | Acc: (67.00%) (23487/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.9038) |  Loss2: (0.0000) | Acc: (67.00%) (24397/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.9053) |  Loss2: (0.0000) | Acc: (67.00%) (25235/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.9049) |  Loss2: (0.0000) | Acc: (67.00%) (26112/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.9041) |  Loss2: (0.0000) | Acc: (67.00%) (26989/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.9032) |  Loss2: (0.0000) | Acc: (67.00%) (27898/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.9030) |  Loss2: (0.0000) | Acc: (67.00%) (28770/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (67.00%) (29640/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.9023) |  Loss2: (0.0000) | Acc: (67.00%) (30515/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.9017) |  Loss2: (0.0000) | Acc: (67.00%) (31402/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.9025) |  Loss2: (0.0000) | Acc: (67.00%) (32246/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.9031) |  Loss2: (0.0000) | Acc: (67.00%) (33112/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.9022) |  Loss2: (0.0000) | Acc: (67.00%) (33952/50000)
# TEST : Loss: (0.9505) | Acc: (66.00%) (6620/10000)
percent tensor([0.4884, 0.5116], device='cuda:0')
percent tensor([0.4487, 0.5513], device='cuda:0')
percent tensor([0.4821, 0.5179], device='cuda:0')
percent tensor([0.4972, 0.5028], device='cuda:0')
percent tensor([0.5337, 0.4663], device='cuda:0')
percent tensor([0.5327, 0.4673], device='cuda:0')
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.8540, 0.1460], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.7321) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8448) |  Loss2: (0.0000) | Acc: (71.00%) (1002/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8823) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8687) |  Loss2: (0.0000) | Acc: (69.00%) (2754/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8793) |  Loss2: (0.0000) | Acc: (68.00%) (3603/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8729) |  Loss2: (0.0000) | Acc: (68.00%) (4501/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8701) |  Loss2: (0.0000) | Acc: (68.00%) (5365/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8684) |  Loss2: (0.0000) | Acc: (68.00%) (6257/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8661) |  Loss2: (0.0000) | Acc: (69.00%) (7154/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8633) |  Loss2: (0.0000) | Acc: (69.00%) (8059/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8637) |  Loss2: (0.0000) | Acc: (69.00%) (8939/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (9852/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8614) |  Loss2: (0.0000) | Acc: (69.00%) (10745/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (11653/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8596) |  Loss2: (0.0000) | Acc: (69.00%) (12535/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8617) |  Loss2: (0.0000) | Acc: (69.00%) (13419/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8624) |  Loss2: (0.0000) | Acc: (69.00%) (14298/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8620) |  Loss2: (0.0000) | Acc: (69.00%) (15186/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8605) |  Loss2: (0.0000) | Acc: (69.00%) (16093/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8610) |  Loss2: (0.0000) | Acc: (69.00%) (16990/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8593) |  Loss2: (0.0000) | Acc: (69.00%) (17899/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8592) |  Loss2: (0.0000) | Acc: (69.00%) (18791/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8601) |  Loss2: (0.0000) | Acc: (69.00%) (19670/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8591) |  Loss2: (0.0000) | Acc: (69.00%) (20591/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8582) |  Loss2: (0.0000) | Acc: (69.00%) (21491/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8577) |  Loss2: (0.0000) | Acc: (69.00%) (22392/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (23286/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8575) |  Loss2: (0.0000) | Acc: (69.00%) (24163/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8571) |  Loss2: (0.0000) | Acc: (69.00%) (25048/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8567) |  Loss2: (0.0000) | Acc: (69.00%) (25949/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (26838/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8569) |  Loss2: (0.0000) | Acc: (69.00%) (27734/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (28629/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8553) |  Loss2: (0.0000) | Acc: (69.00%) (29538/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (30445/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8554) |  Loss2: (0.0000) | Acc: (69.00%) (31343/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8553) |  Loss2: (0.0000) | Acc: (69.00%) (32235/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8538) |  Loss2: (0.0000) | Acc: (69.00%) (33148/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8532) |  Loss2: (0.0000) | Acc: (69.00%) (34044/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8522) |  Loss2: (0.0000) | Acc: (69.00%) (34912/50000)
# TEST : Loss: (0.8629) | Acc: (69.00%) (6914/10000)
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.4491, 0.5509], device='cuda:0')
percent tensor([0.4813, 0.5187], device='cuda:0')
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5518, 0.4482], device='cuda:0')
percent tensor([0.8412, 0.1588], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.7550) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8240) |  Loss2: (0.0000) | Acc: (70.00%) (996/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8816) |  Loss2: (0.0000) | Acc: (68.00%) (1837/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9215) |  Loss2: (0.0000) | Acc: (67.00%) (2661/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9410) |  Loss2: (0.0000) | Acc: (66.00%) (3470/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (65.00%) (4308/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9539) |  Loss2: (0.0000) | Acc: (66.00%) (5155/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9548) |  Loss2: (0.0000) | Acc: (65.00%) (5985/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9572) |  Loss2: (0.0000) | Acc: (65.00%) (6821/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9563) |  Loss2: (0.0000) | Acc: (65.00%) (7674/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (8519/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9583) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9555) |  Loss2: (0.0000) | Acc: (65.00%) (10215/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9519) |  Loss2: (0.0000) | Acc: (66.00%) (11074/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9528) |  Loss2: (0.0000) | Acc: (65.00%) (11902/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9500) |  Loss2: (0.0000) | Acc: (66.00%) (12764/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9504) |  Loss2: (0.0000) | Acc: (65.00%) (13590/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9467) |  Loss2: (0.0000) | Acc: (66.00%) (14472/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9441) |  Loss2: (0.0000) | Acc: (66.00%) (15347/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9451) |  Loss2: (0.0000) | Acc: (66.00%) (16196/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9416) |  Loss2: (0.0000) | Acc: (66.00%) (17084/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9389) |  Loss2: (0.0000) | Acc: (66.00%) (17968/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9377) |  Loss2: (0.0000) | Acc: (66.00%) (18811/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9361) |  Loss2: (0.0000) | Acc: (66.00%) (19677/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (66.00%) (20547/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9346) |  Loss2: (0.0000) | Acc: (66.00%) (21393/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9337) |  Loss2: (0.0000) | Acc: (66.00%) (22258/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9353) |  Loss2: (0.0000) | Acc: (66.00%) (23086/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9338) |  Loss2: (0.0000) | Acc: (66.00%) (23957/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9313) |  Loss2: (0.0000) | Acc: (66.00%) (24856/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9304) |  Loss2: (0.0000) | Acc: (66.00%) (25714/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.9287) |  Loss2: (0.0000) | Acc: (66.00%) (26592/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.9267) |  Loss2: (0.0000) | Acc: (66.00%) (27478/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.9252) |  Loss2: (0.0000) | Acc: (66.00%) (28354/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.9234) |  Loss2: (0.0000) | Acc: (67.00%) (29256/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (30128/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.9194) |  Loss2: (0.0000) | Acc: (67.00%) (31057/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.9185) |  Loss2: (0.0000) | Acc: (67.00%) (31934/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.9180) |  Loss2: (0.0000) | Acc: (67.00%) (32812/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.9178) |  Loss2: (0.0000) | Acc: (67.00%) (33641/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.9096) | Acc: (67.00%) (6792/10000)
percent tensor([0.4960, 0.5040], device='cuda:0')
percent tensor([0.4375, 0.5625], device='cuda:0')
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4930, 0.5070], device='cuda:0')
percent tensor([0.4970, 0.5030], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.9031, 0.0969], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (1.0610) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.9016) |  Loss2: (0.0000) | Acc: (68.00%) (960/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8713) |  Loss2: (0.0000) | Acc: (69.00%) (1873/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8832) |  Loss2: (0.0000) | Acc: (68.00%) (2732/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8733) |  Loss2: (0.0000) | Acc: (69.00%) (3627/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8666) |  Loss2: (0.0000) | Acc: (69.00%) (4533/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8604) |  Loss2: (0.0000) | Acc: (69.00%) (5433/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (6338/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8626) |  Loss2: (0.0000) | Acc: (69.00%) (7200/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8616) |  Loss2: (0.0000) | Acc: (69.00%) (8094/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8620) |  Loss2: (0.0000) | Acc: (69.00%) (8978/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8588) |  Loss2: (0.0000) | Acc: (69.00%) (9882/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (10762/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (11644/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8556) |  Loss2: (0.0000) | Acc: (69.00%) (12535/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (13414/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8572) |  Loss2: (0.0000) | Acc: (69.00%) (14290/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8575) |  Loss2: (0.0000) | Acc: (69.00%) (15180/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8569) |  Loss2: (0.0000) | Acc: (69.00%) (16088/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8554) |  Loss2: (0.0000) | Acc: (69.00%) (16999/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8545) |  Loss2: (0.0000) | Acc: (69.00%) (17906/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8559) |  Loss2: (0.0000) | Acc: (69.00%) (18769/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8533) |  Loss2: (0.0000) | Acc: (69.00%) (19695/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8517) |  Loss2: (0.0000) | Acc: (69.00%) (20592/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (69.00%) (21488/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (69.00%) (22409/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8508) |  Loss2: (0.0000) | Acc: (69.00%) (23300/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (69.00%) (24192/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8495) |  Loss2: (0.0000) | Acc: (69.00%) (25112/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (25988/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (69.00%) (26877/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8479) |  Loss2: (0.0000) | Acc: (69.00%) (27781/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8487) |  Loss2: (0.0000) | Acc: (69.00%) (28665/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8474) |  Loss2: (0.0000) | Acc: (69.00%) (29584/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (69.00%) (30482/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8484) |  Loss2: (0.0000) | Acc: (69.00%) (31343/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8489) |  Loss2: (0.0000) | Acc: (69.00%) (32245/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8495) |  Loss2: (0.0000) | Acc: (69.00%) (33136/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (69.00%) (34030/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (69.00%) (34893/50000)
# TEST : Loss: (0.8649) | Acc: (69.00%) (6956/10000)
percent tensor([0.4962, 0.5038], device='cuda:0')
percent tensor([0.4433, 0.5567], device='cuda:0')
percent tensor([0.4839, 0.5161], device='cuda:0')
percent tensor([0.4964, 0.5036], device='cuda:0')
percent tensor([0.4877, 0.5123], device='cuda:0')
percent tensor([0.5954, 0.4046], device='cuda:0')
percent tensor([0.5768, 0.4232], device='cuda:0')
percent tensor([0.9413, 0.0587], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.7388) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8506) |  Loss2: (0.0000) | Acc: (70.00%) (990/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (72.00%) (1936/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8210) |  Loss2: (0.0000) | Acc: (71.00%) (2830/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (71.00%) (3735/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8220) |  Loss2: (0.0000) | Acc: (71.00%) (4645/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8235) |  Loss2: (0.0000) | Acc: (71.00%) (5547/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8286) |  Loss2: (0.0000) | Acc: (70.00%) (6447/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8308) |  Loss2: (0.0000) | Acc: (70.00%) (7355/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8297) |  Loss2: (0.0000) | Acc: (71.00%) (8277/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8316) |  Loss2: (0.0000) | Acc: (70.00%) (9159/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8356) |  Loss2: (0.0000) | Acc: (70.00%) (10049/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8334) |  Loss2: (0.0000) | Acc: (70.00%) (10942/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8369) |  Loss2: (0.0000) | Acc: (70.00%) (11818/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8406) |  Loss2: (0.0000) | Acc: (70.00%) (12708/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8375) |  Loss2: (0.0000) | Acc: (70.00%) (13636/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8375) |  Loss2: (0.0000) | Acc: (70.00%) (14548/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8376) |  Loss2: (0.0000) | Acc: (70.00%) (15446/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (16345/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8378) |  Loss2: (0.0000) | Acc: (70.00%) (17251/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (18146/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8379) |  Loss2: (0.0000) | Acc: (70.00%) (19072/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8390) |  Loss2: (0.0000) | Acc: (70.00%) (19972/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (20900/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (21828/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8321) |  Loss2: (0.0000) | Acc: (70.00%) (22753/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8313) |  Loss2: (0.0000) | Acc: (70.00%) (23666/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8325) |  Loss2: (0.0000) | Acc: (70.00%) (24544/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8310) |  Loss2: (0.0000) | Acc: (70.00%) (25472/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8306) |  Loss2: (0.0000) | Acc: (70.00%) (26354/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8309) |  Loss2: (0.0000) | Acc: (70.00%) (27257/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8308) |  Loss2: (0.0000) | Acc: (70.00%) (28158/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (29061/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8318) |  Loss2: (0.0000) | Acc: (70.00%) (29930/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8304) |  Loss2: (0.0000) | Acc: (70.00%) (30856/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8306) |  Loss2: (0.0000) | Acc: (70.00%) (31762/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8317) |  Loss2: (0.0000) | Acc: (70.00%) (32640/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8315) |  Loss2: (0.0000) | Acc: (70.00%) (33553/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8296) |  Loss2: (0.0000) | Acc: (70.00%) (34488/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8292) |  Loss2: (0.0000) | Acc: (70.00%) (35375/50000)
# TEST : Loss: (0.8510) | Acc: (70.00%) (7021/10000)
percent tensor([0.4954, 0.5046], device='cuda:0')
percent tensor([0.4428, 0.5572], device='cuda:0')
percent tensor([0.4851, 0.5149], device='cuda:0')
percent tensor([0.5004, 0.4996], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.6042, 0.3958], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.9582, 0.0418], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7718) |  Loss2: (0.0000) | Acc: (71.00%) (92/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.8083) |  Loss2: (0.0000) | Acc: (70.00%) (995/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (1898/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (2787/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.8312) |  Loss2: (0.0000) | Acc: (70.00%) (3677/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.8329) |  Loss2: (0.0000) | Acc: (69.00%) (4562/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8348) |  Loss2: (0.0000) | Acc: (69.00%) (5451/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8322) |  Loss2: (0.0000) | Acc: (70.00%) (6367/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8334) |  Loss2: (0.0000) | Acc: (70.00%) (7270/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8301) |  Loss2: (0.0000) | Acc: (70.00%) (8176/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8296) |  Loss2: (0.0000) | Acc: (70.00%) (9063/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8283) |  Loss2: (0.0000) | Acc: (70.00%) (9966/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8283) |  Loss2: (0.0000) | Acc: (70.00%) (10856/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8261) |  Loss2: (0.0000) | Acc: (70.00%) (11781/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8255) |  Loss2: (0.0000) | Acc: (70.00%) (12708/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8213) |  Loss2: (0.0000) | Acc: (70.00%) (13662/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8219) |  Loss2: (0.0000) | Acc: (70.00%) (14594/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8212) |  Loss2: (0.0000) | Acc: (70.00%) (15490/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8193) |  Loss2: (0.0000) | Acc: (70.00%) (16418/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (17322/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (70.00%) (18212/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (19119/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (70.00%) (20025/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (70.00%) (20930/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8166) |  Loss2: (0.0000) | Acc: (70.00%) (21812/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8165) |  Loss2: (0.0000) | Acc: (70.00%) (22712/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (70.00%) (23613/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (70.00%) (24513/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (70.00%) (25413/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (26314/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8187) |  Loss2: (0.0000) | Acc: (70.00%) (27219/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (28133/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (70.00%) (29057/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8184) |  Loss2: (0.0000) | Acc: (70.00%) (29960/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (30868/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8179) |  Loss2: (0.0000) | Acc: (70.00%) (31797/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (32690/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (33605/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (34516/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (35391/50000)
# TEST : Loss: (0.8387) | Acc: (70.00%) (7037/10000)
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4454, 0.5546], device='cuda:0')
percent tensor([0.4871, 0.5129], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.4930, 0.5070], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.5877, 0.4123], device='cuda:0')
percent tensor([0.9677, 0.0323], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (1012/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (71.00%) (1915/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.8147) |  Loss2: (0.0000) | Acc: (71.00%) (2823/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.8135) |  Loss2: (0.0000) | Acc: (71.00%) (3746/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (71.00%) (4658/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.8148) |  Loss2: (0.0000) | Acc: (71.00%) (5575/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.8133) |  Loss2: (0.0000) | Acc: (71.00%) (6484/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.8156) |  Loss2: (0.0000) | Acc: (71.00%) (7385/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (70.00%) (8270/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (70.00%) (9165/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (71.00%) (10091/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.8150) |  Loss2: (0.0000) | Acc: (71.00%) (11023/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.8132) |  Loss2: (0.0000) | Acc: (71.00%) (11936/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (12840/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (13762/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8124) |  Loss2: (0.0000) | Acc: (71.00%) (14677/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8118) |  Loss2: (0.0000) | Acc: (71.00%) (15600/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (16482/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8157) |  Loss2: (0.0000) | Acc: (71.00%) (17376/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (18304/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (71.00%) (19201/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (71.00%) (20089/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (20992/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (71.00%) (21914/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (22819/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (23732/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8169) |  Loss2: (0.0000) | Acc: (71.00%) (24646/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (71.00%) (25552/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8166) |  Loss2: (0.0000) | Acc: (71.00%) (26493/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (71.00%) (27399/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (28344/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (29290/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8135) |  Loss2: (0.0000) | Acc: (71.00%) (30202/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8138) |  Loss2: (0.0000) | Acc: (71.00%) (31113/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (32029/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8143) |  Loss2: (0.0000) | Acc: (71.00%) (32935/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (33838/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.8140) |  Loss2: (0.0000) | Acc: (71.00%) (34741/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.8130) |  Loss2: (0.0000) | Acc: (71.00%) (35627/50000)
# TEST : Loss: (0.8354) | Acc: (70.00%) (7052/10000)
percent tensor([0.4938, 0.5062], device='cuda:0')
percent tensor([0.4448, 0.5552], device='cuda:0')
percent tensor([0.4870, 0.5130], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.5902, 0.4098], device='cuda:0')
percent tensor([0.9738, 0.0262], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.8817) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8220) |  Loss2: (0.0000) | Acc: (70.00%) (999/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8289) |  Loss2: (0.0000) | Acc: (69.00%) (1874/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8155) |  Loss2: (0.0000) | Acc: (69.00%) (2777/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8173) |  Loss2: (0.0000) | Acc: (70.00%) (3687/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8197) |  Loss2: (0.0000) | Acc: (70.00%) (4587/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8179) |  Loss2: (0.0000) | Acc: (70.00%) (5501/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (6403/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8237) |  Loss2: (0.0000) | Acc: (70.00%) (7276/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8259) |  Loss2: (0.0000) | Acc: (70.00%) (8176/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8215) |  Loss2: (0.0000) | Acc: (70.00%) (9110/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8233) |  Loss2: (0.0000) | Acc: (70.00%) (10009/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8205) |  Loss2: (0.0000) | Acc: (70.00%) (10928/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8214) |  Loss2: (0.0000) | Acc: (70.00%) (11828/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (70.00%) (12751/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8190) |  Loss2: (0.0000) | Acc: (70.00%) (13655/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (70.00%) (14562/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (15465/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (70.00%) (16389/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8203) |  Loss2: (0.0000) | Acc: (70.00%) (17281/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8211) |  Loss2: (0.0000) | Acc: (70.00%) (18181/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8192) |  Loss2: (0.0000) | Acc: (70.00%) (19102/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8172) |  Loss2: (0.0000) | Acc: (70.00%) (20019/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.8159) |  Loss2: (0.0000) | Acc: (70.00%) (20949/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (70.00%) (21841/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (70.00%) (22761/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.8172) |  Loss2: (0.0000) | Acc: (70.00%) (23676/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (70.00%) (24601/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (25565/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.8109) |  Loss2: (0.0000) | Acc: (71.00%) (26487/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (71.00%) (27417/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.8099) |  Loss2: (0.0000) | Acc: (71.00%) (28346/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (29250/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (30162/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.8102) |  Loss2: (0.0000) | Acc: (71.00%) (31078/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.8089) |  Loss2: (0.0000) | Acc: (71.00%) (32019/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.8071) |  Loss2: (0.0000) | Acc: (71.00%) (32952/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (33858/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.8072) |  Loss2: (0.0000) | Acc: (71.00%) (34763/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.8069) |  Loss2: (0.0000) | Acc: (71.00%) (35660/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.9369) | Acc: (67.00%) (6727/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4444, 0.5556], device='cuda:0')
percent tensor([0.4880, 0.5120], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.5935, 0.4065], device='cuda:0')
percent tensor([0.9736, 0.0264], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(169.3348, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(785.1106, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(777.1408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.8564, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(516.4657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2168.6819, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4311.1138, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1434.1517, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.3057, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12183.8496, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4053.5610, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17203.8477, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7557) |  Loss2: (0.0000) | Acc: (73.00%) (1028/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7542) |  Loss2: (0.0000) | Acc: (72.00%) (1959/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7620) |  Loss2: (0.0000) | Acc: (72.00%) (2891/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7569) |  Loss2: (0.0000) | Acc: (72.00%) (3825/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7605) |  Loss2: (0.0000) | Acc: (72.00%) (4752/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7606) |  Loss2: (0.0000) | Acc: (72.00%) (5695/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7659) |  Loss2: (0.0000) | Acc: (72.00%) (6610/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7620) |  Loss2: (0.0000) | Acc: (72.00%) (7568/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (73.00%) (8507/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7651) |  Loss2: (0.0000) | Acc: (72.00%) (9431/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7663) |  Loss2: (0.0000) | Acc: (72.00%) (10344/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7703) |  Loss2: (0.0000) | Acc: (72.00%) (11261/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7657) |  Loss2: (0.0000) | Acc: (72.00%) (12221/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7636) |  Loss2: (0.0000) | Acc: (72.00%) (13156/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7627) |  Loss2: (0.0000) | Acc: (72.00%) (14097/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7641) |  Loss2: (0.0000) | Acc: (72.00%) (15026/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7649) |  Loss2: (0.0000) | Acc: (72.00%) (15959/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7646) |  Loss2: (0.0000) | Acc: (72.00%) (16898/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (17855/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7632) |  Loss2: (0.0000) | Acc: (73.00%) (18782/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7639) |  Loss2: (0.0000) | Acc: (73.00%) (19730/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7617) |  Loss2: (0.0000) | Acc: (73.00%) (20700/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7650) |  Loss2: (0.0000) | Acc: (73.00%) (21609/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7666) |  Loss2: (0.0000) | Acc: (73.00%) (22520/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7671) |  Loss2: (0.0000) | Acc: (72.00%) (23451/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7658) |  Loss2: (0.0000) | Acc: (73.00%) (24413/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (73.00%) (25372/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (73.00%) (26313/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7626) |  Loss2: (0.0000) | Acc: (73.00%) (27280/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7627) |  Loss2: (0.0000) | Acc: (73.00%) (28227/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7630) |  Loss2: (0.0000) | Acc: (73.00%) (29162/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7637) |  Loss2: (0.0000) | Acc: (73.00%) (30081/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (31006/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7631) |  Loss2: (0.0000) | Acc: (73.00%) (31947/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7619) |  Loss2: (0.0000) | Acc: (73.00%) (32905/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7609) |  Loss2: (0.0000) | Acc: (73.00%) (33848/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7608) |  Loss2: (0.0000) | Acc: (73.00%) (34789/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7586) |  Loss2: (0.0000) | Acc: (73.00%) (35754/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7591) |  Loss2: (0.0000) | Acc: (73.00%) (36653/50000)
# TEST : Loss: (0.8186) | Acc: (70.00%) (7063/10000)
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4457, 0.5543], device='cuda:0')
percent tensor([0.4876, 0.5124], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6018, 0.3982], device='cuda:0')
percent tensor([0.5869, 0.4131], device='cuda:0')
percent tensor([0.9714, 0.0286], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7124) |  Loss2: (0.0000) | Acc: (74.00%) (1043/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7203) |  Loss2: (0.0000) | Acc: (74.00%) (1999/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7140) |  Loss2: (0.0000) | Acc: (74.00%) (2960/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7243) |  Loss2: (0.0000) | Acc: (74.00%) (3910/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7214) |  Loss2: (0.0000) | Acc: (74.00%) (4871/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7147) |  Loss2: (0.0000) | Acc: (75.00%) (5867/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7106) |  Loss2: (0.0000) | Acc: (75.00%) (6847/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (7801/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (8748/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (75.00%) (9706/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7140) |  Loss2: (0.0000) | Acc: (75.00%) (10663/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7159) |  Loss2: (0.0000) | Acc: (74.00%) (11602/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7176) |  Loss2: (0.0000) | Acc: (74.00%) (12550/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7208) |  Loss2: (0.0000) | Acc: (74.00%) (13485/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7235) |  Loss2: (0.0000) | Acc: (74.00%) (14433/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7257) |  Loss2: (0.0000) | Acc: (74.00%) (15377/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7229) |  Loss2: (0.0000) | Acc: (74.00%) (16355/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7241) |  Loss2: (0.0000) | Acc: (74.00%) (17293/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7196) |  Loss2: (0.0000) | Acc: (74.00%) (18300/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7187) |  Loss2: (0.0000) | Acc: (74.00%) (19273/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7169) |  Loss2: (0.0000) | Acc: (74.00%) (20248/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7174) |  Loss2: (0.0000) | Acc: (74.00%) (21197/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7175) |  Loss2: (0.0000) | Acc: (74.00%) (22150/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7182) |  Loss2: (0.0000) | Acc: (74.00%) (23097/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7174) |  Loss2: (0.0000) | Acc: (74.00%) (24060/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7192) |  Loss2: (0.0000) | Acc: (74.00%) (25012/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7207) |  Loss2: (0.0000) | Acc: (74.00%) (25948/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7201) |  Loss2: (0.0000) | Acc: (74.00%) (26908/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7205) |  Loss2: (0.0000) | Acc: (74.00%) (27857/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (74.00%) (28817/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (29771/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7192) |  Loss2: (0.0000) | Acc: (74.00%) (30752/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7172) |  Loss2: (0.0000) | Acc: (74.00%) (31734/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7161) |  Loss2: (0.0000) | Acc: (74.00%) (32713/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7153) |  Loss2: (0.0000) | Acc: (74.00%) (33685/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7144) |  Loss2: (0.0000) | Acc: (74.00%) (34647/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7134) |  Loss2: (0.0000) | Acc: (75.00%) (35622/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7124) |  Loss2: (0.0000) | Acc: (75.00%) (36604/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7112) |  Loss2: (0.0000) | Acc: (75.00%) (37539/50000)
# TEST : Loss: (0.8739) | Acc: (69.00%) (6980/10000)
percent tensor([0.4939, 0.5061], device='cuda:0')
percent tensor([0.4469, 0.5531], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6017, 0.3983], device='cuda:0')
percent tensor([0.5902, 0.4098], device='cuda:0')
percent tensor([0.9729, 0.0271], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (1064/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6867) |  Loss2: (0.0000) | Acc: (75.00%) (2027/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6818) |  Loss2: (0.0000) | Acc: (75.00%) (3006/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6931) |  Loss2: (0.0000) | Acc: (75.00%) (3966/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6970) |  Loss2: (0.0000) | Acc: (75.00%) (4921/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6987) |  Loss2: (0.0000) | Acc: (75.00%) (5909/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6953) |  Loss2: (0.0000) | Acc: (75.00%) (6869/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6910) |  Loss2: (0.0000) | Acc: (75.00%) (7849/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (75.00%) (8819/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6881) |  Loss2: (0.0000) | Acc: (75.00%) (9805/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6865) |  Loss2: (0.0000) | Acc: (75.00%) (10772/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6842) |  Loss2: (0.0000) | Acc: (75.00%) (11747/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6864) |  Loss2: (0.0000) | Acc: (75.00%) (12705/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6871) |  Loss2: (0.0000) | Acc: (75.00%) (13671/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6859) |  Loss2: (0.0000) | Acc: (75.00%) (14649/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6845) |  Loss2: (0.0000) | Acc: (75.00%) (15648/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6855) |  Loss2: (0.0000) | Acc: (75.00%) (16608/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6861) |  Loss2: (0.0000) | Acc: (75.00%) (17566/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6843) |  Loss2: (0.0000) | Acc: (75.00%) (18558/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6836) |  Loss2: (0.0000) | Acc: (75.00%) (19518/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6840) |  Loss2: (0.0000) | Acc: (75.00%) (20499/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6837) |  Loss2: (0.0000) | Acc: (75.00%) (21473/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6823) |  Loss2: (0.0000) | Acc: (75.00%) (22465/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6840) |  Loss2: (0.0000) | Acc: (75.00%) (23410/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6828) |  Loss2: (0.0000) | Acc: (75.00%) (24403/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6814) |  Loss2: (0.0000) | Acc: (76.00%) (25393/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6816) |  Loss2: (0.0000) | Acc: (75.00%) (26360/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6821) |  Loss2: (0.0000) | Acc: (75.00%) (27313/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6818) |  Loss2: (0.0000) | Acc: (75.00%) (28287/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6832) |  Loss2: (0.0000) | Acc: (75.00%) (29238/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6817) |  Loss2: (0.0000) | Acc: (75.00%) (30249/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6819) |  Loss2: (0.0000) | Acc: (75.00%) (31212/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6808) |  Loss2: (0.0000) | Acc: (76.00%) (32200/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6804) |  Loss2: (0.0000) | Acc: (76.00%) (33177/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6792) |  Loss2: (0.0000) | Acc: (76.00%) (34178/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6791) |  Loss2: (0.0000) | Acc: (76.00%) (35166/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6793) |  Loss2: (0.0000) | Acc: (76.00%) (36135/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6775) |  Loss2: (0.0000) | Acc: (76.00%) (37159/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (38121/50000)
# TEST : Loss: (0.8411) | Acc: (71.00%) (7187/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4459, 0.5541], device='cuda:0')
percent tensor([0.4867, 0.5133], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.9751, 0.0249], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (1090/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6286) |  Loss2: (0.0000) | Acc: (77.00%) (2072/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (3065/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (4066/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (5069/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (6057/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (7012/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (8013/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6470) |  Loss2: (0.0000) | Acc: (77.00%) (8997/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (9980/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (10952/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6548) |  Loss2: (0.0000) | Acc: (77.00%) (11945/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (12933/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (13921/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (14927/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (15899/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (16873/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (17876/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (18872/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (19858/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (20842/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (21848/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6520) |  Loss2: (0.0000) | Acc: (77.00%) (22838/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6519) |  Loss2: (0.0000) | Acc: (77.00%) (23834/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6503) |  Loss2: (0.0000) | Acc: (77.00%) (24832/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (25807/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6512) |  Loss2: (0.0000) | Acc: (77.00%) (26786/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6512) |  Loss2: (0.0000) | Acc: (77.00%) (27788/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6499) |  Loss2: (0.0000) | Acc: (77.00%) (28802/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (29801/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6485) |  Loss2: (0.0000) | Acc: (77.00%) (30786/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6488) |  Loss2: (0.0000) | Acc: (77.00%) (31762/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (32753/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6493) |  Loss2: (0.0000) | Acc: (77.00%) (33738/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6477) |  Loss2: (0.0000) | Acc: (77.00%) (34735/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6464) |  Loss2: (0.0000) | Acc: (77.00%) (35743/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (36749/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6469) |  Loss2: (0.0000) | Acc: (77.00%) (37733/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6462) |  Loss2: (0.0000) | Acc: (77.00%) (38698/50000)
# TEST : Loss: (0.6948) | Acc: (75.00%) (7579/10000)
percent tensor([0.4940, 0.5060], device='cuda:0')
percent tensor([0.4462, 0.5538], device='cuda:0')
percent tensor([0.4866, 0.5134], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.4987, 0.5013], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.9759, 0.0241], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.7273) |  Loss2: (0.0000) | Acc: (75.00%) (1067/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.7663) |  Loss2: (0.0000) | Acc: (74.00%) (2001/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (73.00%) (2919/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (73.00%) (3834/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (72.00%) (4751/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.8186) |  Loss2: (0.0000) | Acc: (72.00%) (5641/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.8162) |  Loss2: (0.0000) | Acc: (72.00%) (6559/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.8119) |  Loss2: (0.0000) | Acc: (72.00%) (7491/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.8076) |  Loss2: (0.0000) | Acc: (72.00%) (8437/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (72.00%) (9368/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (72.00%) (10319/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7885) |  Loss2: (0.0000) | Acc: (72.00%) (11284/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7844) |  Loss2: (0.0000) | Acc: (72.00%) (12239/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7794) |  Loss2: (0.0000) | Acc: (73.00%) (13204/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7740) |  Loss2: (0.0000) | Acc: (73.00%) (14177/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7678) |  Loss2: (0.0000) | Acc: (73.00%) (15143/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7671) |  Loss2: (0.0000) | Acc: (73.00%) (16058/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7636) |  Loss2: (0.0000) | Acc: (73.00%) (17009/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7611) |  Loss2: (0.0000) | Acc: (73.00%) (17965/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7600) |  Loss2: (0.0000) | Acc: (73.00%) (18923/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7577) |  Loss2: (0.0000) | Acc: (73.00%) (19896/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7548) |  Loss2: (0.0000) | Acc: (73.00%) (20858/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.7506) |  Loss2: (0.0000) | Acc: (73.00%) (21836/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.7493) |  Loss2: (0.0000) | Acc: (73.00%) (22801/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.7459) |  Loss2: (0.0000) | Acc: (74.00%) (23780/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.7438) |  Loss2: (0.0000) | Acc: (74.00%) (24746/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.7425) |  Loss2: (0.0000) | Acc: (74.00%) (25704/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.7421) |  Loss2: (0.0000) | Acc: (74.00%) (26659/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.7386) |  Loss2: (0.0000) | Acc: (74.00%) (27653/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.7372) |  Loss2: (0.0000) | Acc: (74.00%) (28604/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.7357) |  Loss2: (0.0000) | Acc: (74.00%) (29571/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.7338) |  Loss2: (0.0000) | Acc: (74.00%) (30546/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.7315) |  Loss2: (0.0000) | Acc: (74.00%) (31522/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.7290) |  Loss2: (0.0000) | Acc: (74.00%) (32508/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.7273) |  Loss2: (0.0000) | Acc: (74.00%) (33482/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.7262) |  Loss2: (0.0000) | Acc: (74.00%) (34449/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.7248) |  Loss2: (0.0000) | Acc: (74.00%) (35438/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.7249) |  Loss2: (0.0000) | Acc: (74.00%) (36397/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.7230) |  Loss2: (0.0000) | Acc: (74.00%) (37340/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.7211) | Acc: (75.00%) (7520/10000)
percent tensor([0.4760, 0.5240], device='cuda:0')
percent tensor([0.4723, 0.5277], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.9801, 0.0199], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.7072) |  Loss2: (0.0000) | Acc: (75.00%) (1060/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6983) |  Loss2: (0.0000) | Acc: (75.00%) (2024/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (75.00%) (3004/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6903) |  Loss2: (0.0000) | Acc: (75.00%) (3978/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (4960/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6836) |  Loss2: (0.0000) | Acc: (76.00%) (5942/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6834) |  Loss2: (0.0000) | Acc: (76.00%) (6917/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6868) |  Loss2: (0.0000) | Acc: (75.00%) (7866/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6892) |  Loss2: (0.0000) | Acc: (75.00%) (8816/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6868) |  Loss2: (0.0000) | Acc: (75.00%) (9796/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6861) |  Loss2: (0.0000) | Acc: (75.00%) (10779/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6830) |  Loss2: (0.0000) | Acc: (75.00%) (11767/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6827) |  Loss2: (0.0000) | Acc: (75.00%) (12736/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6789) |  Loss2: (0.0000) | Acc: (76.00%) (13730/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (14727/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6760) |  Loss2: (0.0000) | Acc: (76.00%) (15709/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6752) |  Loss2: (0.0000) | Acc: (76.00%) (16698/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6719) |  Loss2: (0.0000) | Acc: (76.00%) (17687/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6731) |  Loss2: (0.0000) | Acc: (76.00%) (18649/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (76.00%) (19616/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6741) |  Loss2: (0.0000) | Acc: (76.00%) (20604/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6753) |  Loss2: (0.0000) | Acc: (76.00%) (21579/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6740) |  Loss2: (0.0000) | Acc: (76.00%) (22566/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6711) |  Loss2: (0.0000) | Acc: (76.00%) (23580/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (76.00%) (24548/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6735) |  Loss2: (0.0000) | Acc: (76.00%) (25508/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6743) |  Loss2: (0.0000) | Acc: (76.00%) (26489/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6734) |  Loss2: (0.0000) | Acc: (76.00%) (27481/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6739) |  Loss2: (0.0000) | Acc: (76.00%) (28454/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6729) |  Loss2: (0.0000) | Acc: (76.00%) (29449/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (30427/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6731) |  Loss2: (0.0000) | Acc: (76.00%) (31402/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (32380/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6724) |  Loss2: (0.0000) | Acc: (76.00%) (33363/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (34326/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6729) |  Loss2: (0.0000) | Acc: (76.00%) (35309/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6721) |  Loss2: (0.0000) | Acc: (76.00%) (36298/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6708) |  Loss2: (0.0000) | Acc: (76.00%) (37305/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6701) |  Loss2: (0.0000) | Acc: (76.00%) (38265/50000)
# TEST : Loss: (0.6946) | Acc: (75.00%) (7589/10000)
percent tensor([0.4801, 0.5199], device='cuda:0')
percent tensor([0.4759, 0.5241], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.6547, 0.3453], device='cuda:0')
percent tensor([0.6207, 0.3793], device='cuda:0')
percent tensor([0.9853, 0.0147], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.6279) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6793) |  Loss2: (0.0000) | Acc: (75.00%) (1068/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (76.00%) (2068/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6326) |  Loss2: (0.0000) | Acc: (77.00%) (3075/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (4085/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6382) |  Loss2: (0.0000) | Acc: (77.00%) (5062/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6459) |  Loss2: (0.0000) | Acc: (77.00%) (6045/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6472) |  Loss2: (0.0000) | Acc: (77.00%) (7015/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6525) |  Loss2: (0.0000) | Acc: (77.00%) (7987/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (8985/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6557) |  Loss2: (0.0000) | Acc: (77.00%) (9956/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (10945/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (11937/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6568) |  Loss2: (0.0000) | Acc: (77.00%) (12920/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6603) |  Loss2: (0.0000) | Acc: (76.00%) (13881/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6622) |  Loss2: (0.0000) | Acc: (76.00%) (14863/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6618) |  Loss2: (0.0000) | Acc: (76.00%) (15855/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6616) |  Loss2: (0.0000) | Acc: (76.00%) (16839/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6595) |  Loss2: (0.0000) | Acc: (76.00%) (17819/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6595) |  Loss2: (0.0000) | Acc: (76.00%) (18820/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6590) |  Loss2: (0.0000) | Acc: (77.00%) (19819/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6576) |  Loss2: (0.0000) | Acc: (77.00%) (20812/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6568) |  Loss2: (0.0000) | Acc: (77.00%) (21806/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6567) |  Loss2: (0.0000) | Acc: (77.00%) (22787/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (77.00%) (23775/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6559) |  Loss2: (0.0000) | Acc: (77.00%) (24768/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6554) |  Loss2: (0.0000) | Acc: (77.00%) (25775/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6552) |  Loss2: (0.0000) | Acc: (77.00%) (26770/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (27783/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6533) |  Loss2: (0.0000) | Acc: (77.00%) (28784/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (29774/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (77.00%) (30752/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (31750/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (32736/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (33758/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (34752/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (35735/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (36741/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (37741/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (38668/50000)
# TEST : Loss: (0.6846) | Acc: (76.00%) (7620/10000)
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.4757, 0.5243], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6632, 0.3368], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.9888, 0.0112], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.5577) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (77.00%) (1087/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (2075/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (3085/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6454) |  Loss2: (0.0000) | Acc: (77.00%) (4076/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (5078/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6475) |  Loss2: (0.0000) | Acc: (77.00%) (6058/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6392) |  Loss2: (0.0000) | Acc: (77.00%) (7075/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6416) |  Loss2: (0.0000) | Acc: (77.00%) (8067/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (78.00%) (9088/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6424) |  Loss2: (0.0000) | Acc: (77.00%) (10079/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (11053/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (12040/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6449) |  Loss2: (0.0000) | Acc: (77.00%) (13018/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6457) |  Loss2: (0.0000) | Acc: (77.00%) (14014/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6455) |  Loss2: (0.0000) | Acc: (77.00%) (14997/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (16019/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6398) |  Loss2: (0.0000) | Acc: (77.00%) (17021/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6406) |  Loss2: (0.0000) | Acc: (77.00%) (18009/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (19008/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (19978/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6432) |  Loss2: (0.0000) | Acc: (77.00%) (20975/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6420) |  Loss2: (0.0000) | Acc: (77.00%) (21986/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6448) |  Loss2: (0.0000) | Acc: (77.00%) (22964/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6453) |  Loss2: (0.0000) | Acc: (77.00%) (23965/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (24978/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (25970/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (26966/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (27964/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6434) |  Loss2: (0.0000) | Acc: (77.00%) (28957/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6433) |  Loss2: (0.0000) | Acc: (77.00%) (29962/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6426) |  Loss2: (0.0000) | Acc: (77.00%) (30968/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6418) |  Loss2: (0.0000) | Acc: (77.00%) (31993/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6422) |  Loss2: (0.0000) | Acc: (77.00%) (32980/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (33990/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (77.00%) (34974/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (35981/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6408) |  Loss2: (0.0000) | Acc: (77.00%) (36980/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (37989/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (38929/50000)
# TEST : Loss: (0.6726) | Acc: (76.00%) (7676/10000)
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4776, 0.5224], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6320, 0.3680], device='cuda:0')
percent tensor([0.9911, 0.0089], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5746) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6492) |  Loss2: (0.0000) | Acc: (78.00%) (1108/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (78.00%) (2112/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6298) |  Loss2: (0.0000) | Acc: (78.00%) (3108/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (78.00%) (4096/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (78.00%) (5096/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6471) |  Loss2: (0.0000) | Acc: (77.00%) (6051/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6477) |  Loss2: (0.0000) | Acc: (77.00%) (7017/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6506) |  Loss2: (0.0000) | Acc: (77.00%) (8001/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6439) |  Loss2: (0.0000) | Acc: (77.00%) (9013/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6415) |  Loss2: (0.0000) | Acc: (77.00%) (10022/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6374) |  Loss2: (0.0000) | Acc: (77.00%) (11021/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6377) |  Loss2: (0.0000) | Acc: (77.00%) (12019/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6364) |  Loss2: (0.0000) | Acc: (77.00%) (13029/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6353) |  Loss2: (0.0000) | Acc: (77.00%) (14030/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (77.00%) (15026/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (16018/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6360) |  Loss2: (0.0000) | Acc: (77.00%) (17024/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (18032/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6347) |  Loss2: (0.0000) | Acc: (77.00%) (19028/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (20008/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (77.00%) (21024/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (22016/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6339) |  Loss2: (0.0000) | Acc: (77.00%) (23030/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (78.00%) (24062/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (78.00%) (25063/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (78.00%) (26071/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (78.00%) (27067/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (78.00%) (28062/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (29082/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (78.00%) (30091/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (78.00%) (31092/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (32077/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6323) |  Loss2: (0.0000) | Acc: (78.00%) (33067/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (34068/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6319) |  Loss2: (0.0000) | Acc: (78.00%) (35059/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (36050/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6324) |  Loss2: (0.0000) | Acc: (77.00%) (37035/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6328) |  Loss2: (0.0000) | Acc: (77.00%) (38034/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (78.00%) (39001/50000)
# TEST : Loss: (0.6640) | Acc: (77.00%) (7712/10000)
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6752, 0.3248], device='cuda:0')
percent tensor([0.6255, 0.3745], device='cuda:0')
percent tensor([0.9927, 0.0073], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (78.00%) (1107/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6479) |  Loss2: (0.0000) | Acc: (78.00%) (2101/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6510) |  Loss2: (0.0000) | Acc: (77.00%) (3084/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6351) |  Loss2: (0.0000) | Acc: (78.00%) (4103/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (5123/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (78.00%) (6112/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (7119/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (78.00%) (8110/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (78.00%) (9091/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (78.00%) (10106/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (78.00%) (11102/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (78.00%) (12107/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (78.00%) (13116/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (14131/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (78.00%) (15131/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6276) |  Loss2: (0.0000) | Acc: (78.00%) (16148/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (17137/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6282) |  Loss2: (0.0000) | Acc: (78.00%) (18138/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (19145/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (78.00%) (20176/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6257) |  Loss2: (0.0000) | Acc: (78.00%) (21151/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6268) |  Loss2: (0.0000) | Acc: (78.00%) (22137/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (23166/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (24155/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (25134/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6251) |  Loss2: (0.0000) | Acc: (78.00%) (26145/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6250) |  Loss2: (0.0000) | Acc: (78.00%) (27152/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (78.00%) (28161/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6229) |  Loss2: (0.0000) | Acc: (78.00%) (29191/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (30187/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6224) |  Loss2: (0.0000) | Acc: (78.00%) (31204/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (32213/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (33236/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6197) |  Loss2: (0.0000) | Acc: (78.00%) (34261/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (35265/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (36288/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6167) |  Loss2: (0.0000) | Acc: (78.00%) (37306/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (38299/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6167) |  Loss2: (0.0000) | Acc: (78.00%) (39279/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.8294) | Acc: (72.00%) (7247/10000)
percent tensor([0.4828, 0.5172], device='cuda:0')
percent tensor([0.4769, 0.5231], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6291, 0.3709], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.4595, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(791.2344, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(782.5692, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.1936, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(514.8881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2175.9382, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4303.1338, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1429.2252, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6094.5366, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12139.0479, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4037.7922, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17128.1250, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5889) |  Loss2: (0.0000) | Acc: (80.00%) (1130/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (80.00%) (2158/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5951) |  Loss2: (0.0000) | Acc: (80.00%) (3182/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5924) |  Loss2: (0.0000) | Acc: (79.00%) (4195/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5874) |  Loss2: (0.0000) | Acc: (80.00%) (5224/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (80.00%) (6249/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5834) |  Loss2: (0.0000) | Acc: (80.00%) (7272/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5884) |  Loss2: (0.0000) | Acc: (79.00%) (8280/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (79.00%) (9316/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5804) |  Loss2: (0.0000) | Acc: (80.00%) (10371/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5830) |  Loss2: (0.0000) | Acc: (80.00%) (11375/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5850) |  Loss2: (0.0000) | Acc: (80.00%) (12392/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5866) |  Loss2: (0.0000) | Acc: (79.00%) (13406/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (14417/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5892) |  Loss2: (0.0000) | Acc: (79.00%) (15441/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5919) |  Loss2: (0.0000) | Acc: (79.00%) (16428/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5897) |  Loss2: (0.0000) | Acc: (79.00%) (17471/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (18472/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5933) |  Loss2: (0.0000) | Acc: (79.00%) (19487/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5934) |  Loss2: (0.0000) | Acc: (79.00%) (20505/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5910) |  Loss2: (0.0000) | Acc: (79.00%) (21547/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5894) |  Loss2: (0.0000) | Acc: (79.00%) (22573/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (23568/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5902) |  Loss2: (0.0000) | Acc: (79.00%) (24574/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5891) |  Loss2: (0.0000) | Acc: (79.00%) (25593/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (26616/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (27643/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5867) |  Loss2: (0.0000) | Acc: (79.00%) (28675/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (29675/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (30699/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5878) |  Loss2: (0.0000) | Acc: (79.00%) (31714/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5890) |  Loss2: (0.0000) | Acc: (79.00%) (32708/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (33760/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (34765/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5889) |  Loss2: (0.0000) | Acc: (79.00%) (35768/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (36807/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (37825/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5885) |  Loss2: (0.0000) | Acc: (79.00%) (38853/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (39832/50000)
# TEST : Loss: (0.6959) | Acc: (76.00%) (7629/10000)
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5230, 0.4770], device='cuda:0')
percent tensor([0.6790, 0.3210], device='cuda:0')
percent tensor([0.6373, 0.3627], device='cuda:0')
percent tensor([0.9944, 0.0056], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (82.00%) (1157/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (81.00%) (2188/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5533) |  Loss2: (0.0000) | Acc: (81.00%) (3215/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5604) |  Loss2: (0.0000) | Acc: (80.00%) (4224/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5668) |  Loss2: (0.0000) | Acc: (80.00%) (5251/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5722) |  Loss2: (0.0000) | Acc: (80.00%) (6250/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (7277/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5651) |  Loss2: (0.0000) | Acc: (80.00%) (8327/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5674) |  Loss2: (0.0000) | Acc: (80.00%) (9351/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (10378/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (80.00%) (11400/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5659) |  Loss2: (0.0000) | Acc: (80.00%) (12428/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5677) |  Loss2: (0.0000) | Acc: (80.00%) (13431/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (14466/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (80.00%) (15511/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (16541/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (17569/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (18599/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5631) |  Loss2: (0.0000) | Acc: (80.00%) (19622/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (20682/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (21705/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5592) |  Loss2: (0.0000) | Acc: (80.00%) (22746/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (23787/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (24824/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5588) |  Loss2: (0.0000) | Acc: (80.00%) (25841/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (26865/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (27884/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (28877/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5631) |  Loss2: (0.0000) | Acc: (80.00%) (29905/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5634) |  Loss2: (0.0000) | Acc: (80.00%) (30919/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5638) |  Loss2: (0.0000) | Acc: (80.00%) (31942/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (32980/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (34025/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5607) |  Loss2: (0.0000) | Acc: (80.00%) (35083/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5616) |  Loss2: (0.0000) | Acc: (80.00%) (36109/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5619) |  Loss2: (0.0000) | Acc: (80.00%) (37152/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (38192/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (39217/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (40205/50000)
# TEST : Loss: (0.8264) | Acc: (73.00%) (7324/10000)
percent tensor([0.4822, 0.5178], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.6302, 0.3698], device='cuda:0')
percent tensor([0.9934, 0.0066], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (2170/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5395) |  Loss2: (0.0000) | Acc: (81.00%) (3215/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5412) |  Loss2: (0.0000) | Acc: (80.00%) (4240/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (80.00%) (5282/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (6345/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5323) |  Loss2: (0.0000) | Acc: (81.00%) (7394/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5315) |  Loss2: (0.0000) | Acc: (81.00%) (8438/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (9466/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (81.00%) (10496/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5360) |  Loss2: (0.0000) | Acc: (81.00%) (11553/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (12571/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (13616/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (14670/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (15684/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (16694/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (81.00%) (17738/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (81.00%) (18769/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5430) |  Loss2: (0.0000) | Acc: (81.00%) (19835/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (20882/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (21975/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5386) |  Loss2: (0.0000) | Acc: (81.00%) (23022/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (24066/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (25104/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5380) |  Loss2: (0.0000) | Acc: (81.00%) (26157/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (27212/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (28266/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (29324/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (30378/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (81.00%) (31422/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5359) |  Loss2: (0.0000) | Acc: (81.00%) (32449/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5371) |  Loss2: (0.0000) | Acc: (81.00%) (33476/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (34513/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (35554/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (36597/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5381) |  Loss2: (0.0000) | Acc: (81.00%) (37627/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5379) |  Loss2: (0.0000) | Acc: (81.00%) (38678/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (39725/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5367) |  Loss2: (0.0000) | Acc: (81.00%) (40750/50000)
# TEST : Loss: (0.7211) | Acc: (75.00%) (7504/10000)
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5219, 0.4781], device='cuda:0')
percent tensor([0.6731, 0.3269], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.9931, 0.0069], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (83.00%) (2241/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (83.00%) (3298/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (4355/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (5384/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (6429/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (7464/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (8534/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (9581/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (10654/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5162) |  Loss2: (0.0000) | Acc: (82.00%) (11719/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (12764/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (13832/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5136) |  Loss2: (0.0000) | Acc: (82.00%) (14886/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (15943/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5110) |  Loss2: (0.0000) | Acc: (82.00%) (17008/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5121) |  Loss2: (0.0000) | Acc: (82.00%) (18049/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (19119/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (20181/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (21218/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5127) |  Loss2: (0.0000) | Acc: (82.00%) (22266/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5128) |  Loss2: (0.0000) | Acc: (82.00%) (23327/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5147) |  Loss2: (0.0000) | Acc: (82.00%) (24352/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (25392/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5159) |  Loss2: (0.0000) | Acc: (82.00%) (26459/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (27487/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (28546/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (29609/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (30649/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (31691/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (32730/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (33791/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (34848/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (35916/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5169) |  Loss2: (0.0000) | Acc: (82.00%) (36974/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (38020/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (39056/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (40106/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (41117/50000)
# TEST : Loss: (0.6679) | Acc: (77.00%) (7729/10000)
percent tensor([0.4826, 0.5174], device='cuda:0')
percent tensor([0.4764, 0.5236], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6750, 0.3250], device='cuda:0')
percent tensor([0.6337, 0.3663], device='cuda:0')
percent tensor([0.9940, 0.0060], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5120) |  Loss2: (0.0000) | Acc: (82.00%) (1159/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (2164/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5977) |  Loss2: (0.0000) | Acc: (79.00%) (3137/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (4110/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (77.00%) (5064/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (6060/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (7062/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (8045/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6375) |  Loss2: (0.0000) | Acc: (77.00%) (9021/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (10002/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6374) |  Loss2: (0.0000) | Acc: (77.00%) (11008/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (12001/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (77.00%) (12999/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (13993/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (15020/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (16020/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6298) |  Loss2: (0.0000) | Acc: (77.00%) (17034/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (18034/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (77.00%) (19050/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (77.00%) (20066/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6285) |  Loss2: (0.0000) | Acc: (77.00%) (21053/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6272) |  Loss2: (0.0000) | Acc: (77.00%) (22049/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6253) |  Loss2: (0.0000) | Acc: (78.00%) (23092/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (24120/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (25103/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6231) |  Loss2: (0.0000) | Acc: (78.00%) (26116/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6217) |  Loss2: (0.0000) | Acc: (78.00%) (27138/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6206) |  Loss2: (0.0000) | Acc: (78.00%) (28155/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (29191/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (30211/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (31247/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6136) |  Loss2: (0.0000) | Acc: (78.00%) (32258/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6132) |  Loss2: (0.0000) | Acc: (78.00%) (33263/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6107) |  Loss2: (0.0000) | Acc: (78.00%) (34314/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6096) |  Loss2: (0.0000) | Acc: (78.00%) (35318/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6081) |  Loss2: (0.0000) | Acc: (78.00%) (36359/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6072) |  Loss2: (0.0000) | Acc: (78.00%) (37379/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6066) |  Loss2: (0.0000) | Acc: (78.00%) (38409/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6058) |  Loss2: (0.0000) | Acc: (78.00%) (39394/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6393) | Acc: (78.00%) (7813/10000)
percent tensor([0.4786, 0.5214], device='cuda:0')
percent tensor([0.4690, 0.5310], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.6304, 0.3696], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.9933, 0.0067], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (79.00%) (2147/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (80.00%) (3176/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5639) |  Loss2: (0.0000) | Acc: (80.00%) (4212/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (5237/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5666) |  Loss2: (0.0000) | Acc: (80.00%) (6269/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (7304/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (8360/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5564) |  Loss2: (0.0000) | Acc: (80.00%) (9402/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5528) |  Loss2: (0.0000) | Acc: (80.00%) (10463/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (80.00%) (11496/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (12518/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5597) |  Loss2: (0.0000) | Acc: (80.00%) (13539/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (14574/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5557) |  Loss2: (0.0000) | Acc: (80.00%) (15636/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (80.00%) (16679/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5536) |  Loss2: (0.0000) | Acc: (80.00%) (17706/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5515) |  Loss2: (0.0000) | Acc: (80.00%) (18766/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (80.00%) (19798/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (20868/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (81.00%) (21920/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5483) |  Loss2: (0.0000) | Acc: (81.00%) (22975/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (23988/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5496) |  Loss2: (0.0000) | Acc: (81.00%) (25035/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5503) |  Loss2: (0.0000) | Acc: (81.00%) (26068/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5494) |  Loss2: (0.0000) | Acc: (81.00%) (27113/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5505) |  Loss2: (0.0000) | Acc: (81.00%) (28137/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (80.00%) (29134/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (80.00%) (30168/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (31204/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5517) |  Loss2: (0.0000) | Acc: (81.00%) (32259/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5535) |  Loss2: (0.0000) | Acc: (80.00%) (33277/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5535) |  Loss2: (0.0000) | Acc: (81.00%) (34324/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (81.00%) (35362/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5558) |  Loss2: (0.0000) | Acc: (80.00%) (36376/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5555) |  Loss2: (0.0000) | Acc: (80.00%) (37411/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (38457/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (39493/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (40468/50000)
# TEST : Loss: (0.6011) | Acc: (79.00%) (7926/10000)
percent tensor([0.4809, 0.5191], device='cuda:0')
percent tensor([0.4671, 0.5329], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5211, 0.4789], device='cuda:0')
percent tensor([0.6319, 0.3681], device='cuda:0')
percent tensor([0.6063, 0.3937], device='cuda:0')
percent tensor([0.9941, 0.0059], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5427) |  Loss2: (0.0000) | Acc: (82.00%) (1160/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5325) |  Loss2: (0.0000) | Acc: (82.00%) (2210/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (81.00%) (3217/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5545) |  Loss2: (0.0000) | Acc: (80.00%) (4245/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (5276/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (80.00%) (6298/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (7322/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (8359/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5528) |  Loss2: (0.0000) | Acc: (80.00%) (9432/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (10467/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5487) |  Loss2: (0.0000) | Acc: (81.00%) (11526/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (81.00%) (12574/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (81.00%) (13594/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (14651/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (15708/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (16733/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (81.00%) (17759/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (18820/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (81.00%) (19888/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5399) |  Loss2: (0.0000) | Acc: (81.00%) (20951/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5405) |  Loss2: (0.0000) | Acc: (81.00%) (21996/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (23057/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (24124/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5359) |  Loss2: (0.0000) | Acc: (81.00%) (25171/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5362) |  Loss2: (0.0000) | Acc: (81.00%) (26215/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (27255/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (28266/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5375) |  Loss2: (0.0000) | Acc: (81.00%) (29319/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5375) |  Loss2: (0.0000) | Acc: (81.00%) (30381/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (31433/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (32479/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (33540/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5349) |  Loss2: (0.0000) | Acc: (81.00%) (34610/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (35637/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5354) |  Loss2: (0.0000) | Acc: (81.00%) (36679/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (37723/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (38767/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5339) |  Loss2: (0.0000) | Acc: (81.00%) (39824/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5343) |  Loss2: (0.0000) | Acc: (81.00%) (40823/50000)
# TEST : Loss: (0.5830) | Acc: (79.00%) (7984/10000)
percent tensor([0.4840, 0.5160], device='cuda:0')
percent tensor([0.4684, 0.5316], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.6223, 0.3777], device='cuda:0')
percent tensor([0.9950, 0.0050], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.5311) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (1150/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (81.00%) (2199/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5374) |  Loss2: (0.0000) | Acc: (82.00%) (3256/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (4303/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (82.00%) (5367/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (82.00%) (6411/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5355) |  Loss2: (0.0000) | Acc: (82.00%) (7473/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (82.00%) (8523/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (82.00%) (9579/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (82.00%) (10629/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5329) |  Loss2: (0.0000) | Acc: (82.00%) (11684/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (82.00%) (12727/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (82.00%) (13765/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5287) |  Loss2: (0.0000) | Acc: (82.00%) (14820/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (82.00%) (15857/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (82.00%) (16902/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (81.00%) (17946/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (18983/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (20046/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (81.00%) (21075/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (22126/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (23190/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (24279/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (25328/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (82.00%) (26377/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (27407/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (82.00%) (28462/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (82.00%) (29511/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (30582/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (82.00%) (31634/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (32659/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (82.00%) (33724/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5239) |  Loss2: (0.0000) | Acc: (82.00%) (34748/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (82.00%) (35819/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (82.00%) (36874/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (82.00%) (37915/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (38977/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (40019/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (82.00%) (41032/50000)
# TEST : Loss: (0.5729) | Acc: (80.00%) (8039/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4687, 0.5313], device='cuda:0')
percent tensor([0.5141, 0.4859], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5276, 0.4724], device='cuda:0')
percent tensor([0.6507, 0.3493], device='cuda:0')
percent tensor([0.6365, 0.3635], device='cuda:0')
percent tensor([0.9957, 0.0043], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (83.00%) (1170/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (2194/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (3246/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (4278/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (5325/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (81.00%) (6373/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (7422/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (81.00%) (8497/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (81.00%) (9539/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (10587/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (11624/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (12670/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (13725/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (14784/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (15830/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (16886/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5244) |  Loss2: (0.0000) | Acc: (81.00%) (17925/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5248) |  Loss2: (0.0000) | Acc: (81.00%) (18960/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (20002/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (81.00%) (21050/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (22086/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (23164/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (81.00%) (24209/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (25271/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (26302/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (27353/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (28408/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (29459/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5222) |  Loss2: (0.0000) | Acc: (81.00%) (30511/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (81.00%) (31581/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (32647/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (33721/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (34788/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (35839/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (36902/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (82.00%) (37960/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (38990/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (82.00%) (40051/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (41076/50000)
# TEST : Loss: (0.5658) | Acc: (80.00%) (8074/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4696, 0.5304], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.6436, 0.3564], device='cuda:0')
percent tensor([0.9962, 0.0038], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4825) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (82.00%) (1163/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (2227/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (3284/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5126) |  Loss2: (0.0000) | Acc: (82.00%) (4321/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (5394/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (6467/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (7512/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (8560/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (9616/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (10707/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.5048) |  Loss2: (0.0000) | Acc: (82.00%) (11759/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (12826/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (13888/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (14943/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (15988/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.5119) |  Loss2: (0.0000) | Acc: (82.00%) (17030/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (18082/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (19171/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (20228/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (21320/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.5054) |  Loss2: (0.0000) | Acc: (82.00%) (22368/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (23427/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (24454/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (25501/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (26521/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (27576/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (28634/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (29683/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.5101) |  Loss2: (0.0000) | Acc: (82.00%) (30728/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (31785/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (32832/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (33903/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (34963/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (36023/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (37080/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (38125/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (39178/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (40248/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (41271/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.5971) | Acc: (79.00%) (7988/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4698, 0.5302], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.9967, 0.0033], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(173.3459, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(795.5269, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(786.8863, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.0380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(513.1813, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2184.1340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.7358, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1424.2054, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6098.9497, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12097.9209, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4022.0383, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17055.4238, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.5547) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4996) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4813) |  Loss2: (0.0000) | Acc: (83.00%) (2244/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (3303/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (4381/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4676) |  Loss2: (0.0000) | Acc: (83.00%) (5478/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (6524/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (7576/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4809) |  Loss2: (0.0000) | Acc: (83.00%) (8646/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4775) |  Loss2: (0.0000) | Acc: (83.00%) (9743/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4742) |  Loss2: (0.0000) | Acc: (83.00%) (10832/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4769) |  Loss2: (0.0000) | Acc: (83.00%) (11893/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4792) |  Loss2: (0.0000) | Acc: (83.00%) (12947/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (14006/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4836) |  Loss2: (0.0000) | Acc: (83.00%) (15044/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (16129/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4818) |  Loss2: (0.0000) | Acc: (83.00%) (17196/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4822) |  Loss2: (0.0000) | Acc: (83.00%) (18257/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (19345/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (20404/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (21462/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4810) |  Loss2: (0.0000) | Acc: (83.00%) (22497/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4810) |  Loss2: (0.0000) | Acc: (83.00%) (23557/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4809) |  Loss2: (0.0000) | Acc: (83.00%) (24620/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4821) |  Loss2: (0.0000) | Acc: (83.00%) (25675/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4830) |  Loss2: (0.0000) | Acc: (83.00%) (26742/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4828) |  Loss2: (0.0000) | Acc: (83.00%) (27811/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (28849/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (29901/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (30973/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (32043/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4848) |  Loss2: (0.0000) | Acc: (83.00%) (33124/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4846) |  Loss2: (0.0000) | Acc: (83.00%) (34203/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (35259/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (36302/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (37379/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (38451/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (83.00%) (39521/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (83.00%) (40574/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (41598/50000)
# TEST : Loss: (0.5475) | Acc: (80.00%) (8091/10000)
percent tensor([0.4847, 0.5153], device='cuda:0')
percent tensor([0.4705, 0.5295], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.6467, 0.3533], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5814) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (83.00%) (2237/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4661) |  Loss2: (0.0000) | Acc: (83.00%) (3306/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4683) |  Loss2: (0.0000) | Acc: (83.00%) (4376/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (5467/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (83.00%) (6554/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (83.00%) (7611/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (83.00%) (8670/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4628) |  Loss2: (0.0000) | Acc: (83.00%) (9752/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (10816/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4657) |  Loss2: (0.0000) | Acc: (83.00%) (11884/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (12963/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4670) |  Loss2: (0.0000) | Acc: (83.00%) (14038/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (15117/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (83.00%) (16201/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (83.00%) (17271/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4637) |  Loss2: (0.0000) | Acc: (83.00%) (18336/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (19401/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (20472/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (21549/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (22613/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4690) |  Loss2: (0.0000) | Acc: (83.00%) (23675/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (24747/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (25812/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (26911/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (27966/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (29043/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (30132/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (31200/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (32244/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (33311/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (34389/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4723) |  Loss2: (0.0000) | Acc: (83.00%) (35470/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (36542/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (37615/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (83.00%) (38699/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (39779/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (83.00%) (40857/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (83.00%) (41887/50000)
# TEST : Loss: (0.5588) | Acc: (80.00%) (8083/10000)
percent tensor([0.4848, 0.5152], device='cuda:0')
percent tensor([0.4704, 0.5296], device='cuda:0')
percent tensor([0.5157, 0.4843], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6574, 0.3426], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (1187/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4479) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (85.00%) (3376/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (4460/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4492) |  Loss2: (0.0000) | Acc: (85.00%) (5553/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (6623/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (8805/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (9882/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (10930/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4539) |  Loss2: (0.0000) | Acc: (84.00%) (12003/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4563) |  Loss2: (0.0000) | Acc: (84.00%) (13077/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (14147/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4597) |  Loss2: (0.0000) | Acc: (84.00%) (15216/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (84.00%) (16301/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (17412/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (18506/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (19568/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (20653/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (84.00%) (21707/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (84.00%) (22762/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (23847/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (24938/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (26004/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (27069/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4573) |  Loss2: (0.0000) | Acc: (84.00%) (28155/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (29255/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (30340/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (31414/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (32503/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (33579/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (34681/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4557) |  Loss2: (0.0000) | Acc: (84.00%) (35753/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (36843/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (37924/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (39015/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4541) |  Loss2: (0.0000) | Acc: (84.00%) (40103/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4538) |  Loss2: (0.0000) | Acc: (84.00%) (41181/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4542) |  Loss2: (0.0000) | Acc: (84.00%) (42213/50000)
# TEST : Loss: (0.6350) | Acc: (78.00%) (7870/10000)
percent tensor([0.4846, 0.5154], device='cuda:0')
percent tensor([0.4696, 0.5304], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (84.00%) (1184/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (2282/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (3354/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (4426/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (5510/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (6597/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4445) |  Loss2: (0.0000) | Acc: (84.00%) (7684/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (8754/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4415) |  Loss2: (0.0000) | Acc: (84.00%) (9853/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (10925/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (12011/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (13094/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (14161/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (15261/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (16360/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (17456/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (18538/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (19607/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (20674/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (21733/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (22824/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (23912/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (24999/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (26078/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (27174/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (28272/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (29372/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (30470/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (31553/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (32639/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (33730/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (34803/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (35893/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (36990/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (38083/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (39181/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (40296/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (41370/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (42405/50000)
# TEST : Loss: (0.6114) | Acc: (79.00%) (7966/10000)
percent tensor([0.4847, 0.5153], device='cuda:0')
percent tensor([0.4705, 0.5295], device='cuda:0')
percent tensor([0.5161, 0.4839], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5315, 0.4685], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.6463, 0.3537], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.6918) |  Loss2: (0.0000) | Acc: (76.00%) (1071/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.8134) |  Loss2: (0.0000) | Acc: (72.00%) (1943/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.8855) |  Loss2: (0.0000) | Acc: (70.00%) (2801/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.9202) |  Loss2: (0.0000) | Acc: (69.00%) (3652/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.9410) |  Loss2: (0.0000) | Acc: (69.00%) (4510/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.9302) |  Loss2: (0.0000) | Acc: (69.00%) (5411/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.9230) |  Loss2: (0.0000) | Acc: (69.00%) (6312/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (69.00%) (7240/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.8942) |  Loss2: (0.0000) | Acc: (70.00%) (8210/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.8795) |  Loss2: (0.0000) | Acc: (70.00%) (9161/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.8670) |  Loss2: (0.0000) | Acc: (71.00%) (10117/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.8504) |  Loss2: (0.0000) | Acc: (71.00%) (11108/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.8402) |  Loss2: (0.0000) | Acc: (72.00%) (12077/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.8295) |  Loss2: (0.0000) | Acc: (72.00%) (13058/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (72.00%) (14057/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.8065) |  Loss2: (0.0000) | Acc: (72.00%) (15038/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (73.00%) (16009/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (73.00%) (16998/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.7858) |  Loss2: (0.0000) | Acc: (73.00%) (17993/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.7821) |  Loss2: (0.0000) | Acc: (73.00%) (18951/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.7759) |  Loss2: (0.0000) | Acc: (73.00%) (19941/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.7690) |  Loss2: (0.0000) | Acc: (73.00%) (20930/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (74.00%) (21925/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.7568) |  Loss2: (0.0000) | Acc: (74.00%) (22944/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.7510) |  Loss2: (0.0000) | Acc: (74.00%) (23959/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (74.00%) (24966/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.7387) |  Loss2: (0.0000) | Acc: (74.00%) (25993/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.7334) |  Loss2: (0.0000) | Acc: (75.00%) (27003/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.7284) |  Loss2: (0.0000) | Acc: (75.00%) (28014/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.7242) |  Loss2: (0.0000) | Acc: (75.00%) (29038/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.7190) |  Loss2: (0.0000) | Acc: (75.00%) (30077/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.7167) |  Loss2: (0.0000) | Acc: (75.00%) (31089/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.7133) |  Loss2: (0.0000) | Acc: (75.00%) (32085/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.7098) |  Loss2: (0.0000) | Acc: (75.00%) (33094/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.7066) |  Loss2: (0.0000) | Acc: (75.00%) (34103/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.7036) |  Loss2: (0.0000) | Acc: (76.00%) (35127/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.7004) |  Loss2: (0.0000) | Acc: (76.00%) (36129/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.6958) |  Loss2: (0.0000) | Acc: (76.00%) (37184/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.6923) |  Loss2: (0.0000) | Acc: (76.00%) (38171/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.6555) | Acc: (77.00%) (7763/10000)
percent tensor([0.4931, 0.5069], device='cuda:0')
percent tensor([0.4495, 0.5505], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.5528, 0.4472], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.5447) |  Loss2: (0.0000) | Acc: (80.00%) (1131/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (2163/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (3191/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (4207/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (5238/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.5519) |  Loss2: (0.0000) | Acc: (80.00%) (6274/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.5520) |  Loss2: (0.0000) | Acc: (80.00%) (7316/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.5578) |  Loss2: (0.0000) | Acc: (80.00%) (8332/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.5554) |  Loss2: (0.0000) | Acc: (80.00%) (9368/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.5550) |  Loss2: (0.0000) | Acc: (80.00%) (10414/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (11459/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.5538) |  Loss2: (0.0000) | Acc: (80.00%) (12487/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (13500/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.5553) |  Loss2: (0.0000) | Acc: (80.00%) (14522/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (15537/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.5555) |  Loss2: (0.0000) | Acc: (80.00%) (16577/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (17620/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.5544) |  Loss2: (0.0000) | Acc: (80.00%) (18654/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (19685/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (20754/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.5499) |  Loss2: (0.0000) | Acc: (80.00%) (21826/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.5498) |  Loss2: (0.0000) | Acc: (80.00%) (22860/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.5482) |  Loss2: (0.0000) | Acc: (80.00%) (23916/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (80.00%) (24964/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.5476) |  Loss2: (0.0000) | Acc: (80.00%) (25990/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.5475) |  Loss2: (0.0000) | Acc: (80.00%) (27039/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.5476) |  Loss2: (0.0000) | Acc: (80.00%) (28067/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.5464) |  Loss2: (0.0000) | Acc: (80.00%) (29129/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.5459) |  Loss2: (0.0000) | Acc: (81.00%) (30174/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.5449) |  Loss2: (0.0000) | Acc: (81.00%) (31228/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.5450) |  Loss2: (0.0000) | Acc: (81.00%) (32256/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.5444) |  Loss2: (0.0000) | Acc: (81.00%) (33296/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (34365/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.5422) |  Loss2: (0.0000) | Acc: (81.00%) (35396/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.5413) |  Loss2: (0.0000) | Acc: (81.00%) (36449/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.5404) |  Loss2: (0.0000) | Acc: (81.00%) (37511/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (38571/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (39611/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (40626/50000)
# TEST : Loss: (0.5961) | Acc: (79.00%) (7953/10000)
percent tensor([0.4952, 0.5048], device='cuda:0')
percent tensor([0.4479, 0.5521], device='cuda:0')
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.5580, 0.4420], device='cuda:0')
percent tensor([0.9962, 0.0038], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (1165/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (2211/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (3273/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (82.00%) (4337/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (5376/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (6433/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (81.00%) (7439/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.5160) |  Loss2: (0.0000) | Acc: (81.00%) (8497/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (81.00%) (9543/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (81.00%) (10594/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (81.00%) (11643/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (12675/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.5227) |  Loss2: (0.0000) | Acc: (81.00%) (13691/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (81.00%) (14734/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.5251) |  Loss2: (0.0000) | Acc: (81.00%) (15768/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (16835/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (17911/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (18947/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.5230) |  Loss2: (0.0000) | Acc: (81.00%) (19973/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.5239) |  Loss2: (0.0000) | Acc: (81.00%) (21014/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (22080/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (81.00%) (23122/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (24194/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (81.00%) (25245/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (81.00%) (26301/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (81.00%) (27347/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (28392/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (29464/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (81.00%) (30498/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (31555/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (81.00%) (32617/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (81.00%) (33680/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (81.00%) (34740/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (81.00%) (35791/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (36858/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (37940/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.5158) |  Loss2: (0.0000) | Acc: (82.00%) (39015/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (82.00%) (40080/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.5145) |  Loss2: (0.0000) | Acc: (82.00%) (41103/50000)
# TEST : Loss: (0.5688) | Acc: (80.00%) (8040/10000)
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.4494, 0.5506], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.9965, 0.0035], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.5603) |  Loss2: (0.0000) | Acc: (80.00%) (1129/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.5569) |  Loss2: (0.0000) | Acc: (80.00%) (2165/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.5437) |  Loss2: (0.0000) | Acc: (81.00%) (3223/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (4278/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (5322/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (6401/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (7452/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (8519/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.5128) |  Loss2: (0.0000) | Acc: (82.00%) (9596/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (10634/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.5134) |  Loss2: (0.0000) | Acc: (82.00%) (11709/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.5116) |  Loss2: (0.0000) | Acc: (82.00%) (12768/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (13851/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (14905/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (15969/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.5069) |  Loss2: (0.0000) | Acc: (82.00%) (17023/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (18085/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (19141/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (20173/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (21244/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (22321/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (23392/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (24445/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (82.00%) (25473/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (26520/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (27590/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (28648/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (29722/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.5005) |  Loss2: (0.0000) | Acc: (82.00%) (30784/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (82.00%) (31874/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (32923/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (33977/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (35043/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4986) |  Loss2: (0.0000) | Acc: (82.00%) (36096/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (82.00%) (37149/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (38207/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (39252/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (40299/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (41319/50000)
# TEST : Loss: (0.5497) | Acc: (81.00%) (8103/10000)
percent tensor([0.4957, 0.5043], device='cuda:0')
percent tensor([0.4519, 0.5481], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (2250/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (3305/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (4333/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (5392/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (6462/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (7532/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (8590/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (82.00%) (9661/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (82.00%) (10724/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (11774/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4955) |  Loss2: (0.0000) | Acc: (82.00%) (12842/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (82.00%) (13899/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (14979/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (16059/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (17112/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (18175/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (19263/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (83.00%) (20333/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (83.00%) (21372/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (22447/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (83.00%) (23502/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4925) |  Loss2: (0.0000) | Acc: (83.00%) (24592/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (25637/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4918) |  Loss2: (0.0000) | Acc: (83.00%) (26739/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (27808/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (83.00%) (28897/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (83.00%) (29933/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (30992/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (32041/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (83.00%) (33104/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (83.00%) (34182/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (35250/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (83.00%) (36294/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (37357/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4922) |  Loss2: (0.0000) | Acc: (83.00%) (38432/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (39513/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (83.00%) (40588/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (41629/50000)
# TEST : Loss: (0.5429) | Acc: (81.00%) (8109/10000)
percent tensor([0.4947, 0.5053], device='cuda:0')
percent tensor([0.4541, 0.5459], device='cuda:0')
percent tensor([0.5086, 0.4914], device='cuda:0')
percent tensor([0.5192, 0.4808], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.5871, 0.4129], device='cuda:0')
percent tensor([0.9971, 0.0029], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (83.00%) (2235/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (3292/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (4359/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4904) |  Loss2: (0.0000) | Acc: (82.00%) (5403/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4883) |  Loss2: (0.0000) | Acc: (82.00%) (6471/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (83.00%) (7555/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (83.00%) (8636/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4777) |  Loss2: (0.0000) | Acc: (83.00%) (9721/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4724) |  Loss2: (0.0000) | Acc: (83.00%) (10805/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (11882/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4715) |  Loss2: (0.0000) | Acc: (83.00%) (12957/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4695) |  Loss2: (0.0000) | Acc: (83.00%) (14034/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (15106/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4655) |  Loss2: (0.0000) | Acc: (83.00%) (16198/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4642) |  Loss2: (0.0000) | Acc: (83.00%) (17286/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4643) |  Loss2: (0.0000) | Acc: (83.00%) (18354/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (84.00%) (19463/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4603) |  Loss2: (0.0000) | Acc: (84.00%) (20545/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (83.00%) (21611/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (84.00%) (22692/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4593) |  Loss2: (0.0000) | Acc: (84.00%) (23783/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (24874/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (25965/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (27042/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4538) |  Loss2: (0.0000) | Acc: (84.00%) (28136/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (29234/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (30302/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (31380/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (32455/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (33527/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4542) |  Loss2: (0.0000) | Acc: (84.00%) (34594/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4539) |  Loss2: (0.0000) | Acc: (84.00%) (35680/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4537) |  Loss2: (0.0000) | Acc: (84.00%) (36752/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (37842/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (38923/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (40020/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (41111/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (42167/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.6228) | Acc: (79.00%) (7934/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4541, 0.5459], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.7578, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.2773, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(790.8948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.9656, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(511.4439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2191.7561, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4291.2891, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1419.2577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6107.1919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12058.5498, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4006.4846, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16984.6348, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (1186/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4510) |  Loss2: (0.0000) | Acc: (84.00%) (2270/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (3345/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (4410/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (5494/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (6583/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (7680/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (8777/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (9887/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (10979/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (12079/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (13188/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (14283/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (85.00%) (15367/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (16445/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (17532/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (18640/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (19737/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (20838/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4325) |  Loss2: (0.0000) | Acc: (85.00%) (21935/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (85.00%) (23010/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (24127/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (25222/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (26332/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (27424/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (28512/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (85.00%) (29610/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (30697/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (31796/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (32883/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (33982/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (35051/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (36167/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (37249/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (38355/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (39445/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (40537/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (41636/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4277) |  Loss2: (0.0000) | Acc: (85.00%) (42698/50000)
# TEST : Loss: (0.5321) | Acc: (82.00%) (8265/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4548, 0.5452], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.5871, 0.4129], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (86.00%) (2326/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (86.00%) (3427/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (86.00%) (4523/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (86.00%) (5640/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4058) |  Loss2: (0.0000) | Acc: (86.00%) (6759/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (86.00%) (7837/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (86.00%) (8939/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (86.00%) (10044/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4094) |  Loss2: (0.0000) | Acc: (86.00%) (11145/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (12258/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (86.00%) (13361/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (86.00%) (14444/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (15578/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (86.00%) (16663/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (86.00%) (17759/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (86.00%) (18838/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (86.00%) (19939/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (86.00%) (21035/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (86.00%) (22150/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (86.00%) (23253/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (24364/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (25460/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (86.00%) (26570/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (86.00%) (27670/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (86.00%) (28775/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (86.00%) (29839/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (86.00%) (30973/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (86.00%) (32075/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (86.00%) (33182/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (86.00%) (34302/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (86.00%) (35400/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (86.00%) (36496/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (86.00%) (37597/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (86.00%) (38669/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (86.00%) (39774/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (86.00%) (40875/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (86.00%) (41965/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (86.00%) (43003/50000)
# TEST : Loss: (0.5679) | Acc: (80.00%) (8070/10000)
percent tensor([0.4945, 0.5055], device='cuda:0')
percent tensor([0.4537, 0.5463], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.6147, 0.3853], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (1199/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (85.00%) (2305/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (85.00%) (3404/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (4504/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (85.00%) (5612/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (6703/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (7796/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (8893/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (10006/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (11117/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (85.00%) (12211/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (13317/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (85.00%) (14420/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (15530/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (16636/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.4012) |  Loss2: (0.0000) | Acc: (86.00%) (17763/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (18863/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (19963/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (21067/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (22166/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (23265/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (24385/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (25480/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (26574/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (27693/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (28799/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3992) |  Loss2: (0.0000) | Acc: (86.00%) (29917/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (31013/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3996) |  Loss2: (0.0000) | Acc: (86.00%) (32126/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (33217/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (34303/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (35417/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (36513/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (37600/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (38687/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.4034) |  Loss2: (0.0000) | Acc: (86.00%) (39795/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (40899/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (42008/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (43053/50000)
# TEST : Loss: (0.5011) | Acc: (83.00%) (8343/10000)
percent tensor([0.4944, 0.5056], device='cuda:0')
percent tensor([0.4544, 0.5456], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.6152, 0.3848], device='cuda:0')
percent tensor([0.5885, 0.4115], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3990) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (4559/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (5633/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3984) |  Loss2: (0.0000) | Acc: (86.00%) (6744/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (7846/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (8971/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (10093/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (11195/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (12320/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (13431/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (14566/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (15677/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (16785/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (17878/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (18978/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (20079/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (21195/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (22309/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (23402/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (24504/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (25600/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (26709/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (27788/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (28898/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (30030/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (31149/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (32255/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (33369/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (34489/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (35585/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (36699/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (37801/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (38905/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (40001/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (41102/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (42220/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (43284/50000)
# TEST : Loss: (0.5580) | Acc: (81.00%) (8192/10000)
percent tensor([0.4944, 0.5056], device='cuda:0')
percent tensor([0.4543, 0.5457], device='cuda:0')
percent tensor([0.5088, 0.4912], device='cuda:0')
percent tensor([0.5192, 0.4808], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6153, 0.3847], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3976) |  Loss2: (0.0000) | Acc: (86.00%) (2315/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (85.00%) (3399/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4275) |  Loss2: (0.0000) | Acc: (85.00%) (4466/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (5533/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (6601/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (7672/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (8737/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (9831/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (10922/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4445) |  Loss2: (0.0000) | Acc: (84.00%) (11987/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (13067/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (14138/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (15228/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4477) |  Loss2: (0.0000) | Acc: (84.00%) (16310/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (17428/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (18518/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (19597/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (20683/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (21793/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (22871/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (23949/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (25043/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (26140/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (27211/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (28303/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (29383/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (30467/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (31559/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (32616/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (33716/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (34797/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (35894/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (37001/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (38084/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (39178/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (40244/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (41336/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (42370/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.5007) | Acc: (82.00%) (8292/10000)
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.4551, 0.5449], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.5502, 0.4498], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (1192/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (3376/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (85.00%) (4488/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (5581/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (6667/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (7749/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (8818/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (9915/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (10993/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (12095/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (13188/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4207) |  Loss2: (0.0000) | Acc: (85.00%) (14282/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (15384/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (16485/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (17580/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (18668/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (19769/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (20879/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (21998/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (23088/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (24209/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (25323/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (26422/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (27536/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (28624/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (29725/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (30821/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (31920/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (33031/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (34158/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (35261/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (36366/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (37463/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (38555/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (39646/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (85.00%) (40782/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (41905/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (85.00%) (42998/50000)
# TEST : Loss: (0.4765) | Acc: (83.00%) (8370/10000)
percent tensor([0.5020, 0.4980], device='cuda:0')
percent tensor([0.4553, 0.5447], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.6195, 0.3805], device='cuda:0')
percent tensor([0.5532, 0.4468], device='cuda:0')
percent tensor([0.9971, 0.0029], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (2328/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (3435/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (4541/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (5648/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (6728/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (7845/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.4051) |  Loss2: (0.0000) | Acc: (86.00%) (8935/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (10050/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (11137/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.4062) |  Loss2: (0.0000) | Acc: (86.00%) (12246/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (86.00%) (13339/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (86.00%) (14429/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (15506/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (16618/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (85.00%) (17715/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (86.00%) (18840/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (19931/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (21050/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (22150/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (23250/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (24360/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.4001) |  Loss2: (0.0000) | Acc: (86.00%) (25486/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (26579/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3992) |  Loss2: (0.0000) | Acc: (86.00%) (27714/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (28833/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (29934/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (31076/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3957) |  Loss2: (0.0000) | Acc: (86.00%) (32167/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (33274/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (34344/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (35474/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (36576/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (37693/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (38792/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (39890/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (40976/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (42108/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (43146/50000)
# TEST : Loss: (0.4649) | Acc: (84.00%) (8421/10000)
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.4531, 0.5469], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.6192, 0.3808], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (86.00%) (2336/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (3426/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (4536/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3882) |  Loss2: (0.0000) | Acc: (86.00%) (5635/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (6747/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (7854/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (8956/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (10079/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (11182/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (13429/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (14553/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (15677/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (16774/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (17907/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (18998/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (20100/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (21220/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (22351/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (23469/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (24575/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (25665/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (26773/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (27886/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (29021/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (30145/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (31245/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (32349/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (33466/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (34596/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (35707/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (86.00%) (36827/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (37950/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (39055/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (40165/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (41258/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (42349/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (43419/50000)
# TEST : Loss: (0.4588) | Acc: (84.00%) (8443/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4551, 0.5449], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5377, 0.4623], device='cuda:0')
percent tensor([0.6233, 0.3767], device='cuda:0')
percent tensor([0.5706, 0.4294], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (3437/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (4527/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (5622/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (6719/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (7845/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (8958/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (10083/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (11191/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (13416/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (14542/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (15648/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (16754/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (17876/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (18987/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (20114/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (21222/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (22338/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (23448/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (24578/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (25685/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3833) |  Loss2: (0.0000) | Acc: (86.00%) (26781/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (27902/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (29025/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (30140/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (31258/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (32365/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (33466/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (34556/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (35667/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (36754/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (37874/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (38982/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (40087/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (41208/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (42327/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (43397/50000)
# TEST : Loss: (0.4505) | Acc: (84.00%) (8465/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4523, 0.5477], device='cuda:0')
percent tensor([0.5170, 0.4830], device='cuda:0')
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.5413, 0.4587], device='cuda:0')
percent tensor([0.6275, 0.3725], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3667) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (2350/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (87.00%) (3464/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (87.00%) (4574/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (87.00%) (5685/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (6792/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (87.00%) (7913/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (9040/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (10175/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (87.00%) (11309/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (87.00%) (12412/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3755) |  Loss2: (0.0000) | Acc: (87.00%) (13529/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (14635/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (15737/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (16854/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (17972/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (19088/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (87.00%) (20211/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (21305/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (22421/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (87.00%) (23540/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (24649/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (25774/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (87.00%) (26880/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (28009/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (29129/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (30215/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (31325/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (32445/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (33554/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (34661/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (87.00%) (35782/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (36891/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (87.00%) (37993/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (39115/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3809) |  Loss2: (0.0000) | Acc: (87.00%) (40204/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (41326/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (42433/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (43521/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.5263) | Acc: (82.00%) (8249/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4525, 0.5475], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.6240, 0.3760], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.0744, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.8675, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.8819, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1520.3204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(509.7896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2200.1914, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4287.3887, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1414.2598, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6116.5371, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12019.6758, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3990.9797, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16915.0938, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (87.00%) (3490/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (4613/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3518) |  Loss2: (0.0000) | Acc: (87.00%) (5740/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (6876/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (7983/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (9102/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (10205/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (11328/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (12417/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (13566/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (14661/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (15760/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (16889/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (17998/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (19125/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (20235/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (21355/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (22476/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (23589/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (24706/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (25843/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (26953/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (28083/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (29187/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (30318/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (31431/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (32552/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (33654/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (34760/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (35859/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (36955/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (38076/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (39196/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (40301/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (41419/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (42527/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (43598/50000)
# TEST : Loss: (0.5342) | Acc: (82.00%) (8222/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4522, 0.5478], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.6289, 0.3711], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (1231/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (2370/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (3470/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (4596/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (5702/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (6820/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (7932/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (9049/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (10159/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (11268/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (12375/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (13481/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (86.00%) (14581/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (15707/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (86.00%) (16813/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (17937/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (19076/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (20201/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (21323/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (22455/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (23575/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (24704/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (25831/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (26966/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (28106/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (29228/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (30346/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (31457/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (32596/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (33727/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (34853/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (35965/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (37085/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (38218/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (39341/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (40471/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (41586/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (42711/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (43791/50000)
# TEST : Loss: (0.4852) | Acc: (83.00%) (8369/10000)
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.4522, 0.5478], device='cuda:0')
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5433, 0.4567], device='cuda:0')
percent tensor([0.5408, 0.4592], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (2361/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (3456/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (4594/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (5722/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (6852/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (7974/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (9119/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (10236/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (11361/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (12486/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (13607/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (14752/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (15894/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (17007/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (18134/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (19257/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (20383/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (21529/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (22655/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (23790/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (24930/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (26076/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (27188/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (28311/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (29441/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (30567/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (31704/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (32824/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (33921/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (35040/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (88.00%) (36175/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (88.00%) (37310/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (88.00%) (38435/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (88.00%) (39550/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (40652/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3525) |  Loss2: (0.0000) | Acc: (87.00%) (41760/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (42901/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (43959/50000)
# TEST : Loss: (0.4788) | Acc: (84.00%) (8402/10000)
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.4526, 0.5474], device='cuda:0')
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.6257, 0.3743], device='cuda:0')
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (87.00%) (1237/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (87.00%) (2361/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (3524/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (89.00%) (4676/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (89.00%) (5813/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (6942/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (8070/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (9209/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (10345/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (11486/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (12617/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (13769/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (14905/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (16043/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (17190/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (18315/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (19448/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (20575/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (21700/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (22838/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (23965/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (25103/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (26238/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (27353/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (28496/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (29608/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (30750/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (31882/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (33023/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (34155/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35306/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (36445/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (37588/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (38707/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (39831/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (40939/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (42078/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (43189/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (44243/50000)
# TEST : Loss: (0.5127) | Acc: (83.00%) (8308/10000)
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4528, 0.5472], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5436, 0.4564], device='cuda:0')
percent tensor([0.5421, 0.4579], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (1246/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (2342/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (3409/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (4481/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (5555/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (6652/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.4291) |  Loss2: (0.0000) | Acc: (84.00%) (8799/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.4295) |  Loss2: (0.0000) | Acc: (84.00%) (9890/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4273) |  Loss2: (0.0000) | Acc: (84.00%) (10979/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (84.00%) (12075/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (84.00%) (13159/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (14266/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4250) |  Loss2: (0.0000) | Acc: (85.00%) (15353/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (16441/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (17533/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.4233) |  Loss2: (0.0000) | Acc: (85.00%) (18632/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.4199) |  Loss2: (0.0000) | Acc: (85.00%) (19743/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (20848/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (21964/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (23054/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (24160/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (25271/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (26352/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (27441/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (28531/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (29613/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (30708/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (31803/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (32922/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (34028/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (35118/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (36194/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (37311/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (38418/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (39519/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (40627/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (41760/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (42819/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.5064) | Acc: (83.00%) (8362/10000)
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.4619, 0.5381], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.6405, 0.3595], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3762) |  Loss2: (0.0000) | Acc: (86.00%) (2327/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (3437/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (4554/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (6760/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (7879/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (9008/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (10116/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (11229/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (12347/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (13448/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (14542/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (15674/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (16807/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (17917/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (19031/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (20149/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (21259/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (22356/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (23460/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (24582/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (25694/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (26811/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (27926/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (29013/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (30134/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (31263/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (32368/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (33504/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (34618/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (35742/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (36872/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (87.00%) (37997/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (87.00%) (39103/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (87.00%) (40226/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (41353/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (87.00%) (42487/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (43563/50000)
# TEST : Loss: (0.4702) | Acc: (84.00%) (8463/10000)
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.4608, 0.5392], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5664, 0.4336], device='cuda:0')
percent tensor([0.5259, 0.4741], device='cuda:0')
percent tensor([0.6349, 0.3651], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (1239/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (2357/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (5743/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (6852/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (7980/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (9090/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (10207/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (11326/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (12448/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (13571/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (14691/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (15823/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (16932/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (18063/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (19173/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (20276/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (21393/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (22529/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (23653/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (24757/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (25868/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (26996/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (28099/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (29221/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (87.00%) (30324/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (31458/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (32566/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (33679/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (34819/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (35951/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (37087/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (38212/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (39338/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (40469/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (41587/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (42713/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (43786/50000)
# TEST : Loss: (0.4556) | Acc: (85.00%) (8508/10000)
percent tensor([0.5124, 0.4876], device='cuda:0')
percent tensor([0.4603, 0.5397], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.5280, 0.4720], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.5874, 0.4126], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (4608/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (6853/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (7981/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (9108/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (10236/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (11373/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (88.00%) (12510/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (88.00%) (13649/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (14753/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (15867/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (17000/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (18120/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (19241/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (20360/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (21483/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (22614/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (23734/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (24873/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (26009/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (27121/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (28249/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (29369/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (30491/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (31650/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (32771/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (88.00%) (33911/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (35045/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (36155/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (37293/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (38430/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (39527/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (87.00%) (40656/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (41747/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (42898/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (43982/50000)
# TEST : Loss: (0.4442) | Acc: (85.00%) (8531/10000)
percent tensor([0.5125, 0.4875], device='cuda:0')
percent tensor([0.4601, 0.5399], device='cuda:0')
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5728, 0.4272], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6360, 0.3640], device='cuda:0')
percent tensor([0.5893, 0.4107], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3734) |  Loss2: (0.0000) | Acc: (86.00%) (1215/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3667) |  Loss2: (0.0000) | Acc: (87.00%) (2339/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (3472/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (5722/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (6841/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (7973/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (87.00%) (9115/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (87.00%) (10247/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (11386/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (12520/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (13661/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (14800/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (15933/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (17066/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (88.00%) (18199/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (19319/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (20452/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (21552/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (88.00%) (22655/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (23788/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (88.00%) (24898/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (26011/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (27146/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (28296/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (29452/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (30567/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (31714/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (32856/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (33977/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (35105/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (88.00%) (36223/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (37362/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (38482/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (39609/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (40750/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (41872/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (43005/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (44096/50000)
# TEST : Loss: (0.4345) | Acc: (85.00%) (8542/10000)
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.5707, 0.4293], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.6351, 0.3649], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (89.00%) (2393/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (3507/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (4630/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (5742/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (6863/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (8011/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (9151/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (10284/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (11421/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (12563/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (13691/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (14815/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (15946/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (17095/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (18221/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (19358/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (20512/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (21645/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (22784/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (23904/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (25004/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (26118/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (27256/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (28397/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (29538/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (30644/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (31782/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (32918/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (34058/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (35209/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (36327/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (37446/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (38573/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (39721/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (40850/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (42003/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (43128/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (44201/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4518) | Acc: (84.00%) (8476/10000)
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.6017, 0.3983], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.3239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.8654, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.7286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.0846, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.0699, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2208.5417, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4284.1353, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1409.1840, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6127.8037, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11981.9990, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3975.5818, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16846.3320, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (3508/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (88.00%) (4658/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (5781/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (6911/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (8066/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (9222/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (88.00%) (10346/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (11479/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (12609/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (88.00%) (13757/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (88.00%) (14916/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (16066/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (17197/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (18342/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (19479/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (20612/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (21762/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (22905/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (24034/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (25195/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (26332/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (27454/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (28571/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (29711/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (30839/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (31966/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (33108/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (34262/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3260) |  Loss2: (0.0000) | Acc: (88.00%) (35381/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (36536/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (37675/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (38801/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3255) |  Loss2: (0.0000) | Acc: (88.00%) (39937/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (41085/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (42182/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (43310/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (44392/50000)
# TEST : Loss: (0.4978) | Acc: (83.00%) (8364/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4609, 0.5391], device='cuda:0')
percent tensor([0.5455, 0.4545], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.5328, 0.4672], device='cuda:0')
percent tensor([0.6328, 0.3672], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.4639) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (3537/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (4678/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (5844/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (6994/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (8128/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (9274/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (10413/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3103) |  Loss2: (0.0000) | Acc: (89.00%) (11559/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (12687/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (13824/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (14968/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (16111/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (17262/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (18406/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (19556/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (20704/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (21842/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (22969/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (24103/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (25244/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (26385/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (27518/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (28668/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (29801/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (30933/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (32075/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (33210/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (34370/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (35491/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (36615/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (37758/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (38893/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (40022/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (41141/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (42281/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (43432/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (44524/50000)
# TEST : Loss: (0.5010) | Acc: (83.00%) (8387/10000)
percent tensor([0.5120, 0.4880], device='cuda:0')
percent tensor([0.4608, 0.5392], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (89.00%) (1264/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (2408/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (89.00%) (3537/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (89.00%) (4678/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (5831/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (6970/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (8108/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (9250/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (10390/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (11531/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (12697/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (13843/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (14984/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (16131/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (17284/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (18417/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (19559/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (20697/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (21840/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (22977/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (24110/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (25221/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (26364/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (27506/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (28651/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (29798/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (30937/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (32076/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (33221/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (34365/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (35514/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (36672/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (37815/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (38945/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (89.00%) (40094/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (41224/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (42370/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (43528/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (44631/50000)
# TEST : Loss: (0.4258) | Acc: (85.00%) (8592/10000)
percent tensor([0.5123, 0.4877], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.6328, 0.3672], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.4561) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (3545/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (4697/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (5835/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (6990/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (9248/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (10378/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (11531/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (12692/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (13848/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (15017/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (16169/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (17318/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (18476/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (19639/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (20804/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (21951/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (23097/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (24245/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (25396/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (26543/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (89.00%) (27703/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (28841/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (29986/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (31137/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (32271/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (33425/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (34580/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (35732/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (36888/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (38038/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (39176/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (40305/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (41452/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (42597/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (43756/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (44848/50000)
# TEST : Loss: (0.4709) | Acc: (84.00%) (8456/10000)
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.4621, 0.5379], device='cuda:0')
percent tensor([0.5451, 0.4549], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.6326, 0.3674], device='cuda:0')
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (3433/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (86.00%) (4560/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (86.00%) (5672/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (86.00%) (6756/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (7846/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (8961/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (86.00%) (10096/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (86.00%) (11216/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (86.00%) (12327/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (86.00%) (13428/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (86.00%) (14530/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (86.00%) (15638/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (86.00%) (16759/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (86.00%) (17874/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (86.00%) (18975/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (86.00%) (20085/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (86.00%) (21208/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (86.00%) (22325/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (86.00%) (23439/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (86.00%) (24570/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (86.00%) (25684/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (86.00%) (26824/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (86.00%) (27944/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (29081/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (30208/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (31320/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (32432/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3653) |  Loss2: (0.0000) | Acc: (87.00%) (33544/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (34677/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (35806/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (36948/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (38070/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (39190/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (40336/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (41462/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (42605/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (43674/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4699) | Acc: (84.00%) (8433/10000)
percent tensor([0.5142, 0.4858], device='cuda:0')
percent tensor([0.4619, 0.5381], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.5863, 0.4137], device='cuda:0')
percent tensor([0.5529, 0.4471], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (1240/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (87.00%) (3486/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (87.00%) (4611/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (6873/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (8006/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (9150/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (10277/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (11415/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (12546/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (13691/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (14823/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (15958/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (17103/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (18232/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (19371/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (20526/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (21657/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (22809/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (23939/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (25072/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (26197/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (27333/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (28491/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (29620/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (30775/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (31908/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (33062/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (88.00%) (34204/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (88.00%) (35333/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (36492/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (37634/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (38786/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (39917/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (41060/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (42208/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (43353/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (44446/50000)
# TEST : Loss: (0.4452) | Acc: (85.00%) (8524/10000)
percent tensor([0.5153, 0.4847], device='cuda:0')
percent tensor([0.4609, 0.5391], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5553, 0.4447], device='cuda:0')
percent tensor([0.6297, 0.3703], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (90.00%) (1268/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (3519/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (5827/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (6974/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (9243/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (10376/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (11528/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (12689/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (13839/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (14995/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (16144/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (17299/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (18442/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (19589/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (20714/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (21849/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (22987/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (24143/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (25290/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (26439/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (27565/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (28720/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (29874/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (31030/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (32177/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (33341/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (34481/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (35618/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (36778/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (37939/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (39083/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (40235/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (89.00%) (41395/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (42540/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (43691/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (44798/50000)
# TEST : Loss: (0.4310) | Acc: (85.00%) (8577/10000)
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.4611, 0.5389], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (3514/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (4670/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (5820/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (6971/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (8140/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (9275/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (10417/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (11576/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (12730/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (13870/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (15024/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (16171/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (17328/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (18492/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (19627/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (20763/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (21920/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (23066/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (24208/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (25368/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (26512/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (27660/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (28817/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (29960/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (31116/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (32270/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (33426/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (34585/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (35739/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (36879/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (38030/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (39179/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (40336/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (41483/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (42620/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (43766/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (44872/50000)
# TEST : Loss: (0.4236) | Acc: (86.00%) (8615/10000)
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.4614, 0.5386], device='cuda:0')
percent tensor([0.5375, 0.4625], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5526, 0.4474], device='cuda:0')
percent tensor([0.6391, 0.3609], device='cuda:0')
percent tensor([0.5873, 0.4127], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (4725/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (7032/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (8184/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (9342/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (10496/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (90.00%) (11645/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (12775/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (13929/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (15076/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (16250/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (90.00%) (17406/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (90.00%) (18552/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (90.00%) (19705/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (20851/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (22001/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (23164/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (90.00%) (24343/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (25489/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (26640/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (27791/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (28956/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (30119/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (90.00%) (31265/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (32400/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (33557/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (90.00%) (34721/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (90.00%) (35894/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (37039/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (38197/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (39335/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (90.00%) (40492/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (90.00%) (41641/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (42777/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (43943/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (90.00%) (45058/50000)
# TEST : Loss: (0.4193) | Acc: (86.00%) (8610/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4617, 0.5383], device='cuda:0')
percent tensor([0.5385, 0.4615], device='cuda:0')
percent tensor([0.5863, 0.4137], device='cuda:0')
percent tensor([0.5521, 0.4479], device='cuda:0')
percent tensor([0.6417, 0.3583], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (2413/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (4733/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (5883/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (7020/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (8194/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (9350/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (10498/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (11644/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (12805/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (90.00%) (13956/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (15081/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (16234/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (17397/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (18547/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (19697/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (20871/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (22016/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (23175/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (24314/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (25455/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (26605/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (27750/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (28880/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (30029/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2935) |  Loss2: (0.0000) | Acc: (89.00%) (31187/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (32347/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (33488/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (34623/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (35779/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (36918/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (38066/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (39190/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (40335/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (41482/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (42632/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (43755/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (44839/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4733) | Acc: (84.00%) (8499/10000)
percent tensor([0.5158, 0.4842], device='cuda:0')
percent tensor([0.4620, 0.5380], device='cuda:0')
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5864, 0.4136], device='cuda:0')
percent tensor([0.5520, 0.4480], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.5929, 0.4071], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.3325, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.1806, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.2275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.8506, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.3567, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2216.4490, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4280.1230, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1404.2413, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6140.6919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11945.0078, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3960.2556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16778.2305, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (90.00%) (2421/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (4736/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (5888/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (90.00%) (7039/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (8200/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (9352/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (10502/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (11654/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (12805/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (13984/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (15150/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (16296/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (17444/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (18605/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (19765/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (20916/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (22066/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (23218/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (24359/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (25501/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (26653/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (27797/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (28949/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (30101/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (31247/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (32417/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (33571/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (34716/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (35872/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (37004/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (38167/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (39323/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (40465/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (90.00%) (41611/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (42780/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (43934/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (90.00%) (45049/50000)
# TEST : Loss: (0.4808) | Acc: (84.00%) (8487/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4618, 0.5382], device='cuda:0')
percent tensor([0.5388, 0.4612], device='cuda:0')
percent tensor([0.5860, 0.4140], device='cuda:0')
percent tensor([0.5524, 0.4476], device='cuda:0')
percent tensor([0.6422, 0.3578], device='cuda:0')
percent tensor([0.5920, 0.4080], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (3620/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (4797/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (7118/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (8278/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (9442/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (10604/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (11766/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (12918/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (91.00%) (14098/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (15263/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (16407/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (17572/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (18725/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (19872/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (21022/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (22184/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (23336/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (24489/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (25648/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (26788/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (27928/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (29098/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (30251/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (31387/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (32522/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (33658/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (34802/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (35957/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (37096/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (38259/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (39420/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (40566/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (41726/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (42876/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (44023/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (45127/50000)
# TEST : Loss: (0.4957) | Acc: (84.00%) (8415/10000)
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.4620, 0.5380], device='cuda:0')
percent tensor([0.5388, 0.4612], device='cuda:0')
percent tensor([0.5862, 0.4138], device='cuda:0')
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.5943, 0.4057], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (3601/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (4738/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (5900/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (7068/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (8223/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (9388/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (10556/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (11730/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (12898/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (14058/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (15239/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (16405/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (17567/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (18728/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (19891/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (21031/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (22177/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (23345/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (24522/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (25682/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (26842/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (28023/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (29184/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (30349/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (31493/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (32633/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (33787/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (34935/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (36099/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (37250/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (38426/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (39579/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (40723/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (41891/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (43034/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (44193/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (45298/50000)
# TEST : Loss: (0.4733) | Acc: (84.00%) (8455/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.4622, 0.5378], device='cuda:0')
percent tensor([0.5403, 0.4597], device='cuda:0')
percent tensor([0.5858, 0.4142], device='cuda:0')
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.6407, 0.3593], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (5949/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (7109/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (8272/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (91.00%) (9435/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (10596/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (11767/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (12937/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (14087/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (15245/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (16409/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (17556/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (18731/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (19889/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (21067/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (22233/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (23404/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (24563/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (25722/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (26889/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (28055/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (29212/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (30395/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (31549/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (32703/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (33871/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (35044/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (36205/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37366/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (38519/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (39697/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (40838/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (42005/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (43170/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (44322/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (45435/50000)
# TEST : Loss: (0.4513) | Acc: (85.00%) (8523/10000)
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.4623, 0.5377], device='cuda:0')
percent tensor([0.5391, 0.4609], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5522, 0.4478], device='cuda:0')
percent tensor([0.6385, 0.3615], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (1265/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (2399/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (3543/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (88.00%) (5789/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (6893/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (8035/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (9163/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.3334) |  Loss2: (0.0000) | Acc: (88.00%) (10300/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (11441/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (12595/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (13742/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (14890/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (88.00%) (16039/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (17174/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (88.00%) (18334/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (88.00%) (19465/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (88.00%) (20593/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (88.00%) (21744/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (22882/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (88.00%) (24017/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (88.00%) (25164/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (26326/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (27482/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (28619/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (29740/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (30901/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (32048/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (33191/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (34329/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (35480/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (36633/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (37775/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (38914/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (40063/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (41203/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (42343/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (43489/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (44591/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4626) | Acc: (85.00%) (8551/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4591, 0.5409], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5873, 0.4127], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.6853, 0.3147], device='cuda:0')
percent tensor([0.6188, 0.3812], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (2410/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (3550/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (4684/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (5830/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (6989/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (8144/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (9310/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (10468/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (11623/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (12786/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (13960/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (15096/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (16251/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (17397/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (89.00%) (18530/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (89.00%) (19676/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (89.00%) (20824/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (21966/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (23130/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (89.00%) (24293/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (89.00%) (25447/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (26614/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (89.00%) (27761/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (89.00%) (28903/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (89.00%) (30061/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (31231/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (32390/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (33532/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (34694/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (35836/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (37004/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (38162/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (39324/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (40502/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (41669/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (42820/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (43970/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (45089/50000)
# TEST : Loss: (0.4372) | Acc: (86.00%) (8626/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4563, 0.5437], device='cuda:0')
percent tensor([0.5483, 0.4517], device='cuda:0')
percent tensor([0.5908, 0.4092], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.6231, 0.3769], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (2434/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (3593/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (4756/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (5913/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (7074/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (8235/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (9392/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (10552/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (11708/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (12860/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (14025/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (15191/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (16360/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (17514/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (18685/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (19855/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (21025/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (22200/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (23353/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (24496/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (25660/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (26824/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (27968/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (29138/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (30300/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (31457/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (32616/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (33770/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (34938/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (36116/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (37254/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (38418/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (39560/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (40746/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (41925/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (43086/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (44270/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (45391/50000)
# TEST : Loss: (0.4246) | Acc: (86.00%) (8661/10000)
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4571, 0.5429], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5898, 0.4102], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.6930, 0.3070], device='cuda:0')
percent tensor([0.6326, 0.3674], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (3607/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (5945/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (7093/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (8256/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (9426/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (10578/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (11740/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (12907/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (14070/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (15236/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (16393/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (17552/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (18705/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (19871/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (21055/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (22221/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (23397/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (24545/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (25710/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (26876/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (28034/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (29206/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (30376/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (31550/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (32706/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (33870/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (35053/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (36223/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (37386/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (38540/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (39712/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (40875/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (42030/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (43181/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (44349/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (45463/50000)
# TEST : Loss: (0.4158) | Acc: (86.00%) (8674/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4572, 0.5428], device='cuda:0')
percent tensor([0.5474, 0.4526], device='cuda:0')
percent tensor([0.5920, 0.4080], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6932, 0.3068], device='cuda:0')
percent tensor([0.6363, 0.3637], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (3611/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (4783/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (5950/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (7125/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (8292/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (9483/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (10661/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (11824/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (12981/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (14137/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (16455/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (17636/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (18805/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (19956/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (21125/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (22287/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (23454/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (24604/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (25783/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (26948/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (28124/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (29294/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (30460/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (31642/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (32806/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (33968/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (35123/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (36288/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (37429/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (38584/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (39756/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (40910/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (91.00%) (42067/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (43229/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (44397/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (45514/50000)
# TEST : Loss: (0.4110) | Acc: (86.00%) (8693/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5472, 0.4528], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.6914, 0.3086], device='cuda:0')
percent tensor([0.6411, 0.3589], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (2471/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (3636/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (90.00%) (4772/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (90.00%) (5928/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (7089/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (90.00%) (8261/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (90.00%) (9431/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (10611/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (11771/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (12932/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (14105/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (15267/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (16426/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (17573/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (18726/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (19875/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (21030/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (22200/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (23355/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (24517/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (26849/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (28012/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (29175/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (30347/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (31493/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (32665/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (33827/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (34991/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (36151/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (37306/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (38463/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (39623/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (40781/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (41944/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (43121/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (44323/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (45429/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4759) | Acc: (84.00%) (8456/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5476, 0.4524], device='cuda:0')
percent tensor([0.5901, 0.4099], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.3275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.3481, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.4761, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1516.7422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.5753, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2223.9951, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4276.9971, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1399.2454, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6154.4019, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11909.0967, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3945.0779, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16710.4844, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (3636/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (4818/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (5981/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (7159/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (8333/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (9497/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (10663/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (11823/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (12992/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (14177/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (15327/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (16488/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (17665/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (18831/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (19996/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (21159/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (22329/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (23486/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (24658/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (25830/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (27017/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (28174/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (29341/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (30533/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (31692/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (32861/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (34023/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (35173/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (36329/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (37485/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (38642/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (39821/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (40987/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (42162/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (43324/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (44488/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (45591/50000)
# TEST : Loss: (0.4305) | Acc: (86.00%) (8628/10000)
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.4584, 0.5416], device='cuda:0')
percent tensor([0.5466, 0.4534], device='cuda:0')
percent tensor([0.5909, 0.4091], device='cuda:0')
percent tensor([0.5740, 0.4260], device='cuda:0')
percent tensor([0.6913, 0.3087], device='cuda:0')
percent tensor([0.6423, 0.3577], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (5988/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (7162/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (8334/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (9524/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (10668/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (11848/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (13022/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (14203/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (15386/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (16558/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (17731/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (20081/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (21261/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (22441/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (23620/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (24783/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (25965/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (27148/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (28318/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (29476/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (30646/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (31821/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (32996/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (34167/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (35322/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (36485/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (37648/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (38800/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (39960/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (41134/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (42305/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (43471/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (44646/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (45785/50000)
# TEST : Loss: (0.4125) | Acc: (87.00%) (8700/10000)
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.4583, 0.5417], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5910, 0.4090], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.6392, 0.3608], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (3627/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (5956/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (7126/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (8300/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (9480/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (10659/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (11843/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (12997/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (14178/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (15364/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (16521/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (17680/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (18854/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (20040/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (21217/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (22376/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (23549/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (24730/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (25905/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (27066/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (28247/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (29407/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (30579/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (31745/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (32926/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (34109/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (35273/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (36445/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (37618/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (38781/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (39956/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (41128/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (42297/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (43473/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (44644/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (45769/50000)
# TEST : Loss: (0.4927) | Acc: (85.00%) (8528/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4589, 0.5411], device='cuda:0')
percent tensor([0.5469, 0.4531], device='cuda:0')
percent tensor([0.5914, 0.4086], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.6920, 0.3080], device='cuda:0')
percent tensor([0.6464, 0.3536], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (3676/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (92.00%) (4848/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (92.00%) (6025/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (7209/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (8390/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (92.00%) (9558/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (92.00%) (10732/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (92.00%) (11909/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (92.00%) (13088/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (92.00%) (14256/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (16591/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (17737/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (18918/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (20093/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (21249/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (22429/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (23599/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (24768/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (25940/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (27106/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (28282/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (29462/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (30646/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (31829/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (32999/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (34175/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (35359/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (36530/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (37672/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (38837/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (40010/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (41179/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (42345/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (43508/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (44680/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (45792/50000)
# TEST : Loss: (0.4521) | Acc: (85.00%) (8534/10000)
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.4587, 0.5413], device='cuda:0')
percent tensor([0.5462, 0.4538], device='cuda:0')
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.6883, 0.3117], device='cuda:0')
percent tensor([0.6407, 0.3593], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (2429/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (3570/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (4718/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (5852/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (6998/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (8153/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (9293/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (10419/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (11567/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (12703/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (13846/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (14985/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (16150/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (17302/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (18444/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (19601/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (20751/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (21886/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (23044/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (24173/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (25336/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (26490/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (27642/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (28806/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (29960/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (31132/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (32269/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (33428/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (34588/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (35753/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (36885/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (38051/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (39211/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (40366/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (41512/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (42668/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (43841/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (44963/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4604) | Acc: (85.00%) (8556/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.4650, 0.5350], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5587, 0.4413], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (4784/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (5943/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (90.00%) (7098/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (8265/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (9441/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (10613/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (11785/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (12944/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (14122/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (15277/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (16435/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (17607/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (18771/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (19939/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (21119/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (22273/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (23443/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (24617/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (25776/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (26946/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (28106/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (29271/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (30426/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (31600/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (32762/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (33925/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (35095/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (36258/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (37425/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (38592/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (39751/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (40918/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (42080/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (43247/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (44422/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (45551/50000)
# TEST : Loss: (0.4395) | Acc: (86.00%) (8618/10000)
percent tensor([0.5182, 0.4818], device='cuda:0')
percent tensor([0.4631, 0.5369], device='cuda:0')
percent tensor([0.5389, 0.4611], device='cuda:0')
percent tensor([0.5866, 0.4134], device='cuda:0')
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.6549, 0.3451], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (3615/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (4781/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (5962/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (7125/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (9452/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (10630/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (11803/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (12965/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (14133/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (15306/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (16479/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (17652/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (18823/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (20001/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (21185/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (22367/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (23550/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (24716/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (25889/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (27064/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (28229/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (29397/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (30573/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (31739/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (32932/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (34101/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (35288/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (36451/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (37626/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (38808/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (39977/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (41150/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (42322/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (43503/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (44686/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (45832/50000)
# TEST : Loss: (0.4284) | Acc: (86.00%) (8622/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.4639, 0.5361], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.5835, 0.4165], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.6619, 0.3381], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (3648/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (5995/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (7174/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (8346/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (9516/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (10703/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (11872/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (13039/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (14210/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (15371/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (16553/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (17732/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (18908/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (20083/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (21252/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (22432/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (23598/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (24786/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (25965/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (27159/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (28337/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (29505/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (30693/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (31857/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (33020/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (34191/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (35362/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (36549/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (37712/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (38880/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (40066/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (41242/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (42430/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (43621/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (44790/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (45932/50000)
# TEST : Loss: (0.4151) | Acc: (86.00%) (8648/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.6227, 0.3773], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (4874/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (6045/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (7222/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (8408/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (9574/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (10753/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (11948/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (13120/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (92.00%) (14291/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (92.00%) (15477/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (92.00%) (16649/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (17835/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (19013/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (20193/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (92.00%) (21384/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (22559/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (92.00%) (23731/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (24922/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (26111/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (27295/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (28478/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (29648/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (30811/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (32001/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (33179/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (34363/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (35538/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (36709/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (37899/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (39089/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (40281/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (41460/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (42640/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (43809/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (44975/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (46112/50000)
# TEST : Loss: (0.4127) | Acc: (86.00%) (8669/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4626, 0.5374], device='cuda:0')
percent tensor([0.5474, 0.4526], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.6296, 0.3704], device='cuda:0')
percent tensor([0.6675, 0.3325], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (92.00%) (2473/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (3643/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (4816/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (6004/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (92.00%) (7185/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (8349/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (9526/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (10690/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (11860/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (13051/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (14234/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (15400/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (16583/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (17753/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (18933/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (20098/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (21278/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (22454/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (23633/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (24796/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (25980/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (27159/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (28331/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (29506/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (30669/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (31816/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (33002/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (34169/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (35336/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (36498/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (37664/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (38837/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (39994/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41163/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (42346/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (43525/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (44708/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (45844/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.4712) | Acc: (84.00%) (8477/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5856, 0.4144], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.1171, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.9636, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(803.6728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.3483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.7923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2230.9001, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4274.3154, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1394.2455, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6169.0474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11874.9443, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3929.8618, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16643.3652, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (3666/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (4859/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (6055/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (7242/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (9607/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (10787/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (11958/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (13150/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (14327/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (15519/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (16681/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (17844/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (19024/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (20210/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (21394/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (22573/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (23755/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (24916/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (26100/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (27289/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (28471/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (29661/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (30839/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (32019/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (33193/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (34385/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (35554/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (36712/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (37867/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (39033/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (40220/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (41394/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (42576/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (43745/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (44911/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (46033/50000)
# TEST : Loss: (0.4380) | Acc: (86.00%) (8602/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4629, 0.5371], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6601, 0.3399], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (2485/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (4870/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (6065/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (8442/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (9616/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (10772/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (11961/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (13147/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (14323/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (15495/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (16674/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (17839/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (19023/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (20204/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (21362/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (22538/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (23727/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (24906/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (26084/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (27272/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (28447/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (29628/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (30807/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (31970/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (33148/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (34320/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (35503/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (36675/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (37851/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (39035/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (40218/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (41409/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (42584/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (43759/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (44950/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (46079/50000)
# TEST : Loss: (0.4122) | Acc: (87.00%) (8705/10000)
percent tensor([0.5205, 0.4795], device='cuda:0')
percent tensor([0.4629, 0.5371], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.6677, 0.3323], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (4912/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6088/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (7280/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (8462/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (93.00%) (9643/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (10826/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (12003/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (13178/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (14360/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (15538/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (16722/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (17891/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (19077/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (20259/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (21447/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (22652/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (23838/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (25016/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (26193/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (27362/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (28548/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (29726/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (30913/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (32099/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (33287/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (34464/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (35634/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (36840/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (38022/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (39204/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (40382/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (41565/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (42752/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (43928/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (45106/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (46247/50000)
# TEST : Loss: (0.4849) | Acc: (84.00%) (8474/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4625, 0.5375], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (3664/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (6032/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (7212/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (8408/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (9597/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (10792/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (11975/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (13153/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (14353/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (15556/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (16750/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (17940/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (19127/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (20324/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (21511/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (22696/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (23898/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (25093/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (26280/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (27467/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (28652/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (29839/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (31024/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (32207/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (33377/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (34560/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (35749/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (36927/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (38108/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (39283/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (40463/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (41647/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (42827/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (44020/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (45209/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (46368/50000)
# TEST : Loss: (0.4476) | Acc: (86.00%) (8638/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.6289, 0.3711], device='cuda:0')
percent tensor([0.6650, 0.3350], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (3633/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (4785/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (7111/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (8271/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2619) |  Loss2: (0.0000) | Acc: (90.00%) (9412/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (10572/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (11741/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (12905/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (14062/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (15219/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (16375/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (17537/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (18704/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (19872/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (21021/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (22185/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (23347/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (24503/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (25668/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (26838/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (28005/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (29161/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (30300/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31465/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (32639/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (33787/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (34950/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (36130/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (37302/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (38476/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (39649/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (40809/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (41989/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (43152/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (44329/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (45454/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4579) | Acc: (85.00%) (8580/10000)
percent tensor([0.5231, 0.4769], device='cuda:0')
percent tensor([0.4627, 0.5373], device='cuda:0')
percent tensor([0.5414, 0.4586], device='cuda:0')
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (92.00%) (3653/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (4819/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (5992/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (7167/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (8358/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (92.00%) (9539/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (10691/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (11854/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (13025/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (14196/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (16556/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (17724/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (18904/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (20080/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (21266/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (22457/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (23608/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (24765/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (25942/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (27111/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (28283/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (29450/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (30634/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (31811/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (33003/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (34193/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (35397/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (36570/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (37758/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (38931/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (40104/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (41289/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (42485/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (43675/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (44853/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (45996/50000)
# TEST : Loss: (0.4332) | Acc: (86.00%) (8675/10000)
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4628, 0.5372], device='cuda:0')
percent tensor([0.5399, 0.4601], device='cuda:0')
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.6080, 0.3920], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (91.00%) (2467/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (3658/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (4844/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (6013/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (7189/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (8371/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (9555/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (10746/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (11934/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (13092/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (14262/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (16616/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (18987/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (20175/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (21370/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (22564/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (23742/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (24924/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (26111/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (27302/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (28490/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (29649/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (30834/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (32027/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (33178/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (34365/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (35525/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (36711/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (37888/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (39066/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (40247/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (41438/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (42628/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (43806/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (44993/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (46117/50000)
# TEST : Loss: (0.4222) | Acc: (86.00%) (8681/10000)
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4655, 0.5345], device='cuda:0')
percent tensor([0.5417, 0.4583], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (4844/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (6038/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (7213/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (8394/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (9567/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (10741/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (11928/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (13108/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (14279/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (15485/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (16692/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (17870/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (19058/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (20233/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (21429/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (22596/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (23778/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (24963/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (26159/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (27350/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (28527/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (29713/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (30898/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (32087/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (33273/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (34446/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (35635/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (36833/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (38023/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (39196/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (40395/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (41574/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (42772/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (43974/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (45159/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (46314/50000)
# TEST : Loss: (0.4105) | Acc: (87.00%) (8711/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.4666, 0.5334], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6194, 0.3806], device='cuda:0')
percent tensor([0.6767, 0.3233], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (6071/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (7253/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (8431/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (9622/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (10807/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (12004/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (13201/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (14390/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (15578/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (16779/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (93.00%) (17981/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (93.00%) (19168/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (93.00%) (20359/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (21559/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (22755/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (23938/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (93.00%) (25119/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (26305/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (27493/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (28672/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (29865/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (93.00%) (31070/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32249/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (33426/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (34602/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (35799/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (36982/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (38165/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (39368/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (40584/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (41770/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (42950/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (93.00%) (44168/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (45353/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (93.00%) (46503/50000)
# TEST : Loss: (0.4062) | Acc: (87.00%) (8743/10000)
percent tensor([0.5235, 0.4765], device='cuda:0')
percent tensor([0.4672, 0.5328], device='cuda:0')
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.6783, 0.3217], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (3692/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (4868/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (6066/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (7246/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (8422/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (9603/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (10781/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (11959/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (14329/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (15511/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (16685/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (17869/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (19050/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (20238/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (21429/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (22628/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (23813/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (24997/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (26183/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (27355/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (28543/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (29746/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (30929/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (32123/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (33310/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (34500/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (35696/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (36895/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (38089/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (39266/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (40457/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (41633/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (42817/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (43997/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (45179/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (46322/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4623) | Acc: (85.00%) (8595/10000)
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5433, 0.4567], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.5727, 0.4273], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.6770, 0.3230], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.7989, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.7266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.8602, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.9132, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.1037, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2238.0481, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4271.6284, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1389.4530, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6182.6260, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11841.5127, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3914.6375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16576.2480, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (4897/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (6089/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (7283/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (8474/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (9668/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (10872/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (12060/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (13249/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (14441/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (15640/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (16842/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (18028/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (19224/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (20396/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (21578/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (22776/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (23962/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (25146/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (93.00%) (26326/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (27515/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (28688/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (29871/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (31054/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (32229/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (33410/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (34600/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (35783/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (36979/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (38159/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (39359/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (40552/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (41744/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (42925/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (44109/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (45313/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (46464/50000)
# TEST : Loss: (0.4200) | Acc: (87.00%) (8705/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.6265, 0.3735], device='cuda:0')
percent tensor([0.6749, 0.3251], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (3706/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (4885/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (6062/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (7245/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (9632/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (10822/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (12019/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (13214/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (14401/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (16786/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (17971/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (19159/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (20353/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (21546/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (22729/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (23918/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (25110/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (26296/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (27497/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (28717/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (29896/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (31088/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (32263/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (33454/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (34639/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (35840/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (37016/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (38210/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (39404/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (40585/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (41774/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (42972/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (44161/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (45350/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (46474/50000)
# TEST : Loss: (0.4691) | Acc: (86.00%) (8607/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.4672, 0.5328], device='cuda:0')
percent tensor([0.5438, 0.4562], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6796, 0.3204], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (3714/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (4906/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (6101/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (7302/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (8496/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (9683/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (10871/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (12057/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (13249/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (14444/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (15638/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (16824/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (18026/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (19223/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (20430/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (21621/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (22809/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (24000/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (25192/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (26374/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (27545/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (28748/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (29932/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (31133/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (32315/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (33512/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (34695/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (35890/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (37076/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (38275/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (39450/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (40644/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (41840/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (43024/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (44215/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (45406/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (46562/50000)
# TEST : Loss: (0.4492) | Acc: (86.00%) (8654/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4682, 0.5318], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.6229, 0.3771], device='cuda:0')
percent tensor([0.6747, 0.3253], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (4934/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (6132/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (7328/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8530/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (9739/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (12137/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (13340/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (14544/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (15735/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (16917/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (18119/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (19310/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (20494/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (21682/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (22877/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (24066/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (25259/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (26465/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (27666/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (28865/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (30071/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (31278/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (32481/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (33682/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (34871/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (36076/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (37256/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (38445/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (39651/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (40839/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (42046/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (43226/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (44423/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (45617/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (46755/50000)
# TEST : Loss: (0.4266) | Acc: (86.00%) (8663/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.4677, 0.5323], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5729, 0.4271], device='cuda:0')
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.6788, 0.3212], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (3657/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (5982/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (7141/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (8310/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (9495/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (10671/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (11827/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (12991/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (14164/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (15338/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (16518/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (17688/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (18856/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (20035/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (21206/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (22361/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (23536/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (24704/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (25891/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (27081/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (28249/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (29438/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (30612/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (31784/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (32959/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (34153/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (35338/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (36523/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (37695/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (38852/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (40029/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (41216/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (42401/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (43587/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (44780/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (45923/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4277) | Acc: (87.00%) (8702/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4673, 0.5327], device='cuda:0')
percent tensor([0.5584, 0.4416], device='cuda:0')
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.6726, 0.3274], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (2479/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (3668/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (4864/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (6060/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (7246/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (8442/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (9640/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (10828/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (12011/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (13216/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (14383/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (15567/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (16750/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (17937/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (19135/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (20318/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (21520/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (22688/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (23898/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (25099/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (26286/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (27491/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (28686/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (29866/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (31041/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32229/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (33420/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (34605/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (35798/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (36986/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (38167/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (39372/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (40559/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (41763/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (42953/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (44151/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (45330/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (46488/50000)
# TEST : Loss: (0.4076) | Acc: (87.00%) (8749/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.4657, 0.5343], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.5906, 0.4094], device='cuda:0')
percent tensor([0.5740, 0.4260], device='cuda:0')
percent tensor([0.6662, 0.3338], device='cuda:0')
percent tensor([0.6822, 0.3178], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (6101/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (7286/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (8488/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (9695/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (10880/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (12062/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (13264/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (14457/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (15641/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (16827/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (18029/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (19225/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (20415/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (21598/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (22788/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (23992/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (25189/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (26388/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (27583/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (28785/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (29987/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (31183/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (32389/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (33588/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (34772/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (35962/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (37149/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (38345/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (39549/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (40748/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (41953/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (43140/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (44341/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (45520/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (46673/50000)
# TEST : Loss: (0.3944) | Acc: (87.00%) (8782/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.4648, 0.5352], device='cuda:0')
percent tensor([0.5621, 0.4379], device='cuda:0')
percent tensor([0.5944, 0.4056], device='cuda:0')
percent tensor([0.5752, 0.4248], device='cuda:0')
percent tensor([0.6732, 0.3268], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (4907/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (6108/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (7314/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (9715/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (10887/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (12088/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (13289/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (14489/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (15684/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (16871/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (18066/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (19264/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (20457/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (21657/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (22851/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (24054/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (25266/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (26462/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (27667/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (28854/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (30054/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (31257/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (32475/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (33688/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (34902/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (36103/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (37297/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (38484/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (39680/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (40868/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (42071/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (43278/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (44470/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (45673/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (46822/50000)
# TEST : Loss: (0.3905) | Acc: (88.00%) (8805/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.4652, 0.5348], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.5919, 0.4081], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.6707, 0.3293], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (94.00%) (2538/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (94.00%) (4940/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (6158/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (7378/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (94.00%) (8571/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (9774/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (10976/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (12180/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (13386/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (14585/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (15781/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (94.00%) (16977/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (94.00%) (18183/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (94.00%) (19380/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (20569/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (21748/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (22947/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (24152/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (25361/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (26554/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (27754/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (28948/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (30147/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (31354/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (32547/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (33743/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (34930/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (36130/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (37339/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (38539/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (39749/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (40947/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (42147/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (43339/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (44551/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (45757/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (46906/50000)
# TEST : Loss: (0.3854) | Acc: (88.00%) (8806/10000)
percent tensor([0.5181, 0.4819], device='cuda:0')
percent tensor([0.4642, 0.5358], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.6709, 0.3291], device='cuda:0')
percent tensor([0.6961, 0.3039], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (3728/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (4934/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (7338/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (8519/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (9711/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (10892/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (12084/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (13288/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (14473/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (15669/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (16868/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (18057/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (19256/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (20448/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (21643/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (22827/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (24009/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (25196/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (26385/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (27600/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (28790/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (29983/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (31160/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (32359/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (33557/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (34737/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (35914/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (37096/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (38272/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (39468/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (40670/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (41887/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (43092/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (44285/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (45480/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (46642/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4214) | Acc: (86.00%) (8699/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.4639, 0.5361], device='cuda:0')
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.6682, 0.3318], device='cuda:0')
percent tensor([0.6897, 0.3103], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.6576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(815.0617, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(807.9974, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.1487, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.3472, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.6589, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.4136, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1384.5138, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6197.1504, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11808.5938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3899.5923, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16509.7891, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (4944/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (6153/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8538/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (94.00%) (9748/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (94.00%) (10952/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (12136/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (13346/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (14540/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (15730/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (16918/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (18113/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (19300/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (20496/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (21686/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (22897/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (24097/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (25295/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (26500/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (27694/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (28878/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (30077/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (31273/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (32465/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (33661/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (34848/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (36038/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (37229/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (38429/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (39624/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (40818/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (42031/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (43241/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (44451/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (45635/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (46791/50000)
# TEST : Loss: (0.4171) | Acc: (87.00%) (8723/10000)
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.4645, 0.5355], device='cuda:0')
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.5978, 0.4022], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.6687, 0.3313], device='cuda:0')
percent tensor([0.6873, 0.3127], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (2532/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (3729/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (6102/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (7313/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (8522/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (9724/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (10922/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (12116/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (13318/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (14527/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (15746/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (16944/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (18142/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (19352/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (20558/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (21758/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (22966/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (24152/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (25353/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (26568/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (27753/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (28949/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (30155/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (31368/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (32568/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (33771/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (34967/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (36152/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (37342/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (38548/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (39734/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (40931/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (42121/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (43334/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (44529/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (45735/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (46887/50000)
# TEST : Loss: (0.5014) | Acc: (85.00%) (8515/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.4644, 0.5356], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5753, 0.4247], device='cuda:0')
percent tensor([0.6670, 0.3330], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (3728/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (4941/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (8544/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (94.00%) (9746/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (10957/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (12168/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (14568/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (15759/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (16960/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (18171/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (19367/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (20573/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (21768/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (22960/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (24165/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (25376/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (26582/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (27768/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (28962/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (30174/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (31386/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (32583/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (33783/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (34988/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (36194/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (37402/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (38606/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (39804/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (41012/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (42193/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (43392/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (44597/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (45785/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (46940/50000)
# TEST : Loss: (0.4462) | Acc: (86.00%) (8671/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4640, 0.5360], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.6697, 0.3303], device='cuda:0')
percent tensor([0.7005, 0.2995], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (8584/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (9786/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (10986/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (12187/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (13392/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (14589/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (15788/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (16997/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (19409/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (20620/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (21830/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (23018/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (24211/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (25408/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (26603/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (27811/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (29014/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (30208/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (31390/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (32577/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (33802/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (35005/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (36197/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (37406/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (38605/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (39813/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (41014/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (42221/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (43425/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (93.00%) (44621/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (45817/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (46975/50000)
# TEST : Loss: (0.4415) | Acc: (86.00%) (8663/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5611, 0.4389], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.6671, 0.3329], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (4853/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (6039/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (7224/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (8411/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (9602/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (10804/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (11987/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (13153/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (14335/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (15520/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (16704/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (17886/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (19074/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (20269/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (21470/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (22658/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (23841/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (25022/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (26215/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (27404/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (28616/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (29806/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (31014/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (32196/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (33394/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (34605/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (35798/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (37002/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (38196/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (39404/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (40599/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (41803/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (43004/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (44183/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (45372/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (46525/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4107) | Acc: (87.00%) (8741/10000)
percent tensor([0.5180, 0.4820], device='cuda:0')
percent tensor([0.4653, 0.5347], device='cuda:0')
percent tensor([0.5582, 0.4418], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6400, 0.3600], device='cuda:0')
percent tensor([0.6499, 0.3501], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (2516/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (3710/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (4910/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (6114/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (7309/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (8518/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (9721/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (12125/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (13338/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (14549/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (15749/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (16948/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (18149/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (19347/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (20544/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (21742/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (22926/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (24137/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (25332/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (26540/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (27759/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28958/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (30171/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (31372/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (32571/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (33762/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (34960/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (36162/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (37368/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (38579/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (39780/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (40983/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (42167/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (43370/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (44559/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (45771/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (46935/50000)
# TEST : Loss: (0.4000) | Acc: (87.00%) (8775/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4653, 0.5347], device='cuda:0')
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.6448, 0.3552], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (4925/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (6130/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (7341/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (8553/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (9756/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (10958/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (12173/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (13375/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (14594/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (15805/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (17012/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (18216/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (19419/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (20625/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (21817/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (23029/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (24240/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (25455/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (26652/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (27851/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (29053/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (30264/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (31467/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (32670/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (33885/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (35105/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (36308/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (37488/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (38690/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (39898/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (41097/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (42306/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (43507/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (44704/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (45908/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (47074/50000)
# TEST : Loss: (0.3901) | Acc: (87.00%) (8799/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4652, 0.5348], device='cuda:0')
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.6477, 0.3523], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (4945/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (7366/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (8578/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (9787/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (11000/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (12207/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (13423/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (14619/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (15823/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (17031/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (18234/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (19452/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (20668/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (21862/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (23077/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (24284/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (25503/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (26705/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (27917/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (29130/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (30337/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (31549/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (32763/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (33974/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (35184/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (36388/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (37604/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (38825/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (40046/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (41243/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (42439/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (43650/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (44864/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (46076/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (47235/50000)
# TEST : Loss: (0.3864) | Acc: (88.00%) (8812/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4649, 0.5351], device='cuda:0')
percent tensor([0.5615, 0.4385], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.5796, 0.4204], device='cuda:0')
percent tensor([0.6459, 0.3541], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (7386/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (11044/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (12246/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (13458/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (14675/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (15897/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (17109/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (18328/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (19539/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (20759/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (21954/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (23162/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (24370/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (25566/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (26786/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (28000/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (29208/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (30417/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (31625/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (32840/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (34040/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (35241/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (36449/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (37645/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (38868/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (40085/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (41288/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (42498/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (43709/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (44928/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (46134/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (47291/50000)
# TEST : Loss: (0.3804) | Acc: (88.00%) (8842/10000)
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.4634, 0.5366], device='cuda:0')
percent tensor([0.5601, 0.4399], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.6658, 0.3342], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (6177/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (7393/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (8591/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (9791/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (10994/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (12212/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (13399/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (14605/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (15806/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (17007/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (18210/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (19403/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (20597/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (21797/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (23011/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (24209/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (25408/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (26615/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (27824/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (29014/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (30204/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (31412/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (32623/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (33818/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (35010/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (36207/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (37419/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (94.00%) (38623/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (39815/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (41015/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (42220/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (43422/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (44642/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (45849/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (47009/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4217) | Acc: (87.00%) (8734/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4630, 0.5370], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.2843, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.2087, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.9854, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.8651, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.6857, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2251.1531, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.2109, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.5607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6212.1899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11775.9512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3884.4653, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16443.7930, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (3759/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (4970/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (6193/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (7412/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (8622/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (9818/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (11029/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (12233/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (13428/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (15844/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (17058/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (18268/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (19470/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (20685/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (21890/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23114/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (24334/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (25531/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (26740/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (27938/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (29126/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (30324/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (31525/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (32731/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (33925/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (35130/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (36343/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (37550/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (38762/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (39965/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (41160/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (42366/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (43574/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (44785/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (46010/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (47178/50000)
# TEST : Loss: (0.4490) | Acc: (86.00%) (8604/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.6657, 0.3343], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (95.00%) (4987/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (7408/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8620/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (9831/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (11028/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (12255/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (13455/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (14668/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (15876/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (17076/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (18284/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (19502/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (20712/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (21910/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (23139/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (24342/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (25543/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (26751/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (29167/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (31599/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (32800/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (34006/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (35215/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (36407/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (37609/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (38799/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (39998/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (41208/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (42406/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (43619/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (44807/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (46016/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (47179/50000)
# TEST : Loss: (0.4461) | Acc: (86.00%) (8674/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4634, 0.5366], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.6538, 0.3462], device='cuda:0')
percent tensor([0.6657, 0.3343], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (3738/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (93.00%) (4930/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (6139/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (7367/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (8586/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (9788/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (10982/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (12180/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (13387/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (14615/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (15827/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (17033/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (18235/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (19447/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (20657/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (21867/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (23073/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (24266/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (25472/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (26679/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (27886/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (29087/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (30279/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (31498/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (32712/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (33916/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (35139/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (36356/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (37571/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (38787/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (39994/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (41200/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (42404/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (43598/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (44790/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (45996/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (47170/50000)
# TEST : Loss: (0.4081) | Acc: (87.00%) (8793/10000)
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.4633, 0.5367], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.6534, 0.3466], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (3769/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (4979/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (95.00%) (7431/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (95.00%) (8655/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (95.00%) (9870/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (95.00%) (11086/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (95.00%) (12294/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (13497/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (95.00%) (14717/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (15929/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (95.00%) (17155/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (18380/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (19577/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (20780/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (21988/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (23182/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (24400/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (25616/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (26819/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (28019/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (29227/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (30427/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (31632/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (32840/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (34031/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (35241/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (36448/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (37659/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38886/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (40110/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (41311/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (42518/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (43736/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (44935/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (46145/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (47305/50000)
# TEST : Loss: (0.4659) | Acc: (86.00%) (8653/10000)
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.4635, 0.5365], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.6056, 0.3944], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.6728, 0.3272], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (2502/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (3676/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (6021/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (7204/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (8379/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (9557/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (10749/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (11933/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (13122/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (14307/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (15483/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (16676/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (17863/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (19056/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (20234/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (21423/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (22625/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (23815/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (25005/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (26186/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (27373/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (28570/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (29757/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (30940/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (32138/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (33330/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (34519/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (35707/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (36892/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (38079/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (39285/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (40486/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (41673/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (42863/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (44057/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (45256/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (46422/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4322) | Acc: (86.00%) (8687/10000)
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.4586, 0.5414], device='cuda:0')
percent tensor([0.5672, 0.4328], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.5697, 0.4303], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.6650, 0.3350], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (1317/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (6111/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (8526/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (9727/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (10928/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (12130/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (13323/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (14535/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (15724/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (16943/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (18153/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (19371/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (20571/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (94.00%) (21782/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (22994/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (24187/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (25384/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (26582/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (27784/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (28997/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (30210/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (31415/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (32620/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (94.00%) (33822/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (35027/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (36248/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (37450/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (38640/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (39836/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (41044/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (42255/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (43465/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (44667/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (45871/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (47037/50000)
# TEST : Loss: (0.4171) | Acc: (87.00%) (8734/10000)
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.4560, 0.5440], device='cuda:0')
percent tensor([0.5640, 0.4360], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.5728, 0.4272], device='cuda:0')
percent tensor([0.6500, 0.3500], device='cuda:0')
percent tensor([0.6747, 0.3253], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (3758/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (4977/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (8608/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (12235/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (13444/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (14659/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (15867/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (17076/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (18293/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (19501/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (20712/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (21917/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (23128/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (24331/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (25533/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (26746/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (27960/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (29163/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (30342/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (31541/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (32756/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (33967/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (35175/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (36386/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (37594/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (38797/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (39999/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (41201/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (42418/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (43620/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (44844/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (46064/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (47224/50000)
# TEST : Loss: (0.4007) | Acc: (87.00%) (8773/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.4586, 0.5414], device='cuda:0')
percent tensor([0.5632, 0.4368], device='cuda:0')
percent tensor([0.5977, 0.4023], device='cuda:0')
percent tensor([0.5775, 0.4225], device='cuda:0')
percent tensor([0.6541, 0.3459], device='cuda:0')
percent tensor([0.6829, 0.3171], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (3755/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (6189/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (7398/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (9810/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (11021/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (12237/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (13443/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (14660/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (17069/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (18291/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (19516/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (20722/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (21934/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (23132/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (24352/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (25573/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (26773/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (27985/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (29194/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (31614/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (32820/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (34015/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (35234/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (36456/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (37650/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (38862/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (40078/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (41289/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (42496/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (43717/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (44929/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (46129/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (47300/50000)
# TEST : Loss: (0.3996) | Acc: (87.00%) (8799/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4588, 0.5412], device='cuda:0')
percent tensor([0.5626, 0.4374], device='cuda:0')
percent tensor([0.5981, 0.4019], device='cuda:0')
percent tensor([0.5760, 0.4240], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6905, 0.3095], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (94.00%) (2551/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (3783/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (6212/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (95.00%) (7428/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (8639/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (9865/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (11085/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (12310/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (13519/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (14724/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (15927/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (17139/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (18356/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (19572/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (20773/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (21976/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (23187/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (24399/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (25610/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (26818/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (28028/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (29247/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (30468/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (31668/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (32879/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (34086/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (35289/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (36500/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (37713/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (38929/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (40144/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (41366/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (42578/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (43782/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (44995/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (46220/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (47383/50000)
# TEST : Loss: (0.3938) | Acc: (88.00%) (8809/10000)
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.4574, 0.5426], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6015, 0.3985], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (7439/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (95.00%) (8646/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (95.00%) (9853/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (11065/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (12268/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (13497/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (14707/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (15922/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (17137/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (18344/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (19566/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (20782/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (21985/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (23194/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (24395/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (25608/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (26831/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (28037/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (29244/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (30444/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (31658/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (32864/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (34058/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (35266/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (36466/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (37679/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38898/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (40105/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (41331/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (42539/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (43741/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (44942/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (46150/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (47316/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4500) | Acc: (86.00%) (8686/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.4571, 0.5429], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.5786, 0.4214], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.6971, 0.3029], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.8739, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.1645, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.7325, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.2257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.9375, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2257.4609, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.2510, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.7454, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6227.5732, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11743.0430, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3869.4504, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16377.9883, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (5000/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (6211/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (95.00%) (8637/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (9838/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (11044/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (12256/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (13477/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (15911/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (17105/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (18319/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (19539/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (20746/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (21951/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (23160/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (24368/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (25570/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (26788/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (28012/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (29231/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (30453/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (31655/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (32867/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (34077/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (35282/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (36483/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (37699/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (38918/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (40119/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (41335/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (42552/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (43752/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (44961/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (46160/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (47332/50000)
# TEST : Loss: (0.4261) | Acc: (87.00%) (8706/10000)
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.4580, 0.5420], device='cuda:0')
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6938, 0.3062], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (3777/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (5002/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (6225/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (7437/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (8647/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (9857/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (11078/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (12293/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (13506/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (14712/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (15932/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (17156/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (18373/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (19581/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (20807/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (22005/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (23200/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (24407/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (25627/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (26843/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (28064/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (29282/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (30476/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (31673/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (32883/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (34096/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (35301/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (36514/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (37732/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (38946/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (40147/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (41362/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (43786/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (44993/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (46198/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (47358/50000)
# TEST : Loss: (0.3982) | Acc: (88.00%) (8816/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.4579, 0.5421], device='cuda:0')
percent tensor([0.5624, 0.4376], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.6542, 0.3458], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (3781/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (5000/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (6222/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (7438/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (8664/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (9884/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (11098/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (12293/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (13507/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (14733/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (15954/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (17173/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (18384/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (19613/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (20828/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (22040/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (23262/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (24482/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (25694/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (28117/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (29336/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (30552/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (31778/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (32997/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (34212/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (35426/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (36637/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (37858/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (39066/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (40284/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (41496/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (42713/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (43930/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (45137/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (46342/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (47522/50000)
# TEST : Loss: (0.4097) | Acc: (87.00%) (8780/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.4578, 0.5422], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.6558, 0.3442], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (5024/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (7457/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (8676/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (9899/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (11127/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (12354/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (13580/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (14787/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (15994/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (17205/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (18435/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (19662/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (20882/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (22086/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (23300/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (24523/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (25746/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (26961/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (28183/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (29392/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (30620/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (31846/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (33049/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (34251/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (35479/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (36706/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (37923/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (39153/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (40373/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (41589/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (42813/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (44035/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (45238/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (47625/50000)
# TEST : Loss: (0.4255) | Acc: (87.00%) (8760/10000)
percent tensor([0.5198, 0.4802], device='cuda:0')
percent tensor([0.4576, 0.5424], device='cuda:0')
percent tensor([0.5629, 0.4371], device='cuda:0')
percent tensor([0.6014, 0.3986], device='cuda:0')
percent tensor([0.5792, 0.4208], device='cuda:0')
percent tensor([0.6533, 0.3467], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (2519/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (4909/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (6119/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (7307/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (8502/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (9715/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (10927/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (12124/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (13329/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (14524/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (15714/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (16912/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (18117/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (19320/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (20515/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (21723/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (22919/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (24130/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (25342/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (26566/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (27777/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (28981/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (30184/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (31380/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (93.00%) (32598/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (33796/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (35006/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (36219/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (37448/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (38643/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (39842/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (41045/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (42249/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (43465/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (44676/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (45888/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (47059/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4327) | Acc: (87.00%) (8722/10000)
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.4695, 0.5305], device='cuda:0')
percent tensor([0.5674, 0.4326], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.6664, 0.3336], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')


Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (3766/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (4978/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (6181/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (7396/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (8600/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (9830/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (11043/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (12243/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (13445/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (14647/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (17065/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (18279/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (19498/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (20723/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (21939/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (23167/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (24384/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (25598/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (26823/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (28031/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (29239/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (30455/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (31677/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (32895/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (34107/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (35313/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (36521/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (37755/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (38980/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (40198/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (41411/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (42646/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (43851/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (45057/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (46276/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (47448/50000)
# TEST : Loss: (0.4072) | Acc: (87.00%) (8792/10000)
percent tensor([0.5206, 0.4794], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5714, 0.4286], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.6649, 0.3351], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (1325/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (2546/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (3754/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (4970/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (6191/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (7405/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (8620/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (9835/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (11056/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (12265/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (13496/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (14715/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (15926/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (17145/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (18380/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (19590/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (20805/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (22009/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (23217/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (24440/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (25666/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (26883/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (28108/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (29322/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (30545/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (31753/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (32975/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (34179/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (35392/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (36613/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (37822/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (39036/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (40248/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (41472/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (42689/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (43922/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (45150/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (46363/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (47543/50000)
# TEST : Loss: (0.4039) | Acc: (88.00%) (8807/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4726, 0.5274], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.6063, 0.3937], device='cuda:0')
percent tensor([0.5704, 0.4296], device='cuda:0')
percent tensor([0.6909, 0.3091], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (5015/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (7465/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (8689/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (9908/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (11128/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (12354/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (13581/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (14799/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (16018/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (17245/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (18458/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (19680/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (20913/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (22146/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (23359/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (24581/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (25800/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (27022/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (28249/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (29474/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (30698/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (31931/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (33151/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (34375/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (35580/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (36792/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (38024/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (39247/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (40473/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (41688/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (42907/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (44121/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (45343/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (46564/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (47738/50000)
# TEST : Loss: (0.3951) | Acc: (88.00%) (8816/10000)
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.4733, 0.5267], device='cuda:0')
percent tensor([0.5722, 0.4278], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.6958, 0.3042], device='cuda:0')
percent tensor([0.6761, 0.3239], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (2565/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (3795/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (6240/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (7450/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (8663/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (9887/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (11113/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (12340/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (13562/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (14781/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (15991/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (17215/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (18431/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (19641/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (20847/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (22060/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (23270/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (24503/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (25726/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (26940/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (28151/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (29378/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (30600/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (31821/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (33046/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (34267/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (35485/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (36698/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (37921/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (39142/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (40354/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (41584/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (42801/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (44024/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (45250/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (46479/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (47660/50000)
# TEST : Loss: (0.3915) | Acc: (88.00%) (8841/10000)
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.6917, 0.3083], device='cuda:0')
percent tensor([0.6834, 0.3166], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (6230/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (7433/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (8659/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (9879/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (11096/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (12331/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (13554/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (14768/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (15995/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (17233/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (18461/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (19684/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (20900/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (22130/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (23368/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (24590/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (25808/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (27025/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (28239/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (29441/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (30653/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (31868/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (33093/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (34313/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (35527/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (36747/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (37968/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (39175/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (40385/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (41607/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (42818/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (44037/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (45256/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (46465/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (47619/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
# TEST : Loss: (0.4635) | Acc: (86.00%) (8689/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.4717, 0.5283], device='cuda:0')
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.6897, 0.3103], device='cuda:0')
percent tensor([0.6866, 0.3134], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.7126, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.3984, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.3984, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.1160, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.0110, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2266.6584, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4268.9985, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1371.9889, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6252.8701, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11728.7715, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3860.5881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16338.1709, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 151 | Batch_idx: 0 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (2571/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (6229/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (7456/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (8677/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (9901/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (11129/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (12335/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (13547/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (14782/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (16005/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (17217/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (18436/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (19648/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (20883/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (22101/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (23310/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (24537/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (25747/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (26961/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (28166/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (29380/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (30598/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (31817/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (33029/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (34254/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (35470/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (36673/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (37896/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (39108/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (40327/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (41542/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (42761/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (43992/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (45219/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (46442/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (47619/50000)
# TEST : Loss: (0.4401) | Acc: (87.00%) (8719/10000)
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.4723, 0.5277], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6018, 0.3982], device='cuda:0')
percent tensor([0.5720, 0.4280], device='cuda:0')
percent tensor([0.6900, 0.3100], device='cuda:0')
percent tensor([0.6857, 0.3143], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (7456/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (8687/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (9902/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (11138/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (12368/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (13599/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (14823/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (16045/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (17266/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (18484/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (19698/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (20921/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (22145/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (23380/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (24602/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (25834/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (27057/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (28270/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (29500/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (30699/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (31913/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (33131/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (34339/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (35554/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (36770/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (37992/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (39209/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (40431/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (41662/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (42896/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (44109/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (45324/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (46542/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (47712/50000)
# TEST : Loss: (0.4233) | Acc: (87.00%) (8789/10000)
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.4723, 0.5277], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.5735, 0.4265], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.6851, 0.3149], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (5029/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (6262/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (7485/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (8702/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (9924/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (11154/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (12391/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (13612/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (14852/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (16092/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (17311/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (18529/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (19748/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (20959/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (22179/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (23407/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (24632/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (25845/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (27070/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (28285/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (29508/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (30726/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (31950/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (33170/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (34392/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (35607/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (36838/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (38055/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (39270/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (40498/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (41712/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (42939/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (44162/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (45399/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (46613/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (47793/50000)
# TEST : Loss: (0.4221) | Acc: (87.00%) (8760/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.4722, 0.5278], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.6023, 0.3977], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6930, 0.3070], device='cuda:0')
percent tensor([0.6842, 0.3158], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (7485/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (8704/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (9928/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (11148/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (12380/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (13613/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (14843/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (16058/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (17290/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (18516/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (19751/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (20989/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (22215/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (23438/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (24660/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (25897/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (27133/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (28357/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (29588/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (30825/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (32051/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (33274/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (34498/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (35733/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (36954/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (38178/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (39404/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (40623/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (41834/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (43045/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (44260/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (45471/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (46686/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (47864/50000)
# TEST : Loss: (0.4399) | Acc: (87.00%) (8717/10000)
percent tensor([0.5187, 0.4813], device='cuda:0')
percent tensor([0.4724, 0.5276], device='cuda:0')
percent tensor([0.5714, 0.4286], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6872, 0.3128], device='cuda:0')
percent tensor([0.6815, 0.3185], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (2565/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (5023/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (7442/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (8654/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (9870/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (11084/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (12302/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (13523/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (14733/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (15957/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (17171/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (18385/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (19619/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (20843/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (22061/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (23276/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (24485/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (25717/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (26938/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (28147/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (29357/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (30554/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (31787/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (33000/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (34205/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (35432/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (36673/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (37908/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (39123/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (40345/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (41561/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (42780/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (43996/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (45223/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (46451/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (47620/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_155.pth.tar'
# TEST : Loss: (0.4066) | Acc: (88.00%) (8821/10000)
percent tensor([0.5214, 0.4786], device='cuda:0')
percent tensor([0.4688, 0.5312], device='cuda:0')
percent tensor([0.5596, 0.4404], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (6252/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (7484/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (8706/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (9931/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (11151/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (12373/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (13608/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (14835/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (16077/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (17296/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (18523/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (19752/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (20984/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (22195/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (23421/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (24653/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (25880/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (27105/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (28331/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (29555/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (30783/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (32014/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (33242/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (34475/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (35706/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (36945/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (38178/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (39399/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (40622/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (41842/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (43064/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (44294/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (45525/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (46733/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (47917/50000)
# TEST : Loss: (0.3899) | Acc: (88.00%) (8866/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4665, 0.5335], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.7266, 0.2734], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (3804/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (6276/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (7502/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (8733/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (9966/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (11192/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (12422/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (13639/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (14874/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (96.00%) (16102/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (17324/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (18565/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (96.00%) (19796/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (21028/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (22260/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (23484/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (24724/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (25957/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (27185/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (28420/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (96.00%) (29647/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (30876/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (96.00%) (32116/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (33330/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (34555/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (35782/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (37016/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (38245/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (39462/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (40692/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (96.00%) (41924/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (43157/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (44382/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (96.00%) (45608/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (46846/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (96.00%) (48032/50000)
# TEST : Loss: (0.3877) | Acc: (88.00%) (8860/10000)
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.4676, 0.5324], device='cuda:0')
percent tensor([0.5603, 0.4397], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.5818, 0.4182], device='cuda:0')
percent tensor([0.7262, 0.2738], device='cuda:0')
percent tensor([0.6653, 0.3347], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (5054/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (6297/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (7533/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (8761/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (9996/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (11207/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (12438/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (13671/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (14904/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (16141/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (17360/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (18592/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (19832/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (21060/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (22286/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (23511/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (24733/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (25965/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (27192/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (28428/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (29658/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (30891/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (32121/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (33358/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (34581/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (35808/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (37040/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (38264/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (39497/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (40714/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (41942/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (43167/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (44409/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (45650/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (46882/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (48060/50000)
# TEST : Loss: (0.3828) | Acc: (88.00%) (8889/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.4678, 0.5322], device='cuda:0')
percent tensor([0.5598, 0.4402], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.7242, 0.2758], device='cuda:0')
percent tensor([0.6749, 0.3251], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (5053/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (6289/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (7523/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (8734/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (9973/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (11205/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (12428/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (13662/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (14899/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (16137/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (17362/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (18607/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (19846/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (21077/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (22302/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (23534/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (24762/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (25986/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (27209/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (28446/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (29690/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (30924/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (32165/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (33402/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (34642/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (35877/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (37104/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (38341/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (39571/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (40802/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (42033/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (43256/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (44491/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (45725/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (46959/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (48145/50000)
# TEST : Loss: (0.3773) | Acc: (88.00%) (8888/10000)
percent tensor([0.5185, 0.4815], device='cuda:0')
percent tensor([0.4680, 0.5320], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.6027, 0.3973], device='cuda:0')
percent tensor([0.5829, 0.4171], device='cuda:0')
percent tensor([0.7241, 0.2759], device='cuda:0')
percent tensor([0.6779, 0.3221], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (3821/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (6281/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (7518/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (8760/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (9999/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (11239/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (12476/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (13700/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (14927/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (16154/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (17384/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (18614/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (19846/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (21067/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (22290/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (23523/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (24757/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (25972/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (27202/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (28442/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (29662/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (30892/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (32120/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (33347/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (34576/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (35788/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (37003/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (38217/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (39448/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (40676/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (41896/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (43130/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (44349/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (45574/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (46796/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (47971/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_160.pth.tar'
# TEST : Loss: (0.4434) | Acc: (87.00%) (8739/10000)
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.4682, 0.5318], device='cuda:0')
percent tensor([0.5597, 0.4403], device='cuda:0')
percent tensor([0.6027, 0.3973], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.7247, 0.2753], device='cuda:0')
percent tensor([0.6796, 0.3204], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.1425, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.1368, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(815.3831, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.6073, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.2590, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2271.5305, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.2954, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1367.3134, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6267.3242, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11695.8965, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3845.6924, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16272.7891, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (2568/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (3794/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (5026/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (6261/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (7498/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (8738/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (9966/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (11202/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (12437/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (13660/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (14887/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (16121/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (17359/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (18592/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (19819/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (21030/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (96.00%) (22255/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (23484/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (96.00%) (24713/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (25941/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (27158/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (96.00%) (28386/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (29610/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (30839/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (32068/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (33294/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (34509/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (35730/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (36967/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (38183/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (39404/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (40631/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (41866/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (43091/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (44319/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (45548/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (46774/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (47944/50000)
# TEST : Loss: (0.4345) | Acc: (87.00%) (8704/10000)
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.4686, 0.5314], device='cuda:0')
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.7219, 0.2781], device='cuda:0')
percent tensor([0.6753, 0.3247], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (2572/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (95.00%) (5029/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (95.00%) (6264/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (7505/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (8736/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (9954/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (11188/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (12417/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (13651/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (14890/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (16099/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (17327/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (18546/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (95.00%) (19782/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (95.00%) (21012/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (22252/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (23477/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (24704/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (25934/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (27171/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (28393/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (29608/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (30840/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (32068/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (33300/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (34534/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (35755/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (36967/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (38200/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (39416/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (40640/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (41854/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (43092/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (44320/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (45550/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (46773/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (47949/50000)
# TEST : Loss: (0.4385) | Acc: (87.00%) (8756/10000)
percent tensor([0.5187, 0.4813], device='cuda:0')
percent tensor([0.4685, 0.5315], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.7255, 0.2745], device='cuda:0')
percent tensor([0.6799, 0.3201], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (3820/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (5051/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (6280/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (7514/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (8753/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (9970/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (11203/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (12431/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (13661/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (14896/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (16135/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (17368/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (18595/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (19819/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (21049/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (22269/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (23500/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (24729/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (25959/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (27188/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (28416/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (29648/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (30875/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (32102/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (33319/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (34551/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (35780/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (37015/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (38235/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (39463/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (40686/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (41907/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (95.00%) (43129/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (44339/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (45564/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (46788/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (47972/50000)
# TEST : Loss: (0.4243) | Acc: (87.00%) (8786/10000)
percent tensor([0.5190, 0.4810], device='cuda:0')
percent tensor([0.4683, 0.5317], device='cuda:0')
percent tensor([0.5601, 0.4399], device='cuda:0')
percent tensor([0.6026, 0.3974], device='cuda:0')
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.7228, 0.2772], device='cuda:0')
percent tensor([0.6739, 0.3261], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (6272/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (7509/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (9974/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (11211/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (12458/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (13691/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (14924/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (16159/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (17392/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (18615/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (19854/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (21087/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (22315/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (23538/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (24768/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (25994/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (27223/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (28451/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (29681/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (30915/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (32144/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (33384/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (34602/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (35836/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (37058/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (38278/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (39499/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (40718/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (41940/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (43176/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (44398/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (45618/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (46842/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (48025/50000)
# TEST : Loss: (0.4648) | Acc: (86.00%) (8682/10000)
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.4690, 0.5310], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.6020, 0.3980], device='cuda:0')
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.7213, 0.2787], device='cuda:0')
percent tensor([0.6761, 0.3239], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (2571/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (3780/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (95.00%) (6202/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (7410/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (8626/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (9845/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (11064/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (12266/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (13483/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (14685/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (15915/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (17135/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (18357/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (19578/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (20783/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (21983/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (23206/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (24409/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (25636/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (26853/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (28077/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (29295/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (30513/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (31730/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (32948/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (34171/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (35388/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (36608/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (37840/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (39054/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (40274/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (41489/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (42714/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (43931/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (45156/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (46382/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (47560/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_165.pth.tar'
# TEST : Loss: (0.4307) | Acc: (87.00%) (8756/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.4716, 0.5284], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.6629, 0.3371], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 166 | Batch_idx: 0 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (3807/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (7480/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (8704/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (9940/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (11160/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (12388/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (13609/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (14843/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (16061/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (17287/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (18526/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (19761/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (20977/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (22203/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (23426/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (24659/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (25898/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (27118/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (28336/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (29570/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (30801/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (32030/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (33249/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (34484/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (35703/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (36933/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (38156/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (39387/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (40626/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (41850/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (43086/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (44322/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (45555/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (46778/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (47965/50000)
# TEST : Loss: (0.4190) | Acc: (88.00%) (8806/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.4730, 0.5270], device='cuda:0')
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.6139, 0.3861], device='cuda:0')
percent tensor([0.5935, 0.4065], device='cuda:0')
percent tensor([0.7076, 0.2924], device='cuda:0')
percent tensor([0.6662, 0.3338], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (96.00%) (2586/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (96.00%) (5039/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (96.00%) (6269/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (7503/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (8729/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (9941/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (11170/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (12399/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (13620/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (14846/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (16068/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (17301/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (18529/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (19763/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (20988/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (22221/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (23465/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (24696/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (25927/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (27162/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (28397/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (29626/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (30861/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (32088/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (33317/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (34543/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (35775/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (36996/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (38230/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (39455/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (40689/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (41924/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (43147/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (44388/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (45612/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (46841/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (48035/50000)
# TEST : Loss: (0.3989) | Acc: (88.00%) (8846/10000)
percent tensor([0.5153, 0.4847], device='cuda:0')
percent tensor([0.4744, 0.5256], device='cuda:0')
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.6149, 0.3851], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.7186, 0.2814], device='cuda:0')
percent tensor([0.6732, 0.3268], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 168 | Batch_idx: 0 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (6273/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (7501/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (9976/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (11209/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (12442/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (13680/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (14911/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (16156/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (17392/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (18627/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (19861/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (21091/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (22324/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (23546/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (24771/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (25996/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (27227/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (28458/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (29685/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (30906/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (32139/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (33370/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (34607/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (35837/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (37062/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (38303/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (39528/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (40772/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (41998/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (43244/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (44483/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (45713/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (46942/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (48129/50000)
# TEST : Loss: (0.3970) | Acc: (88.00%) (8856/10000)
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.4721, 0.5279], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.6163, 0.3837], device='cuda:0')
percent tensor([0.5946, 0.4054], device='cuda:0')
percent tensor([0.7189, 0.2811], device='cuda:0')
percent tensor([0.6701, 0.3299], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 169 | Batch_idx: 0 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (5065/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (6299/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (7546/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (8776/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (10013/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (11251/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (12497/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (13745/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (14973/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (16209/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (17451/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (18685/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (19915/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (21160/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (22388/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (23629/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (24863/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (26102/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (27350/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (28587/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (29821/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (31064/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (32305/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (33542/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (34782/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (36029/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (37263/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (38506/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (39733/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (40958/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (42189/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (43431/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (44676/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (45915/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (47150/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (48327/50000)
# TEST : Loss: (0.3901) | Acc: (88.00%) (8864/10000)
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.4715, 0.5285], device='cuda:0')
percent tensor([0.5634, 0.4366], device='cuda:0')
percent tensor([0.6169, 0.3831], device='cuda:0')
percent tensor([0.6007, 0.3993], device='cuda:0')
percent tensor([0.7218, 0.2782], device='cuda:0')
percent tensor([0.6769, 0.3231], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (5049/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (6296/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (7529/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (8754/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (9982/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (11207/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (12424/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (13657/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (14889/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (16132/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (17367/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (18582/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (19815/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (21040/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (22273/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (23515/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (24729/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (25955/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (27183/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (28415/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (29638/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (30865/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (32107/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (33340/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (34571/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (35796/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (37019/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (38252/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (39500/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (40735/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (41974/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (43195/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (44422/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (45642/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (46881/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (48061/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_170.pth.tar'
# TEST : Loss: (0.4756) | Acc: (86.00%) (8689/10000)
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.6180, 0.3820], device='cuda:0')
percent tensor([0.6016, 0.3984], device='cuda:0')
percent tensor([0.7199, 0.2801], device='cuda:0')
percent tensor([0.6804, 0.3196], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.4571, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.7634, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.6693, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1507.9946, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(491.6064, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2276.0193, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4261.7915, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1362.5444, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6281.4004, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11662.5820, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3830.8914, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16207.8652, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 171 | Batch_idx: 0 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (3848/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (5077/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (7561/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (8800/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (10028/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (11258/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (12496/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (13732/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (14967/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (16196/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (17438/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (18662/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (19896/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (21133/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (22361/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (23585/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (24811/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (26048/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (27280/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (28504/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (29735/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (30961/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (32187/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (33403/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (34620/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (35857/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (37084/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (38306/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (39556/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (40782/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (42009/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (43243/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (44478/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (45700/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (46926/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (48101/50000)
# TEST : Loss: (0.4900) | Acc: (86.00%) (8633/10000)
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.4716, 0.5284], device='cuda:0')
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.6166, 0.3834], device='cuda:0')
percent tensor([0.6003, 0.3997], device='cuda:0')
percent tensor([0.7203, 0.2797], device='cuda:0')
percent tensor([0.6786, 0.3214], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 172 | Batch_idx: 0 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (5058/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (7530/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (8763/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (10005/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (11245/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (12481/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (13714/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (14953/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (16180/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (17418/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (18663/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (19900/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (21133/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (22364/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (23583/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (24821/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (26051/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (27286/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (28523/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (29757/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (30986/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (32217/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (33441/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (34673/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (35910/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (37127/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (38358/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (39586/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (40818/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (42051/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (43286/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (44514/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (45749/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (46982/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (48160/50000)
# TEST : Loss: (0.4234) | Acc: (87.00%) (8773/10000)
percent tensor([0.5145, 0.4855], device='cuda:0')
percent tensor([0.4718, 0.5282], device='cuda:0')
percent tensor([0.5636, 0.4364], device='cuda:0')
percent tensor([0.6161, 0.3839], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.7172, 0.2828], device='cuda:0')
percent tensor([0.6729, 0.3271], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 173 | Batch_idx: 0 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (2602/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (6305/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (7544/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (8789/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (10023/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (11254/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (12475/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (13709/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (14932/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (16166/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (17407/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (18650/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (19873/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (21108/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (22338/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (23573/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (24819/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (26053/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (27294/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (28529/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (29766/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (31002/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (32249/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (33472/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (34710/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (35949/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (37175/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (38396/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (39627/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (40863/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (42099/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (43342/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (44566/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (45797/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (47033/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (48220/50000)
# TEST : Loss: (0.4912) | Acc: (87.00%) (8720/10000)
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.4718, 0.5282], device='cuda:0')
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.6156, 0.3844], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.7194, 0.2806], device='cuda:0')
percent tensor([0.6760, 0.3240], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 174 | Batch_idx: 0 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (5077/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (6306/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (7532/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (8771/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (10011/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (11252/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (12492/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (13734/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (14974/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (16208/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (17459/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (18692/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (19929/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (21161/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (22402/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (23635/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (24867/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (26105/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (27337/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (28573/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (29811/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (31047/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (32286/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (33525/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (34753/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (35977/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (37207/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (38447/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (39675/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (40910/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (42156/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (43390/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (44639/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (45867/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (47099/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (48296/50000)
# TEST : Loss: (0.4138) | Acc: (88.00%) (8832/10000)
percent tensor([0.5145, 0.4855], device='cuda:0')
percent tensor([0.4721, 0.5279], device='cuda:0')
percent tensor([0.5638, 0.4362], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.7178, 0.2822], device='cuda:0')
percent tensor([0.6762, 0.3238], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (2600/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (3822/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (5052/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (7517/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (8741/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (9980/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (11204/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (12441/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (13677/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (14908/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (16132/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (17346/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (18571/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (19814/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (21036/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (22261/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (23484/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (95.00%) (24697/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (25920/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (27147/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (28369/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (29598/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (95.00%) (30838/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (95.00%) (32069/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (33303/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (34539/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (35776/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (37021/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (38268/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (39514/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (40753/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (41995/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (43224/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (44456/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (45687/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (46930/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (48124/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_175.pth.tar'
# TEST : Loss: (0.4146) | Acc: (88.00%) (8814/10000)
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.4687, 0.5313], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.6961, 0.3039], device='cuda:0')
percent tensor([0.6808, 0.3192], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 176 | Batch_idx: 0 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (5074/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (6310/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (7538/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (8757/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (9997/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (11232/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (12464/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (13711/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (14935/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (16149/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (17382/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (18620/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (19849/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (21097/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (22328/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (23572/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (24800/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (26038/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (27280/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (28525/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (29757/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (31000/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (32245/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (33483/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (34715/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (35961/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (37204/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (38435/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (39676/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (40898/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (42140/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (43392/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (44633/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (45867/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (47111/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (48311/50000)
# TEST : Loss: (0.4021) | Acc: (88.00%) (8855/10000)
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.4668, 0.5332], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.5990, 0.4010], device='cuda:0')
percent tensor([0.6001, 0.3999], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.6867, 0.3133], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 177 | Batch_idx: 0 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (6311/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (7547/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (10011/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (11252/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (12483/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (13726/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (14971/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (16206/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (17443/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (18691/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (19930/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (21174/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (22414/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (23654/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (24890/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (26128/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (27358/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (28590/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (29831/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (31068/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (32308/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (33545/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (34791/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (36033/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (37279/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (38515/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (39759/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (40998/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (42242/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (43483/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (44726/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (45972/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (47224/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (48427/50000)
# TEST : Loss: (0.3971) | Acc: (88.00%) (8873/10000)
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.4655, 0.5345], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.6031, 0.3969], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.6957, 0.3043], device='cuda:0')
percent tensor([0.6902, 0.3098], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 178 | Batch_idx: 0 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (97.00%) (5094/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (6327/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (7575/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (8812/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (10053/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (11287/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (12529/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (13775/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (15020/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (16259/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (17501/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (18748/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (19986/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (97.00%) (21235/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (22471/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (23711/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (24945/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (26181/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (27422/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (28652/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (29885/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (31134/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (32364/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (33602/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (34826/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (36071/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (37317/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (38562/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (39800/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (41044/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (42292/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (43529/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (44769/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (46011/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (47256/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (48457/50000)
# TEST : Loss: (0.3919) | Acc: (88.00%) (8889/10000)
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.4654, 0.5346], device='cuda:0')
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6081, 0.3919], device='cuda:0')
percent tensor([0.6935, 0.3065], device='cuda:0')
percent tensor([0.6900, 0.3100], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 179 | Batch_idx: 0 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (6347/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (7584/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (97.00%) (8830/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (97.00%) (10071/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (11310/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (12563/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (13795/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (97.00%) (15028/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (16256/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (17504/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (18741/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (19982/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (21224/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (22468/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (23710/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (24954/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (26199/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (27445/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (28698/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (29952/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (31190/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (32431/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (33685/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (34924/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (36164/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (37402/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (97.00%) (38641/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (39877/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (41131/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (42370/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (43621/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (44866/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (46097/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (47332/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (48521/50000)
# TEST : Loss: (0.3921) | Acc: (88.00%) (8879/10000)
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.4654, 0.5346], device='cuda:0')
percent tensor([0.5760, 0.4240], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.6955, 0.3045], device='cuda:0')
percent tensor([0.7004, 0.2996], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (5096/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (6328/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (7575/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (8811/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (10055/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (11300/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (12534/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (13778/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (15004/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (16237/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (17459/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (18683/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (19929/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (21172/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (22409/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (23637/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (24872/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (26093/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (27321/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (28551/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (29784/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (31019/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (32247/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (33472/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (34710/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (35950/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (37186/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (38431/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (39673/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (40906/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (42146/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (43390/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (44631/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (45857/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (47099/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (48270/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_180.pth.tar'
# TEST : Loss: (0.4218) | Acc: (88.00%) (8818/10000)
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.4657, 0.5343], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.6078, 0.3922], device='cuda:0')
percent tensor([0.6062, 0.3938], device='cuda:0')
percent tensor([0.6957, 0.3043], device='cuda:0')
percent tensor([0.7041, 0.2959], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.8329, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.3527, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.6793, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1506.3601, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.8781, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2279.4819, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4258.4531, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1357.6328, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6294.1875, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11629.5361, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3816.0857, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16143.1895, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (97.00%) (3849/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (5082/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (6327/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (7560/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (8802/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (10047/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (11281/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (12510/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (13746/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (14978/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (16225/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (17460/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (18696/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (19933/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (21185/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (22426/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (23665/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (24898/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (26141/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (27382/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (28619/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (29861/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (31097/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (32330/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (33559/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (34791/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (36021/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (37253/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (38494/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (39737/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (40968/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (42208/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (43439/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (44678/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (45915/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (47145/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (48330/50000)
# TEST : Loss: (0.4381) | Acc: (87.00%) (8779/10000)
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5751, 0.4249], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.6057, 0.3943], device='cuda:0')
percent tensor([0.6933, 0.3067], device='cuda:0')
percent tensor([0.6963, 0.3037], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 182 | Batch_idx: 0 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (6309/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (7556/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (8797/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (10033/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (11279/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (12519/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (13758/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (14997/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (16227/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (17469/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (18703/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (19944/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (21190/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (22441/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (23670/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (24918/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (26160/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (27389/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (28635/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (29873/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (31108/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (32342/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (33574/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (34809/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (36048/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (37289/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (38532/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (39754/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (40987/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (42226/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (43448/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (44690/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (45920/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (47159/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (48331/50000)
# TEST : Loss: (0.4260) | Acc: (88.00%) (8851/10000)
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4656, 0.5344], device='cuda:0')
percent tensor([0.5761, 0.4239], device='cuda:0')
percent tensor([0.6094, 0.3906], device='cuda:0')
percent tensor([0.6078, 0.3922], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 183 | Batch_idx: 0 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (7564/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (8815/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (10063/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (11312/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (12542/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (13788/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (15017/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (16254/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (17490/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (18723/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (19958/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (21192/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (22437/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (23670/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (24897/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (26142/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (27371/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (28606/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (29850/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (31086/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (32325/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (33560/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (34791/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (36029/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (37264/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (38503/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (39749/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (40982/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (42222/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (43461/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (44682/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (45925/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (47151/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (48338/50000)
# TEST : Loss: (0.4704) | Acc: (87.00%) (8764/10000)
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4657, 0.5343], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.6088, 0.3912], device='cuda:0')
percent tensor([0.6075, 0.3925], device='cuda:0')
percent tensor([0.6969, 0.3031], device='cuda:0')
percent tensor([0.7089, 0.2911], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (3837/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (5073/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (6306/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (7545/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (8797/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (10046/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (11285/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (12522/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (13762/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (14998/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (16231/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (17476/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (18713/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (19948/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (21186/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (22418/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (23659/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (24891/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (26122/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (27362/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (28600/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (29852/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (31089/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (32325/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (33560/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (34801/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (36043/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (37283/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (38515/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (39756/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (40995/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (42235/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (43479/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (44720/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (45960/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (47200/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (48391/50000)
# TEST : Loss: (0.4115) | Acc: (88.00%) (8838/10000)
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4661, 0.5339], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.6075, 0.3925], device='cuda:0')
percent tensor([0.6057, 0.3943], device='cuda:0')
percent tensor([0.6949, 0.3051], device='cuda:0')
percent tensor([0.6984, 0.3016], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (5012/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (7419/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (8639/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (9861/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (11067/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (12264/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (13485/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (14695/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (15920/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (17128/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (18350/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (19564/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (20782/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (22001/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (23213/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (24436/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (94.00%) (25656/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (94.00%) (26871/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (28090/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (29318/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (30538/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (31773/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (32998/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (34219/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (35449/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (36669/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (37905/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (39136/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (40354/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (41583/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (42808/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (44037/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (45269/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (46498/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (47685/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_185.pth.tar'
# TEST : Loss: (0.4410) | Acc: (88.00%) (8820/10000)
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.4644, 0.5356], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.6181, 0.3819], device='cuda:0')
percent tensor([0.6113, 0.3887], device='cuda:0')
percent tensor([0.7125, 0.2875], device='cuda:0')
percent tensor([0.6543, 0.3457], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 186 | Batch_idx: 0 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (5061/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (6293/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (7527/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (8753/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (9981/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (11216/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (12427/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (13662/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (14901/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (16124/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (17342/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (18565/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (19796/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (21025/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (22243/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (23477/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (24704/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (25934/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (27168/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (28393/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (29623/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (30854/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (32088/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (33316/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (34552/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (35793/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (37023/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (38257/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (39498/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (40728/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (41960/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (43194/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (44432/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (45667/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (46911/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (48093/50000)
# TEST : Loss: (0.4253) | Acc: (88.00%) (8843/10000)
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.4651, 0.5349], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.6265, 0.3735], device='cuda:0')
percent tensor([0.6093, 0.3907], device='cuda:0')
percent tensor([0.7198, 0.2802], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (6302/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (7531/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (8773/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (10007/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (11242/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (12483/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (13714/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (14945/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (16168/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (17403/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (18639/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (19877/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (21108/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (22350/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23589/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (24823/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (26056/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (27294/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (28532/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (29772/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (31000/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (32238/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (33460/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (34699/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (35937/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (37173/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (38419/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (39658/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (40892/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (42129/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (43359/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (44591/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (45835/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (47065/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (48257/50000)
# TEST : Loss: (0.4155) | Acc: (88.00%) (8871/10000)
percent tensor([0.5128, 0.4872], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5860, 0.4140], device='cuda:0')
percent tensor([0.6225, 0.3775], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.7207, 0.2793], device='cuda:0')
percent tensor([0.6575, 0.3425], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (3848/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (97.00%) (6336/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (7568/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (8803/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (10042/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (11293/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (12524/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (13757/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (14987/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (16222/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (17458/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (18683/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (19922/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (21160/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (22392/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (23641/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (24885/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (26121/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (27359/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (28594/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (29836/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (31070/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (32311/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (33557/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (34791/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (36020/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (37266/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (38502/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (39744/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (40964/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (42202/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (43440/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (44676/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (45913/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (47139/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (48334/50000)
# TEST : Loss: (0.4120) | Acc: (88.00%) (8873/10000)
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.4655, 0.5345], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.6222, 0.3778], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.7170, 0.2830], device='cuda:0')
percent tensor([0.6649, 0.3351], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 189 | Batch_idx: 0 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (6320/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (7557/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (8802/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (10040/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (11282/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (12516/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (13746/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (14988/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (16231/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (17473/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (18718/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (19957/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (21194/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (22436/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (23675/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (24908/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (26153/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (27395/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (28630/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (29871/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (31120/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (32363/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (33598/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (34832/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (36072/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (37323/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (38560/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (39807/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (41047/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (42291/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (43523/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (44780/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (46015/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (47247/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (48451/50000)
# TEST : Loss: (0.4086) | Acc: (88.00%) (8889/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4658, 0.5342], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.6245, 0.3755], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.7192, 0.2808], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (97.00%) (5094/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (97.00%) (6333/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (7570/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (8813/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (10044/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (11296/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (97.00%) (12543/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (97.00%) (13785/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (15020/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (16258/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (17504/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (18740/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (19986/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (21229/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (22472/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (97.00%) (23717/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (24951/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (26188/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (27431/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (28677/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (29913/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (31147/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (32384/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (33622/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (34865/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (36116/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (37349/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (38590/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (39824/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (41066/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (42297/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (43546/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (44785/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (46024/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (47250/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (48443/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_190.pth.tar'
# TEST : Loss: (0.4429) | Acc: (88.00%) (8835/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4660, 0.5340], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.6122, 0.3878], device='cuda:0')
percent tensor([0.7206, 0.2794], device='cuda:0')
percent tensor([0.6685, 0.3315], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.1285, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.7075, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.9661, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.7111, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.0474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2282.9929, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4255.0166, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1352.7550, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6306.3657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11596.8975, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3801.2961, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16078.9609, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (3873/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (5119/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (6364/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (7608/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (8860/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (10097/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (11341/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (12583/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (13831/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (15078/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (16313/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (17556/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (18797/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (20029/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (21268/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (22517/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (23761/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (25006/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (97.00%) (26230/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (97.00%) (27471/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (28714/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (29953/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (31195/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (32441/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (33686/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (34918/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (36168/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (37405/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (38644/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (39886/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (41127/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (42366/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (43607/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (44842/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (46068/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (47313/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (48502/50000)
# TEST : Loss: (0.4362) | Acc: (88.00%) (8831/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4659, 0.5341], device='cuda:0')
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.7222, 0.2778], device='cuda:0')
percent tensor([0.6740, 0.3260], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (6334/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (7583/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (8829/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (10066/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (11312/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (12548/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (13790/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (15034/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (16284/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (17529/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (18777/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (20024/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (21268/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (22514/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (23764/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (25005/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (26241/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (27491/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (28739/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (29977/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (31218/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (32471/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (33712/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (34945/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (36186/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (37432/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (38665/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (39902/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (41143/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (42381/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (97.00%) (43616/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (44857/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (46099/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (47336/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (48531/50000)
# TEST : Loss: (0.4754) | Acc: (87.00%) (8742/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4661, 0.5339], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.6227, 0.3773], device='cuda:0')
percent tensor([0.6092, 0.3908], device='cuda:0')
percent tensor([0.7191, 0.2809], device='cuda:0')
percent tensor([0.6666, 0.3334], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 193 | Batch_idx: 0 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (3872/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (5110/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (7582/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (8821/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (10078/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (11317/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (12569/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (13809/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (15052/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (16300/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (17535/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (18784/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (20026/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (21273/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (22528/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (23765/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (25007/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (26254/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (27502/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (28743/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (29988/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (31229/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (32471/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (33708/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (34942/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (36191/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (37440/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (38686/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (39931/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (41178/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (42424/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (43662/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (44908/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (46149/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (47400/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (48592/50000)
# TEST : Loss: (0.4571) | Acc: (88.00%) (8819/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4663, 0.5337], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.7207, 0.2793], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 194 | Batch_idx: 0 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (5114/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (7612/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (8859/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (10102/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (11351/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (12595/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (13842/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (15096/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (16338/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (17583/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (18828/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (20057/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (21289/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (22536/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (23766/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (25019/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (26264/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (27508/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (28746/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (29998/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (31250/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (32492/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (33736/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (34983/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (36227/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (37471/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (38712/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (39957/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (41198/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (42437/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (43667/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (44905/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (46148/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (47404/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (48606/50000)
# TEST : Loss: (0.5235) | Acc: (86.00%) (8647/10000)
percent tensor([0.5135, 0.4865], device='cuda:0')
percent tensor([0.4654, 0.5346], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.6242, 0.3758], device='cuda:0')
percent tensor([0.6107, 0.3893], device='cuda:0')
percent tensor([0.7178, 0.2822], device='cuda:0')
percent tensor([0.6718, 0.3282], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (3795/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (5003/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (6236/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (7450/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (8659/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (9874/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (11088/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (12296/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (13512/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (14739/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (15953/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (17176/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (18399/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (19607/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (20828/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (22068/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (23292/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (24510/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (25747/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (26976/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (28217/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (29437/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (30660/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (31879/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (33094/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (34336/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (35566/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (36801/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (38021/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (39252/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (40498/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (41725/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (42951/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (44192/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (45415/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (46656/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (47852/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_195.pth.tar'
# TEST : Loss: (0.4454) | Acc: (87.00%) (8786/10000)
percent tensor([0.5195, 0.4805], device='cuda:0')
percent tensor([0.4672, 0.5328], device='cuda:0')
percent tensor([0.6169, 0.3831], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.7507, 0.2493], device='cuda:0')
percent tensor([0.6333, 0.3667], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (5073/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (6305/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (7532/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (8758/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (9998/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (11234/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (12463/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (13701/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (14942/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (16179/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (17411/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (18641/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (19866/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (21089/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (22325/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23554/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (24790/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (26026/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (27264/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (28500/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (29730/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (30959/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (32203/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (33443/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (34695/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (35933/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (37175/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (38411/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (39657/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (40899/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (42139/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (43359/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (44596/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (45836/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (47068/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (48269/50000)
# TEST : Loss: (0.4240) | Acc: (88.00%) (8849/10000)
percent tensor([0.5209, 0.4791], device='cuda:0')
percent tensor([0.4681, 0.5319], device='cuda:0')
percent tensor([0.6122, 0.3878], device='cuda:0')
percent tensor([0.5860, 0.4140], device='cuda:0')
percent tensor([0.6177, 0.3823], device='cuda:0')
percent tensor([0.7453, 0.2547], device='cuda:0')
percent tensor([0.6320, 0.3680], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 197 | Batch_idx: 0 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (7557/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (8799/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (10038/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (11271/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (12509/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (13754/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (15006/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (16247/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (17489/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (18732/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (19980/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (21216/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (22457/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (23694/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (24940/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (26183/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (27410/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (28647/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (29896/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (31137/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (32374/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (33611/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (34862/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (36108/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (37334/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (38586/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (39833/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (41066/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (42305/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (43549/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (44788/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (46023/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (47263/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (48465/50000)
# TEST : Loss: (0.4159) | Acc: (88.00%) (8855/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.4687, 0.5313], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.5909, 0.4091], device='cuda:0')
percent tensor([0.6192, 0.3808], device='cuda:0')
percent tensor([0.7429, 0.2571], device='cuda:0')
percent tensor([0.6450, 0.3550], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (3861/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (5105/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (7607/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (10100/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (11338/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (12584/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (13830/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (15073/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (16318/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (17566/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (18809/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (20063/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (21314/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (22553/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (23777/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (25026/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (26259/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (27511/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (28755/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (30002/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (31235/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (32475/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (33719/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (34952/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (36205/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (37448/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (38693/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (39935/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (41174/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (42421/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (43659/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (44894/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (46130/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (47378/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (48577/50000)
# TEST : Loss: (0.4116) | Acc: (88.00%) (8876/10000)
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.4696, 0.5304], device='cuda:0')
percent tensor([0.6116, 0.3884], device='cuda:0')
percent tensor([0.5906, 0.4094], device='cuda:0')
percent tensor([0.6197, 0.3803], device='cuda:0')
percent tensor([0.7413, 0.2587], device='cuda:0')
percent tensor([0.6510, 0.3490], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 199 | Batch_idx: 0 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (3854/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (5106/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (6356/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (7607/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (10092/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (11342/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (12586/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (13838/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (15088/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (16338/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (17590/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (18834/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (20082/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (21331/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (22581/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (23824/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (25070/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (26311/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (27554/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (28799/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (30046/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (31287/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (32535/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (33782/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (35028/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (36280/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (37532/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (38781/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (40036/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (41281/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (42535/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (43783/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (45034/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (46275/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (47531/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (48735/50000)
# TEST : Loss: (0.4086) | Acc: (88.00%) (8857/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4691, 0.5309], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.5933, 0.4067], device='cuda:0')
percent tensor([0.6196, 0.3804], device='cuda:0')
percent tensor([0.7376, 0.2624], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (3865/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (6343/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (7591/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (8830/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (10072/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (11330/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (12559/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (13809/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (15055/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (16297/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (17550/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (18798/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (20049/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (21296/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (22543/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (23777/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (25030/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (26276/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (27523/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (28768/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (30006/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (31256/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (32498/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (33735/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (34983/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (36227/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (37472/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (38716/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (39959/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (41193/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (42435/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (43678/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (44920/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (46167/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (47410/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (48607/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_200.pth.tar'
# TEST : Loss: (0.4302) | Acc: (88.00%) (8853/10000)
percent tensor([0.5195, 0.4805], device='cuda:0')
percent tensor([0.4691, 0.5309], device='cuda:0')
percent tensor([0.6000, 0.4000], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6199, 0.3801], device='cuda:0')
percent tensor([0.7384, 0.2616], device='cuda:0')
percent tensor([0.6798, 0.3202], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.4328, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.8209, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.2860, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1502.6399, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(486.2762, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2286.0720, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4250.7788, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1347.8218, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6316.3516, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11562.6709, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3786.7288, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16014.8027, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (5125/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (6381/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (7640/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (8873/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (10119/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (11360/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (12610/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (13853/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (15103/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (16342/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (17597/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (18846/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (20091/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (21342/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (22583/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (23827/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (25065/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (26310/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (27555/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (28801/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (30047/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (31287/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (32528/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (33782/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (35023/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (36267/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (37515/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (38762/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (40010/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (41254/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (42496/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (43730/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (44970/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (46207/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (47452/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (48658/50000)
# TEST : Loss: (0.4484) | Acc: (88.00%) (8838/10000)
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.4707, 0.5293], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6163, 0.3837], device='cuda:0')
percent tensor([0.6187, 0.3813], device='cuda:0')
percent tensor([0.7429, 0.2571], device='cuda:0')
percent tensor([0.6820, 0.3180], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (96.00%) (5085/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (6338/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (7587/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (8837/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (11348/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (12595/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (13849/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (15093/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (16344/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (17587/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (18838/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (20083/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (21328/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (22567/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (23807/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (25053/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (26287/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (27538/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (28793/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (30037/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (31277/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (32526/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (33782/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (35023/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (36263/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (37511/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (38759/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (40009/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (41249/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (42487/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (43730/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (44978/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (46221/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (47469/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (48668/50000)
# TEST : Loss: (0.4232) | Acc: (88.00%) (8846/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.4708, 0.5292], device='cuda:0')
percent tensor([0.5961, 0.4039], device='cuda:0')
percent tensor([0.6222, 0.3778], device='cuda:0')
percent tensor([0.6206, 0.3794], device='cuda:0')
percent tensor([0.7434, 0.2566], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 203 | Batch_idx: 0 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (3885/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (5134/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (6389/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (7640/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (8886/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (10130/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (11386/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (12637/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (13890/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (15125/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (16384/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (17633/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (18877/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (20128/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (21375/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (22631/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (23878/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (25122/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (26357/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (27598/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (28842/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (30087/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (31333/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (32576/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (33816/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (35072/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (36322/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (37566/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (38795/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (40044/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (41274/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (42521/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (43775/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (45026/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (46271/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (47514/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (48720/50000)
# TEST : Loss: (0.4321) | Acc: (88.00%) (8875/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4706, 0.5294], device='cuda:0')
percent tensor([0.5931, 0.4069], device='cuda:0')
percent tensor([0.6252, 0.3748], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.7448, 0.2552], device='cuda:0')
percent tensor([0.6914, 0.3086], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 204 | Batch_idx: 0 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (3875/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (5121/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (6382/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (7628/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (8873/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (10127/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (11375/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (12615/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (13861/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (15105/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (16355/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (17605/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (18842/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (20081/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (21333/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (22580/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (23816/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (25068/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (26312/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (27558/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (28802/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (30048/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (31293/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (32543/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (33779/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (35015/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (36258/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (37504/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (38741/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (39990/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (41232/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (42463/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (43708/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (44947/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (46200/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (47441/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (48645/50000)
# TEST : Loss: (0.4399) | Acc: (88.00%) (8860/10000)
percent tensor([0.5208, 0.4792], device='cuda:0')
percent tensor([0.4715, 0.5285], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.6257, 0.3743], device='cuda:0')
percent tensor([0.6226, 0.3774], device='cuda:0')
percent tensor([0.7435, 0.2565], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 205 | Batch_idx: 0 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (2621/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (5120/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (6374/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (7630/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (8885/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (10140/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (11383/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (12634/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (13887/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (15135/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (16393/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (17641/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (18891/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (20136/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (21390/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (22647/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (23897/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (25149/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (26394/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (27637/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (28890/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (30135/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (31387/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (32630/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (33884/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (35133/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (36380/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (37630/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (38882/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (40141/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (41378/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (42621/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (43871/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (45114/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (46364/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (47610/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (48804/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_205.pth.tar'
# TEST : Loss: (0.4251) | Acc: (88.00%) (8835/10000)
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.4711, 0.5289], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.7470, 0.2530], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (5127/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (6387/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (7643/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (8891/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (10147/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (11398/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (12656/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (13904/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (15150/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (16401/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (17649/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (18892/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (20148/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (21405/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (22657/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (23892/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (25119/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (26373/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (27614/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (28861/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (30108/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (31349/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (32601/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (33845/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (35096/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (36343/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (37593/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (38831/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (40078/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (41324/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (42576/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (43812/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (45056/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (46298/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (47545/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (48750/50000)
# TEST : Loss: (0.4498) | Acc: (88.00%) (8810/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.4711, 0.5289], device='cuda:0')
percent tensor([0.5979, 0.4021], device='cuda:0')
percent tensor([0.6318, 0.3682], device='cuda:0')
percent tensor([0.6287, 0.3713], device='cuda:0')
percent tensor([0.7456, 0.2544], device='cuda:0')
percent tensor([0.7010, 0.2990], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 207 | Batch_idx: 0 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (5142/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (6397/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (7651/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (8906/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (10161/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (11410/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (12658/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (13902/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (15155/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (16408/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (17664/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (18917/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (20160/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (21412/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (22653/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (23901/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (25150/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (26401/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (27649/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (28890/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (30128/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (31377/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (32616/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (33868/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (35112/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (36357/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (37619/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (38866/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (40118/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (41368/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (42625/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (43867/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (45118/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (46366/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (47612/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (48814/50000)
# TEST : Loss: (0.4297) | Acc: (88.00%) (8867/10000)
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.4719, 0.5281], device='cuda:0')
percent tensor([0.5947, 0.4053], device='cuda:0')
percent tensor([0.6359, 0.3641], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.7476, 0.2524], device='cuda:0')
percent tensor([0.7004, 0.2996], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (2625/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (5118/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (6363/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (7613/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (8861/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (10115/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (11367/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (12621/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (13861/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (15105/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (16352/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (17604/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (18839/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (20095/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (21345/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (22600/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (23855/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (25106/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (26356/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (27606/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (28858/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (30100/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (31359/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (32599/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (33849/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (35090/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (36342/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (37590/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (38835/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (40098/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (41357/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (42601/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (43849/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (45095/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (46339/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (47582/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (48780/50000)
# TEST : Loss: (0.4752) | Acc: (87.00%) (8770/10000)
percent tensor([0.5250, 0.4750], device='cuda:0')
percent tensor([0.4727, 0.5273], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.6355, 0.3645], device='cuda:0')
percent tensor([0.6274, 0.3726], device='cuda:0')
percent tensor([0.7489, 0.2511], device='cuda:0')
percent tensor([0.6995, 0.3005], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (2624/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (6366/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (7620/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (8880/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (10134/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (11391/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (12642/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (13891/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (15150/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (16397/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (17654/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (18901/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (20146/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (21390/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (22646/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (23895/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (25144/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (26406/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (27663/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (28921/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (30171/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (31418/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (32670/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (33926/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (35181/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (36431/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (37678/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (38929/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (40172/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (41431/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (42676/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (43927/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (45179/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (46432/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (47687/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (48886/50000)
# TEST : Loss: (0.4194) | Acc: (89.00%) (8917/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.4736, 0.5264], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.6264, 0.3736], device='cuda:0')
percent tensor([0.7543, 0.2457], device='cuda:0')
percent tensor([0.7038, 0.2962], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 210 | Batch_idx: 0 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (2635/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (5134/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (6390/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (7647/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (8901/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (10159/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (11411/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (12666/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (13913/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (15169/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (16420/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (17681/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (18932/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (20181/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (21432/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (22696/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (23948/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (25193/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (26441/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (27691/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (28943/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (30189/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (31440/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (32686/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (33941/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (35187/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (36431/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (37682/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (38930/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (40178/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (41426/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (42670/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (43917/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (45166/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (46418/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (47669/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (48870/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_210.pth.tar'
# TEST : Loss: (0.4627) | Acc: (88.00%) (8817/10000)
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.4731, 0.5269], device='cuda:0')
percent tensor([0.5987, 0.4013], device='cuda:0')
percent tensor([0.6394, 0.3606], device='cuda:0')
percent tensor([0.6259, 0.3741], device='cuda:0')
percent tensor([0.7524, 0.2476], device='cuda:0')
percent tensor([0.7058, 0.2942], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.4086, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.3173, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.4787, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.1067, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(484.7444, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2299.8132, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4258.7817, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1343.1914, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6358.8423, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11540.3740, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3772.1362, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15950.0977, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (6385/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (7646/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (8900/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (10147/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (11404/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (12661/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (13917/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (15171/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (16423/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (17671/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (18927/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (20179/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (21428/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (22676/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (23920/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (25168/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (26421/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (27671/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (28916/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (30162/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (31418/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (32665/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (33915/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (35161/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36407/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (37654/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (38901/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (40158/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (41410/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (42671/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (43927/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (45181/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (46435/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (47679/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (48887/50000)
# TEST : Loss: (0.4398) | Acc: (88.00%) (8879/10000)
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.4741, 0.5259], device='cuda:0')
percent tensor([0.5993, 0.4007], device='cuda:0')
percent tensor([0.6401, 0.3599], device='cuda:0')
percent tensor([0.6267, 0.3733], device='cuda:0')
percent tensor([0.7526, 0.2474], device='cuda:0')
percent tensor([0.7067, 0.2933], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (3886/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (6396/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (7653/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (8916/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (10166/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (11424/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (12676/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (98.00%) (13930/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (15180/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (16436/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (17692/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (18940/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (20195/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (21449/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (22707/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (23953/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (25210/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (26465/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (27715/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (28962/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (30206/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (31460/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (32700/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (33958/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (35210/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (36459/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (37720/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (38969/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (40209/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (41460/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (42704/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (43949/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (45200/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (46456/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (47704/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (48895/50000)
# TEST : Loss: (0.5300) | Acc: (87.00%) (8724/10000)
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.4745, 0.5255], device='cuda:0')
percent tensor([0.5972, 0.4028], device='cuda:0')
percent tensor([0.6377, 0.3623], device='cuda:0')
percent tensor([0.6294, 0.3706], device='cuda:0')
percent tensor([0.7553, 0.2447], device='cuda:0')
percent tensor([0.7022, 0.2978], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (6392/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (7642/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (8899/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (10156/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (11402/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (12659/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (13921/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (15181/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (16438/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (17695/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (18958/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (20221/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (21485/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (22742/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (23998/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (25257/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (26507/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (27757/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (29022/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (30279/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (31528/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (32782/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (34037/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (35298/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (36558/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (37815/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (39059/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (40306/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (41560/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (42816/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (44067/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (45321/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (46582/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (47832/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (49035/50000)
# TEST : Loss: (0.5458) | Acc: (86.00%) (8646/10000)
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.4750, 0.5250], device='cuda:0')
percent tensor([0.5970, 0.4030], device='cuda:0')
percent tensor([0.6347, 0.3653], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.7574, 0.2426], device='cuda:0')
percent tensor([0.7027, 0.2973], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (5160/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (6418/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (7670/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (8924/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (10182/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (11443/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (12701/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (13955/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (15207/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (16465/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (17723/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (18982/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (20232/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (21485/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (22738/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (23988/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (25235/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (26485/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (98.00%) (27731/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (28985/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (98.00%) (30245/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (31502/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (32760/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (34021/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (35279/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (36531/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (37783/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (39039/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (40297/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (41554/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (42814/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (44065/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (45316/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (46569/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (47821/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (49020/50000)
# TEST : Loss: (0.4288) | Acc: (89.00%) (8903/10000)
percent tensor([0.5267, 0.4733], device='cuda:0')
percent tensor([0.4754, 0.5246], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.6419, 0.3581], device='cuda:0')
percent tensor([0.6268, 0.3732], device='cuda:0')
percent tensor([0.7583, 0.2417], device='cuda:0')
percent tensor([0.7118, 0.2882], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0216) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (5154/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (6405/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (7662/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (8922/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (10179/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (11436/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (12699/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (13964/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (15228/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (16489/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (17750/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (19008/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (20263/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (21505/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (22766/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (24010/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (25256/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (26503/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (27754/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (29014/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (30258/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (31519/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (32778/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (34036/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (35301/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (36563/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (37825/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (39088/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (40345/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (41593/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (42845/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (44094/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (45345/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (46596/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (47846/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (49048/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_215.pth.tar'
# TEST : Loss: (0.4414) | Acc: (88.00%) (8865/10000)
percent tensor([0.5282, 0.4718], device='cuda:0')
percent tensor([0.4753, 0.5247], device='cuda:0')
percent tensor([0.6033, 0.3967], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.7623, 0.2377], device='cuda:0')
percent tensor([0.7104, 0.2896], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (7661/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (8915/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (10169/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (11428/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (12688/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (13955/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (15206/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (16466/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (17721/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (18974/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (20221/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (21478/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (22729/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (23979/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (25231/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (26480/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (27738/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (28986/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (30242/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (31492/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (32752/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (34009/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (35266/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (36523/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (37788/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (39041/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (40291/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (41549/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (42788/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (44043/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (45292/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (46550/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (47811/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (49026/50000)
# TEST : Loss: (0.4117) | Acc: (89.00%) (8943/10000)
percent tensor([0.5277, 0.4723], device='cuda:0')
percent tensor([0.4764, 0.5236], device='cuda:0')
percent tensor([0.6037, 0.3963], device='cuda:0')
percent tensor([0.6410, 0.3590], device='cuda:0')
percent tensor([0.6320, 0.3680], device='cuda:0')
percent tensor([0.7648, 0.2352], device='cuda:0')
percent tensor([0.7097, 0.2903], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (3912/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (5172/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (6429/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (7699/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (8953/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (10212/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (11479/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (12735/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (13995/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (15248/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (16508/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (17763/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (19016/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (20270/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (21524/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (22784/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (24044/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (25290/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (26544/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (27798/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (29049/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (30304/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (31565/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (32819/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (34078/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (35340/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (36595/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (37838/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (39097/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (40356/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (41615/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (42868/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (44110/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (45365/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (46612/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (47867/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (49069/50000)
# TEST : Loss: (0.4744) | Acc: (88.00%) (8807/10000)
percent tensor([0.5274, 0.4726], device='cuda:0')
percent tensor([0.4773, 0.5227], device='cuda:0')
percent tensor([0.6031, 0.3969], device='cuda:0')
percent tensor([0.6435, 0.3565], device='cuda:0')
percent tensor([0.6332, 0.3668], device='cuda:0')
percent tensor([0.7653, 0.2347], device='cuda:0')
percent tensor([0.7139, 0.2861], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 218 | Batch_idx: 0 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (2645/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (3893/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (5151/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (6397/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (97.00%) (7648/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (8883/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (10141/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (97.00%) (11399/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (12659/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (97.00%) (13922/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (97.00%) (15175/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (16433/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (17691/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (18944/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (20204/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (21461/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (22720/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (23984/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (25237/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (26496/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (27757/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (29014/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (30274/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (31538/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (32804/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (34058/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (35319/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (36575/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (37830/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (39092/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (40343/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (41606/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (42859/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (44117/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (45374/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (46638/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (47900/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (49112/50000)
# TEST : Loss: (0.4348) | Acc: (89.00%) (8902/10000)
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.4774, 0.5226], device='cuda:0')
percent tensor([0.6013, 0.3987], device='cuda:0')
percent tensor([0.6426, 0.3574], device='cuda:0')
percent tensor([0.6354, 0.3646], device='cuda:0')
percent tensor([0.7664, 0.2336], device='cuda:0')
percent tensor([0.7149, 0.2851], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (3884/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (5146/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (6405/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (7667/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (8930/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (10196/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (12713/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (13975/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (15235/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (16496/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (17756/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (19017/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (20273/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (21531/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (22787/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (24052/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (25314/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (26572/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (27837/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (29094/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (30360/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (31623/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (32887/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (34150/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (35408/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (36667/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (37919/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (39174/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (40436/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (41686/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (42946/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (44205/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (45461/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (46713/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (47966/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (49177/50000)
# TEST : Loss: (0.4564) | Acc: (88.00%) (8866/10000)
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.4775, 0.5225], device='cuda:0')
percent tensor([0.6024, 0.3976], device='cuda:0')
percent tensor([0.6440, 0.3560], device='cuda:0')
percent tensor([0.6350, 0.3650], device='cuda:0')
percent tensor([0.7676, 0.2324], device='cuda:0')
percent tensor([0.7119, 0.2881], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (3895/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (5155/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (7668/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (8927/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (11448/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (12706/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (13962/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (15214/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (16473/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (17733/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (18989/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (20245/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (21514/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (22769/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (24035/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (25294/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (26549/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (27809/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (29069/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (30333/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (31593/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (32849/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (34106/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (35360/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (36618/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (37876/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (39128/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (40389/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (41650/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (42913/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (44179/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (45439/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (46701/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (47951/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (49151/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_220.pth.tar'
# TEST : Loss: (0.4650) | Acc: (87.00%) (8794/10000)
percent tensor([0.5300, 0.4700], device='cuda:0')
percent tensor([0.4776, 0.5224], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.6461, 0.3539], device='cuda:0')
percent tensor([0.6385, 0.3615], device='cuda:0')
percent tensor([0.7697, 0.2303], device='cuda:0')
percent tensor([0.7180, 0.2820], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.0441, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.6334, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.6034, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.4257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(483.0931, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2310.1475, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4263.8608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1338.5765, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6392.6333, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11515.5049, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3757.6960, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15885.4961, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (3899/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (5155/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (7667/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (8931/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (10186/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (11443/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (12703/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (13963/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (15222/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (16467/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (17717/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (18969/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (20230/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (21494/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (22753/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (24007/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (25277/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (26537/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (27802/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (29059/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (30315/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (31576/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (32830/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (34087/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (35342/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (36600/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (37856/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (39116/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (40369/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (41629/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (42889/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (44146/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (45410/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (46662/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (47920/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (49127/50000)
# TEST : Loss: (0.4559) | Acc: (88.00%) (8850/10000)
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.4785, 0.5215], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.6453, 0.3547], device='cuda:0')
percent tensor([0.6322, 0.3678], device='cuda:0')
percent tensor([0.7698, 0.2302], device='cuda:0')
percent tensor([0.7214, 0.2786], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0299) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (3910/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (5171/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (6434/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (7693/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (8958/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (10214/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (11476/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (12732/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (13993/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (15255/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (16512/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (17767/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (19030/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (20285/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (21539/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (22792/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (24053/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (25301/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (26554/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (27817/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (29076/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (30339/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (31603/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (32861/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (34122/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (35384/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (36637/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (37898/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (39159/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (40422/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (41684/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (42940/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (44198/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (45454/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (46711/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (47973/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (49189/50000)
# TEST : Loss: (0.4635) | Acc: (88.00%) (8834/10000)
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.4775, 0.5225], device='cuda:0')
percent tensor([0.6005, 0.3995], device='cuda:0')
percent tensor([0.6486, 0.3514], device='cuda:0')
percent tensor([0.6344, 0.3656], device='cuda:0')
percent tensor([0.7702, 0.2298], device='cuda:0')
percent tensor([0.7131, 0.2869], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (97.00%) (3884/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (5153/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (6411/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (7671/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (8929/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (10192/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (12717/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (13981/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (15236/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (16488/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (17748/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (19013/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (20275/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (21533/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (22793/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (24052/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (25319/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (26580/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (27832/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (29092/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (30344/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (31605/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (32871/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (34126/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (35384/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (36651/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (37912/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (39171/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (40434/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (41686/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (42946/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (44198/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (45446/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (46699/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (47956/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (49163/50000)
# TEST : Loss: (0.4489) | Acc: (88.00%) (8882/10000)
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.4782, 0.5218], device='cuda:0')
percent tensor([0.6033, 0.3967], device='cuda:0')
percent tensor([0.6520, 0.3480], device='cuda:0')
percent tensor([0.6349, 0.3651], device='cuda:0')
percent tensor([0.7665, 0.2335], device='cuda:0')
percent tensor([0.7041, 0.2959], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (99.00%) (1396/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (2657/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (3926/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (5193/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (7715/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (8969/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (10227/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (11486/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (12749/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (14009/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (15267/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (16524/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (17786/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (19047/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (20298/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (21560/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (22817/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (24073/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (25327/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (26590/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (27854/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (29122/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (30383/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (31644/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (32898/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (34153/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (35419/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (36683/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (37944/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (39199/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (40456/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (41708/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (42966/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (44224/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (45481/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (46733/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (47990/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (49198/50000)
# TEST : Loss: (0.4406) | Acc: (88.00%) (8897/10000)
percent tensor([0.5316, 0.4684], device='cuda:0')
percent tensor([0.4792, 0.5208], device='cuda:0')
percent tensor([0.6054, 0.3946], device='cuda:0')
percent tensor([0.6520, 0.3480], device='cuda:0')
percent tensor([0.6396, 0.3604], device='cuda:0')
percent tensor([0.7675, 0.2325], device='cuda:0')
percent tensor([0.7073, 0.2927], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (3891/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (6423/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (7674/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (8937/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (10192/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (12720/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (13985/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (15247/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (16507/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (17764/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (19024/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (20288/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (21557/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (22814/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (24078/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (25342/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (26600/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (27858/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (29117/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (30382/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (31645/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (32910/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (34179/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (35443/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (36698/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (37960/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (39227/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (40491/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (41748/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (43005/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (44260/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (45518/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (46782/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (48049/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (49265/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_225.pth.tar'
# TEST : Loss: (0.4258) | Acc: (89.00%) (8944/10000)
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.4796, 0.5204], device='cuda:0')
percent tensor([0.6031, 0.3969], device='cuda:0')
percent tensor([0.6524, 0.3476], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.7714, 0.2286], device='cuda:0')
percent tensor([0.7095, 0.2905], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0229) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (99.00%) (1398/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (99.00%) (2666/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (99.00%) (3930/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (99.00%) (5197/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (6460/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (7724/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (8985/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (10240/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (11502/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (12759/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (14023/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (15287/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (16543/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (17805/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (19069/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (20327/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (21585/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (22853/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (24111/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (25377/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (26640/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (27902/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (29168/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (30427/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (31691/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (32956/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (34215/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (35478/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (36737/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (38001/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (39255/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (40516/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (41777/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (43032/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (44298/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (45555/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (46805/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (48058/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (49274/50000)
# TEST : Loss: (0.4680) | Acc: (88.00%) (8841/10000)
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4795, 0.5205], device='cuda:0')
percent tensor([0.6043, 0.3957], device='cuda:0')
percent tensor([0.6496, 0.3504], device='cuda:0')
percent tensor([0.6344, 0.3656], device='cuda:0')
percent tensor([0.7744, 0.2256], device='cuda:0')
percent tensor([0.7159, 0.2841], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0239) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (2659/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (3920/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (5184/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (7719/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (8974/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (10228/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (11490/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (12756/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (14022/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (15291/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (16550/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (17809/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (19077/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (20340/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (21600/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (22859/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (24114/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (25373/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (26644/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (27905/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (29163/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (30430/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (31682/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (32946/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (34209/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (35469/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (36723/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (37982/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (39244/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (40503/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (41758/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (43012/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (44267/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (45521/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (46783/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (48045/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (49248/50000)
# TEST : Loss: (0.4969) | Acc: (88.00%) (8817/10000)
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.4800, 0.5200], device='cuda:0')
percent tensor([0.6037, 0.3963], device='cuda:0')
percent tensor([0.6516, 0.3484], device='cuda:0')
percent tensor([0.6352, 0.3648], device='cuda:0')
percent tensor([0.7706, 0.2294], device='cuda:0')
percent tensor([0.7154, 0.2846], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (3909/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (5163/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (6429/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (7688/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (8948/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (10214/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (11479/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (12734/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (13990/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (15247/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (16510/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (17766/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (19031/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (20299/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (21557/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (22818/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (24082/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (25346/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (26607/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (27876/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (29138/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (30399/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (31668/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (32929/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (34192/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (35444/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (36703/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (37961/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (39224/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (40493/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (41753/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (43017/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (44275/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (45537/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (46793/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (48049/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (49269/50000)
# TEST : Loss: (0.4342) | Acc: (89.00%) (8925/10000)
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.4795, 0.5205], device='cuda:0')
percent tensor([0.6040, 0.3960], device='cuda:0')
percent tensor([0.6501, 0.3499], device='cuda:0')
percent tensor([0.6345, 0.3655], device='cuda:0')
percent tensor([0.7714, 0.2286], device='cuda:0')
percent tensor([0.7138, 0.2862], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (2658/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (3927/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (99.00%) (5196/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (6461/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (99.00%) (7730/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (8992/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (11516/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (12782/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (14053/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (15312/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (16573/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (17835/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (19094/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (20356/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (21619/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (22886/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (24147/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (25415/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (26679/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (27947/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (29217/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (30469/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (31732/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (32996/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (34257/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (35513/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (36769/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (38028/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (39273/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (40532/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (41792/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (43051/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (44315/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (45570/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (46832/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (48089/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (49304/50000)
# TEST : Loss: (0.4398) | Acc: (89.00%) (8914/10000)
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.4795, 0.5205], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6512, 0.3488], device='cuda:0')
percent tensor([0.6384, 0.3616], device='cuda:0')
percent tensor([0.7734, 0.2266], device='cuda:0')
percent tensor([0.7172, 0.2828], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (3911/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (5178/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (6441/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (7700/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (8958/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (10226/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (11488/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (12754/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (14013/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (15267/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (16528/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (17784/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (19049/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (20307/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (21569/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (22821/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (24081/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (25345/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (26612/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (27878/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (29135/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (30401/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (31662/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (32923/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (34186/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (35446/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (36712/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (37973/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (39223/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (40489/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (41753/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (43013/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (44277/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (45537/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (46797/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (48056/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (49270/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_230.pth.tar'
# TEST : Loss: (0.4633) | Acc: (88.00%) (8890/10000)
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.4798, 0.5202], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6535, 0.3465], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.7740, 0.2260], device='cuda:0')
percent tensor([0.7171, 0.2829], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.7119, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.2078, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.5873, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.6234, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(481.4304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2319.0955, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.9624, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1333.7920, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6421.0283, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11489.2070, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3743.1855, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15821.6113, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (3916/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (5176/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (6439/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (7696/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (8958/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (10221/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (11483/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (12742/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (14004/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (15271/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (16529/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (17794/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (19054/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (20319/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (21584/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (22837/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (24103/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (25360/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (26622/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (27886/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (29140/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (30403/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (31664/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (32922/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (34185/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (35450/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (36706/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (37966/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (39230/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (40488/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (41755/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (43015/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (44275/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (45537/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (46799/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (48058/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (49269/50000)
# TEST : Loss: (0.4659) | Acc: (89.00%) (8905/10000)
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.4796, 0.5204], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.6560, 0.3440], device='cuda:0')
percent tensor([0.6398, 0.3602], device='cuda:0')
percent tensor([0.7773, 0.2227], device='cuda:0')
percent tensor([0.7173, 0.2827], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (2650/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (3914/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (5176/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (6437/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (7701/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (8963/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (10227/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (11489/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (12751/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (14014/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (15277/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (16539/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (17797/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (19066/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (20328/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (21589/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (22854/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (24122/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (25388/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (26649/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (27912/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (29179/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (30434/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (31692/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (32956/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (34215/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (35472/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (36736/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (38000/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (39256/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (40517/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (41776/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (43039/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (44302/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (45568/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (46831/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (48096/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (49312/50000)
# TEST : Loss: (0.4583) | Acc: (88.00%) (8893/10000)
percent tensor([0.5346, 0.4654], device='cuda:0')
percent tensor([0.4797, 0.5203], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6411, 0.3589], device='cuda:0')
percent tensor([0.7813, 0.2187], device='cuda:0')
percent tensor([0.7192, 0.2808], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0199) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (6431/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (7694/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (8957/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (10222/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (11485/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (12749/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (14020/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (16548/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (17810/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (19062/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (20326/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (21589/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (22848/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (24109/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (25378/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (26639/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (27901/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (29162/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (30429/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (31688/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (32949/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (34217/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (35483/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (36749/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (38010/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (39276/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (40539/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (41794/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (43056/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (44308/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (45560/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (46818/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (48078/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (49294/50000)
# TEST : Loss: (0.4667) | Acc: (88.00%) (8890/10000)
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.4801, 0.5199], device='cuda:0')
percent tensor([0.6062, 0.3938], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.7786, 0.2214], device='cuda:0')
percent tensor([0.7212, 0.2788], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (99.00%) (2665/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (99.00%) (3935/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (5193/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (6456/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (7726/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (8992/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (10261/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (11529/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (12789/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (14055/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (15322/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (16584/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (17846/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (19109/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (20370/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (21637/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (22899/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (24154/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (25412/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (26670/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (27927/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (29183/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (30438/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (31701/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (32964/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (34220/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (35470/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (36734/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (38004/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (39277/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (40542/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (41805/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (43074/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (44342/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (45606/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (46870/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (48139/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (49358/50000)
# TEST : Loss: (0.4532) | Acc: (89.00%) (8928/10000)
percent tensor([0.5352, 0.4648], device='cuda:0')
percent tensor([0.4803, 0.5197], device='cuda:0')
percent tensor([0.6102, 0.3898], device='cuda:0')
percent tensor([0.6567, 0.3433], device='cuda:0')
percent tensor([0.6374, 0.3626], device='cuda:0')
percent tensor([0.7761, 0.2239], device='cuda:0')
percent tensor([0.7218, 0.2782], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0204) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (2660/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (3924/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (5185/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (6453/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (7715/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (8981/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (10244/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (11504/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (12768/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (14030/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (15292/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (16559/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (17822/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (19084/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (20348/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (21605/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (22863/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (24129/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (25393/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (26660/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (27924/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (29185/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (30452/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (31708/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (32970/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (34234/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (35503/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (36760/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (38015/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (39281/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (40546/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (41810/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (43075/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (44336/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (45599/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (46861/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (48115/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (49334/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_235.pth.tar'
# TEST : Loss: (0.4727) | Acc: (88.00%) (8855/10000)
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.6102, 0.3898], device='cuda:0')
percent tensor([0.6590, 0.3410], device='cuda:0')
percent tensor([0.6404, 0.3596], device='cuda:0')
percent tensor([0.7786, 0.2214], device='cuda:0')
percent tensor([0.7197, 0.2803], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (2655/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (5187/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (6455/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (7717/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (8984/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (10249/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (11514/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (12777/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (14042/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (15302/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (16565/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (17825/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (19087/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (20352/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (21618/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (22879/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (24147/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (25414/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (26677/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (27940/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (29197/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (30458/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (31719/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (32988/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (34249/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (35517/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (36774/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (38037/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (39299/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (40558/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (41817/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (43085/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (44352/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (45614/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (46882/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (48143/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (49364/50000)
# TEST : Loss: (0.4594) | Acc: (88.00%) (8858/10000)
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.4810, 0.5190], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6565, 0.3435], device='cuda:0')
percent tensor([0.6382, 0.3618], device='cuda:0')
percent tensor([0.7792, 0.2208], device='cuda:0')
percent tensor([0.7215, 0.2785], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0321) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (3916/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (5171/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (6432/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (7700/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (8967/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (10229/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (11496/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (12762/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (14022/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (15296/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (16566/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (17832/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (19095/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (20364/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (21628/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (22893/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (24153/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (25422/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (26686/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (27956/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (29222/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (30491/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (31753/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (33017/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (34282/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (35540/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (36803/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (38065/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (39326/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (40589/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (41856/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (43117/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (44380/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (45648/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (46916/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (48176/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (49389/50000)
# TEST : Loss: (0.4873) | Acc: (88.00%) (8878/10000)
percent tensor([0.5352, 0.4648], device='cuda:0')
percent tensor([0.4806, 0.5194], device='cuda:0')
percent tensor([0.6073, 0.3927], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.6467, 0.3533], device='cuda:0')
percent tensor([0.7784, 0.2216], device='cuda:0')
percent tensor([0.7185, 0.2815], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0216) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (6427/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (7692/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (8950/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (10216/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (11486/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (12751/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (14013/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (15275/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (16546/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (17807/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (19069/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (20332/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (21600/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (22864/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (24130/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (25397/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (26654/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (27923/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (29185/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (30451/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (31716/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (32983/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (34239/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (35506/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (36769/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (38032/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (39294/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (40553/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (41817/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (43085/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (44348/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (45613/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (46883/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (48141/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (49358/50000)
# TEST : Loss: (0.4759) | Acc: (88.00%) (8898/10000)
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.4802, 0.5198], device='cuda:0')
percent tensor([0.6085, 0.3915], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.6439, 0.3561], device='cuda:0')
percent tensor([0.7799, 0.2201], device='cuda:0')
percent tensor([0.7157, 0.2843], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (2661/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (5183/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (6449/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (7721/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (8987/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (10247/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (11511/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (12773/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (14035/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (15302/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (16574/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (17834/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (19097/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (20365/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (21633/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (22901/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (24165/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (25436/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (26702/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (27966/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (29235/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (30499/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (31769/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (33034/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (34298/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (35560/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (36820/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (38087/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (39350/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (40617/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (41875/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (43135/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (44388/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (45651/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (46915/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (48180/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (49395/50000)
# TEST : Loss: (0.4971) | Acc: (88.00%) (8839/10000)
percent tensor([0.5369, 0.4631], device='cuda:0')
percent tensor([0.4801, 0.5199], device='cuda:0')
percent tensor([0.6098, 0.3902], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.6445, 0.3555], device='cuda:0')
percent tensor([0.7816, 0.2184], device='cuda:0')
percent tensor([0.7178, 0.2822], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(188.0523, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.0920, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.6897, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1504.3336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(479.8828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2325.8691, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.3691, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1329.6527, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6442.8135, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11464.3838, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3730.2673, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15764.6045, device='cuda:0', grad_fn=<NormBackward0>)
2 hours 34 mins 26 secs for training