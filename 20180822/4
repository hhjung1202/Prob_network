Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.2948) |  Loss2: (0.0000) | Acc: (15.00%) (20/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.2963) |  Loss2: (0.0000) | Acc: (13.00%) (189/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2943) |  Loss2: (0.0000) | Acc: (12.00%) (340/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2862) |  Loss2: (0.0000) | Acc: (14.00%) (559/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2773) |  Loss2: (0.0000) | Acc: (14.00%) (781/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2694) |  Loss2: (0.0000) | Acc: (15.00%) (1008/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2618) |  Loss2: (0.0000) | Acc: (16.00%) (1265/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2515) |  Loss2: (0.0000) | Acc: (17.00%) (1565/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2429) |  Loss2: (0.0000) | Acc: (17.00%) (1841/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2351) |  Loss2: (0.0000) | Acc: (18.00%) (2126/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2271) |  Loss2: (0.0000) | Acc: (18.00%) (2427/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2185) |  Loss2: (0.0000) | Acc: (19.00%) (2714/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2099) |  Loss2: (0.0000) | Acc: (19.00%) (3034/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2014) |  Loss2: (0.0000) | Acc: (20.00%) (3359/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1927) |  Loss2: (0.0000) | Acc: (20.00%) (3679/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1842) |  Loss2: (0.0000) | Acc: (20.00%) (4017/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1758) |  Loss2: (0.0000) | Acc: (21.00%) (4360/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1675) |  Loss2: (0.0000) | Acc: (21.00%) (4710/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1593) |  Loss2: (0.0000) | Acc: (21.00%) (5038/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1518) |  Loss2: (0.0000) | Acc: (22.00%) (5379/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1436) |  Loss2: (0.0000) | Acc: (22.00%) (5760/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1355) |  Loss2: (0.0000) | Acc: (22.00%) (6121/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1279) |  Loss2: (0.0000) | Acc: (22.00%) (6490/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1201) |  Loss2: (0.0000) | Acc: (23.00%) (6856/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1140) |  Loss2: (0.0000) | Acc: (23.00%) (7228/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1066) |  Loss2: (0.0000) | Acc: (23.00%) (7596/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.0991) |  Loss2: (0.0000) | Acc: (23.00%) (7987/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0927) |  Loss2: (0.0000) | Acc: (24.00%) (8367/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0851) |  Loss2: (0.0000) | Acc: (24.00%) (8764/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0784) |  Loss2: (0.0000) | Acc: (24.00%) (9151/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0725) |  Loss2: (0.0000) | Acc: (24.00%) (9537/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0662) |  Loss2: (0.0000) | Acc: (24.00%) (9931/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0607) |  Loss2: (0.0000) | Acc: (25.00%) (10348/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0550) |  Loss2: (0.0000) | Acc: (25.00%) (10769/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0492) |  Loss2: (0.0000) | Acc: (25.00%) (11165/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0439) |  Loss2: (0.0000) | Acc: (25.00%) (11581/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0378) |  Loss2: (0.0000) | Acc: (26.00%) (12020/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0319) |  Loss2: (0.0000) | Acc: (26.00%) (12446/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0271) |  Loss2: (0.0000) | Acc: (26.00%) (12829/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0222) |  Loss2: (0.0000) | Acc: (26.00%) (13221/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.7821) | Acc: (34.00%) (3412/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(164.6626, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(767.8034, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.6410, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1537.7130, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.1446, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2176.4060, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4343.5093, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1452.4517, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6140.1162, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12264.0498, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4083.7295, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17370.3984, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.7577) |  Loss2: (0.0000) | Acc: (32.00%) (42/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.7919) |  Loss2: (0.0000) | Acc: (33.00%) (476/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8052) |  Loss2: (0.0000) | Acc: (33.00%) (911/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8104) |  Loss2: (0.0000) | Acc: (33.00%) (1341/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8064) |  Loss2: (0.0000) | Acc: (34.00%) (1794/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8073) |  Loss2: (0.0000) | Acc: (34.00%) (2237/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8022) |  Loss2: (0.0000) | Acc: (34.00%) (2713/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7962) |  Loss2: (0.0000) | Acc: (34.00%) (3173/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7946) |  Loss2: (0.0000) | Acc: (34.00%) (3620/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7927) |  Loss2: (0.0000) | Acc: (34.00%) (4040/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7886) |  Loss2: (0.0000) | Acc: (34.00%) (4511/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7848) |  Loss2: (0.0000) | Acc: (34.00%) (4956/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7795) |  Loss2: (0.0000) | Acc: (34.00%) (5411/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7766) |  Loss2: (0.0000) | Acc: (34.00%) (5868/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7734) |  Loss2: (0.0000) | Acc: (35.00%) (6325/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7703) |  Loss2: (0.0000) | Acc: (35.00%) (6796/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7699) |  Loss2: (0.0000) | Acc: (35.00%) (7238/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7677) |  Loss2: (0.0000) | Acc: (35.00%) (7677/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7648) |  Loss2: (0.0000) | Acc: (35.00%) (8152/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7602) |  Loss2: (0.0000) | Acc: (35.00%) (8632/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7572) |  Loss2: (0.0000) | Acc: (35.00%) (9126/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7540) |  Loss2: (0.0000) | Acc: (35.00%) (9635/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7518) |  Loss2: (0.0000) | Acc: (35.00%) (10115/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7484) |  Loss2: (0.0000) | Acc: (35.00%) (10609/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7455) |  Loss2: (0.0000) | Acc: (36.00%) (11111/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7433) |  Loss2: (0.0000) | Acc: (36.00%) (11583/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7400) |  Loss2: (0.0000) | Acc: (36.00%) (12086/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7366) |  Loss2: (0.0000) | Acc: (36.00%) (12579/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7336) |  Loss2: (0.0000) | Acc: (36.00%) (13088/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7305) |  Loss2: (0.0000) | Acc: (36.00%) (13608/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7267) |  Loss2: (0.0000) | Acc: (36.00%) (14162/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7228) |  Loss2: (0.0000) | Acc: (36.00%) (14691/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7207) |  Loss2: (0.0000) | Acc: (37.00%) (15214/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7177) |  Loss2: (0.0000) | Acc: (37.00%) (15717/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7156) |  Loss2: (0.0000) | Acc: (37.00%) (16237/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7129) |  Loss2: (0.0000) | Acc: (37.00%) (16744/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7096) |  Loss2: (0.0000) | Acc: (37.00%) (17272/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7071) |  Loss2: (0.0000) | Acc: (37.00%) (17773/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7035) |  Loss2: (0.0000) | Acc: (37.00%) (18316/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7016) |  Loss2: (0.0000) | Acc: (37.00%) (18805/50000)
# TEST : Loss: (1.5757) | Acc: (41.00%) (4149/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5705) |  Loss2: (0.0000) | Acc: (46.00%) (59/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5801) |  Loss2: (0.0000) | Acc: (43.00%) (610/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5811) |  Loss2: (0.0000) | Acc: (42.00%) (1139/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5727) |  Loss2: (0.0000) | Acc: (42.00%) (1693/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5748) |  Loss2: (0.0000) | Acc: (42.00%) (2244/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5789) |  Loss2: (0.0000) | Acc: (42.00%) (2783/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5788) |  Loss2: (0.0000) | Acc: (42.00%) (3313/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5784) |  Loss2: (0.0000) | Acc: (42.00%) (3836/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5720) |  Loss2: (0.0000) | Acc: (42.00%) (4392/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5686) |  Loss2: (0.0000) | Acc: (42.00%) (4957/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5674) |  Loss2: (0.0000) | Acc: (42.00%) (5488/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5672) |  Loss2: (0.0000) | Acc: (42.00%) (6035/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5670) |  Loss2: (0.0000) | Acc: (42.00%) (6584/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5666) |  Loss2: (0.0000) | Acc: (42.00%) (7122/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5653) |  Loss2: (0.0000) | Acc: (42.00%) (7682/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5605) |  Loss2: (0.0000) | Acc: (42.00%) (8244/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5563) |  Loss2: (0.0000) | Acc: (42.00%) (8830/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5534) |  Loss2: (0.0000) | Acc: (42.00%) (9397/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5538) |  Loss2: (0.0000) | Acc: (42.00%) (9935/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5517) |  Loss2: (0.0000) | Acc: (42.00%) (10498/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5495) |  Loss2: (0.0000) | Acc: (43.00%) (11078/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5481) |  Loss2: (0.0000) | Acc: (43.00%) (11646/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5458) |  Loss2: (0.0000) | Acc: (43.00%) (12226/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5442) |  Loss2: (0.0000) | Acc: (43.00%) (12785/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5405) |  Loss2: (0.0000) | Acc: (43.00%) (13404/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5384) |  Loss2: (0.0000) | Acc: (43.00%) (13997/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5363) |  Loss2: (0.0000) | Acc: (43.00%) (14559/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5347) |  Loss2: (0.0000) | Acc: (43.00%) (15146/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5318) |  Loss2: (0.0000) | Acc: (43.00%) (15755/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5292) |  Loss2: (0.0000) | Acc: (43.00%) (16366/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5269) |  Loss2: (0.0000) | Acc: (44.00%) (16978/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5252) |  Loss2: (0.0000) | Acc: (44.00%) (17547/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5242) |  Loss2: (0.0000) | Acc: (44.00%) (18139/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5216) |  Loss2: (0.0000) | Acc: (44.00%) (18762/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5189) |  Loss2: (0.0000) | Acc: (44.00%) (19389/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5161) |  Loss2: (0.0000) | Acc: (44.00%) (20016/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5137) |  Loss2: (0.0000) | Acc: (44.00%) (20639/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5119) |  Loss2: (0.0000) | Acc: (44.00%) (21252/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5111) |  Loss2: (0.0000) | Acc: (44.00%) (21835/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5084) |  Loss2: (0.0000) | Acc: (44.00%) (22432/50000)
# TEST : Loss: (1.4186) | Acc: (47.00%) (4746/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.3086) |  Loss2: (0.0000) | Acc: (54.00%) (70/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4394) |  Loss2: (0.0000) | Acc: (48.00%) (685/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4254) |  Loss2: (0.0000) | Acc: (48.00%) (1314/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4164) |  Loss2: (0.0000) | Acc: (48.00%) (1936/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4134) |  Loss2: (0.0000) | Acc: (48.00%) (2566/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4204) |  Loss2: (0.0000) | Acc: (48.00%) (3177/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4159) |  Loss2: (0.0000) | Acc: (48.00%) (3786/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4138) |  Loss2: (0.0000) | Acc: (48.00%) (4410/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4138) |  Loss2: (0.0000) | Acc: (48.00%) (5017/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4095) |  Loss2: (0.0000) | Acc: (48.00%) (5648/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4062) |  Loss2: (0.0000) | Acc: (48.00%) (6303/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4026) |  Loss2: (0.0000) | Acc: (48.00%) (6926/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4038) |  Loss2: (0.0000) | Acc: (48.00%) (7541/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4040) |  Loss2: (0.0000) | Acc: (48.00%) (8159/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4026) |  Loss2: (0.0000) | Acc: (48.00%) (8807/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4005) |  Loss2: (0.0000) | Acc: (48.00%) (9435/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.3986) |  Loss2: (0.0000) | Acc: (48.00%) (10082/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.3983) |  Loss2: (0.0000) | Acc: (49.00%) (10731/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.3977) |  Loss2: (0.0000) | Acc: (49.00%) (11361/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.3973) |  Loss2: (0.0000) | Acc: (49.00%) (11982/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.3979) |  Loss2: (0.0000) | Acc: (48.00%) (12599/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (49.00%) (13243/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.3968) |  Loss2: (0.0000) | Acc: (49.00%) (13862/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.3949) |  Loss2: (0.0000) | Acc: (49.00%) (14536/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.3921) |  Loss2: (0.0000) | Acc: (49.00%) (15196/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.3916) |  Loss2: (0.0000) | Acc: (49.00%) (15850/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3903) |  Loss2: (0.0000) | Acc: (49.00%) (16507/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3890) |  Loss2: (0.0000) | Acc: (49.00%) (17152/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3880) |  Loss2: (0.0000) | Acc: (49.00%) (17784/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3857) |  Loss2: (0.0000) | Acc: (49.00%) (18452/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3841) |  Loss2: (0.0000) | Acc: (49.00%) (19113/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3817) |  Loss2: (0.0000) | Acc: (49.00%) (19786/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3806) |  Loss2: (0.0000) | Acc: (49.00%) (20444/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3800) |  Loss2: (0.0000) | Acc: (49.00%) (21088/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3774) |  Loss2: (0.0000) | Acc: (49.00%) (21796/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3751) |  Loss2: (0.0000) | Acc: (50.00%) (22465/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3724) |  Loss2: (0.0000) | Acc: (50.00%) (23165/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3700) |  Loss2: (0.0000) | Acc: (50.00%) (23857/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3688) |  Loss2: (0.0000) | Acc: (50.00%) (24504/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3677) |  Loss2: (0.0000) | Acc: (50.00%) (25157/50000)
# TEST : Loss: (1.3312) | Acc: (51.00%) (5100/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.2566) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.2364) |  Loss2: (0.0000) | Acc: (55.00%) (782/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.2640) |  Loss2: (0.0000) | Acc: (55.00%) (1480/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.2894) |  Loss2: (0.0000) | Acc: (54.00%) (2167/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.2933) |  Loss2: (0.0000) | Acc: (54.00%) (2843/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3004) |  Loss2: (0.0000) | Acc: (53.00%) (3507/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2949) |  Loss2: (0.0000) | Acc: (53.00%) (4209/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2942) |  Loss2: (0.0000) | Acc: (53.00%) (4901/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2887) |  Loss2: (0.0000) | Acc: (54.00%) (5624/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2794) |  Loss2: (0.0000) | Acc: (54.00%) (6338/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2783) |  Loss2: (0.0000) | Acc: (54.00%) (7027/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2776) |  Loss2: (0.0000) | Acc: (54.00%) (7729/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2738) |  Loss2: (0.0000) | Acc: (54.00%) (8467/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2765) |  Loss2: (0.0000) | Acc: (54.00%) (9153/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2756) |  Loss2: (0.0000) | Acc: (54.00%) (9858/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2752) |  Loss2: (0.0000) | Acc: (54.00%) (10573/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2745) |  Loss2: (0.0000) | Acc: (54.00%) (11290/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2708) |  Loss2: (0.0000) | Acc: (54.00%) (12035/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2710) |  Loss2: (0.0000) | Acc: (54.00%) (12741/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2688) |  Loss2: (0.0000) | Acc: (54.00%) (13445/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2687) |  Loss2: (0.0000) | Acc: (54.00%) (14136/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2654) |  Loss2: (0.0000) | Acc: (55.00%) (14877/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2653) |  Loss2: (0.0000) | Acc: (54.00%) (15543/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2633) |  Loss2: (0.0000) | Acc: (55.00%) (16276/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2605) |  Loss2: (0.0000) | Acc: (55.00%) (17031/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2596) |  Loss2: (0.0000) | Acc: (55.00%) (17727/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2601) |  Loss2: (0.0000) | Acc: (55.00%) (18410/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2567) |  Loss2: (0.0000) | Acc: (55.00%) (19153/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2548) |  Loss2: (0.0000) | Acc: (55.00%) (19872/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2511) |  Loss2: (0.0000) | Acc: (55.00%) (20625/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2497) |  Loss2: (0.0000) | Acc: (55.00%) (21348/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2473) |  Loss2: (0.0000) | Acc: (55.00%) (22086/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2452) |  Loss2: (0.0000) | Acc: (55.00%) (22831/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2444) |  Loss2: (0.0000) | Acc: (55.00%) (23553/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2433) |  Loss2: (0.0000) | Acc: (55.00%) (24287/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2414) |  Loss2: (0.0000) | Acc: (55.00%) (25035/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2401) |  Loss2: (0.0000) | Acc: (55.00%) (25762/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2390) |  Loss2: (0.0000) | Acc: (55.00%) (26500/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2379) |  Loss2: (0.0000) | Acc: (55.00%) (27240/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2370) |  Loss2: (0.0000) | Acc: (55.00%) (27945/50000)
# TEST : Loss: (1.2641) | Acc: (54.00%) (5403/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.1024) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2193) |  Loss2: (0.0000) | Acc: (56.00%) (796/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (1447/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3390) |  Loss2: (0.0000) | Acc: (51.00%) (2047/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.3540) |  Loss2: (0.0000) | Acc: (50.00%) (2673/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.3482) |  Loss2: (0.0000) | Acc: (51.00%) (3345/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.3614) |  Loss2: (0.0000) | Acc: (50.00%) (3960/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.3605) |  Loss2: (0.0000) | Acc: (50.00%) (4601/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.3593) |  Loss2: (0.0000) | Acc: (50.00%) (5246/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.3582) |  Loss2: (0.0000) | Acc: (50.00%) (5889/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.3507) |  Loss2: (0.0000) | Acc: (50.00%) (6579/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3468) |  Loss2: (0.0000) | Acc: (51.00%) (7261/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3448) |  Loss2: (0.0000) | Acc: (51.00%) (7917/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3430) |  Loss2: (0.0000) | Acc: (51.00%) (8596/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3410) |  Loss2: (0.0000) | Acc: (51.00%) (9259/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3385) |  Loss2: (0.0000) | Acc: (51.00%) (9937/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3364) |  Loss2: (0.0000) | Acc: (51.00%) (10595/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3348) |  Loss2: (0.0000) | Acc: (51.00%) (11268/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3334) |  Loss2: (0.0000) | Acc: (51.00%) (11949/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3330) |  Loss2: (0.0000) | Acc: (51.00%) (12622/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3295) |  Loss2: (0.0000) | Acc: (51.00%) (13304/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3280) |  Loss2: (0.0000) | Acc: (51.00%) (14006/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3244) |  Loss2: (0.0000) | Acc: (51.00%) (14692/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3230) |  Loss2: (0.0000) | Acc: (51.00%) (15361/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3207) |  Loss2: (0.0000) | Acc: (52.00%) (16043/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3180) |  Loss2: (0.0000) | Acc: (52.00%) (16746/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3142) |  Loss2: (0.0000) | Acc: (52.00%) (17455/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3131) |  Loss2: (0.0000) | Acc: (52.00%) (18150/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3117) |  Loss2: (0.0000) | Acc: (52.00%) (18846/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3111) |  Loss2: (0.0000) | Acc: (52.00%) (19526/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3100) |  Loss2: (0.0000) | Acc: (52.00%) (20216/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3088) |  Loss2: (0.0000) | Acc: (52.00%) (20910/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3065) |  Loss2: (0.0000) | Acc: (52.00%) (21612/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3056) |  Loss2: (0.0000) | Acc: (52.00%) (22310/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3057) |  Loss2: (0.0000) | Acc: (52.00%) (22963/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3035) |  Loss2: (0.0000) | Acc: (52.00%) (23666/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3015) |  Loss2: (0.0000) | Acc: (52.00%) (24391/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.2999) |  Loss2: (0.0000) | Acc: (52.00%) (25111/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.2972) |  Loss2: (0.0000) | Acc: (52.00%) (25830/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2968) |  Loss2: (0.0000) | Acc: (52.00%) (26488/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2169) | Acc: (55.00%) (5559/10000)
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.4851, 0.5149], device='cuda:0')
percent tensor([0.4873, 0.5127], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.4859, 0.5141], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.3169) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2029) |  Loss2: (0.0000) | Acc: (55.00%) (786/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2170) |  Loss2: (0.0000) | Acc: (55.00%) (1490/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.2349) |  Loss2: (0.0000) | Acc: (55.00%) (2185/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.2269) |  Loss2: (0.0000) | Acc: (55.00%) (2919/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2196) |  Loss2: (0.0000) | Acc: (55.00%) (3652/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2146) |  Loss2: (0.0000) | Acc: (56.00%) (4374/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2137) |  Loss2: (0.0000) | Acc: (55.00%) (5088/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2109) |  Loss2: (0.0000) | Acc: (56.00%) (5839/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2124) |  Loss2: (0.0000) | Acc: (56.00%) (6572/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2095) |  Loss2: (0.0000) | Acc: (56.00%) (7307/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2073) |  Loss2: (0.0000) | Acc: (56.00%) (8047/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2110) |  Loss2: (0.0000) | Acc: (56.00%) (8742/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2081) |  Loss2: (0.0000) | Acc: (56.00%) (9493/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2136) |  Loss2: (0.0000) | Acc: (56.00%) (10181/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2150) |  Loss2: (0.0000) | Acc: (56.00%) (10876/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2147) |  Loss2: (0.0000) | Acc: (56.00%) (11580/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2127) |  Loss2: (0.0000) | Acc: (56.00%) (12317/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2122) |  Loss2: (0.0000) | Acc: (56.00%) (13036/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2134) |  Loss2: (0.0000) | Acc: (56.00%) (13748/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2126) |  Loss2: (0.0000) | Acc: (56.00%) (14460/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2128) |  Loss2: (0.0000) | Acc: (56.00%) (15181/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2123) |  Loss2: (0.0000) | Acc: (56.00%) (15913/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2121) |  Loss2: (0.0000) | Acc: (56.00%) (16628/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2101) |  Loss2: (0.0000) | Acc: (56.00%) (17374/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2104) |  Loss2: (0.0000) | Acc: (56.00%) (18092/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2092) |  Loss2: (0.0000) | Acc: (56.00%) (18806/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2085) |  Loss2: (0.0000) | Acc: (56.00%) (19543/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2069) |  Loss2: (0.0000) | Acc: (56.00%) (20280/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2055) |  Loss2: (0.0000) | Acc: (56.00%) (21019/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2070) |  Loss2: (0.0000) | Acc: (56.00%) (21710/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2065) |  Loss2: (0.0000) | Acc: (56.00%) (22429/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2056) |  Loss2: (0.0000) | Acc: (56.00%) (23144/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2050) |  Loss2: (0.0000) | Acc: (56.00%) (23879/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (56.00%) (24596/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2046) |  Loss2: (0.0000) | Acc: (56.00%) (25321/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2032) |  Loss2: (0.0000) | Acc: (56.00%) (26100/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2023) |  Loss2: (0.0000) | Acc: (56.00%) (26853/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2024) |  Loss2: (0.0000) | Acc: (56.00%) (27564/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2007) |  Loss2: (0.0000) | Acc: (56.00%) (28273/50000)
# TEST : Loss: (1.1763) | Acc: (57.00%) (5761/10000)
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.4910, 0.5090], device='cuda:0')
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.4813, 0.5187], device='cuda:0')
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.0940) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.1686) |  Loss2: (0.0000) | Acc: (57.00%) (816/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.1579) |  Loss2: (0.0000) | Acc: (57.00%) (1552/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.1664) |  Loss2: (0.0000) | Acc: (57.00%) (2276/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.1570) |  Loss2: (0.0000) | Acc: (57.00%) (3017/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1566) |  Loss2: (0.0000) | Acc: (57.00%) (3740/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.1576) |  Loss2: (0.0000) | Acc: (57.00%) (4486/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.1555) |  Loss2: (0.0000) | Acc: (57.00%) (5233/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1585) |  Loss2: (0.0000) | Acc: (57.00%) (5975/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1671) |  Loss2: (0.0000) | Acc: (57.00%) (6684/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1666) |  Loss2: (0.0000) | Acc: (57.00%) (7418/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1682) |  Loss2: (0.0000) | Acc: (57.00%) (8153/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1670) |  Loss2: (0.0000) | Acc: (57.00%) (8883/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.1687) |  Loss2: (0.0000) | Acc: (57.00%) (9619/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.1670) |  Loss2: (0.0000) | Acc: (57.00%) (10373/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.1678) |  Loss2: (0.0000) | Acc: (57.00%) (11109/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.1681) |  Loss2: (0.0000) | Acc: (57.00%) (11866/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.1678) |  Loss2: (0.0000) | Acc: (57.00%) (12593/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1699) |  Loss2: (0.0000) | Acc: (57.00%) (13325/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1686) |  Loss2: (0.0000) | Acc: (57.00%) (14067/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1685) |  Loss2: (0.0000) | Acc: (57.00%) (14780/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1677) |  Loss2: (0.0000) | Acc: (57.00%) (15524/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1699) |  Loss2: (0.0000) | Acc: (57.00%) (16225/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1699) |  Loss2: (0.0000) | Acc: (57.00%) (16947/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1703) |  Loss2: (0.0000) | Acc: (57.00%) (17674/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1714) |  Loss2: (0.0000) | Acc: (57.00%) (18404/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1720) |  Loss2: (0.0000) | Acc: (57.00%) (19132/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1709) |  Loss2: (0.0000) | Acc: (57.00%) (19885/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1710) |  Loss2: (0.0000) | Acc: (57.00%) (20640/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1681) |  Loss2: (0.0000) | Acc: (57.00%) (21407/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1682) |  Loss2: (0.0000) | Acc: (57.00%) (22152/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1687) |  Loss2: (0.0000) | Acc: (57.00%) (22892/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1689) |  Loss2: (0.0000) | Acc: (57.00%) (23636/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1706) |  Loss2: (0.0000) | Acc: (57.00%) (24359/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1707) |  Loss2: (0.0000) | Acc: (57.00%) (25088/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1712) |  Loss2: (0.0000) | Acc: (57.00%) (25812/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1714) |  Loss2: (0.0000) | Acc: (57.00%) (26554/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1714) |  Loss2: (0.0000) | Acc: (57.00%) (27296/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1707) |  Loss2: (0.0000) | Acc: (57.00%) (28034/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1704) |  Loss2: (0.0000) | Acc: (57.00%) (28757/50000)
# TEST : Loss: (1.1583) | Acc: (58.00%) (5865/10000)
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.4844, 0.5156], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.4774, 0.5226], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.7984, 0.2016], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1957) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1362) |  Loss2: (0.0000) | Acc: (59.00%) (831/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1330) |  Loss2: (0.0000) | Acc: (58.00%) (1575/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1331) |  Loss2: (0.0000) | Acc: (58.00%) (2319/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1364) |  Loss2: (0.0000) | Acc: (58.00%) (3061/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (58.00%) (3803/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1538) |  Loss2: (0.0000) | Acc: (58.00%) (4550/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1497) |  Loss2: (0.0000) | Acc: (58.00%) (5302/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1491) |  Loss2: (0.0000) | Acc: (58.00%) (6067/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1470) |  Loss2: (0.0000) | Acc: (58.00%) (6840/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (7587/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1484) |  Loss2: (0.0000) | Acc: (58.00%) (8330/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1529) |  Loss2: (0.0000) | Acc: (58.00%) (9072/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1519) |  Loss2: (0.0000) | Acc: (58.00%) (9823/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1520) |  Loss2: (0.0000) | Acc: (58.00%) (10579/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1530) |  Loss2: (0.0000) | Acc: (58.00%) (11292/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1535) |  Loss2: (0.0000) | Acc: (58.00%) (12041/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1528) |  Loss2: (0.0000) | Acc: (58.00%) (12797/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1522) |  Loss2: (0.0000) | Acc: (58.00%) (13534/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1526) |  Loss2: (0.0000) | Acc: (58.00%) (14252/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1545) |  Loss2: (0.0000) | Acc: (58.00%) (14973/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1557) |  Loss2: (0.0000) | Acc: (58.00%) (15688/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1596) |  Loss2: (0.0000) | Acc: (57.00%) (16389/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1598) |  Loss2: (0.0000) | Acc: (57.00%) (17127/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1592) |  Loss2: (0.0000) | Acc: (57.00%) (17872/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1588) |  Loss2: (0.0000) | Acc: (58.00%) (18641/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1600) |  Loss2: (0.0000) | Acc: (57.00%) (19355/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1609) |  Loss2: (0.0000) | Acc: (57.00%) (20087/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1599) |  Loss2: (0.0000) | Acc: (57.00%) (20841/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1594) |  Loss2: (0.0000) | Acc: (57.00%) (21601/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1588) |  Loss2: (0.0000) | Acc: (58.00%) (22352/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1595) |  Loss2: (0.0000) | Acc: (58.00%) (23095/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1593) |  Loss2: (0.0000) | Acc: (58.00%) (23839/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1588) |  Loss2: (0.0000) | Acc: (58.00%) (24585/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1580) |  Loss2: (0.0000) | Acc: (58.00%) (25342/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1578) |  Loss2: (0.0000) | Acc: (58.00%) (26094/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1572) |  Loss2: (0.0000) | Acc: (58.00%) (26819/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1564) |  Loss2: (0.0000) | Acc: (58.00%) (27581/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1554) |  Loss2: (0.0000) | Acc: (58.00%) (28339/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1546) |  Loss2: (0.0000) | Acc: (58.00%) (29051/50000)
# TEST : Loss: (1.1485) | Acc: (58.00%) (5896/10000)
percent tensor([0.5125, 0.4875], device='cuda:0')
percent tensor([0.4856, 0.5144], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.4767, 0.5233], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.5013, 0.4987], device='cuda:0')
percent tensor([0.8335, 0.1665], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.2201) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1865) |  Loss2: (0.0000) | Acc: (57.00%) (810/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1805) |  Loss2: (0.0000) | Acc: (57.00%) (1534/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1798) |  Loss2: (0.0000) | Acc: (56.00%) (2259/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1710) |  Loss2: (0.0000) | Acc: (57.00%) (2999/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1529) |  Loss2: (0.0000) | Acc: (57.00%) (3754/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1433) |  Loss2: (0.0000) | Acc: (58.00%) (4535/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1424) |  Loss2: (0.0000) | Acc: (58.00%) (5293/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1433) |  Loss2: (0.0000) | Acc: (58.00%) (6042/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1433) |  Loss2: (0.0000) | Acc: (58.00%) (6806/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1423) |  Loss2: (0.0000) | Acc: (58.00%) (7572/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1428) |  Loss2: (0.0000) | Acc: (58.00%) (8307/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (9043/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1388) |  Loss2: (0.0000) | Acc: (58.00%) (9789/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1381) |  Loss2: (0.0000) | Acc: (58.00%) (10535/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1396) |  Loss2: (0.0000) | Acc: (58.00%) (11307/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1412) |  Loss2: (0.0000) | Acc: (58.00%) (12053/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1437) |  Loss2: (0.0000) | Acc: (58.00%) (12783/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1465) |  Loss2: (0.0000) | Acc: (58.00%) (13494/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1464) |  Loss2: (0.0000) | Acc: (58.00%) (14251/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1456) |  Loss2: (0.0000) | Acc: (58.00%) (14982/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1447) |  Loss2: (0.0000) | Acc: (58.00%) (15749/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (58.00%) (16503/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1439) |  Loss2: (0.0000) | Acc: (58.00%) (17265/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1450) |  Loss2: (0.0000) | Acc: (58.00%) (18005/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1478) |  Loss2: (0.0000) | Acc: (58.00%) (18721/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1485) |  Loss2: (0.0000) | Acc: (58.00%) (19456/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1511) |  Loss2: (0.0000) | Acc: (58.00%) (20164/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1518) |  Loss2: (0.0000) | Acc: (58.00%) (20890/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1519) |  Loss2: (0.0000) | Acc: (58.00%) (21660/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1509) |  Loss2: (0.0000) | Acc: (58.00%) (22418/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1484) |  Loss2: (0.0000) | Acc: (58.00%) (23194/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1482) |  Loss2: (0.0000) | Acc: (58.00%) (23937/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1480) |  Loss2: (0.0000) | Acc: (58.00%) (24686/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1486) |  Loss2: (0.0000) | Acc: (58.00%) (25412/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1491) |  Loss2: (0.0000) | Acc: (58.00%) (26137/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1486) |  Loss2: (0.0000) | Acc: (58.00%) (26904/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1478) |  Loss2: (0.0000) | Acc: (58.00%) (27668/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1465) |  Loss2: (0.0000) | Acc: (58.00%) (28439/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1454) |  Loss2: (0.0000) | Acc: (58.00%) (29183/50000)
# TEST : Loss: (1.1448) | Acc: (58.00%) (5888/10000)
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.4866, 0.5134], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4765, 0.5235], device='cuda:0')
percent tensor([0.5273, 0.4727], device='cuda:0')
percent tensor([0.5013, 0.4987], device='cuda:0')
percent tensor([0.8499, 0.1501], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1516) |  Loss2: (0.0000) | Acc: (50.00%) (65/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1590) |  Loss2: (0.0000) | Acc: (58.00%) (817/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1655) |  Loss2: (0.0000) | Acc: (57.00%) (1555/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1628) |  Loss2: (0.0000) | Acc: (58.00%) (2306/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1631) |  Loss2: (0.0000) | Acc: (57.00%) (3040/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1606) |  Loss2: (0.0000) | Acc: (57.00%) (3784/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1595) |  Loss2: (0.0000) | Acc: (57.00%) (4517/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1559) |  Loss2: (0.0000) | Acc: (57.00%) (5264/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1499) |  Loss2: (0.0000) | Acc: (58.00%) (6025/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1456) |  Loss2: (0.0000) | Acc: (58.00%) (6787/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1419) |  Loss2: (0.0000) | Acc: (58.00%) (7556/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1406) |  Loss2: (0.0000) | Acc: (58.00%) (8303/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1391) |  Loss2: (0.0000) | Acc: (58.00%) (9062/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1370) |  Loss2: (0.0000) | Acc: (58.00%) (9833/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1355) |  Loss2: (0.0000) | Acc: (58.00%) (10590/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1369) |  Loss2: (0.0000) | Acc: (58.00%) (11320/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (58.00%) (12102/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1353) |  Loss2: (0.0000) | Acc: (58.00%) (12855/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1356) |  Loss2: (0.0000) | Acc: (58.00%) (13614/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1324) |  Loss2: (0.0000) | Acc: (58.00%) (14409/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1290) |  Loss2: (0.0000) | Acc: (59.00%) (15204/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1277) |  Loss2: (0.0000) | Acc: (59.00%) (15980/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1243) |  Loss2: (0.0000) | Acc: (59.00%) (16779/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1217) |  Loss2: (0.0000) | Acc: (59.00%) (17558/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1207) |  Loss2: (0.0000) | Acc: (59.00%) (18335/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1193) |  Loss2: (0.0000) | Acc: (59.00%) (19110/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1184) |  Loss2: (0.0000) | Acc: (59.00%) (19892/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1176) |  Loss2: (0.0000) | Acc: (59.00%) (20680/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1158) |  Loss2: (0.0000) | Acc: (59.00%) (21465/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1145) |  Loss2: (0.0000) | Acc: (59.00%) (22246/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1134) |  Loss2: (0.0000) | Acc: (59.00%) (23027/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1113) |  Loss2: (0.0000) | Acc: (59.00%) (23799/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1098) |  Loss2: (0.0000) | Acc: (59.00%) (24566/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1086) |  Loss2: (0.0000) | Acc: (59.00%) (25343/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1085) |  Loss2: (0.0000) | Acc: (59.00%) (26119/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1059) |  Loss2: (0.0000) | Acc: (59.00%) (26956/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1034) |  Loss2: (0.0000) | Acc: (60.00%) (27789/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1024) |  Loss2: (0.0000) | Acc: (60.00%) (28599/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1015) |  Loss2: (0.0000) | Acc: (60.00%) (29376/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1013) |  Loss2: (0.0000) | Acc: (60.00%) (30104/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.4459) | Acc: (51.00%) (5130/10000)
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.4865, 0.5135], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5323, 0.4677], device='cuda:0')
percent tensor([0.4758, 0.5242], device='cuda:0')
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.8649, 0.1351], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(165.5059, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(774.5266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(770.4261, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1533.6591, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.3642, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.2458, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4326.2109, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1446.7700, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6111.5400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12210.4170, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4066.8228, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17282.5605, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (0.9831) |  Loss2: (0.0000) | Acc: (67.00%) (86/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0178) |  Loss2: (0.0000) | Acc: (63.00%) (888/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0406) |  Loss2: (0.0000) | Acc: (63.00%) (1696/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0486) |  Loss2: (0.0000) | Acc: (62.00%) (2461/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0382) |  Loss2: (0.0000) | Acc: (62.00%) (3282/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0309) |  Loss2: (0.0000) | Acc: (63.00%) (4114/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0339) |  Loss2: (0.0000) | Acc: (63.00%) (4926/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0342) |  Loss2: (0.0000) | Acc: (62.00%) (5724/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0323) |  Loss2: (0.0000) | Acc: (62.00%) (6522/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0342) |  Loss2: (0.0000) | Acc: (62.00%) (7319/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0347) |  Loss2: (0.0000) | Acc: (62.00%) (8104/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0369) |  Loss2: (0.0000) | Acc: (62.00%) (8920/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0349) |  Loss2: (0.0000) | Acc: (62.00%) (9755/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0319) |  Loss2: (0.0000) | Acc: (63.00%) (10600/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0319) |  Loss2: (0.0000) | Acc: (63.00%) (11411/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0314) |  Loss2: (0.0000) | Acc: (63.00%) (12222/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0292) |  Loss2: (0.0000) | Acc: (63.00%) (13057/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0283) |  Loss2: (0.0000) | Acc: (63.00%) (13879/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0292) |  Loss2: (0.0000) | Acc: (63.00%) (14685/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0296) |  Loss2: (0.0000) | Acc: (63.00%) (15478/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0267) |  Loss2: (0.0000) | Acc: (63.00%) (16318/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0270) |  Loss2: (0.0000) | Acc: (63.00%) (17123/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0254) |  Loss2: (0.0000) | Acc: (63.00%) (17943/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0227) |  Loss2: (0.0000) | Acc: (63.00%) (18780/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0205) |  Loss2: (0.0000) | Acc: (63.00%) (19625/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0206) |  Loss2: (0.0000) | Acc: (63.00%) (20417/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0175) |  Loss2: (0.0000) | Acc: (63.00%) (21286/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0160) |  Loss2: (0.0000) | Acc: (63.00%) (22117/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0165) |  Loss2: (0.0000) | Acc: (63.00%) (22908/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0154) |  Loss2: (0.0000) | Acc: (63.00%) (23752/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0134) |  Loss2: (0.0000) | Acc: (63.00%) (24608/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0117) |  Loss2: (0.0000) | Acc: (63.00%) (25452/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0096) |  Loss2: (0.0000) | Acc: (64.00%) (26306/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0085) |  Loss2: (0.0000) | Acc: (64.00%) (27142/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0092) |  Loss2: (0.0000) | Acc: (64.00%) (27963/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0079) |  Loss2: (0.0000) | Acc: (64.00%) (28798/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0066) |  Loss2: (0.0000) | Acc: (64.00%) (29646/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0057) |  Loss2: (0.0000) | Acc: (64.00%) (30482/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0058) |  Loss2: (0.0000) | Acc: (64.00%) (31285/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0053) |  Loss2: (0.0000) | Acc: (64.00%) (32072/50000)
# TEST : Loss: (1.0451) | Acc: (62.00%) (6243/10000)
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.4760, 0.5240], device='cuda:0')
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.8655, 0.1345], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (1.0539) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (65.00%) (928/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9598) |  Loss2: (0.0000) | Acc: (65.00%) (1766/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9605) |  Loss2: (0.0000) | Acc: (65.00%) (2610/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9652) |  Loss2: (0.0000) | Acc: (65.00%) (3443/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9590) |  Loss2: (0.0000) | Acc: (65.00%) (4288/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9579) |  Loss2: (0.0000) | Acc: (65.00%) (5129/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9551) |  Loss2: (0.0000) | Acc: (65.00%) (5980/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9572) |  Loss2: (0.0000) | Acc: (65.00%) (6802/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9561) |  Loss2: (0.0000) | Acc: (65.00%) (7656/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9525) |  Loss2: (0.0000) | Acc: (65.00%) (8529/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9514) |  Loss2: (0.0000) | Acc: (66.00%) (9384/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9523) |  Loss2: (0.0000) | Acc: (66.00%) (10224/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9529) |  Loss2: (0.0000) | Acc: (66.00%) (11072/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9514) |  Loss2: (0.0000) | Acc: (66.00%) (11919/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9514) |  Loss2: (0.0000) | Acc: (65.00%) (12751/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9513) |  Loss2: (0.0000) | Acc: (65.00%) (13598/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9526) |  Loss2: (0.0000) | Acc: (65.00%) (14441/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9530) |  Loss2: (0.0000) | Acc: (65.00%) (15272/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9504) |  Loss2: (0.0000) | Acc: (66.00%) (16152/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9483) |  Loss2: (0.0000) | Acc: (66.00%) (17013/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9489) |  Loss2: (0.0000) | Acc: (66.00%) (17861/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9476) |  Loss2: (0.0000) | Acc: (66.00%) (18734/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9463) |  Loss2: (0.0000) | Acc: (66.00%) (19586/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9473) |  Loss2: (0.0000) | Acc: (66.00%) (20418/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9477) |  Loss2: (0.0000) | Acc: (66.00%) (21263/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9460) |  Loss2: (0.0000) | Acc: (66.00%) (22139/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9472) |  Loss2: (0.0000) | Acc: (66.00%) (22973/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9486) |  Loss2: (0.0000) | Acc: (66.00%) (23804/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9477) |  Loss2: (0.0000) | Acc: (66.00%) (24651/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9475) |  Loss2: (0.0000) | Acc: (66.00%) (25513/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9460) |  Loss2: (0.0000) | Acc: (66.00%) (26408/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9450) |  Loss2: (0.0000) | Acc: (66.00%) (27271/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9437) |  Loss2: (0.0000) | Acc: (66.00%) (28125/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9429) |  Loss2: (0.0000) | Acc: (66.00%) (28996/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9437) |  Loss2: (0.0000) | Acc: (66.00%) (29829/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9437) |  Loss2: (0.0000) | Acc: (66.00%) (30685/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9431) |  Loss2: (0.0000) | Acc: (66.00%) (31551/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9411) |  Loss2: (0.0000) | Acc: (66.00%) (32431/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9409) |  Loss2: (0.0000) | Acc: (66.00%) (33252/50000)
# TEST : Loss: (1.1535) | Acc: (60.00%) (6091/10000)
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4862, 0.5138], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5329, 0.4671], device='cuda:0')
percent tensor([0.4751, 0.5249], device='cuda:0')
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.8538, 0.1462], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.8778) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9420) |  Loss2: (0.0000) | Acc: (66.00%) (941/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9176) |  Loss2: (0.0000) | Acc: (67.00%) (1811/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.8974) |  Loss2: (0.0000) | Acc: (68.00%) (2706/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9078) |  Loss2: (0.0000) | Acc: (67.00%) (3531/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9000) |  Loss2: (0.0000) | Acc: (67.00%) (4400/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9000) |  Loss2: (0.0000) | Acc: (67.00%) (5261/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.8985) |  Loss2: (0.0000) | Acc: (67.00%) (6138/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.8979) |  Loss2: (0.0000) | Acc: (67.00%) (7021/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.8985) |  Loss2: (0.0000) | Acc: (67.00%) (7894/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8966) |  Loss2: (0.0000) | Acc: (67.00%) (8775/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8968) |  Loss2: (0.0000) | Acc: (67.00%) (9660/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8979) |  Loss2: (0.0000) | Acc: (67.00%) (10527/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8963) |  Loss2: (0.0000) | Acc: (68.00%) (11410/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8985) |  Loss2: (0.0000) | Acc: (67.00%) (12272/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8965) |  Loss2: (0.0000) | Acc: (68.00%) (13184/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8992) |  Loss2: (0.0000) | Acc: (68.00%) (14036/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9001) |  Loss2: (0.0000) | Acc: (68.00%) (14898/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9003) |  Loss2: (0.0000) | Acc: (68.00%) (15769/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9019) |  Loss2: (0.0000) | Acc: (67.00%) (16612/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9008) |  Loss2: (0.0000) | Acc: (67.00%) (17487/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8985) |  Loss2: (0.0000) | Acc: (68.00%) (18391/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8976) |  Loss2: (0.0000) | Acc: (68.00%) (19259/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8961) |  Loss2: (0.0000) | Acc: (68.00%) (20137/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8951) |  Loss2: (0.0000) | Acc: (68.00%) (21018/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8946) |  Loss2: (0.0000) | Acc: (68.00%) (21901/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (22776/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8924) |  Loss2: (0.0000) | Acc: (68.00%) (23679/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8919) |  Loss2: (0.0000) | Acc: (68.00%) (24560/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8933) |  Loss2: (0.0000) | Acc: (68.00%) (25420/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (26293/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8918) |  Loss2: (0.0000) | Acc: (68.00%) (27177/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8918) |  Loss2: (0.0000) | Acc: (68.00%) (28044/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8908) |  Loss2: (0.0000) | Acc: (68.00%) (28922/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (29781/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8897) |  Loss2: (0.0000) | Acc: (68.00%) (30689/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8893) |  Loss2: (0.0000) | Acc: (68.00%) (31541/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8886) |  Loss2: (0.0000) | Acc: (68.00%) (32419/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8892) |  Loss2: (0.0000) | Acc: (68.00%) (33277/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8888) |  Loss2: (0.0000) | Acc: (68.00%) (34111/50000)
# TEST : Loss: (0.9810) | Acc: (65.00%) (6553/10000)
percent tensor([0.5133, 0.4867], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.4771, 0.5229], device='cuda:0')
percent tensor([0.5281, 0.4719], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.8660, 0.1340], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.9139) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8782) |  Loss2: (0.0000) | Acc: (66.00%) (943/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8576) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8507) |  Loss2: (0.0000) | Acc: (69.00%) (2754/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8447) |  Loss2: (0.0000) | Acc: (69.00%) (3658/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (69.00%) (4555/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (69.00%) (5444/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8483) |  Loss2: (0.0000) | Acc: (69.00%) (6335/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8527) |  Loss2: (0.0000) | Acc: (69.00%) (7207/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (8097/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8547) |  Loss2: (0.0000) | Acc: (69.00%) (9002/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8539) |  Loss2: (0.0000) | Acc: (69.00%) (9905/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8519) |  Loss2: (0.0000) | Acc: (69.00%) (10817/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8503) |  Loss2: (0.0000) | Acc: (69.00%) (11710/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8516) |  Loss2: (0.0000) | Acc: (69.00%) (12584/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8508) |  Loss2: (0.0000) | Acc: (69.00%) (13495/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8505) |  Loss2: (0.0000) | Acc: (69.00%) (14387/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8518) |  Loss2: (0.0000) | Acc: (69.00%) (15263/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8520) |  Loss2: (0.0000) | Acc: (69.00%) (16161/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8494) |  Loss2: (0.0000) | Acc: (69.00%) (17086/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8500) |  Loss2: (0.0000) | Acc: (69.00%) (17993/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (18903/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (70.00%) (19807/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8491) |  Loss2: (0.0000) | Acc: (69.00%) (20693/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (21580/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8493) |  Loss2: (0.0000) | Acc: (69.00%) (22477/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8513) |  Loss2: (0.0000) | Acc: (69.00%) (23343/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8500) |  Loss2: (0.0000) | Acc: (69.00%) (24241/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8502) |  Loss2: (0.0000) | Acc: (69.00%) (25133/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8500) |  Loss2: (0.0000) | Acc: (69.00%) (26032/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (69.00%) (26931/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8487) |  Loss2: (0.0000) | Acc: (69.00%) (27822/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8486) |  Loss2: (0.0000) | Acc: (69.00%) (28700/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8485) |  Loss2: (0.0000) | Acc: (69.00%) (29603/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8464) |  Loss2: (0.0000) | Acc: (69.00%) (30517/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8448) |  Loss2: (0.0000) | Acc: (69.00%) (31422/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8443) |  Loss2: (0.0000) | Acc: (69.00%) (32335/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8431) |  Loss2: (0.0000) | Acc: (70.00%) (33272/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8442) |  Loss2: (0.0000) | Acc: (70.00%) (34151/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8429) |  Loss2: (0.0000) | Acc: (70.00%) (35019/50000)
# TEST : Loss: (0.9900) | Acc: (65.00%) (6551/10000)
percent tensor([0.5129, 0.4871], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.4773, 0.5227], device='cuda:0')
percent tensor([0.5275, 0.4725], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.8743, 0.1257], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.8757) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8231) |  Loss2: (0.0000) | Acc: (71.00%) (1009/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8576) |  Loss2: (0.0000) | Acc: (69.00%) (1881/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.8741) |  Loss2: (0.0000) | Acc: (69.00%) (2760/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.8918) |  Loss2: (0.0000) | Acc: (68.00%) (3595/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.8968) |  Loss2: (0.0000) | Acc: (68.00%) (4470/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.8964) |  Loss2: (0.0000) | Acc: (68.00%) (5355/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.8995) |  Loss2: (0.0000) | Acc: (68.00%) (6221/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.8949) |  Loss2: (0.0000) | Acc: (68.00%) (7117/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.8940) |  Loss2: (0.0000) | Acc: (68.00%) (7992/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.8928) |  Loss2: (0.0000) | Acc: (68.00%) (8851/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.8898) |  Loss2: (0.0000) | Acc: (68.00%) (9735/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.8894) |  Loss2: (0.0000) | Acc: (68.00%) (10600/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.8917) |  Loss2: (0.0000) | Acc: (68.00%) (11474/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.8919) |  Loss2: (0.0000) | Acc: (68.00%) (12342/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.8934) |  Loss2: (0.0000) | Acc: (68.00%) (13212/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.8911) |  Loss2: (0.0000) | Acc: (68.00%) (14095/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.8899) |  Loss2: (0.0000) | Acc: (68.00%) (14971/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.8873) |  Loss2: (0.0000) | Acc: (68.00%) (15875/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.8857) |  Loss2: (0.0000) | Acc: (68.00%) (16763/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.8846) |  Loss2: (0.0000) | Acc: (68.00%) (17642/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.8832) |  Loss2: (0.0000) | Acc: (68.00%) (18536/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.8802) |  Loss2: (0.0000) | Acc: (68.00%) (19450/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.8787) |  Loss2: (0.0000) | Acc: (68.00%) (20343/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.8775) |  Loss2: (0.0000) | Acc: (68.00%) (21240/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.8760) |  Loss2: (0.0000) | Acc: (68.00%) (22135/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.8771) |  Loss2: (0.0000) | Acc: (68.00%) (23018/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.8764) |  Loss2: (0.0000) | Acc: (68.00%) (23889/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.8752) |  Loss2: (0.0000) | Acc: (68.00%) (24765/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.8742) |  Loss2: (0.0000) | Acc: (68.00%) (25676/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.8722) |  Loss2: (0.0000) | Acc: (69.00%) (26605/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.8707) |  Loss2: (0.0000) | Acc: (69.00%) (27507/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.8685) |  Loss2: (0.0000) | Acc: (69.00%) (28412/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.8658) |  Loss2: (0.0000) | Acc: (69.00%) (29349/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.8655) |  Loss2: (0.0000) | Acc: (69.00%) (30238/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.8649) |  Loss2: (0.0000) | Acc: (69.00%) (31123/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8634) |  Loss2: (0.0000) | Acc: (69.00%) (32036/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (32946/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8613) |  Loss2: (0.0000) | Acc: (69.00%) (33856/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8588) |  Loss2: (0.0000) | Acc: (69.00%) (34762/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.8407) | Acc: (70.00%) (7039/10000)
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.4832, 0.5168], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5209, 0.4791], device='cuda:0')
percent tensor([0.4849, 0.5151], device='cuda:0')
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.9321, 0.0679], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (1.0821) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8328) |  Loss2: (0.0000) | Acc: (70.00%) (995/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8221) |  Loss2: (0.0000) | Acc: (71.00%) (1910/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (71.00%) (2827/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (3758/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8110) |  Loss2: (0.0000) | Acc: (71.00%) (4674/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (5605/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (6527/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8073) |  Loss2: (0.0000) | Acc: (71.00%) (7436/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8120) |  Loss2: (0.0000) | Acc: (71.00%) (8324/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8088) |  Loss2: (0.0000) | Acc: (71.00%) (9241/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8061) |  Loss2: (0.0000) | Acc: (71.00%) (10174/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8095) |  Loss2: (0.0000) | Acc: (71.00%) (11071/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8112) |  Loss2: (0.0000) | Acc: (71.00%) (11992/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (12896/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (13812/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8081) |  Loss2: (0.0000) | Acc: (71.00%) (14718/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8076) |  Loss2: (0.0000) | Acc: (71.00%) (15621/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8070) |  Loss2: (0.0000) | Acc: (71.00%) (16554/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (17479/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8071) |  Loss2: (0.0000) | Acc: (71.00%) (18399/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8087) |  Loss2: (0.0000) | Acc: (71.00%) (19311/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8073) |  Loss2: (0.0000) | Acc: (71.00%) (20224/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8085) |  Loss2: (0.0000) | Acc: (71.00%) (21134/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8087) |  Loss2: (0.0000) | Acc: (71.00%) (22034/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8089) |  Loss2: (0.0000) | Acc: (71.00%) (22940/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8088) |  Loss2: (0.0000) | Acc: (71.00%) (23861/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8087) |  Loss2: (0.0000) | Acc: (71.00%) (24774/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8077) |  Loss2: (0.0000) | Acc: (71.00%) (25694/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (26620/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8048) |  Loss2: (0.0000) | Acc: (71.00%) (27532/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8062) |  Loss2: (0.0000) | Acc: (71.00%) (28433/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8059) |  Loss2: (0.0000) | Acc: (71.00%) (29354/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (30295/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8046) |  Loss2: (0.0000) | Acc: (71.00%) (31210/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (32111/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (33022/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (33950/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (71.00%) (34881/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8026) |  Loss2: (0.0000) | Acc: (71.00%) (35747/50000)
# TEST : Loss: (0.8211) | Acc: (70.00%) (7098/10000)
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.4861, 0.5139], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.4868, 0.5132], device='cuda:0')
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.9638, 0.0362], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.7147) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.7796) |  Loss2: (0.0000) | Acc: (71.00%) (1005/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (70.00%) (1887/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.7898) |  Loss2: (0.0000) | Acc: (71.00%) (2818/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.7879) |  Loss2: (0.0000) | Acc: (71.00%) (3733/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.7787) |  Loss2: (0.0000) | Acc: (71.00%) (4679/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.7845) |  Loss2: (0.0000) | Acc: (71.00%) (5577/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.7841) |  Loss2: (0.0000) | Acc: (71.00%) (6496/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.7856) |  Loss2: (0.0000) | Acc: (71.00%) (7424/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (71.00%) (8323/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.7867) |  Loss2: (0.0000) | Acc: (71.00%) (9258/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.7849) |  Loss2: (0.0000) | Acc: (71.00%) (10201/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (11109/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.7837) |  Loss2: (0.0000) | Acc: (71.00%) (12056/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.7847) |  Loss2: (0.0000) | Acc: (71.00%) (12969/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.7848) |  Loss2: (0.0000) | Acc: (71.00%) (13897/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (72.00%) (14839/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.7833) |  Loss2: (0.0000) | Acc: (72.00%) (15761/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (71.00%) (16668/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.7835) |  Loss2: (0.0000) | Acc: (71.00%) (17572/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.7836) |  Loss2: (0.0000) | Acc: (71.00%) (18493/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.7847) |  Loss2: (0.0000) | Acc: (71.00%) (19402/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.7855) |  Loss2: (0.0000) | Acc: (71.00%) (20318/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (71.00%) (21210/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.7873) |  Loss2: (0.0000) | Acc: (71.00%) (22126/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.7870) |  Loss2: (0.0000) | Acc: (71.00%) (23063/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.7869) |  Loss2: (0.0000) | Acc: (71.00%) (23993/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.7870) |  Loss2: (0.0000) | Acc: (71.00%) (24913/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.7884) |  Loss2: (0.0000) | Acc: (71.00%) (25809/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.7869) |  Loss2: (0.0000) | Acc: (71.00%) (26758/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.7871) |  Loss2: (0.0000) | Acc: (71.00%) (27674/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.7891) |  Loss2: (0.0000) | Acc: (71.00%) (28552/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.7894) |  Loss2: (0.0000) | Acc: (71.00%) (29474/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.7881) |  Loss2: (0.0000) | Acc: (71.00%) (30411/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (71.00%) (31343/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (71.00%) (32273/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.7891) |  Loss2: (0.0000) | Acc: (71.00%) (33179/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.7888) |  Loss2: (0.0000) | Acc: (71.00%) (34103/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.7890) |  Loss2: (0.0000) | Acc: (71.00%) (35017/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.7879) |  Loss2: (0.0000) | Acc: (71.00%) (35914/50000)
# TEST : Loss: (0.8114) | Acc: (71.00%) (7141/10000)
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.4889, 0.5111], device='cuda:0')
percent tensor([0.5120, 0.4880], device='cuda:0')
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.4885, 0.5115], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.9756, 0.0244], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.7266) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.8245) |  Loss2: (0.0000) | Acc: (69.00%) (984/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8108) |  Loss2: (0.0000) | Acc: (70.00%) (1906/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (71.00%) (2832/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (3772/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (4687/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.7949) |  Loss2: (0.0000) | Acc: (71.00%) (5619/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.7927) |  Loss2: (0.0000) | Acc: (72.00%) (6546/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (72.00%) (7486/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.7944) |  Loss2: (0.0000) | Acc: (72.00%) (8391/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.7954) |  Loss2: (0.0000) | Acc: (72.00%) (9319/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (72.00%) (10272/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (72.00%) (11192/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (72.00%) (12118/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.7867) |  Loss2: (0.0000) | Acc: (72.00%) (13055/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.7889) |  Loss2: (0.0000) | Acc: (72.00%) (13979/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.7872) |  Loss2: (0.0000) | Acc: (72.00%) (14924/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.7866) |  Loss2: (0.0000) | Acc: (72.00%) (15858/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.7847) |  Loss2: (0.0000) | Acc: (72.00%) (16797/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.7846) |  Loss2: (0.0000) | Acc: (72.00%) (17705/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.7848) |  Loss2: (0.0000) | Acc: (72.00%) (18632/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.7819) |  Loss2: (0.0000) | Acc: (72.00%) (19582/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.7809) |  Loss2: (0.0000) | Acc: (72.00%) (20528/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.7812) |  Loss2: (0.0000) | Acc: (72.00%) (21450/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.7797) |  Loss2: (0.0000) | Acc: (72.00%) (22382/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.7814) |  Loss2: (0.0000) | Acc: (72.00%) (23283/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.7811) |  Loss2: (0.0000) | Acc: (72.00%) (24225/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.7805) |  Loss2: (0.0000) | Acc: (72.00%) (25163/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.7798) |  Loss2: (0.0000) | Acc: (72.00%) (26106/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.7781) |  Loss2: (0.0000) | Acc: (72.00%) (27047/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.7784) |  Loss2: (0.0000) | Acc: (72.00%) (27962/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.7776) |  Loss2: (0.0000) | Acc: (72.00%) (28900/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.7771) |  Loss2: (0.0000) | Acc: (72.00%) (29842/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.7788) |  Loss2: (0.0000) | Acc: (72.00%) (30741/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.7800) |  Loss2: (0.0000) | Acc: (72.00%) (31630/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.7796) |  Loss2: (0.0000) | Acc: (72.00%) (32568/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.7801) |  Loss2: (0.0000) | Acc: (72.00%) (33482/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.7810) |  Loss2: (0.0000) | Acc: (72.00%) (34395/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.7807) |  Loss2: (0.0000) | Acc: (72.00%) (35324/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.7804) |  Loss2: (0.0000) | Acc: (72.00%) (36214/50000)
# TEST : Loss: (0.8069) | Acc: (71.00%) (7151/10000)
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.4904, 0.5096], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.4896, 0.5104], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.9828, 0.0172], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.7526) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7919) |  Loss2: (0.0000) | Acc: (71.00%) (1010/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.7815) |  Loss2: (0.0000) | Acc: (71.00%) (1928/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7785) |  Loss2: (0.0000) | Acc: (71.00%) (2848/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7892) |  Loss2: (0.0000) | Acc: (71.00%) (3744/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7915) |  Loss2: (0.0000) | Acc: (71.00%) (4655/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (71.00%) (5581/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (71.00%) (6499/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (71.00%) (7431/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7862) |  Loss2: (0.0000) | Acc: (71.00%) (8372/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7866) |  Loss2: (0.0000) | Acc: (71.00%) (9304/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7854) |  Loss2: (0.0000) | Acc: (72.00%) (10237/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7816) |  Loss2: (0.0000) | Acc: (72.00%) (11172/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7831) |  Loss2: (0.0000) | Acc: (72.00%) (12090/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7825) |  Loss2: (0.0000) | Acc: (71.00%) (12991/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.7822) |  Loss2: (0.0000) | Acc: (71.00%) (13914/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (71.00%) (14828/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.7828) |  Loss2: (0.0000) | Acc: (71.00%) (15744/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.7849) |  Loss2: (0.0000) | Acc: (71.00%) (16664/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.7841) |  Loss2: (0.0000) | Acc: (71.00%) (17599/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7829) |  Loss2: (0.0000) | Acc: (72.00%) (18525/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7828) |  Loss2: (0.0000) | Acc: (72.00%) (19458/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7837) |  Loss2: (0.0000) | Acc: (71.00%) (20362/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7834) |  Loss2: (0.0000) | Acc: (71.00%) (21279/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.7825) |  Loss2: (0.0000) | Acc: (71.00%) (22209/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7814) |  Loss2: (0.0000) | Acc: (72.00%) (23141/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.7820) |  Loss2: (0.0000) | Acc: (72.00%) (24059/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.7814) |  Loss2: (0.0000) | Acc: (72.00%) (24984/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.7802) |  Loss2: (0.0000) | Acc: (72.00%) (25934/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.7816) |  Loss2: (0.0000) | Acc: (72.00%) (26832/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.7820) |  Loss2: (0.0000) | Acc: (72.00%) (27756/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.7833) |  Loss2: (0.0000) | Acc: (71.00%) (28650/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (71.00%) (29563/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.7829) |  Loss2: (0.0000) | Acc: (71.00%) (30503/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (72.00%) (31433/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.7834) |  Loss2: (0.0000) | Acc: (72.00%) (32381/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.7836) |  Loss2: (0.0000) | Acc: (72.00%) (33305/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7841) |  Loss2: (0.0000) | Acc: (72.00%) (34216/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7834) |  Loss2: (0.0000) | Acc: (72.00%) (35143/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7810) |  Loss2: (0.0000) | Acc: (72.00%) (36078/50000)
# TEST : Loss: (0.8007) | Acc: (71.00%) (7150/10000)
percent tensor([0.5366, 0.4634], device='cuda:0')
percent tensor([0.4913, 0.5087], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.4907, 0.5093], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.9863, 0.0137], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7475) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.7936) |  Loss2: (0.0000) | Acc: (70.00%) (990/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (71.00%) (1915/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (71.00%) (2825/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8019) |  Loss2: (0.0000) | Acc: (70.00%) (3724/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8055) |  Loss2: (0.0000) | Acc: (70.00%) (4610/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8059) |  Loss2: (0.0000) | Acc: (70.00%) (5536/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (6474/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (7366/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8126) |  Loss2: (0.0000) | Acc: (70.00%) (8257/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8105) |  Loss2: (0.0000) | Acc: (70.00%) (9174/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (70.00%) (10071/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8156) |  Loss2: (0.0000) | Acc: (70.00%) (10976/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8128) |  Loss2: (0.0000) | Acc: (70.00%) (11898/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8109) |  Loss2: (0.0000) | Acc: (71.00%) (12816/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8093) |  Loss2: (0.0000) | Acc: (70.00%) (13720/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8119) |  Loss2: (0.0000) | Acc: (70.00%) (14612/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8096) |  Loss2: (0.0000) | Acc: (71.00%) (15542/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8087) |  Loss2: (0.0000) | Acc: (71.00%) (16456/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (71.00%) (17366/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8085) |  Loss2: (0.0000) | Acc: (71.00%) (18301/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8058) |  Loss2: (0.0000) | Acc: (71.00%) (19247/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (20152/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.8085) |  Loss2: (0.0000) | Acc: (71.00%) (21050/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.8080) |  Loss2: (0.0000) | Acc: (71.00%) (21979/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.8069) |  Loss2: (0.0000) | Acc: (71.00%) (22917/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.8051) |  Loss2: (0.0000) | Acc: (71.00%) (23840/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.8041) |  Loss2: (0.0000) | Acc: (71.00%) (24768/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.8041) |  Loss2: (0.0000) | Acc: (71.00%) (25702/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.8054) |  Loss2: (0.0000) | Acc: (71.00%) (26588/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (27532/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (71.00%) (28462/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.8026) |  Loss2: (0.0000) | Acc: (71.00%) (29362/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.8026) |  Loss2: (0.0000) | Acc: (71.00%) (30272/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.8012) |  Loss2: (0.0000) | Acc: (71.00%) (31214/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (32141/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7994) |  Loss2: (0.0000) | Acc: (71.00%) (33082/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7994) |  Loss2: (0.0000) | Acc: (71.00%) (34010/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7987) |  Loss2: (0.0000) | Acc: (71.00%) (34927/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7973) |  Loss2: (0.0000) | Acc: (71.00%) (35832/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.8974) | Acc: (68.00%) (6882/10000)
percent tensor([0.5361, 0.4639], device='cuda:0')
percent tensor([0.4915, 0.5085], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5328, 0.4672], device='cuda:0')
percent tensor([0.4911, 0.5089], device='cuda:0')
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.5134, 0.4866], device='cuda:0')
percent tensor([0.9848, 0.0152], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.1948, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.3374, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(777.7216, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1532.8204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.5517, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2179.9036, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4313.9556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1441.5154, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6095.8286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12161.9795, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4050.8940, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17203.4688, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7670) |  Loss2: (0.0000) | Acc: (67.00%) (86/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7880) |  Loss2: (0.0000) | Acc: (71.00%) (1009/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7717) |  Loss2: (0.0000) | Acc: (72.00%) (1950/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7927) |  Loss2: (0.0000) | Acc: (71.00%) (2844/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7921) |  Loss2: (0.0000) | Acc: (71.00%) (3771/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (71.00%) (4690/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (71.00%) (5602/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7860) |  Loss2: (0.0000) | Acc: (71.00%) (6533/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7758) |  Loss2: (0.0000) | Acc: (72.00%) (7487/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7749) |  Loss2: (0.0000) | Acc: (72.00%) (8399/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7698) |  Loss2: (0.0000) | Acc: (72.00%) (9345/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7732) |  Loss2: (0.0000) | Acc: (72.00%) (10259/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7702) |  Loss2: (0.0000) | Acc: (72.00%) (11202/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7700) |  Loss2: (0.0000) | Acc: (72.00%) (12145/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7713) |  Loss2: (0.0000) | Acc: (72.00%) (13084/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7700) |  Loss2: (0.0000) | Acc: (72.00%) (14015/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7697) |  Loss2: (0.0000) | Acc: (72.00%) (14944/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7716) |  Loss2: (0.0000) | Acc: (72.00%) (15852/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7720) |  Loss2: (0.0000) | Acc: (72.00%) (16779/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7710) |  Loss2: (0.0000) | Acc: (72.00%) (17716/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7708) |  Loss2: (0.0000) | Acc: (72.00%) (18637/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7705) |  Loss2: (0.0000) | Acc: (72.00%) (19571/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7685) |  Loss2: (0.0000) | Acc: (72.00%) (20530/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7669) |  Loss2: (0.0000) | Acc: (72.00%) (21471/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7653) |  Loss2: (0.0000) | Acc: (72.00%) (22405/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7641) |  Loss2: (0.0000) | Acc: (72.00%) (23367/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7637) |  Loss2: (0.0000) | Acc: (72.00%) (24309/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7630) |  Loss2: (0.0000) | Acc: (72.00%) (25266/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7612) |  Loss2: (0.0000) | Acc: (72.00%) (26239/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7606) |  Loss2: (0.0000) | Acc: (73.00%) (27193/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7598) |  Loss2: (0.0000) | Acc: (73.00%) (28160/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7603) |  Loss2: (0.0000) | Acc: (73.00%) (29102/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7602) |  Loss2: (0.0000) | Acc: (73.00%) (30057/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7602) |  Loss2: (0.0000) | Acc: (73.00%) (30998/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7590) |  Loss2: (0.0000) | Acc: (73.00%) (31951/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7582) |  Loss2: (0.0000) | Acc: (73.00%) (32893/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7581) |  Loss2: (0.0000) | Acc: (73.00%) (33829/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7564) |  Loss2: (0.0000) | Acc: (73.00%) (34772/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7560) |  Loss2: (0.0000) | Acc: (73.00%) (35710/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7553) |  Loss2: (0.0000) | Acc: (73.00%) (36625/50000)
# TEST : Loss: (0.9149) | Acc: (68.00%) (6883/10000)
percent tensor([0.5352, 0.4648], device='cuda:0')
percent tensor([0.4914, 0.5086], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.4904, 0.5096], device='cuda:0')
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.9886, 0.0114], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7419) |  Loss2: (0.0000) | Acc: (72.00%) (1021/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7100) |  Loss2: (0.0000) | Acc: (74.00%) (1997/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7134) |  Loss2: (0.0000) | Acc: (74.00%) (2952/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7193) |  Loss2: (0.0000) | Acc: (74.00%) (3918/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7256) |  Loss2: (0.0000) | Acc: (74.00%) (4840/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7289) |  Loss2: (0.0000) | Acc: (74.00%) (5787/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7294) |  Loss2: (0.0000) | Acc: (74.00%) (6746/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7237) |  Loss2: (0.0000) | Acc: (74.00%) (7713/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7233) |  Loss2: (0.0000) | Acc: (74.00%) (8665/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7199) |  Loss2: (0.0000) | Acc: (74.00%) (9635/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7219) |  Loss2: (0.0000) | Acc: (74.00%) (10571/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7196) |  Loss2: (0.0000) | Acc: (74.00%) (11531/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7188) |  Loss2: (0.0000) | Acc: (74.00%) (12492/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7231) |  Loss2: (0.0000) | Acc: (74.00%) (13435/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7218) |  Loss2: (0.0000) | Acc: (74.00%) (14401/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7212) |  Loss2: (0.0000) | Acc: (74.00%) (15365/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7215) |  Loss2: (0.0000) | Acc: (74.00%) (16316/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7211) |  Loss2: (0.0000) | Acc: (74.00%) (17277/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (18240/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (19193/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7214) |  Loss2: (0.0000) | Acc: (74.00%) (20148/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7210) |  Loss2: (0.0000) | Acc: (74.00%) (21093/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7215) |  Loss2: (0.0000) | Acc: (74.00%) (22037/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7203) |  Loss2: (0.0000) | Acc: (74.00%) (22998/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (23959/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7206) |  Loss2: (0.0000) | Acc: (74.00%) (24933/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7202) |  Loss2: (0.0000) | Acc: (74.00%) (25866/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7186) |  Loss2: (0.0000) | Acc: (74.00%) (26852/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7184) |  Loss2: (0.0000) | Acc: (74.00%) (27829/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7182) |  Loss2: (0.0000) | Acc: (74.00%) (28769/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7181) |  Loss2: (0.0000) | Acc: (74.00%) (29736/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7177) |  Loss2: (0.0000) | Acc: (74.00%) (30695/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7174) |  Loss2: (0.0000) | Acc: (74.00%) (31648/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7170) |  Loss2: (0.0000) | Acc: (74.00%) (32620/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7171) |  Loss2: (0.0000) | Acc: (74.00%) (33582/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7160) |  Loss2: (0.0000) | Acc: (74.00%) (34565/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7144) |  Loss2: (0.0000) | Acc: (74.00%) (35547/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7145) |  Loss2: (0.0000) | Acc: (74.00%) (36504/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7135) |  Loss2: (0.0000) | Acc: (74.00%) (37438/50000)
# TEST : Loss: (0.7728) | Acc: (72.00%) (7281/10000)
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.4914, 0.5086], device='cuda:0')
percent tensor([0.5170, 0.4830], device='cuda:0')
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.9889, 0.0111], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.7150) |  Loss2: (0.0000) | Acc: (76.00%) (1074/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.7159) |  Loss2: (0.0000) | Acc: (76.00%) (2047/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.7046) |  Loss2: (0.0000) | Acc: (75.00%) (3000/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6959) |  Loss2: (0.0000) | Acc: (75.00%) (3986/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.7004) |  Loss2: (0.0000) | Acc: (75.00%) (4937/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.7053) |  Loss2: (0.0000) | Acc: (75.00%) (5889/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.7024) |  Loss2: (0.0000) | Acc: (75.00%) (6856/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6983) |  Loss2: (0.0000) | Acc: (75.00%) (7854/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6994) |  Loss2: (0.0000) | Acc: (75.00%) (8802/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6976) |  Loss2: (0.0000) | Acc: (75.00%) (9788/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (10742/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6959) |  Loss2: (0.0000) | Acc: (75.00%) (11736/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6949) |  Loss2: (0.0000) | Acc: (75.00%) (12708/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (13663/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6942) |  Loss2: (0.0000) | Acc: (75.00%) (14636/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6912) |  Loss2: (0.0000) | Acc: (75.00%) (15615/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (75.00%) (16593/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6892) |  Loss2: (0.0000) | Acc: (75.00%) (17570/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6874) |  Loss2: (0.0000) | Acc: (75.00%) (18565/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6887) |  Loss2: (0.0000) | Acc: (75.00%) (19518/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6874) |  Loss2: (0.0000) | Acc: (75.00%) (20503/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6886) |  Loss2: (0.0000) | Acc: (75.00%) (21458/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6860) |  Loss2: (0.0000) | Acc: (75.00%) (22454/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6863) |  Loss2: (0.0000) | Acc: (75.00%) (23438/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6862) |  Loss2: (0.0000) | Acc: (75.00%) (24414/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6852) |  Loss2: (0.0000) | Acc: (76.00%) (25399/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6838) |  Loss2: (0.0000) | Acc: (76.00%) (26382/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6851) |  Loss2: (0.0000) | Acc: (76.00%) (27337/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6845) |  Loss2: (0.0000) | Acc: (76.00%) (28319/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6829) |  Loss2: (0.0000) | Acc: (76.00%) (29312/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6816) |  Loss2: (0.0000) | Acc: (76.00%) (30315/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6805) |  Loss2: (0.0000) | Acc: (76.00%) (31301/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6799) |  Loss2: (0.0000) | Acc: (76.00%) (32289/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6782) |  Loss2: (0.0000) | Acc: (76.00%) (33299/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6783) |  Loss2: (0.0000) | Acc: (76.00%) (34247/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6780) |  Loss2: (0.0000) | Acc: (76.00%) (35234/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6767) |  Loss2: (0.0000) | Acc: (76.00%) (36221/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6755) |  Loss2: (0.0000) | Acc: (76.00%) (37225/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6753) |  Loss2: (0.0000) | Acc: (76.00%) (38172/50000)
# TEST : Loss: (0.8123) | Acc: (71.00%) (7151/10000)
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.4917, 0.5083], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.4910, 0.5090], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.9889, 0.0111], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6031) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6818) |  Loss2: (0.0000) | Acc: (76.00%) (1073/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6665) |  Loss2: (0.0000) | Acc: (76.00%) (2058/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6606) |  Loss2: (0.0000) | Acc: (76.00%) (3049/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6670) |  Loss2: (0.0000) | Acc: (76.00%) (4007/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6650) |  Loss2: (0.0000) | Acc: (76.00%) (4977/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6645) |  Loss2: (0.0000) | Acc: (76.00%) (5981/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6581) |  Loss2: (0.0000) | Acc: (76.00%) (6985/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6620) |  Loss2: (0.0000) | Acc: (76.00%) (7940/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6660) |  Loss2: (0.0000) | Acc: (76.00%) (8917/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6630) |  Loss2: (0.0000) | Acc: (76.00%) (9924/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (76.00%) (10925/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6589) |  Loss2: (0.0000) | Acc: (77.00%) (11932/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6591) |  Loss2: (0.0000) | Acc: (77.00%) (12915/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6548) |  Loss2: (0.0000) | Acc: (77.00%) (13925/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6535) |  Loss2: (0.0000) | Acc: (77.00%) (14930/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (15925/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6531) |  Loss2: (0.0000) | Acc: (77.00%) (16908/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6516) |  Loss2: (0.0000) | Acc: (77.00%) (17899/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6521) |  Loss2: (0.0000) | Acc: (77.00%) (18886/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6513) |  Loss2: (0.0000) | Acc: (77.00%) (19871/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (20865/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6502) |  Loss2: (0.0000) | Acc: (77.00%) (21864/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6508) |  Loss2: (0.0000) | Acc: (77.00%) (22841/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6516) |  Loss2: (0.0000) | Acc: (77.00%) (23813/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6507) |  Loss2: (0.0000) | Acc: (77.00%) (24806/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6482) |  Loss2: (0.0000) | Acc: (77.00%) (25830/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6461) |  Loss2: (0.0000) | Acc: (77.00%) (26839/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6453) |  Loss2: (0.0000) | Acc: (77.00%) (27839/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6449) |  Loss2: (0.0000) | Acc: (77.00%) (28843/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6462) |  Loss2: (0.0000) | Acc: (77.00%) (29802/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6447) |  Loss2: (0.0000) | Acc: (77.00%) (30820/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (31823/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (32810/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (33828/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (34815/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6440) |  Loss2: (0.0000) | Acc: (77.00%) (35812/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (36803/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (37806/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6426) |  Loss2: (0.0000) | Acc: (77.00%) (38795/50000)
# TEST : Loss: (0.8858) | Acc: (70.00%) (7098/10000)
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.4915, 0.5085], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5329, 0.4671], device='cuda:0')
percent tensor([0.4907, 0.5093], device='cuda:0')
percent tensor([0.5704, 0.4296], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.9894, 0.0106], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6810) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (76.00%) (1082/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6637) |  Loss2: (0.0000) | Acc: (76.00%) (2055/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6765) |  Loss2: (0.0000) | Acc: (76.00%) (3021/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.6879) |  Loss2: (0.0000) | Acc: (76.00%) (3989/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.7000) |  Loss2: (0.0000) | Acc: (75.00%) (4936/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.7019) |  Loss2: (0.0000) | Acc: (75.00%) (5896/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.7066) |  Loss2: (0.0000) | Acc: (75.00%) (6849/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.7062) |  Loss2: (0.0000) | Acc: (75.00%) (7824/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7088) |  Loss2: (0.0000) | Acc: (75.00%) (8764/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7096) |  Loss2: (0.0000) | Acc: (75.00%) (9714/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7142) |  Loss2: (0.0000) | Acc: (75.00%) (10663/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7146) |  Loss2: (0.0000) | Acc: (75.00%) (11625/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7124) |  Loss2: (0.0000) | Acc: (75.00%) (12599/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7132) |  Loss2: (0.0000) | Acc: (75.00%) (13555/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7103) |  Loss2: (0.0000) | Acc: (75.00%) (14532/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7130) |  Loss2: (0.0000) | Acc: (75.00%) (15474/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7144) |  Loss2: (0.0000) | Acc: (75.00%) (16423/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7108) |  Loss2: (0.0000) | Acc: (75.00%) (17422/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7113) |  Loss2: (0.0000) | Acc: (75.00%) (18363/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7113) |  Loss2: (0.0000) | Acc: (75.00%) (19318/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7109) |  Loss2: (0.0000) | Acc: (75.00%) (20278/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7083) |  Loss2: (0.0000) | Acc: (75.00%) (21277/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.7070) |  Loss2: (0.0000) | Acc: (75.00%) (22242/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.7063) |  Loss2: (0.0000) | Acc: (75.00%) (23224/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.7073) |  Loss2: (0.0000) | Acc: (75.00%) (24172/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.7081) |  Loss2: (0.0000) | Acc: (75.00%) (25122/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.7068) |  Loss2: (0.0000) | Acc: (75.00%) (26093/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.7054) |  Loss2: (0.0000) | Acc: (75.00%) (27070/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.7052) |  Loss2: (0.0000) | Acc: (75.00%) (28034/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.7053) |  Loss2: (0.0000) | Acc: (75.00%) (28999/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.7042) |  Loss2: (0.0000) | Acc: (75.00%) (29971/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.7027) |  Loss2: (0.0000) | Acc: (75.00%) (30965/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (31946/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.7008) |  Loss2: (0.0000) | Acc: (75.00%) (32931/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.7003) |  Loss2: (0.0000) | Acc: (75.00%) (33925/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6990) |  Loss2: (0.0000) | Acc: (75.00%) (34920/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6990) |  Loss2: (0.0000) | Acc: (75.00%) (35883/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (75.00%) (36857/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (75.00%) (37775/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.7038) | Acc: (75.00%) (7522/10000)
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.4946, 0.5054], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.4946, 0.5054], device='cuda:0')
percent tensor([0.5785, 0.4215], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.9899, 0.0101], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.7250) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.6937) |  Loss2: (0.0000) | Acc: (75.00%) (1066/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6625) |  Loss2: (0.0000) | Acc: (76.00%) (2065/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6578) |  Loss2: (0.0000) | Acc: (76.00%) (3055/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (4051/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6558) |  Loss2: (0.0000) | Acc: (77.00%) (5043/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6609) |  Loss2: (0.0000) | Acc: (77.00%) (6031/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6638) |  Loss2: (0.0000) | Acc: (77.00%) (7000/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6654) |  Loss2: (0.0000) | Acc: (77.00%) (7990/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6678) |  Loss2: (0.0000) | Acc: (76.00%) (8959/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6616) |  Loss2: (0.0000) | Acc: (77.00%) (9976/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6633) |  Loss2: (0.0000) | Acc: (76.00%) (10934/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6624) |  Loss2: (0.0000) | Acc: (76.00%) (11896/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6592) |  Loss2: (0.0000) | Acc: (76.00%) (12911/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6580) |  Loss2: (0.0000) | Acc: (77.00%) (13897/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6563) |  Loss2: (0.0000) | Acc: (77.00%) (14890/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6571) |  Loss2: (0.0000) | Acc: (77.00%) (15882/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6588) |  Loss2: (0.0000) | Acc: (76.00%) (16843/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6579) |  Loss2: (0.0000) | Acc: (77.00%) (17847/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6548) |  Loss2: (0.0000) | Acc: (77.00%) (18853/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6553) |  Loss2: (0.0000) | Acc: (77.00%) (19822/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6553) |  Loss2: (0.0000) | Acc: (77.00%) (20813/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6554) |  Loss2: (0.0000) | Acc: (77.00%) (21783/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6533) |  Loss2: (0.0000) | Acc: (77.00%) (22776/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6517) |  Loss2: (0.0000) | Acc: (77.00%) (23790/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (24791/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6492) |  Loss2: (0.0000) | Acc: (77.00%) (25806/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6509) |  Loss2: (0.0000) | Acc: (77.00%) (26782/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6511) |  Loss2: (0.0000) | Acc: (77.00%) (27764/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6515) |  Loss2: (0.0000) | Acc: (77.00%) (28745/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6508) |  Loss2: (0.0000) | Acc: (77.00%) (29759/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6507) |  Loss2: (0.0000) | Acc: (77.00%) (30734/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6507) |  Loss2: (0.0000) | Acc: (77.00%) (31724/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6504) |  Loss2: (0.0000) | Acc: (77.00%) (32714/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6493) |  Loss2: (0.0000) | Acc: (77.00%) (33726/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (34715/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (35694/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6495) |  Loss2: (0.0000) | Acc: (77.00%) (36687/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6505) |  Loss2: (0.0000) | Acc: (77.00%) (37657/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6499) |  Loss2: (0.0000) | Acc: (77.00%) (38617/50000)
# TEST : Loss: (0.6733) | Acc: (76.00%) (7613/10000)
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.4986, 0.5014], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.5008, 0.4992], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.7061) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (77.00%) (1098/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (2088/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6359) |  Loss2: (0.0000) | Acc: (77.00%) (3087/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (4062/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6286) |  Loss2: (0.0000) | Acc: (77.00%) (5082/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (6068/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (7059/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (77.00%) (8054/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (9035/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (10033/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (11007/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6454) |  Loss2: (0.0000) | Acc: (77.00%) (12002/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6441) |  Loss2: (0.0000) | Acc: (77.00%) (13011/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6416) |  Loss2: (0.0000) | Acc: (77.00%) (14017/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6421) |  Loss2: (0.0000) | Acc: (77.00%) (15012/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (15978/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (16973/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6423) |  Loss2: (0.0000) | Acc: (77.00%) (17979/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (77.00%) (18968/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (19982/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (20959/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6410) |  Loss2: (0.0000) | Acc: (77.00%) (21965/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (77.00%) (22974/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (77.00%) (23982/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6378) |  Loss2: (0.0000) | Acc: (77.00%) (24989/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (25988/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (26987/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (77.00%) (27979/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (28983/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (29982/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (30971/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6386) |  Loss2: (0.0000) | Acc: (77.00%) (31947/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (77.00%) (32978/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6375) |  Loss2: (0.0000) | Acc: (77.00%) (33985/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6378) |  Loss2: (0.0000) | Acc: (77.00%) (34979/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6373) |  Loss2: (0.0000) | Acc: (77.00%) (35982/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6358) |  Loss2: (0.0000) | Acc: (77.00%) (37006/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6349) |  Loss2: (0.0000) | Acc: (77.00%) (38017/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6346) |  Loss2: (0.0000) | Acc: (77.00%) (38978/50000)
# TEST : Loss: (0.6600) | Acc: (76.00%) (7660/10000)
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5124, 0.4876], device='cuda:0')
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.9944, 0.0056], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.5871) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.5964) |  Loss2: (0.0000) | Acc: (78.00%) (1111/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (79.00%) (2132/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (79.00%) (3136/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (4120/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6260) |  Loss2: (0.0000) | Acc: (78.00%) (5133/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6286) |  Loss2: (0.0000) | Acc: (78.00%) (6132/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6239) |  Loss2: (0.0000) | Acc: (78.00%) (7145/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6258) |  Loss2: (0.0000) | Acc: (78.00%) (8122/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (78.00%) (9105/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (77.00%) (10079/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6290) |  Loss2: (0.0000) | Acc: (77.00%) (11070/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (77.00%) (12059/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6325) |  Loss2: (0.0000) | Acc: (77.00%) (13059/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (77.00%) (14051/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (15015/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (77.00%) (16011/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6329) |  Loss2: (0.0000) | Acc: (77.00%) (17026/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (77.00%) (18056/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (77.00%) (19052/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (77.00%) (20048/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (78.00%) (21085/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6268) |  Loss2: (0.0000) | Acc: (78.00%) (22107/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6263) |  Loss2: (0.0000) | Acc: (78.00%) (23117/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6269) |  Loss2: (0.0000) | Acc: (78.00%) (24115/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6282) |  Loss2: (0.0000) | Acc: (78.00%) (25105/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6277) |  Loss2: (0.0000) | Acc: (78.00%) (26118/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6275) |  Loss2: (0.0000) | Acc: (78.00%) (27117/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6273) |  Loss2: (0.0000) | Acc: (78.00%) (28134/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6276) |  Loss2: (0.0000) | Acc: (78.00%) (29115/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6268) |  Loss2: (0.0000) | Acc: (78.00%) (30108/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6251) |  Loss2: (0.0000) | Acc: (78.00%) (31133/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6247) |  Loss2: (0.0000) | Acc: (78.00%) (32146/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6256) |  Loss2: (0.0000) | Acc: (78.00%) (33151/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (34159/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (35175/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (78.00%) (36173/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (78.00%) (37208/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6226) |  Loss2: (0.0000) | Acc: (78.00%) (38231/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6221) |  Loss2: (0.0000) | Acc: (78.00%) (39189/50000)
# TEST : Loss: (0.6510) | Acc: (76.00%) (7692/10000)
percent tensor([0.5583, 0.4417], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5196, 0.4804], device='cuda:0')
percent tensor([0.5073, 0.4927], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.5122, 0.4878], device='cuda:0')
percent tensor([0.9957, 0.0043], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5493) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6079) |  Loss2: (0.0000) | Acc: (78.00%) (1106/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6098) |  Loss2: (0.0000) | Acc: (78.00%) (2119/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (3126/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (78.00%) (4128/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6283) |  Loss2: (0.0000) | Acc: (78.00%) (5117/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6302) |  Loss2: (0.0000) | Acc: (78.00%) (6123/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (7133/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6273) |  Loss2: (0.0000) | Acc: (78.00%) (8142/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6234) |  Loss2: (0.0000) | Acc: (78.00%) (9157/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (10179/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (11209/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6198) |  Loss2: (0.0000) | Acc: (78.00%) (12209/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6224) |  Loss2: (0.0000) | Acc: (78.00%) (13197/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6227) |  Loss2: (0.0000) | Acc: (78.00%) (14196/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6229) |  Loss2: (0.0000) | Acc: (78.00%) (15207/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6212) |  Loss2: (0.0000) | Acc: (78.00%) (16216/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6218) |  Loss2: (0.0000) | Acc: (78.00%) (17201/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (78.00%) (18207/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6203) |  Loss2: (0.0000) | Acc: (78.00%) (19212/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (78.00%) (20207/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (21208/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6225) |  Loss2: (0.0000) | Acc: (78.00%) (22192/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6218) |  Loss2: (0.0000) | Acc: (78.00%) (23200/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6184) |  Loss2: (0.0000) | Acc: (78.00%) (24240/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6181) |  Loss2: (0.0000) | Acc: (78.00%) (25235/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6198) |  Loss2: (0.0000) | Acc: (78.00%) (26231/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6193) |  Loss2: (0.0000) | Acc: (78.00%) (27245/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6189) |  Loss2: (0.0000) | Acc: (78.00%) (28268/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6196) |  Loss2: (0.0000) | Acc: (78.00%) (29249/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6202) |  Loss2: (0.0000) | Acc: (78.00%) (30256/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6219) |  Loss2: (0.0000) | Acc: (78.00%) (31235/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6207) |  Loss2: (0.0000) | Acc: (78.00%) (32261/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6198) |  Loss2: (0.0000) | Acc: (78.00%) (33280/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6203) |  Loss2: (0.0000) | Acc: (78.00%) (34272/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6198) |  Loss2: (0.0000) | Acc: (78.00%) (35279/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (36303/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (37339/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6167) |  Loss2: (0.0000) | Acc: (78.00%) (38349/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (39334/50000)
# TEST : Loss: (0.6455) | Acc: (77.00%) (7730/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5218, 0.4782], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.6103, 0.3897], device='cuda:0')
percent tensor([0.5185, 0.4815], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (1122/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (79.00%) (2124/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (3095/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6526) |  Loss2: (0.0000) | Acc: (77.00%) (4073/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6489) |  Loss2: (0.0000) | Acc: (77.00%) (5074/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6452) |  Loss2: (0.0000) | Acc: (77.00%) (6074/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (7070/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6457) |  Loss2: (0.0000) | Acc: (77.00%) (8058/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6436) |  Loss2: (0.0000) | Acc: (77.00%) (9052/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6367) |  Loss2: (0.0000) | Acc: (77.00%) (10078/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (77.00%) (11081/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (78.00%) (12110/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6286) |  Loss2: (0.0000) | Acc: (78.00%) (13118/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6272) |  Loss2: (0.0000) | Acc: (78.00%) (14132/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (15129/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6263) |  Loss2: (0.0000) | Acc: (78.00%) (16143/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6251) |  Loss2: (0.0000) | Acc: (78.00%) (17145/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6220) |  Loss2: (0.0000) | Acc: (78.00%) (18167/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6223) |  Loss2: (0.0000) | Acc: (78.00%) (19177/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (20203/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (21208/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (22209/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (23217/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6188) |  Loss2: (0.0000) | Acc: (78.00%) (24239/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6186) |  Loss2: (0.0000) | Acc: (78.00%) (25246/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6181) |  Loss2: (0.0000) | Acc: (78.00%) (26256/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (27266/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6186) |  Loss2: (0.0000) | Acc: (78.00%) (28259/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (29287/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (30277/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (31308/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6173) |  Loss2: (0.0000) | Acc: (78.00%) (32299/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6181) |  Loss2: (0.0000) | Acc: (78.00%) (33295/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6179) |  Loss2: (0.0000) | Acc: (78.00%) (34307/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6175) |  Loss2: (0.0000) | Acc: (78.00%) (35295/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (36312/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6164) |  Loss2: (0.0000) | Acc: (78.00%) (37316/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6157) |  Loss2: (0.0000) | Acc: (78.00%) (38329/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6162) |  Loss2: (0.0000) | Acc: (78.00%) (39299/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.6897) | Acc: (76.00%) (7633/10000)
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5222, 0.4778], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.5187, 0.4813], device='cuda:0')
percent tensor([0.9964, 0.0036], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.8339, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(790.1340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.8483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1532.2013, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.9851, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2189.6133, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4305.4629, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1436.5563, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6090.9111, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12116.1680, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4035.2932, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17127.0840, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.6105) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5940) |  Loss2: (0.0000) | Acc: (79.00%) (1124/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.6006) |  Loss2: (0.0000) | Acc: (79.00%) (2127/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (3174/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5941) |  Loss2: (0.0000) | Acc: (79.00%) (4198/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5949) |  Loss2: (0.0000) | Acc: (79.00%) (5213/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (6220/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5910) |  Loss2: (0.0000) | Acc: (79.00%) (7252/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (80.00%) (8303/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5875) |  Loss2: (0.0000) | Acc: (79.00%) (9307/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5868) |  Loss2: (0.0000) | Acc: (79.00%) (10319/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5868) |  Loss2: (0.0000) | Acc: (79.00%) (11327/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5879) |  Loss2: (0.0000) | Acc: (79.00%) (12334/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5895) |  Loss2: (0.0000) | Acc: (79.00%) (13339/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5867) |  Loss2: (0.0000) | Acc: (79.00%) (14376/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5868) |  Loss2: (0.0000) | Acc: (79.00%) (15390/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5878) |  Loss2: (0.0000) | Acc: (79.00%) (16411/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (17398/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5905) |  Loss2: (0.0000) | Acc: (79.00%) (18418/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5899) |  Loss2: (0.0000) | Acc: (79.00%) (19427/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5887) |  Loss2: (0.0000) | Acc: (79.00%) (20456/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (21487/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5866) |  Loss2: (0.0000) | Acc: (79.00%) (22521/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (23546/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5856) |  Loss2: (0.0000) | Acc: (79.00%) (24568/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5867) |  Loss2: (0.0000) | Acc: (79.00%) (25576/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5884) |  Loss2: (0.0000) | Acc: (79.00%) (26581/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (27609/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (28621/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5884) |  Loss2: (0.0000) | Acc: (79.00%) (29639/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5877) |  Loss2: (0.0000) | Acc: (79.00%) (30665/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5885) |  Loss2: (0.0000) | Acc: (79.00%) (31662/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5887) |  Loss2: (0.0000) | Acc: (79.00%) (32676/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5874) |  Loss2: (0.0000) | Acc: (79.00%) (33716/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5874) |  Loss2: (0.0000) | Acc: (79.00%) (34727/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5881) |  Loss2: (0.0000) | Acc: (79.00%) (35736/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (36737/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5883) |  Loss2: (0.0000) | Acc: (79.00%) (37762/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (38783/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5864) |  Loss2: (0.0000) | Acc: (79.00%) (39803/50000)
# TEST : Loss: (0.7132) | Acc: (75.00%) (7536/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5215, 0.4785], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.6116, 0.3884], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.9965, 0.0035], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.5356) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5651) |  Loss2: (0.0000) | Acc: (79.00%) (1119/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.6090) |  Loss2: (0.0000) | Acc: (77.00%) (2096/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5944) |  Loss2: (0.0000) | Acc: (78.00%) (3110/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5820) |  Loss2: (0.0000) | Acc: (79.00%) (4158/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5748) |  Loss2: (0.0000) | Acc: (79.00%) (5188/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5679) |  Loss2: (0.0000) | Acc: (79.00%) (6226/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (7274/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (8308/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (9327/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5648) |  Loss2: (0.0000) | Acc: (79.00%) (10334/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5613) |  Loss2: (0.0000) | Acc: (80.00%) (11372/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5592) |  Loss2: (0.0000) | Acc: (80.00%) (12406/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (13438/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5578) |  Loss2: (0.0000) | Acc: (80.00%) (14462/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5577) |  Loss2: (0.0000) | Acc: (80.00%) (15489/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (16528/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (17547/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (18578/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5614) |  Loss2: (0.0000) | Acc: (80.00%) (19582/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5621) |  Loss2: (0.0000) | Acc: (80.00%) (20607/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5611) |  Loss2: (0.0000) | Acc: (80.00%) (21649/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5621) |  Loss2: (0.0000) | Acc: (80.00%) (22670/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5601) |  Loss2: (0.0000) | Acc: (80.00%) (23717/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (24750/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5589) |  Loss2: (0.0000) | Acc: (80.00%) (25793/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (26805/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5601) |  Loss2: (0.0000) | Acc: (80.00%) (27820/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (28839/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5612) |  Loss2: (0.0000) | Acc: (80.00%) (29866/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5612) |  Loss2: (0.0000) | Acc: (80.00%) (30911/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5616) |  Loss2: (0.0000) | Acc: (80.00%) (31933/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5618) |  Loss2: (0.0000) | Acc: (80.00%) (32964/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (33982/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (35013/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (36046/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5651) |  Loss2: (0.0000) | Acc: (80.00%) (37057/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5653) |  Loss2: (0.0000) | Acc: (80.00%) (38096/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5649) |  Loss2: (0.0000) | Acc: (80.00%) (39134/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (40147/50000)
# TEST : Loss: (0.6670) | Acc: (77.00%) (7753/10000)
percent tensor([0.5558, 0.4442], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5147, 0.4853], device='cuda:0')
percent tensor([0.5225, 0.4775], device='cuda:0')
percent tensor([0.5110, 0.4889], device='cuda:0')
percent tensor([0.6147, 0.3853], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.9964, 0.0036], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.5393) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5567) |  Loss2: (0.0000) | Acc: (81.00%) (1141/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5437) |  Loss2: (0.0000) | Acc: (81.00%) (2192/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5474) |  Loss2: (0.0000) | Acc: (81.00%) (3221/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (4280/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (5324/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5314) |  Loss2: (0.0000) | Acc: (81.00%) (6379/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (7429/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (8441/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (9460/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5430) |  Loss2: (0.0000) | Acc: (81.00%) (10494/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5454) |  Loss2: (0.0000) | Acc: (81.00%) (11529/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5474) |  Loss2: (0.0000) | Acc: (81.00%) (12552/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (80.00%) (13574/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5465) |  Loss2: (0.0000) | Acc: (81.00%) (14620/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5490) |  Loss2: (0.0000) | Acc: (80.00%) (15642/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (80.00%) (16669/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5483) |  Loss2: (0.0000) | Acc: (80.00%) (17711/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5490) |  Loss2: (0.0000) | Acc: (80.00%) (18738/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (19757/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5480) |  Loss2: (0.0000) | Acc: (80.00%) (20808/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5485) |  Loss2: (0.0000) | Acc: (80.00%) (21846/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5474) |  Loss2: (0.0000) | Acc: (80.00%) (22884/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5481) |  Loss2: (0.0000) | Acc: (80.00%) (23903/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (80.00%) (24961/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (80.00%) (25987/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (80.00%) (27036/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (80.00%) (28067/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (80.00%) (29105/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5476) |  Loss2: (0.0000) | Acc: (80.00%) (30154/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (31213/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5460) |  Loss2: (0.0000) | Acc: (81.00%) (32246/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (33286/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5457) |  Loss2: (0.0000) | Acc: (80.00%) (34309/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (80.00%) (35351/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (80.00%) (36371/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5459) |  Loss2: (0.0000) | Acc: (80.00%) (37385/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (80.00%) (38390/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5466) |  Loss2: (0.0000) | Acc: (80.00%) (39435/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5462) |  Loss2: (0.0000) | Acc: (80.00%) (40451/50000)
# TEST : Loss: (0.6251) | Acc: (78.00%) (7866/10000)
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5150, 0.4850], device='cuda:0')
percent tensor([0.5226, 0.4774], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.6093, 0.3907], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4877) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (82.00%) (1158/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (83.00%) (2241/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (3282/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (4332/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (5399/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (6454/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5009) |  Loss2: (0.0000) | Acc: (82.00%) (7524/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (8576/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (9642/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (10691/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (11720/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5100) |  Loss2: (0.0000) | Acc: (82.00%) (12780/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5115) |  Loss2: (0.0000) | Acc: (82.00%) (13825/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5129) |  Loss2: (0.0000) | Acc: (82.00%) (14868/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5131) |  Loss2: (0.0000) | Acc: (82.00%) (15913/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5145) |  Loss2: (0.0000) | Acc: (82.00%) (16958/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5167) |  Loss2: (0.0000) | Acc: (82.00%) (17999/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (82.00%) (19028/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (20036/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (21087/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (22164/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (23210/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (82.00%) (24259/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (25312/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (26362/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (27416/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (28456/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (29518/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (82.00%) (30569/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (31631/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (82.00%) (32663/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (33716/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (34743/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (81.00%) (35767/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (81.00%) (36798/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (81.00%) (37859/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (81.00%) (38920/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (81.00%) (39986/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (82.00%) (41013/50000)
# TEST : Loss: (0.5768) | Acc: (80.00%) (8001/10000)
percent tensor([0.5543, 0.4457], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.6104, 0.3896], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.6512) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (1168/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5371) |  Loss2: (0.0000) | Acc: (81.00%) (2178/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (3196/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (4221/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.5516) |  Loss2: (0.0000) | Acc: (80.00%) (5269/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.5580) |  Loss2: (0.0000) | Acc: (80.00%) (6292/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.5606) |  Loss2: (0.0000) | Acc: (80.00%) (7311/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.5640) |  Loss2: (0.0000) | Acc: (80.00%) (8345/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (9372/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.5671) |  Loss2: (0.0000) | Acc: (80.00%) (10385/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (11415/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (12463/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (80.00%) (13492/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (14547/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (15561/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.5574) |  Loss2: (0.0000) | Acc: (80.00%) (16601/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (17637/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.5566) |  Loss2: (0.0000) | Acc: (80.00%) (18661/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.5556) |  Loss2: (0.0000) | Acc: (80.00%) (19711/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (20746/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.5537) |  Loss2: (0.0000) | Acc: (80.00%) (21778/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.5535) |  Loss2: (0.0000) | Acc: (80.00%) (22818/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.5517) |  Loss2: (0.0000) | Acc: (80.00%) (23878/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.5500) |  Loss2: (0.0000) | Acc: (80.00%) (24932/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.5497) |  Loss2: (0.0000) | Acc: (80.00%) (25970/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (80.00%) (27010/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.5488) |  Loss2: (0.0000) | Acc: (80.00%) (28057/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (29076/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (30119/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (80.00%) (31162/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.5489) |  Loss2: (0.0000) | Acc: (80.00%) (32217/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (80.00%) (33269/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (81.00%) (34327/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (35391/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.5444) |  Loss2: (0.0000) | Acc: (81.00%) (36445/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (37506/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (81.00%) (38542/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (81.00%) (39579/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (40569/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.5804) | Acc: (79.00%) (7975/10000)
percent tensor([0.5421, 0.4579], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.6037, 0.3963], device='cuda:0')
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.9968, 0.0032], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5414) |  Loss2: (0.0000) | Acc: (80.00%) (1137/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (2187/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5263) |  Loss2: (0.0000) | Acc: (81.00%) (3240/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (4294/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (81.00%) (5348/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (81.00%) (6397/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (81.00%) (7426/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (81.00%) (8476/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (81.00%) (9546/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5156) |  Loss2: (0.0000) | Acc: (82.00%) (10604/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (11637/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (81.00%) (12688/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5185) |  Loss2: (0.0000) | Acc: (81.00%) (13732/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (81.00%) (14790/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (15852/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (81.00%) (16891/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (17957/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (81.00%) (18996/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (81.00%) (20029/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (81.00%) (21070/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (81.00%) (22111/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (81.00%) (23185/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5164) |  Loss2: (0.0000) | Acc: (81.00%) (24236/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5156) |  Loss2: (0.0000) | Acc: (81.00%) (25290/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (81.00%) (26322/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (81.00%) (27393/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (81.00%) (28421/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (81.00%) (29459/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (81.00%) (30505/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (81.00%) (31539/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (81.00%) (32576/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (81.00%) (33631/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (81.00%) (34696/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5168) |  Loss2: (0.0000) | Acc: (81.00%) (35750/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (81.00%) (36812/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (81.00%) (37884/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (82.00%) (38945/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5150) |  Loss2: (0.0000) | Acc: (82.00%) (39990/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (82.00%) (41020/50000)
# TEST : Loss: (0.5588) | Acc: (80.00%) (8027/10000)
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.5008, 0.4992], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5297, 0.4703], device='cuda:0')
percent tensor([0.5180, 0.4820], device='cuda:0')
percent tensor([0.6057, 0.3943], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.4587) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (83.00%) (2236/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (83.00%) (3299/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (83.00%) (4356/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (5412/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (6475/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (83.00%) (7557/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (8636/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (83.00%) (9679/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (10753/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (83.00%) (11804/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (83.00%) (12871/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.4956) |  Loss2: (0.0000) | Acc: (83.00%) (13925/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.4970) |  Loss2: (0.0000) | Acc: (82.00%) (14968/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.4960) |  Loss2: (0.0000) | Acc: (83.00%) (16044/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.4948) |  Loss2: (0.0000) | Acc: (83.00%) (17107/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (82.00%) (18142/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (19197/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5003) |  Loss2: (0.0000) | Acc: (82.00%) (20247/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (21292/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (22353/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5009) |  Loss2: (0.0000) | Acc: (82.00%) (23432/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (24467/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (25517/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5052) |  Loss2: (0.0000) | Acc: (82.00%) (26562/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (27631/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (28693/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (29751/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (30805/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (31847/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (32911/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (33965/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (35020/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (36075/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (37134/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (38192/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (39231/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (40271/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (41294/50000)
# TEST : Loss: (0.5477) | Acc: (80.00%) (8068/10000)
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.6072, 0.3928], device='cuda:0')
percent tensor([0.5632, 0.4368], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (80.00%) (1133/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (81.00%) (2186/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (81.00%) (3252/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (81.00%) (4282/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (81.00%) (5350/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5136) |  Loss2: (0.0000) | Acc: (81.00%) (6391/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (7476/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (82.00%) (8538/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (9609/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5005) |  Loss2: (0.0000) | Acc: (82.00%) (10657/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (11717/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (12777/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (82.00%) (13860/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (14913/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.4974) |  Loss2: (0.0000) | Acc: (82.00%) (15967/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (17027/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (18081/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.4973) |  Loss2: (0.0000) | Acc: (82.00%) (19149/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.4981) |  Loss2: (0.0000) | Acc: (82.00%) (20201/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (21265/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (82.00%) (22302/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (82.00%) (23350/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (24415/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (25484/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.4991) |  Loss2: (0.0000) | Acc: (82.00%) (26547/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (27576/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5001) |  Loss2: (0.0000) | Acc: (82.00%) (28652/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5003) |  Loss2: (0.0000) | Acc: (82.00%) (29712/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (82.00%) (30780/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (31845/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.4981) |  Loss2: (0.0000) | Acc: (82.00%) (32920/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.4974) |  Loss2: (0.0000) | Acc: (82.00%) (34001/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (35052/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (36116/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (37181/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (38231/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (39310/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (40363/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (41384/50000)
# TEST : Loss: (0.5463) | Acc: (80.00%) (8079/10000)
percent tensor([0.5575, 0.4425], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.6084, 0.3916], device='cuda:0')
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5823) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (82.00%) (1166/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (82.00%) (2225/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (3284/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (82.00%) (4339/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (82.00%) (5400/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (6439/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.4965) |  Loss2: (0.0000) | Acc: (82.00%) (7495/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.4953) |  Loss2: (0.0000) | Acc: (82.00%) (8564/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.4956) |  Loss2: (0.0000) | Acc: (82.00%) (9623/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (10657/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (11718/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (12778/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.4971) |  Loss2: (0.0000) | Acc: (82.00%) (13852/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (82.00%) (14937/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.4915) |  Loss2: (0.0000) | Acc: (82.00%) (16011/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (82.00%) (17059/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.4904) |  Loss2: (0.0000) | Acc: (82.00%) (18157/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (82.00%) (19213/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (82.00%) (20280/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.4903) |  Loss2: (0.0000) | Acc: (83.00%) (21355/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.4917) |  Loss2: (0.0000) | Acc: (82.00%) (22400/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (82.00%) (23459/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (82.00%) (24493/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (82.00%) (25560/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.4910) |  Loss2: (0.0000) | Acc: (82.00%) (26642/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (82.00%) (27727/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (82.00%) (28762/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.4930) |  Loss2: (0.0000) | Acc: (82.00%) (29818/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (82.00%) (30892/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (82.00%) (31941/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (33018/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (34083/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (35156/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (36202/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (37267/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.4934) |  Loss2: (0.0000) | Acc: (82.00%) (38329/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (39392/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (40454/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (41481/50000)
# TEST : Loss: (0.5382) | Acc: (81.00%) (8119/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.6093, 0.3907], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.4696) |  Loss2: (0.0000) | Acc: (83.00%) (2238/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4854) |  Loss2: (0.0000) | Acc: (82.00%) (3289/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (4332/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (5369/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (81.00%) (6400/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (81.00%) (7451/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.5063) |  Loss2: (0.0000) | Acc: (82.00%) (8514/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (9576/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (10621/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (11689/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (12746/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (13805/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (14843/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (15917/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (82.00%) (16963/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.5106) |  Loss2: (0.0000) | Acc: (82.00%) (18007/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (19069/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (20124/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (21191/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (22258/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (23319/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (24351/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (25408/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (26470/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (27522/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (28563/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (29612/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (30670/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (31740/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.5063) |  Loss2: (0.0000) | Acc: (82.00%) (32804/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (33849/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (34896/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (35963/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.5065) |  Loss2: (0.0000) | Acc: (82.00%) (37013/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (38067/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (82.00%) (39140/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (40172/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.5060) |  Loss2: (0.0000) | Acc: (82.00%) (41190/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.6497) | Acc: (78.00%) (7822/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.5266, 0.4734], device='cuda:0')
percent tensor([0.6117, 0.3883], device='cuda:0')
percent tensor([0.5851, 0.4149], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(172.9867, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.4311, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(789.3107, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1531.5833, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.4048, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2199.4636, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4299.1450, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1431.3931, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6093.0757, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12073.7725, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4019.7424, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17054.0547, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (1181/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (82.00%) (2230/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (3301/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4671) |  Loss2: (0.0000) | Acc: (83.00%) (4379/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (83.00%) (5467/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4636) |  Loss2: (0.0000) | Acc: (83.00%) (6543/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4706) |  Loss2: (0.0000) | Acc: (83.00%) (7581/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (8647/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (9709/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4769) |  Loss2: (0.0000) | Acc: (83.00%) (10785/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4775) |  Loss2: (0.0000) | Acc: (83.00%) (11858/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (12917/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4818) |  Loss2: (0.0000) | Acc: (83.00%) (13972/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4814) |  Loss2: (0.0000) | Acc: (83.00%) (15056/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4823) |  Loss2: (0.0000) | Acc: (83.00%) (16114/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (17161/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4850) |  Loss2: (0.0000) | Acc: (83.00%) (18232/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4862) |  Loss2: (0.0000) | Acc: (83.00%) (19273/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (20350/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4847) |  Loss2: (0.0000) | Acc: (83.00%) (21414/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4847) |  Loss2: (0.0000) | Acc: (83.00%) (22483/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4850) |  Loss2: (0.0000) | Acc: (83.00%) (23537/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4866) |  Loss2: (0.0000) | Acc: (83.00%) (24605/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (83.00%) (25659/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4882) |  Loss2: (0.0000) | Acc: (83.00%) (26732/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4872) |  Loss2: (0.0000) | Acc: (83.00%) (27808/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4868) |  Loss2: (0.0000) | Acc: (83.00%) (28876/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4868) |  Loss2: (0.0000) | Acc: (83.00%) (29942/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (83.00%) (31004/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4860) |  Loss2: (0.0000) | Acc: (83.00%) (32058/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (33152/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4842) |  Loss2: (0.0000) | Acc: (83.00%) (34220/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4848) |  Loss2: (0.0000) | Acc: (83.00%) (35283/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (36348/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (37412/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (38464/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4865) |  Loss2: (0.0000) | Acc: (83.00%) (39523/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (83.00%) (40602/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4863) |  Loss2: (0.0000) | Acc: (83.00%) (41620/50000)
# TEST : Loss: (0.6375) | Acc: (78.00%) (7888/10000)
percent tensor([0.5564, 0.4436], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.6100, 0.3900], device='cuda:0')
percent tensor([0.5850, 0.4150], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4703) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4847) |  Loss2: (0.0000) | Acc: (84.00%) (1187/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (2256/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4785) |  Loss2: (0.0000) | Acc: (83.00%) (3333/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (4396/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (84.00%) (5486/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4691) |  Loss2: (0.0000) | Acc: (84.00%) (6578/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (84.00%) (7636/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4726) |  Loss2: (0.0000) | Acc: (83.00%) (8686/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4719) |  Loss2: (0.0000) | Acc: (83.00%) (9770/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (10834/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (11913/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4717) |  Loss2: (0.0000) | Acc: (83.00%) (12985/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4716) |  Loss2: (0.0000) | Acc: (83.00%) (14058/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4703) |  Loss2: (0.0000) | Acc: (83.00%) (15137/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (16228/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4673) |  Loss2: (0.0000) | Acc: (83.00%) (17301/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4676) |  Loss2: (0.0000) | Acc: (83.00%) (18369/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4671) |  Loss2: (0.0000) | Acc: (83.00%) (19441/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4699) |  Loss2: (0.0000) | Acc: (83.00%) (20497/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4701) |  Loss2: (0.0000) | Acc: (83.00%) (21570/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (22636/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4711) |  Loss2: (0.0000) | Acc: (83.00%) (23703/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4704) |  Loss2: (0.0000) | Acc: (83.00%) (24783/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4719) |  Loss2: (0.0000) | Acc: (83.00%) (25843/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (26887/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (27933/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (28998/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4738) |  Loss2: (0.0000) | Acc: (83.00%) (30070/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4730) |  Loss2: (0.0000) | Acc: (83.00%) (31157/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4721) |  Loss2: (0.0000) | Acc: (83.00%) (32237/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4730) |  Loss2: (0.0000) | Acc: (83.00%) (33295/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4733) |  Loss2: (0.0000) | Acc: (83.00%) (34374/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (35466/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4714) |  Loss2: (0.0000) | Acc: (83.00%) (36547/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4714) |  Loss2: (0.0000) | Acc: (83.00%) (37623/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4718) |  Loss2: (0.0000) | Acc: (83.00%) (38688/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (83.00%) (39774/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4707) |  Loss2: (0.0000) | Acc: (83.00%) (40832/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4718) |  Loss2: (0.0000) | Acc: (83.00%) (41845/50000)
# TEST : Loss: (0.5573) | Acc: (80.00%) (8083/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.6077, 0.3923], device='cuda:0')
percent tensor([0.5835, 0.4165], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (85.00%) (2288/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (85.00%) (3375/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (4448/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (5551/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (6603/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (7670/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4448) |  Loss2: (0.0000) | Acc: (84.00%) (8771/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4445) |  Loss2: (0.0000) | Acc: (84.00%) (9848/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (10915/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (11993/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (13073/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (14156/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4482) |  Loss2: (0.0000) | Acc: (84.00%) (15249/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (16320/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (84.00%) (17398/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (18475/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (19563/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4482) |  Loss2: (0.0000) | Acc: (84.00%) (20653/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4499) |  Loss2: (0.0000) | Acc: (84.00%) (21714/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4513) |  Loss2: (0.0000) | Acc: (84.00%) (22792/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4499) |  Loss2: (0.0000) | Acc: (84.00%) (23902/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4509) |  Loss2: (0.0000) | Acc: (84.00%) (24978/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (26094/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4503) |  Loss2: (0.0000) | Acc: (84.00%) (27164/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (28222/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (29305/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (30366/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (31453/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4519) |  Loss2: (0.0000) | Acc: (84.00%) (32530/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (33625/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (34722/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (35788/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (36879/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (37976/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (39048/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (84.00%) (40120/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4524) |  Loss2: (0.0000) | Acc: (84.00%) (41188/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (42212/50000)
# TEST : Loss: (0.5859) | Acc: (80.00%) (8024/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5318, 0.4682], device='cuda:0')
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.6110, 0.3890], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4244) |  Loss2: (0.0000) | Acc: (85.00%) (1209/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (2308/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4211) |  Loss2: (0.0000) | Acc: (85.00%) (4494/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4262) |  Loss2: (0.0000) | Acc: (85.00%) (5573/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (6639/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (7707/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (84.00%) (8810/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (9869/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (84.00%) (10939/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (12016/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (13090/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4395) |  Loss2: (0.0000) | Acc: (84.00%) (14152/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4387) |  Loss2: (0.0000) | Acc: (84.00%) (15247/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (16326/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (17402/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4360) |  Loss2: (0.0000) | Acc: (84.00%) (18497/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (19576/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (84.00%) (20670/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (21742/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (22846/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (84.00%) (23945/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (25047/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (26125/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (27206/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (28304/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (84.00%) (29395/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (30460/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (31541/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4388) |  Loss2: (0.0000) | Acc: (84.00%) (32616/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (33712/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (34808/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (35903/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (36979/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (38063/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (39158/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (40247/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (41332/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (42368/50000)
# TEST : Loss: (0.5445) | Acc: (81.00%) (8166/10000)
percent tensor([0.5559, 0.4441], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.5253, 0.4747], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.5812, 0.4188], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (85.00%) (1203/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4777) |  Loss2: (0.0000) | Acc: (84.00%) (2265/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4930) |  Loss2: (0.0000) | Acc: (83.00%) (3313/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.5140) |  Loss2: (0.0000) | Acc: (82.00%) (4335/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (82.00%) (5364/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.5301) |  Loss2: (0.0000) | Acc: (82.00%) (6403/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (82.00%) (7461/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (8529/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (9593/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (82.00%) (10653/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (11728/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (12795/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (82.00%) (13828/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (14879/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (82.00%) (15952/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (17001/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (18029/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (19088/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (82.00%) (20144/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (21179/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (22264/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (82.00%) (23328/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.5135) |  Loss2: (0.0000) | Acc: (82.00%) (24398/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.5145) |  Loss2: (0.0000) | Acc: (82.00%) (25439/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (82.00%) (26505/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (82.00%) (27578/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.5112) |  Loss2: (0.0000) | Acc: (82.00%) (28637/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (29698/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.5121) |  Loss2: (0.0000) | Acc: (82.00%) (30720/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.5105) |  Loss2: (0.0000) | Acc: (82.00%) (31792/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.5114) |  Loss2: (0.0000) | Acc: (82.00%) (32829/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.5114) |  Loss2: (0.0000) | Acc: (82.00%) (33878/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (34937/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (36011/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (37072/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (38113/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (39178/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.5063) |  Loss2: (0.0000) | Acc: (82.00%) (40255/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (41311/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.5601) | Acc: (81.00%) (8162/10000)
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.5043, 0.4957], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5406, 0.4594], device='cuda:0')
percent tensor([0.5114, 0.4886], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.5829, 0.4171], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.5813) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4684) |  Loss2: (0.0000) | Acc: (83.00%) (1170/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (2248/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4825) |  Loss2: (0.0000) | Acc: (83.00%) (3305/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4897) |  Loss2: (0.0000) | Acc: (82.00%) (4355/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (82.00%) (5416/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (6456/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4898) |  Loss2: (0.0000) | Acc: (82.00%) (7540/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (82.00%) (8603/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (82.00%) (9658/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (82.00%) (10730/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4876) |  Loss2: (0.0000) | Acc: (83.00%) (11801/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (13955/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4799) |  Loss2: (0.0000) | Acc: (83.00%) (15053/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (16113/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (17196/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4794) |  Loss2: (0.0000) | Acc: (83.00%) (18259/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4798) |  Loss2: (0.0000) | Acc: (83.00%) (19335/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (20414/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (21481/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4764) |  Loss2: (0.0000) | Acc: (83.00%) (22572/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (23641/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (24728/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4732) |  Loss2: (0.0000) | Acc: (83.00%) (25803/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4728) |  Loss2: (0.0000) | Acc: (83.00%) (26879/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (27943/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4718) |  Loss2: (0.0000) | Acc: (83.00%) (29027/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (30104/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4716) |  Loss2: (0.0000) | Acc: (83.00%) (31164/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (32238/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4699) |  Loss2: (0.0000) | Acc: (83.00%) (33326/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4689) |  Loss2: (0.0000) | Acc: (83.00%) (34429/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (35498/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4673) |  Loss2: (0.0000) | Acc: (83.00%) (36567/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (83.00%) (37655/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4671) |  Loss2: (0.0000) | Acc: (83.00%) (38728/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4665) |  Loss2: (0.0000) | Acc: (83.00%) (39808/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (40903/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (83.00%) (41942/50000)
# TEST : Loss: (0.5312) | Acc: (82.00%) (8218/10000)
percent tensor([0.5391, 0.4609], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5066, 0.4934], device='cuda:0')
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (1179/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (2258/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (3338/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (4427/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (5493/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (6606/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4457) |  Loss2: (0.0000) | Acc: (84.00%) (7693/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4456) |  Loss2: (0.0000) | Acc: (84.00%) (8777/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (9840/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4490) |  Loss2: (0.0000) | Acc: (84.00%) (10918/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (12007/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (13083/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (14175/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4484) |  Loss2: (0.0000) | Acc: (84.00%) (15238/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4463) |  Loss2: (0.0000) | Acc: (84.00%) (16335/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (17423/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4471) |  Loss2: (0.0000) | Acc: (84.00%) (18488/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (19567/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (20644/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4479) |  Loss2: (0.0000) | Acc: (84.00%) (21723/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (22817/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4468) |  Loss2: (0.0000) | Acc: (84.00%) (23887/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (24965/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (26033/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (27100/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (28201/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (29289/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (30354/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (31426/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (32492/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4515) |  Loss2: (0.0000) | Acc: (84.00%) (33579/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (34652/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (35749/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4490) |  Loss2: (0.0000) | Acc: (84.00%) (36851/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (37922/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (38994/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (40094/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (41177/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (42227/50000)
# TEST : Loss: (0.5169) | Acc: (82.00%) (8251/10000)
percent tensor([0.5452, 0.4548], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5428, 0.4572], device='cuda:0')
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.6054, 0.3946], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4708) |  Loss2: (0.0000) | Acc: (84.00%) (1184/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4648) |  Loss2: (0.0000) | Acc: (84.00%) (2260/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4499) |  Loss2: (0.0000) | Acc: (84.00%) (3358/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4548) |  Loss2: (0.0000) | Acc: (84.00%) (4426/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4545) |  Loss2: (0.0000) | Acc: (84.00%) (5508/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (6588/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4469) |  Loss2: (0.0000) | Acc: (84.00%) (7691/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (8788/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (9855/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (10932/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (12025/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (13090/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (14202/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (15284/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (16384/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (17458/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (18553/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (19646/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (20744/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (21835/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (22923/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (23998/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (25067/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (26126/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (27225/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (28294/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (29389/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (30480/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (31576/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4397) |  Loss2: (0.0000) | Acc: (84.00%) (32668/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4397) |  Loss2: (0.0000) | Acc: (84.00%) (33739/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (34817/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (35912/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4396) |  Loss2: (0.0000) | Acc: (84.00%) (36996/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4397) |  Loss2: (0.0000) | Acc: (84.00%) (38087/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4395) |  Loss2: (0.0000) | Acc: (84.00%) (39169/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (40256/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4395) |  Loss2: (0.0000) | Acc: (84.00%) (41333/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (42385/50000)
# TEST : Loss: (0.5072) | Acc: (82.00%) (8273/10000)
percent tensor([0.5480, 0.4520], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5452, 0.4548], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.6229, 0.3771], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (1190/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (3378/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (85.00%) (4482/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (5563/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (85.00%) (6669/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (85.00%) (7762/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (85.00%) (8846/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (85.00%) (9942/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (85.00%) (11009/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (85.00%) (12099/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (13164/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4387) |  Loss2: (0.0000) | Acc: (85.00%) (14256/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (85.00%) (15342/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (85.00%) (16443/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4346) |  Loss2: (0.0000) | Acc: (85.00%) (17548/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (85.00%) (18610/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (19674/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (20745/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (21835/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (22914/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (24009/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (25089/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (26175/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (27297/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (85.00%) (28403/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (29476/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (85.00%) (30576/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (85.00%) (31667/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (32747/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (84.00%) (33836/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (84.00%) (34918/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (36011/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (37092/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4357) |  Loss2: (0.0000) | Acc: (85.00%) (38204/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (39314/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4339) |  Loss2: (0.0000) | Acc: (85.00%) (40417/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (85.00%) (41474/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (42528/50000)
# TEST : Loss: (0.5012) | Acc: (82.00%) (8287/10000)
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5447, 0.4553], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.6287, 0.3713], device='cuda:0')
percent tensor([0.6155, 0.3845], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4102) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4193) |  Loss2: (0.0000) | Acc: (85.00%) (3406/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (4475/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (5547/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (6609/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (7665/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4453) |  Loss2: (0.0000) | Acc: (84.00%) (8749/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (9846/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (10925/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (12018/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (13112/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (14204/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (15288/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (16365/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (17449/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (18549/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (84.00%) (19639/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (20722/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (21814/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (22913/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (24019/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (25101/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (26172/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4381) |  Loss2: (0.0000) | Acc: (84.00%) (27247/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (28322/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (29404/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (30468/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (84.00%) (31552/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (32636/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (33742/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4381) |  Loss2: (0.0000) | Acc: (84.00%) (34815/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (35916/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (36995/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (38090/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (84.00%) (39185/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (84.00%) (40276/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (41338/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (42375/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.6226) | Acc: (79.00%) (7972/10000)
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.5452, 0.4548], device='cuda:0')
percent tensor([0.5095, 0.4905], device='cuda:0')
percent tensor([0.6296, 0.3704], device='cuda:0')
percent tensor([0.6189, 0.3811], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.9562, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.4247, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.7852, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1530.6227, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.8777, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2209.6016, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4293.1768, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1426.3762, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6098.3179, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12034.0342, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4004.3313, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16983.2949, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (87.00%) (1229/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (2311/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (85.00%) (3408/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (85.00%) (4511/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4071) |  Loss2: (0.0000) | Acc: (86.00%) (5617/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (6713/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (7832/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (8915/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (86.00%) (10024/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (11110/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (12196/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (13303/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4180) |  Loss2: (0.0000) | Acc: (85.00%) (14385/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (15502/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (16590/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4203) |  Loss2: (0.0000) | Acc: (85.00%) (17673/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (18751/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4210) |  Loss2: (0.0000) | Acc: (85.00%) (19847/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (20933/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (22019/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (23122/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4203) |  Loss2: (0.0000) | Acc: (85.00%) (24228/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (25330/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (26437/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (27532/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (28649/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (29751/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (30848/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (31946/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (33057/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (34162/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (35271/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (36350/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (37430/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (38500/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (39593/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (40681/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (41783/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (42847/50000)
# TEST : Loss: (0.5560) | Acc: (81.00%) (8137/10000)
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5456, 0.4544], device='cuda:0')
percent tensor([0.5094, 0.4906], device='cuda:0')
percent tensor([0.6283, 0.3717], device='cuda:0')
percent tensor([0.6154, 0.3846], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (86.00%) (1224/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (3445/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (4552/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (5647/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (86.00%) (6741/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (86.00%) (7840/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (86.00%) (8945/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4057) |  Loss2: (0.0000) | Acc: (86.00%) (10059/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4060) |  Loss2: (0.0000) | Acc: (86.00%) (11170/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (12266/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (86.00%) (13363/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (86.00%) (14454/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4063) |  Loss2: (0.0000) | Acc: (86.00%) (15570/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (16689/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (17781/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (18891/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (19994/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (21126/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (22221/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (23322/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (24401/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4050) |  Loss2: (0.0000) | Acc: (86.00%) (25492/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4037) |  Loss2: (0.0000) | Acc: (86.00%) (26606/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (27711/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (28801/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (86.00%) (29873/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (86.00%) (30958/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (86.00%) (32059/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (86.00%) (33149/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (86.00%) (34266/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (35370/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (86.00%) (36455/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (86.00%) (37551/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (86.00%) (38651/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (86.00%) (39757/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (86.00%) (40857/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (41952/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (43010/50000)
# TEST : Loss: (0.5268) | Acc: (81.00%) (8190/10000)
percent tensor([0.5485, 0.4515], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5452, 0.4548], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.6134, 0.3866], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (1231/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (87.00%) (2351/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (87.00%) (3456/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (4552/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (5673/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (6786/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (7901/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (86.00%) (9013/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (10109/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (11246/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (12355/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (86.00%) (13411/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (14511/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (15617/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (16732/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (17834/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (18947/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (20055/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (21167/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (22279/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (23399/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (24476/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3921) |  Loss2: (0.0000) | Acc: (86.00%) (25578/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (26672/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (27777/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (28879/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (29992/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (31105/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (32208/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (33290/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (34378/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (35474/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (36591/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (37690/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (38797/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (39907/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (41014/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (42135/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3938) |  Loss2: (0.0000) | Acc: (86.00%) (43205/50000)
# TEST : Loss: (0.5097) | Acc: (83.00%) (8315/10000)
percent tensor([0.5484, 0.4516], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5453, 0.4547], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (1230/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (2338/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (87.00%) (3465/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3766) |  Loss2: (0.0000) | Acc: (87.00%) (4592/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3813) |  Loss2: (0.0000) | Acc: (87.00%) (5699/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (6831/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (87.00%) (7947/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (87.00%) (9061/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3753) |  Loss2: (0.0000) | Acc: (87.00%) (10170/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (11293/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (12426/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (13539/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (14663/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (15779/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (16884/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (17997/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (19099/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (20205/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (21303/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3753) |  Loss2: (0.0000) | Acc: (87.00%) (22424/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (23528/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (24665/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (25766/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (87.00%) (26882/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (27998/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (87.00%) (29096/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (87.00%) (30191/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (87.00%) (31299/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (87.00%) (32406/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (33522/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (34615/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (35733/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (36845/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (37954/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (39056/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (40160/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (41277/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (42393/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (43476/50000)
# TEST : Loss: (0.6264) | Acc: (80.00%) (8061/10000)
percent tensor([0.5484, 0.4516], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5450, 0.4550], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.6290, 0.3710], device='cuda:0')
percent tensor([0.6147, 0.3853], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (2271/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (84.00%) (3340/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4773) |  Loss2: (0.0000) | Acc: (83.00%) (4370/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (83.00%) (5421/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4859) |  Loss2: (0.0000) | Acc: (83.00%) (6483/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4885) |  Loss2: (0.0000) | Acc: (82.00%) (7530/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (82.00%) (8578/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (82.00%) (9634/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (82.00%) (10682/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (11728/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (82.00%) (12779/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (82.00%) (13814/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (82.00%) (14880/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (15947/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4958) |  Loss2: (0.0000) | Acc: (82.00%) (16993/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (82.00%) (18052/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (82.00%) (19102/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4936) |  Loss2: (0.0000) | Acc: (82.00%) (20173/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (82.00%) (21232/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (82.00%) (22300/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4896) |  Loss2: (0.0000) | Acc: (82.00%) (23378/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4882) |  Loss2: (0.0000) | Acc: (82.00%) (24458/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (82.00%) (25527/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4870) |  Loss2: (0.0000) | Acc: (82.00%) (26590/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (82.00%) (27680/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (82.00%) (28756/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (82.00%) (29836/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4827) |  Loss2: (0.0000) | Acc: (83.00%) (30926/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4830) |  Loss2: (0.0000) | Acc: (83.00%) (31986/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4825) |  Loss2: (0.0000) | Acc: (82.00%) (33033/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4817) |  Loss2: (0.0000) | Acc: (83.00%) (34114/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4804) |  Loss2: (0.0000) | Acc: (83.00%) (35203/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4800) |  Loss2: (0.0000) | Acc: (83.00%) (36276/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4793) |  Loss2: (0.0000) | Acc: (83.00%) (37346/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (38413/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4788) |  Loss2: (0.0000) | Acc: (83.00%) (39475/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (40542/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (41558/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.5502) | Acc: (81.00%) (8183/10000)
percent tensor([0.5560, 0.4440], device='cuda:0')
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.5440, 0.4560], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.6495, 0.3505], device='cuda:0')
percent tensor([0.5927, 0.4073], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4346) |  Loss2: (0.0000) | Acc: (84.00%) (1183/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4481) |  Loss2: (0.0000) | Acc: (84.00%) (2262/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (3340/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (4413/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (83.00%) (5474/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (83.00%) (6550/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (83.00%) (7629/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (83.00%) (8704/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (9795/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (10894/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4461) |  Loss2: (0.0000) | Acc: (84.00%) (11990/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (13075/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4461) |  Loss2: (0.0000) | Acc: (84.00%) (14141/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (15230/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (16307/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (17403/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (18496/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (19584/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (84.00%) (20674/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (84.00%) (21757/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (22849/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (23945/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (25056/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (84.00%) (26151/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.4360) |  Loss2: (0.0000) | Acc: (84.00%) (27240/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.4357) |  Loss2: (0.0000) | Acc: (84.00%) (28331/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (29399/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (30499/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (31592/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (32651/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (33749/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.4360) |  Loss2: (0.0000) | Acc: (84.00%) (34858/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (35990/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (84.00%) (37066/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (38127/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (84.00%) (39230/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.4339) |  Loss2: (0.0000) | Acc: (84.00%) (40330/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (84.00%) (41412/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (42470/50000)
# TEST : Loss: (0.5195) | Acc: (82.00%) (8257/10000)
percent tensor([0.5540, 0.4460], device='cuda:0')
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.5214, 0.4786], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.5867) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.4383) |  Loss2: (0.0000) | Acc: (84.00%) (1191/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (85.00%) (2305/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (3401/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.4203) |  Loss2: (0.0000) | Acc: (85.00%) (4475/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (5582/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.4224) |  Loss2: (0.0000) | Acc: (85.00%) (6659/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (7771/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (8879/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (9956/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (11074/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (12180/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (85.00%) (13264/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (14338/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.4132) |  Loss2: (0.0000) | Acc: (85.00%) (15455/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (85.00%) (16558/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (17676/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (18762/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (19855/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (20941/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (22045/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (23132/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (24195/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (25297/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (26392/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (27483/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (28581/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (29672/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (30760/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (31853/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (32951/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (34042/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (35138/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (36252/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (37357/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (38457/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (85.00%) (39568/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (40677/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (41765/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.4099) |  Loss2: (0.0000) | Acc: (85.00%) (42822/50000)
# TEST : Loss: (0.4991) | Acc: (83.00%) (8330/10000)
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.5023, 0.4977], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.5460, 0.4540], device='cuda:0')
percent tensor([0.5094, 0.4906], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.6208, 0.3792], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.4457) |  Loss2: (0.0000) | Acc: (83.00%) (1175/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (84.00%) (2276/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (3389/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (85.00%) (4489/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (5596/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.4052) |  Loss2: (0.0000) | Acc: (85.00%) (6696/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (85.00%) (7802/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (85.00%) (8909/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (85.00%) (10012/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (85.00%) (11109/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (86.00%) (12236/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (13360/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (14442/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (15538/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (16625/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (17758/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3972) |  Loss2: (0.0000) | Acc: (86.00%) (18873/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3987) |  Loss2: (0.0000) | Acc: (86.00%) (19967/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (21080/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (22192/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (23275/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3962) |  Loss2: (0.0000) | Acc: (86.00%) (24394/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (25493/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (26584/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (27690/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (28780/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (29872/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (30981/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.4002) |  Loss2: (0.0000) | Acc: (86.00%) (32082/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (33186/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3994) |  Loss2: (0.0000) | Acc: (86.00%) (34296/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3987) |  Loss2: (0.0000) | Acc: (86.00%) (35419/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3989) |  Loss2: (0.0000) | Acc: (86.00%) (36509/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (37609/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (38708/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3985) |  Loss2: (0.0000) | Acc: (86.00%) (39815/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (40922/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (42034/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (43101/50000)
# TEST : Loss: (0.4841) | Acc: (83.00%) (8350/10000)
percent tensor([0.5580, 0.4420], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.5474, 0.4526], device='cuda:0')
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.6277, 0.3723], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (2325/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (3434/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (4537/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (5645/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (6730/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (7839/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (8930/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (10032/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (85.00%) (11114/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (85.00%) (12216/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3919) |  Loss2: (0.0000) | Acc: (86.00%) (13326/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (14440/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (15552/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (16661/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3904) |  Loss2: (0.0000) | Acc: (86.00%) (17774/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3904) |  Loss2: (0.0000) | Acc: (86.00%) (18887/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (19980/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3921) |  Loss2: (0.0000) | Acc: (86.00%) (21077/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (22203/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3882) |  Loss2: (0.0000) | Acc: (86.00%) (23326/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (24436/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (25542/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (26652/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (27742/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3891) |  Loss2: (0.0000) | Acc: (86.00%) (28857/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (29970/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (31082/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (32195/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (33334/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (34453/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (35553/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (36657/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (37766/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (38856/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (39978/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (41087/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (42195/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (43252/50000)
# TEST : Loss: (0.4802) | Acc: (83.00%) (8362/10000)
percent tensor([0.5572, 0.4428], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5247, 0.4753], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6556, 0.3444], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (1230/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (86.00%) (2333/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (87.00%) (3463/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3769) |  Loss2: (0.0000) | Acc: (87.00%) (4581/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (87.00%) (5697/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (6831/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (7948/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3735) |  Loss2: (0.0000) | Acc: (87.00%) (9065/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (10155/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (11279/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (87.00%) (12389/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (87.00%) (13489/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3869) |  Loss2: (0.0000) | Acc: (87.00%) (14594/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (87.00%) (15727/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (16835/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (87.00%) (17954/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (87.00%) (19062/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (87.00%) (20165/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (87.00%) (21277/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (22403/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (23529/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (87.00%) (24654/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (25771/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (87.00%) (26878/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (27970/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (87.00%) (29087/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (30210/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (87.00%) (31320/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (32443/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (87.00%) (33542/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (34657/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (87.00%) (35789/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (87.00%) (36898/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (87.00%) (38031/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (87.00%) (39137/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (87.00%) (40236/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (87.00%) (41350/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (87.00%) (42475/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (87.00%) (43545/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.5731) | Acc: (80.00%) (8098/10000)
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5251, 0.4749], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.5170, 0.4830], device='cuda:0')
percent tensor([0.6581, 0.3419], device='cuda:0')
percent tensor([0.6398, 0.3602], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.3411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(806.0490, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(797.6852, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.6450, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.3684, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2219.5398, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4287.6152, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1421.3867, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6105.4814, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11996.1865, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3988.8765, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16913.4004, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (1217/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (2328/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (3443/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3844) |  Loss2: (0.0000) | Acc: (87.00%) (4571/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (5684/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3750) |  Loss2: (0.0000) | Acc: (87.00%) (6803/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (7945/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (9046/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (10166/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (11284/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (12418/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (13532/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (14649/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (15766/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (16893/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (17984/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (19111/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (20238/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (21353/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (22473/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (23581/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (24690/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (25791/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (26910/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (28022/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (29135/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (30238/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (31372/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (32475/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (33582/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (34684/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (35814/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (36925/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (38028/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (39156/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (40265/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (41390/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (42525/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (43590/50000)
# TEST : Loss: (0.4819) | Acc: (83.00%) (8392/10000)
percent tensor([0.5558, 0.4442], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5242, 0.4758], device='cuda:0')
percent tensor([0.5506, 0.4494], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.6520, 0.3480], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (3508/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (4631/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (87.00%) (5744/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (87.00%) (6852/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (7955/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (9081/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (87.00%) (10200/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (87.00%) (11315/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (12421/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (13534/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (14649/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (15752/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (16888/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (18010/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (19142/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (20249/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (21384/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (22488/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (23623/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (24750/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (25869/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (26992/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (28130/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3571) |  Loss2: (0.0000) | Acc: (87.00%) (29253/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3566) |  Loss2: (0.0000) | Acc: (87.00%) (30371/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (31516/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (32631/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (33765/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (34892/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (36000/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (37125/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (38240/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (39358/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (40474/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (41586/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (42703/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (43781/50000)
# TEST : Loss: (0.5477) | Acc: (82.00%) (8222/10000)
percent tensor([0.5556, 0.4444], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.6564, 0.3436], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (88.00%) (1240/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (2351/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3539) |  Loss2: (0.0000) | Acc: (88.00%) (3495/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (4607/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3571) |  Loss2: (0.0000) | Acc: (87.00%) (5725/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (6829/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (7973/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (9108/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (10226/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (11339/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (12485/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (13621/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (14752/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (15878/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (87.00%) (17000/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (18112/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (88.00%) (19268/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3482) |  Loss2: (0.0000) | Acc: (88.00%) (20399/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (21528/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (22664/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (23794/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (24920/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (88.00%) (26043/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (27166/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (28281/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (29418/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (88.00%) (30534/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (31664/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (32789/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (33928/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (88.00%) (35046/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (87.00%) (36157/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (87.00%) (37268/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (87.00%) (38406/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (39520/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3496) |  Loss2: (0.0000) | Acc: (87.00%) (40630/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (41756/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3488) |  Loss2: (0.0000) | Acc: (87.00%) (42911/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (43985/50000)
# TEST : Loss: (0.4974) | Acc: (83.00%) (8331/10000)
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.6557, 0.3443], device='cuda:0')
percent tensor([0.6454, 0.3546], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (2409/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (3563/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (4685/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (5797/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (6933/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (8075/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (9216/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (10349/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (11485/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (12614/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (13744/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (14856/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (15977/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (17114/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (18227/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (19336/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (20468/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (21605/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (22739/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (23858/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (24995/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (26131/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (27261/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (28361/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (29512/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (30629/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (31751/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (32895/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (34030/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (35175/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (36297/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (37437/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (38556/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (39699/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (40819/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (41961/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (43074/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (44162/50000)
# TEST : Loss: (0.5688) | Acc: (82.00%) (8211/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.6556, 0.3444], device='cuda:0')
percent tensor([0.6401, 0.3599], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (87.00%) (1239/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (2323/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (3415/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (85.00%) (4505/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (5591/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (6669/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (7774/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.4269) |  Loss2: (0.0000) | Acc: (85.00%) (8867/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (85.00%) (9938/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (85.00%) (11006/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (12104/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (13199/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (85.00%) (14264/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (15335/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.4355) |  Loss2: (0.0000) | Acc: (84.00%) (16427/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (17517/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (85.00%) (18607/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (85.00%) (19697/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (20805/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (21899/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (22982/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (24105/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.4258) |  Loss2: (0.0000) | Acc: (85.00%) (25201/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.4263) |  Loss2: (0.0000) | Acc: (85.00%) (26290/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.4260) |  Loss2: (0.0000) | Acc: (85.00%) (27387/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (28502/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (29599/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.4222) |  Loss2: (0.0000) | Acc: (85.00%) (30725/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (85.00%) (31814/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (32913/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (34009/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.4198) |  Loss2: (0.0000) | Acc: (85.00%) (35124/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (36236/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (37351/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.4183) |  Loss2: (0.0000) | Acc: (85.00%) (38462/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (39551/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (40650/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (41759/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (42818/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.5279) | Acc: (83.00%) (8313/10000)
percent tensor([0.5582, 0.4418], device='cuda:0')
percent tensor([0.5070, 0.4930], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.5190, 0.4810], device='cuda:0')
percent tensor([0.6758, 0.3242], device='cuda:0')
percent tensor([0.6222, 0.3778], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (88.00%) (1240/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (2340/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (86.00%) (3452/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (86.00%) (4562/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (5680/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (87.00%) (6793/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (7879/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (8989/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (10105/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (86.00%) (11232/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (86.00%) (12341/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3750) |  Loss2: (0.0000) | Acc: (86.00%) (13453/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (86.00%) (14578/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (15721/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (16856/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (17957/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3693) |  Loss2: (0.0000) | Acc: (87.00%) (19087/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (20206/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (21320/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (22436/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (23537/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (24633/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (25767/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (26888/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (28021/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (29156/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (30255/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (31364/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (32480/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (33578/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (34706/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (35823/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (36938/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3685) |  Loss2: (0.0000) | Acc: (87.00%) (38049/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (39168/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (40304/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (41437/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (42567/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (43658/50000)
# TEST : Loss: (0.4864) | Acc: (84.00%) (8410/10000)
percent tensor([0.5598, 0.4402], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5126, 0.4874], device='cuda:0')
percent tensor([0.5637, 0.4363], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.6352, 0.3648], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3334) |  Loss2: (0.0000) | Acc: (88.00%) (2371/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (3468/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (87.00%) (4601/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (5733/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (87.00%) (6850/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (7969/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (9108/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3539) |  Loss2: (0.0000) | Acc: (87.00%) (10223/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (87.00%) (11355/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (87.00%) (12499/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (13648/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (88.00%) (14775/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (15897/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (17022/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (18144/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (19265/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (87.00%) (20380/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (87.00%) (21498/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3488) |  Loss2: (0.0000) | Acc: (87.00%) (22630/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (23759/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3485) |  Loss2: (0.0000) | Acc: (87.00%) (24884/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (25991/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (27134/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (28247/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (29386/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (30541/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (31665/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (32813/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (33933/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (88.00%) (35070/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (36199/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (37322/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3458) |  Loss2: (0.0000) | Acc: (88.00%) (38461/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (39583/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (40711/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (41830/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (42954/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (44032/50000)
# TEST : Loss: (0.4688) | Acc: (84.00%) (8478/10000)
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.5637, 0.4363], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.6479, 0.3521], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (3498/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (4620/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (88.00%) (5760/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (6897/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (8044/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (9172/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (10323/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (11455/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (12571/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (13699/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (14838/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (15981/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (17112/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (18253/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (19362/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (20487/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (21632/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (22775/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (23897/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (25024/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (26157/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (27262/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (28386/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (29526/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (30652/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (31812/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (32940/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (34061/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (35200/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (36312/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (37435/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3376) |  Loss2: (0.0000) | Acc: (88.00%) (38558/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (39692/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (40837/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (41995/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (43116/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (44203/50000)
# TEST : Loss: (0.4568) | Acc: (85.00%) (8507/10000)
percent tensor([0.5593, 0.4407], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.7109, 0.2891], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (1264/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (2399/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (3543/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (89.00%) (4675/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (5798/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (6933/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (8070/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (9195/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (10314/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (11453/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (12592/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (13726/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (14861/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (15998/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (17124/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (18261/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (19383/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (20538/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (21675/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (22812/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (23963/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (25112/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (26252/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (27382/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (28522/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (29638/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (30768/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (31894/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (33020/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (34160/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (35312/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (36437/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (37561/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (38719/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (39845/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (40961/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (42096/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (43246/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (44323/50000)
# TEST : Loss: (0.4519) | Acc: (85.00%) (8506/10000)
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5116, 0.4884], device='cuda:0')
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7053, 0.2947], device='cuda:0')
percent tensor([0.6609, 0.3391], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (1262/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (3548/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (4684/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (88.00%) (5809/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (6938/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (8086/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (89.00%) (9230/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (10355/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (11495/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3263) |  Loss2: (0.0000) | Acc: (89.00%) (12647/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (13768/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (14890/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (16015/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (17170/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (18307/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (19453/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (20591/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (21735/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (22881/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (24018/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (25144/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (26277/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (27420/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (28558/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (29693/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (30809/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (31935/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (33078/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (34199/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (35320/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (36438/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (37568/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (38713/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (39845/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (40960/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (42111/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (43241/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (44315/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.5555) | Acc: (82.00%) (8251/10000)
percent tensor([0.5618, 0.4382], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5116, 0.4884], device='cuda:0')
percent tensor([0.5636, 0.4364], device='cuda:0')
percent tensor([0.5267, 0.4733], device='cuda:0')
percent tensor([0.7104, 0.2896], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.8864, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(810.2170, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.0926, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.8448, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.8925, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2228.8677, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.8672, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1416.5331, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6114.7197, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11959.8604, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3973.5193, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16844.7539, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (3523/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (4658/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (5807/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (6921/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (8040/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (9177/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (10305/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (11454/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (88.00%) (12595/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (13745/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (14883/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (16014/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (17152/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (18291/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (19434/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (20563/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (21708/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (22848/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (23969/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (25116/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (26243/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (27370/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (28489/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (29642/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (30771/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (31916/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (33079/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (34209/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (35339/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (36475/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (37619/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (38744/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (39868/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (41008/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (42134/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (43277/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (44385/50000)
# TEST : Loss: (0.4715) | Acc: (84.00%) (8472/10000)
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.5265, 0.4735], device='cuda:0')
percent tensor([0.7096, 0.2904], device='cuda:0')
percent tensor([0.6635, 0.3365], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (89.00%) (2395/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (89.00%) (3532/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (4673/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (89.00%) (5826/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (89.00%) (6972/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (8110/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (9271/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (10399/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3145) |  Loss2: (0.0000) | Acc: (89.00%) (11551/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (12677/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (13817/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (14960/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (16103/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (17257/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (18407/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (19549/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3129) |  Loss2: (0.0000) | Acc: (89.00%) (20713/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (21854/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (22992/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (24116/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (25248/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (26395/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (27517/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (28651/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3165) |  Loss2: (0.0000) | Acc: (89.00%) (29784/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (30915/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (32058/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (33199/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3170) |  Loss2: (0.0000) | Acc: (89.00%) (34349/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (35504/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (36627/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (37766/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (38891/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (40021/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (41147/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (42289/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (43439/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (44528/50000)
# TEST : Loss: (0.4739) | Acc: (84.00%) (8434/10000)
percent tensor([0.5624, 0.4376], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.7091, 0.2909], device='cuda:0')
percent tensor([0.6613, 0.3387], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (89.00%) (2418/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (89.00%) (3571/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (4712/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (7005/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (8158/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (9304/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (10471/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (11606/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (12738/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (13900/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (15047/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (16194/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (17355/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (18510/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (19665/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (20816/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (21981/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (23107/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (24259/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (25404/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (26541/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (27674/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (28840/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (29991/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (31124/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (32273/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (33404/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (34556/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (35693/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (36850/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (38000/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (39135/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (40257/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (41388/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (42515/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (43662/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (44755/50000)
# TEST : Loss: (0.4555) | Acc: (84.00%) (8479/10000)
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.5634, 0.4366], device='cuda:0')
percent tensor([0.5264, 0.4736], device='cuda:0')
percent tensor([0.7051, 0.2949], device='cuda:0')
percent tensor([0.6614, 0.3386], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (1266/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (2437/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (3598/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (4753/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (90.00%) (5888/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (7055/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (8204/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (9349/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2935) |  Loss2: (0.0000) | Acc: (89.00%) (10483/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (90.00%) (11641/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (90.00%) (12796/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (90.00%) (13956/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (15079/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (16231/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (17368/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (18504/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (19654/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (20800/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (21941/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (23094/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (24236/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (25377/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (26547/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (27678/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (28834/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (29969/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (31124/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (32284/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (33439/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (34592/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (35730/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (36856/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (37996/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (39112/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (40243/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (41396/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (42543/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (43689/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (44784/50000)
# TEST : Loss: (0.4338) | Acc: (85.00%) (8559/10000)
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.5631, 0.4369], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.7036, 0.2964], device='cuda:0')
percent tensor([0.6536, 0.3464], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (87.00%) (3486/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (5671/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (6775/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (7897/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (9012/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (10119/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (11212/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (12311/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (13402/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (14530/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (15647/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (16770/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3776) |  Loss2: (0.0000) | Acc: (86.00%) (17894/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3762) |  Loss2: (0.0000) | Acc: (86.00%) (19013/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (20120/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (21232/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3776) |  Loss2: (0.0000) | Acc: (86.00%) (22353/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (86.00%) (23485/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3741) |  Loss2: (0.0000) | Acc: (87.00%) (24622/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (87.00%) (25750/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (87.00%) (26882/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (28000/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (29125/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (30235/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (31354/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (32476/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (33586/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (34693/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (35805/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3693) |  Loss2: (0.0000) | Acc: (87.00%) (36941/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (38060/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (39197/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (40332/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (41457/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (42599/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (43688/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4722) | Acc: (84.00%) (8410/10000)
percent tensor([0.5516, 0.4484], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.5366, 0.4634], device='cuda:0')
percent tensor([0.5529, 0.4471], device='cuda:0')
percent tensor([0.6701, 0.3299], device='cuda:0')
percent tensor([0.6325, 0.3675], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (88.00%) (2369/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (88.00%) (3493/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (88.00%) (4640/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (5778/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (6912/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (8046/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (9185/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (10329/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (11444/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (12583/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (13724/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (14863/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (15998/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (17138/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (18264/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (19419/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (20559/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (21708/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (22833/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (23977/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (25104/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (26263/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (27419/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (28563/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (29714/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (30844/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (31976/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (33101/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (34244/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (35398/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (36539/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (88.00%) (37664/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (38801/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (88.00%) (39937/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (41062/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (88.00%) (42188/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3255) |  Loss2: (0.0000) | Acc: (88.00%) (43316/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (44420/50000)
# TEST : Loss: (0.4496) | Acc: (84.00%) (8461/10000)
percent tensor([0.5483, 0.4517], device='cuda:0')
percent tensor([0.5044, 0.4956], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.6737, 0.3263], device='cuda:0')
percent tensor([0.6433, 0.3567], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (88.00%) (3523/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (4658/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (5798/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (6929/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (8071/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (9193/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (88.00%) (10344/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (11486/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (12633/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (13774/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.3183) |  Loss2: (0.0000) | Acc: (88.00%) (14920/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (16060/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (88.00%) (17197/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (18343/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (19478/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (20638/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (21777/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (22913/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (24059/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (89.00%) (25178/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (26326/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (27475/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (28603/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (29747/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (30894/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (32038/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (33184/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (34332/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (35471/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (36625/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (37766/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (38919/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (40071/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (41216/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (42358/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (43499/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (44613/50000)
# TEST : Loss: (0.4369) | Acc: (84.00%) (8496/10000)
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5029, 0.4971], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5398, 0.4602], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.6758, 0.3242], device='cuda:0')
percent tensor([0.6501, 0.3499], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (1257/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (89.00%) (2410/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (3550/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (4689/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (5830/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (6971/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (8113/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (9276/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (10421/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (11564/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (12704/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (13850/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (14994/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (16142/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (17271/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (18424/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (19577/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (20709/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (21866/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (23005/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (24154/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (25316/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (26446/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (27586/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (28732/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (29879/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (31017/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (32175/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (33326/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (34472/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (35637/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (36774/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (37914/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (39052/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (40200/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (41344/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (42507/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (43648/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (44753/50000)
# TEST : Loss: (0.4319) | Acc: (85.00%) (8508/10000)
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5076, 0.4924], device='cuda:0')
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.6778, 0.3222], device='cuda:0')
percent tensor([0.6609, 0.3391], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (2409/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (3556/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (89.00%) (4711/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (89.00%) (5855/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (7013/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (9308/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2935) |  Loss2: (0.0000) | Acc: (89.00%) (10448/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (11606/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (12760/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (13907/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (15030/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (16164/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (17304/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (18446/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (19597/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (20743/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (21896/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (23049/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (24199/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (25362/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (26510/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (27647/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (28809/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (29961/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (31104/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (32253/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (33404/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (34543/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (35704/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (36850/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (38006/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (39136/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (40268/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (41415/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (42555/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (43705/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (44828/50000)
# TEST : Loss: (0.4235) | Acc: (85.00%) (8539/10000)
percent tensor([0.5488, 0.4512], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.6830, 0.3170], device='cuda:0')
percent tensor([0.6655, 0.3345], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (2430/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (90.00%) (3577/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (4733/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (90.00%) (5894/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (7042/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (90.00%) (8184/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (9328/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (90.00%) (10484/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (11621/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (12774/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (13915/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (15053/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (16189/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (17359/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (18511/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (19658/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (20798/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (21947/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (23107/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (24272/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (25422/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (26558/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (27704/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (28856/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (29986/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (31145/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (32290/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (33431/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (34573/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (35709/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (36868/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (38011/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (39150/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (40285/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (41423/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (42584/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (43721/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (44795/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.5024) | Acc: (83.00%) (8342/10000)
percent tensor([0.5486, 0.4514], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5454, 0.4546], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6850, 0.3150], device='cuda:0')
percent tensor([0.6656, 0.3344], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.1511, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.6259, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(804.8654, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.5548, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.4178, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2237.5422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4277.3340, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1411.4608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6125.9971, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11924.0332, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3958.3030, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16776.7676, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2853) |  Loss2: (0.0000) | Acc: (90.00%) (3574/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (4723/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (89.00%) (5875/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (7045/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (8194/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (9355/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (10514/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (11669/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2831) |  Loss2: (0.0000) | Acc: (90.00%) (12836/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (13979/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (15138/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (16294/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (17442/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (18602/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (19762/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (20928/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (22059/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (23199/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (24320/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (25475/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (26628/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (90.00%) (27787/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (28947/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (90.00%) (30085/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (31222/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (32350/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (33508/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (34652/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (35802/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (36940/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (89.00%) (38091/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (89.00%) (39252/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (40383/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (41528/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2907) |  Loss2: (0.0000) | Acc: (89.00%) (42670/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (89.00%) (43828/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (44932/50000)
# TEST : Loss: (0.4650) | Acc: (84.00%) (8462/10000)
percent tensor([0.5492, 0.4508], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5461, 0.4539], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.6831, 0.3169], device='cuda:0')
percent tensor([0.6664, 0.3336], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (91.00%) (1284/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (3602/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (91.00%) (4786/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (5933/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (7090/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (8255/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (9417/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (10580/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (11734/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (12893/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (14044/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (15195/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (16347/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (17494/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (18643/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (19822/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (20970/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (22131/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (23292/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (24458/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (25618/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (26794/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (27939/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (29083/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (30240/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (31420/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (32571/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (33709/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2796) |  Loss2: (0.0000) | Acc: (90.00%) (34877/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (36026/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (37178/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (38318/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (39468/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (40621/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (41755/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (90.00%) (42899/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (44054/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (45166/50000)
# TEST : Loss: (0.4560) | Acc: (85.00%) (8511/10000)
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5447, 0.4553], device='cuda:0')
percent tensor([0.5619, 0.4381], device='cuda:0')
percent tensor([0.6805, 0.3195], device='cuda:0')
percent tensor([0.6674, 0.3326], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (3618/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (90.00%) (4774/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (5929/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (7094/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (8261/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (9411/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (10559/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (11724/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (12884/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (14033/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (15201/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (16354/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (17528/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (18669/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (19848/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (20985/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (22151/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (23335/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (24502/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (25656/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (26789/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (27935/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (29075/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (30222/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (31378/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (32545/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (33696/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (34864/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (36022/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (37183/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (38338/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (39512/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (40679/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (41839/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (42999/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (44170/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (45285/50000)
# TEST : Loss: (0.4653) | Acc: (85.00%) (8512/10000)
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5456, 0.4544], device='cuda:0')
percent tensor([0.5631, 0.4369], device='cuda:0')
percent tensor([0.6860, 0.3140], device='cuda:0')
percent tensor([0.6690, 0.3310], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (2443/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (91.00%) (4812/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (91.00%) (5979/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (91.00%) (7129/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (91.00%) (8277/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (91.00%) (9442/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (91.00%) (10616/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (91.00%) (11768/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (12913/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (14070/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (15221/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (16389/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (17553/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (18701/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (19850/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (20989/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (22151/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (23327/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (24488/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (25656/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (26810/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (27964/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (29117/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (30276/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (31428/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (32596/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (33763/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (34941/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (36094/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (37259/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (38429/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (39589/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (40734/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (41900/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (43074/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (44225/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (45347/50000)
# TEST : Loss: (0.4741) | Acc: (84.00%) (8499/10000)
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5455, 0.4545], device='cuda:0')
percent tensor([0.5626, 0.4374], device='cuda:0')
percent tensor([0.6814, 0.3186], device='cuda:0')
percent tensor([0.6648, 0.3352], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (2405/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (88.00%) (3526/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (4632/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (5733/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (6836/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (7959/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (9064/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (10168/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (11283/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (12417/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.3573) |  Loss2: (0.0000) | Acc: (87.00%) (13530/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (14638/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (15759/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (16902/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (87.00%) (18038/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (87.00%) (19171/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (20310/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (21446/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (87.00%) (22574/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (87.00%) (23717/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (87.00%) (24856/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (87.00%) (25962/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (87.00%) (27111/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (87.00%) (28257/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (87.00%) (29382/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (30529/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (31681/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (32832/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (33984/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (35132/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (36277/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (37417/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (38553/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (39696/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (40851/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (88.00%) (41973/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (43133/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (44224/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4751) | Acc: (84.00%) (8475/10000)
percent tensor([0.5580, 0.4420], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5449, 0.4551], device='cuda:0')
percent tensor([0.5640, 0.4360], device='cuda:0')
percent tensor([0.6795, 0.3205], device='cuda:0')
percent tensor([0.6477, 0.3523], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (2429/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (3567/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (4713/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (5860/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (7003/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (89.00%) (8135/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (9288/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (10416/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (11561/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (89.00%) (12717/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (89.00%) (13868/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (15015/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (89.00%) (16183/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (17329/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (18485/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (19626/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (20785/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2886) |  Loss2: (0.0000) | Acc: (89.00%) (21948/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (89.00%) (23096/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (24247/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (25400/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (26536/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (89.00%) (27699/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (28844/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (89.00%) (29990/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (89.00%) (31151/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (89.00%) (32317/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (33480/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (34628/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (35748/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (89.00%) (36919/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (38084/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (89.00%) (39236/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (40387/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (89.00%) (41549/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (89.00%) (42708/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (89.00%) (43855/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (89.00%) (44969/50000)
# TEST : Loss: (0.4531) | Acc: (85.00%) (8520/10000)
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5673, 0.4327], device='cuda:0')
percent tensor([0.6856, 0.3144], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (90.00%) (2439/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (3590/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (5924/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (7098/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (8259/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (9423/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (10587/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (90.00%) (11755/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (12914/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (14080/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (15228/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (16402/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (17535/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (18680/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (19855/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (21008/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (22149/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (24457/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (25613/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (26771/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (27942/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (29097/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (30254/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (31406/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (32564/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (33721/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (34866/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (36023/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (37191/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (38366/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (39545/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (40676/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (41825/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (42986/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (44137/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (45233/50000)
# TEST : Loss: (0.4396) | Acc: (85.00%) (8556/10000)
percent tensor([0.5571, 0.4429], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5664, 0.4336], device='cuda:0')
percent tensor([0.6896, 0.3104], device='cuda:0')
percent tensor([0.6632, 0.3368], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (3633/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (4792/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (5942/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (7112/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (8259/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (9414/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (10576/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (11730/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (12890/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (14051/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (15211/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (16378/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (17546/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (18695/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (19839/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (20987/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (22152/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (23311/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (24469/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (25640/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (26790/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (27937/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (29110/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (30272/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (31443/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (32618/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (33773/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (34940/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (36119/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (37281/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (38444/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (39604/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (40768/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (41931/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (43116/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (44273/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (45390/50000)
# TEST : Loss: (0.4334) | Acc: (85.00%) (8587/10000)
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5064, 0.4936], device='cuda:0')
percent tensor([0.5495, 0.4505], device='cuda:0')
percent tensor([0.5688, 0.4312], device='cuda:0')
percent tensor([0.6882, 0.3118], device='cuda:0')
percent tensor([0.6756, 0.3244], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (4778/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (5936/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (7088/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (8252/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (90.00%) (9421/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (10575/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (11732/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (12889/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (14060/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (15208/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (16372/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (17536/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (18716/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (19871/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (21037/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (22201/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (23377/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (24551/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (25698/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (26852/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (90.00%) (28012/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (29168/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (30331/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (31518/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (32684/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (33850/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (35018/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (36207/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (37383/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (38543/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (39716/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (40882/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (42045/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (43202/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (91.00%) (44380/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (45512/50000)
# TEST : Loss: (0.4268) | Acc: (85.00%) (8594/10000)
percent tensor([0.5547, 0.4453], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5520, 0.4480], device='cuda:0')
percent tensor([0.5689, 0.4311], device='cuda:0')
percent tensor([0.6875, 0.3125], device='cuda:0')
percent tensor([0.6763, 0.3237], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (3635/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (4798/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (5953/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (7119/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (8290/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (9457/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (10612/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (91.00%) (11770/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (12927/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (14094/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (15256/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (91.00%) (16425/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (17595/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (91.00%) (18759/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (91.00%) (19920/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (21076/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (22249/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (23394/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (24551/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (25692/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (26856/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (28017/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (29181/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (30355/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (31527/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (32682/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (33840/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (34994/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (36173/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (37329/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (38487/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (39650/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (40805/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (41968/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (43136/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (44281/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (45393/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4587) | Acc: (85.00%) (8531/10000)
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5526, 0.4474], device='cuda:0')
percent tensor([0.5696, 0.4304], device='cuda:0')
percent tensor([0.6888, 0.3112], device='cuda:0')
percent tensor([0.6771, 0.3229], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.3037, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.4542, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.1458, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.4341, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.9706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2245.4343, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4273.8379, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1406.5306, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6137.6670, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11889.2529, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3943.0288, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16709.2344, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2796) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (2450/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (3617/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (4794/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (5965/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (7144/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (8322/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (9495/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (10660/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (11823/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (12992/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (14177/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (15343/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (16497/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (17672/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (18840/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (20021/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (21195/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (22368/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (23517/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (24687/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (25842/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2531) |  Loss2: (0.0000) | Acc: (91.00%) (27031/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (28194/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (29344/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (30512/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (31678/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (32840/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (33995/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (35156/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (36313/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (37499/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (38676/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (39834/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (40999/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (42181/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (43336/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (44497/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (45614/50000)
# TEST : Loss: (0.4893) | Acc: (84.00%) (8487/10000)
percent tensor([0.5543, 0.4457], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5671, 0.4329], device='cuda:0')
percent tensor([0.6877, 0.3123], device='cuda:0')
percent tensor([0.6832, 0.3168], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (3630/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (4795/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (5963/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (7130/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (8287/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (9480/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (10649/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (12998/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (14176/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (15346/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (16518/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (17678/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (18857/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (20035/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (21192/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (22352/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (23515/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (24685/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (25850/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (27002/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (28172/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (29339/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (30509/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (31694/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (32871/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (34038/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (35208/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (36382/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (37562/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (38747/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (39920/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (41079/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (42254/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (43411/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (44578/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (45691/50000)
# TEST : Loss: (0.4479) | Acc: (85.00%) (8536/10000)
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5521, 0.4479], device='cuda:0')
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.6857, 0.3143], device='cuda:0')
percent tensor([0.6761, 0.3239], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.4011) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (92.00%) (1299/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (3669/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (92.00%) (4830/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (6003/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (7166/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (9492/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (10680/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (11861/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (13036/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (14188/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (15361/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (16526/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (17669/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (18846/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (20034/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (21195/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (22348/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (23504/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (24671/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (25837/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (27025/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (28196/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (29363/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (30533/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (31713/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (32880/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (34053/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (35229/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (36399/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (37563/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (38738/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (39924/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (41095/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (42251/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (43418/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (44582/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (45713/50000)
# TEST : Loss: (0.4682) | Acc: (85.00%) (8551/10000)
percent tensor([0.5543, 0.4457], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5519, 0.4481], device='cuda:0')
percent tensor([0.5688, 0.4312], device='cuda:0')
percent tensor([0.6872, 0.3128], device='cuda:0')
percent tensor([0.6768, 0.3232], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (1299/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (2469/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (4809/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (5994/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (7151/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (8328/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (9514/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (10683/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (11844/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (13010/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (14198/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (15364/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (16543/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (17728/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (18878/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (20050/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (21223/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (22393/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (23567/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (24744/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (25912/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (27093/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (28264/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (29441/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (30619/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (31800/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (32988/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (34167/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (35329/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (36520/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (37688/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (38858/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (40029/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (41199/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (42376/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (43518/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (44682/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (45792/50000)
# TEST : Loss: (0.4476) | Acc: (86.00%) (8625/10000)
percent tensor([0.5540, 0.4460], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5522, 0.4478], device='cuda:0')
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.6883, 0.3117], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (2404/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (3551/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (4681/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (88.00%) (5803/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (88.00%) (6933/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (8090/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (88.00%) (9216/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (10337/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (88.00%) (11470/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (12606/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (13752/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3165) |  Loss2: (0.0000) | Acc: (88.00%) (14900/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (88.00%) (16042/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (88.00%) (17190/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (88.00%) (18332/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (19443/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (88.00%) (20590/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (21760/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (22903/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (24051/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (25209/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (26345/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.3075) |  Loss2: (0.0000) | Acc: (89.00%) (27494/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.3075) |  Loss2: (0.0000) | Acc: (89.00%) (28635/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (29792/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (30940/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (32105/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (33268/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (34426/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (35576/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (36738/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (37886/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (39057/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (40222/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (41378/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (42535/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (43685/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (44786/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4711) | Acc: (85.00%) (8518/10000)
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5639, 0.4361], device='cuda:0')
percent tensor([0.7325, 0.2675], device='cuda:0')
percent tensor([0.6321, 0.3679], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (3595/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (4732/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (5885/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (7048/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2749) |  Loss2: (0.0000) | Acc: (90.00%) (8209/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (9364/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (10540/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (11711/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (12887/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (14070/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (15243/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (16399/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (17562/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (18713/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (19857/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (21003/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (22152/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (23317/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (24479/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (25644/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (26811/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (27979/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (29152/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (30295/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (31461/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (32619/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (33797/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (34979/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (36118/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (37288/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (38441/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (39611/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (40770/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (41928/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (43083/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (44252/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (45360/50000)
# TEST : Loss: (0.4454) | Acc: (86.00%) (8605/10000)
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5001, 0.4999], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5666, 0.4334], device='cuda:0')
percent tensor([0.5654, 0.4346], device='cuda:0')
percent tensor([0.7359, 0.2641], device='cuda:0')
percent tensor([0.6319, 0.3681], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (3633/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (4814/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (5989/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (8357/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (9519/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (10696/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (11864/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (13035/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (14198/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (15360/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (16528/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (17684/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (18847/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (20025/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (21198/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (22373/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (23533/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (24719/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (25895/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (27081/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (28253/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (29419/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (30571/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (31752/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (32922/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (34088/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (35256/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (36440/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (37620/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (38795/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (39961/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (41119/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (42302/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (43477/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (44650/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (45774/50000)
# TEST : Loss: (0.4363) | Acc: (86.00%) (8629/10000)
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.4997, 0.5003], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5665, 0.4335], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.7355, 0.2645], device='cuda:0')
percent tensor([0.6477, 0.3523], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (92.00%) (2474/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (4807/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (5990/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (7171/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (8331/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (9518/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (10694/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (11872/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (13054/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (14230/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (15413/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (16585/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (17757/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (18914/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (20080/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (21248/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (22420/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (23582/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (24752/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (25933/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (27102/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (28266/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (29420/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (30595/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (31765/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (32919/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (34088/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (35274/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (36447/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (37614/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (38783/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (39951/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (41135/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (42313/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (43498/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (44680/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (45813/50000)
# TEST : Loss: (0.4293) | Acc: (86.00%) (8639/10000)
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.4995, 0.5005], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5665, 0.4335], device='cuda:0')
percent tensor([0.5686, 0.4314], device='cuda:0')
percent tensor([0.7378, 0.2622], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (3634/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (5972/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (7142/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (8330/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (9512/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (10684/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (11856/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (13045/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (14203/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (15367/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (16557/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (17739/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (18906/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (20085/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (21256/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (22439/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (23604/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (24778/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (25948/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (27125/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (28289/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (29465/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (30644/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (31812/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (32990/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (34148/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (35336/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (36506/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (37686/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (38859/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (40058/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (41235/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (42409/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (43587/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (44755/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (45871/50000)
# TEST : Loss: (0.4218) | Acc: (86.00%) (8686/10000)
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.7363, 0.2637], device='cuda:0')
percent tensor([0.6671, 0.3329], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (3668/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (4856/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (6036/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (7206/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (8383/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (9574/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (10755/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (11933/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (13114/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (14293/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (15466/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (16641/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (17809/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (18978/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (20130/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (21297/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (22472/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (23638/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (24821/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (26000/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (27179/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (28361/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (29538/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (30724/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (31906/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (33086/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (34247/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (35412/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2352) |  Loss2: (0.0000) | Acc: (91.00%) (36590/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (37733/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (38892/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (40040/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (41209/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (42376/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (43530/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (44707/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (45830/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.4318) | Acc: (86.00%) (8619/10000)
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5735, 0.4265], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.6709, 0.3291], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.2921, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.7538, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.0720, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.1669, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(492.5722, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2252.9280, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4270.4199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1401.4960, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6149.8984, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11855.0439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3927.8831, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16641.9824, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (2476/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (5994/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (7174/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (8368/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (9537/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (91.00%) (10708/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (11882/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (13065/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (14251/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (91.00%) (15424/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (16611/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (91.00%) (17778/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (91.00%) (18938/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (20110/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (21295/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (22478/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (23642/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (91.00%) (24820/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (91.00%) (25986/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (27168/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (91.00%) (28346/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (29519/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (30681/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (31852/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (33032/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (34190/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (35355/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (36525/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (37695/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (38885/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (40030/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (41205/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (42389/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (43558/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (44735/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (45884/50000)
# TEST : Loss: (0.4973) | Acc: (85.00%) (8531/10000)
percent tensor([0.5440, 0.4560], device='cuda:0')
percent tensor([0.4993, 0.5007], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5689, 0.4311], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.7372, 0.2628], device='cuda:0')
percent tensor([0.6723, 0.3277], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (2498/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (7254/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (8443/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (9624/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (10785/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (11954/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (14321/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (15490/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (16655/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (17828/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (19014/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (20195/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (21381/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (22568/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (23762/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (24943/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (26112/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (27301/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (28468/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (29651/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (30843/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (32033/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (33204/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (34395/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (35573/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (36756/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (37937/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (39127/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (40289/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (41483/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (42642/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (43809/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (44974/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (46084/50000)
# TEST : Loss: (0.5668) | Acc: (83.00%) (8357/10000)
percent tensor([0.5438, 0.4562], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5682, 0.4318], device='cuda:0')
percent tensor([0.5732, 0.4268], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.6715, 0.3285], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (93.00%) (1319/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (3689/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (4870/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (6055/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (7229/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (8424/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (9593/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (10778/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (11958/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (13156/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (14329/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (15504/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (16680/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (17859/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (19046/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (20220/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (21411/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (22593/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (23769/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (24948/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (26105/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (27284/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (28481/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (29672/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (30843/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (32009/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (33200/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (34369/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (35538/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (36735/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (37924/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (39093/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (40270/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (41459/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (42645/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (43836/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (45015/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (46172/50000)
# TEST : Loss: (0.5051) | Acc: (84.00%) (8425/10000)
percent tensor([0.5436, 0.4564], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.5731, 0.4269], device='cuda:0')
percent tensor([0.7301, 0.2699], device='cuda:0')
percent tensor([0.6677, 0.3323], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (3701/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (93.00%) (4896/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (6088/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (93.00%) (7279/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (93.00%) (8452/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (9641/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (10827/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (12015/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (13199/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (14377/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (15551/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (16738/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (17921/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (19099/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (20285/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (21466/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (22638/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (23824/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (25023/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (26213/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (27406/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (28602/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (29792/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (30969/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (32164/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (33348/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (34533/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (35713/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (36891/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (38089/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (39261/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (40443/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (41621/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (42799/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (43988/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (45185/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (46308/50000)
# TEST : Loss: (0.4254) | Acc: (86.00%) (8640/10000)
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.4994, 0.5006], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5685, 0.4315], device='cuda:0')
percent tensor([0.5735, 0.4265], device='cuda:0')
percent tensor([0.7315, 0.2685], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (3653/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (4809/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (5957/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (7087/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (8228/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (9389/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (10552/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (11723/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (12886/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (14062/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (15219/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (16383/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (17538/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (18694/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (19855/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (21020/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (22178/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (23358/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (24524/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (25683/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (26841/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (28004/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (29177/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (30351/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (31524/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (32672/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (33834/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (90.00%) (35016/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (90.00%) (36182/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (90.00%) (37360/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (90.00%) (38527/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (39719/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (40901/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (42080/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (43241/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (44413/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (45554/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4602) | Acc: (85.00%) (8580/10000)
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5670, 0.4330], device='cuda:0')
percent tensor([0.5661, 0.4339], device='cuda:0')
percent tensor([0.7540, 0.2460], device='cuda:0')
percent tensor([0.6444, 0.3556], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (4811/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (5989/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (8341/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (9523/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (10705/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (11877/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (13069/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (14230/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (15417/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (16600/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (17771/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (18933/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (20118/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (21306/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (22487/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (23671/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (24870/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (26036/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (27199/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (28383/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (29549/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (30729/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (92.00%) (31927/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (33114/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (34296/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (35480/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (36659/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (37844/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (39028/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (40211/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (41379/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (42562/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (43743/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (44943/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (46068/50000)
# TEST : Loss: (0.4358) | Acc: (86.00%) (8618/10000)
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5013, 0.4987], device='cuda:0')
percent tensor([0.5063, 0.4937], device='cuda:0')
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.7642, 0.2358], device='cuda:0')
percent tensor([0.6410, 0.3590], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (2491/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (3657/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (4839/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (6026/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (7191/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (8367/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (9548/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (10740/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (11917/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (13097/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (14282/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (15474/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (16663/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (17862/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (19059/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (20241/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (21433/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (22597/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (23783/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (24953/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (26139/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (27321/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (28510/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (29690/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (30887/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (32080/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (33274/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (34475/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (35674/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (36856/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (38041/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (39210/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (40392/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (41582/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (42787/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (43980/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (45166/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (46309/50000)
# TEST : Loss: (0.4310) | Acc: (86.00%) (8623/10000)
percent tensor([0.5417, 0.4583], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5059, 0.4941], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.5707, 0.4293], device='cuda:0')
percent tensor([0.7639, 0.2361], device='cuda:0')
percent tensor([0.6572, 0.3428], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (2502/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (93.00%) (4896/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (8428/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (10802/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (11990/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (13182/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (14384/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (15559/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (16755/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (17931/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (19139/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (20313/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (21497/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (22690/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (23876/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (25059/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (26225/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (27413/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (28606/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (29786/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (30970/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (32155/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (33342/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (34531/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (35724/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (36913/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (38107/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2132) |  Loss2: (0.0000) | Acc: (92.00%) (39287/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (40488/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (41699/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (42884/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (44070/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (45250/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (46400/50000)
# TEST : Loss: (0.4229) | Acc: (86.00%) (8647/10000)
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.7654, 0.2346], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (2510/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (6050/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (7245/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (8445/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (9631/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (10816/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (11995/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (13180/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (14363/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (15557/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (16738/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (17922/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (19116/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (20310/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (21510/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (22686/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (23867/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (25062/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (26242/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (27442/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (28605/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (29794/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (30967/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (32163/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (33366/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (34558/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (35762/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (36957/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (38154/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (39348/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (40535/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (41721/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (42906/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (44105/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (45295/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (46430/50000)
# TEST : Loss: (0.4184) | Acc: (86.00%) (8649/10000)
percent tensor([0.5419, 0.4581], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5768, 0.4232], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.7632, 0.2368], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (3670/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (4853/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (6034/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (7223/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (8409/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (9586/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (10784/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (11975/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (13163/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (14347/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (15545/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (16723/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (17896/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (19073/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (20249/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (21425/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (22613/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (23804/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (24989/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (26166/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (27330/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (28507/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (29687/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (30868/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (32047/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (33213/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (34411/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (35600/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (36784/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (37964/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (39169/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (40368/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (41553/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (42740/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (43937/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (45126/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (46264/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4303) | Acc: (86.00%) (8671/10000)
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5754, 0.4246], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.7630, 0.2370], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.2304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.1593, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.0663, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.3114, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(491.0279, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2260.4368, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4267.3950, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1396.4554, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6163.4844, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11820.8408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3912.7290, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16575.1895, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (93.00%) (3692/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (93.00%) (4892/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (6085/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (7286/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (8463/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (9663/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (10845/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (93.00%) (12034/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (13211/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (93.00%) (14422/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (93.00%) (15614/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (93.00%) (16813/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (93.00%) (17988/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (93.00%) (19186/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (93.00%) (20370/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (93.00%) (21563/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (93.00%) (22757/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (93.00%) (23943/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (93.00%) (25128/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (93.00%) (26327/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (93.00%) (27500/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (28685/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (29875/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (31062/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (32248/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (33446/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (34625/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (35808/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (36998/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (38184/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (39369/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (40554/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (41750/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (42949/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (44127/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (45323/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (46453/50000)
# TEST : Loss: (0.4392) | Acc: (86.00%) (8651/10000)
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5758, 0.4242], device='cuda:0')
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.7588, 0.2412], device='cuda:0')
percent tensor([0.6586, 0.3414], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (4914/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (6094/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (7292/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (8484/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (9682/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (10891/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (12078/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (13253/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (14449/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (15643/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (16843/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (18031/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (19223/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (20421/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (21584/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (22783/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (23984/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (25179/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (26381/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (27563/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (28731/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (29926/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (31112/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (32300/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (33480/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (34675/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (35862/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (37049/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (93.00%) (38234/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (39423/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (40609/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (93.00%) (41796/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (42983/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (93.00%) (44165/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (45342/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (46489/50000)
# TEST : Loss: (0.4688) | Acc: (85.00%) (8555/10000)
percent tensor([0.5420, 0.4580], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5767, 0.4233], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.7615, 0.2385], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (4885/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (6080/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (7277/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (8467/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (9659/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (10854/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (12034/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (13235/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (14438/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (15635/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (16834/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (18013/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (19203/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (20392/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (21589/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (22792/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (23966/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (25184/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (26376/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (27573/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (28759/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (29953/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (31156/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (32352/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (33524/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (34730/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (35934/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (37123/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (38309/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (39505/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (40687/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (41874/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (43046/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (44243/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (45432/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (46595/50000)
# TEST : Loss: (0.4386) | Acc: (86.00%) (8644/10000)
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.7604, 0.2396], device='cuda:0')
percent tensor([0.6576, 0.3424], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (2509/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (4908/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (6102/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (7308/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (8510/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (9713/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (10907/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (12110/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (13301/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (14490/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (15690/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (16881/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (18072/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (19268/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (20467/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (21676/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (22872/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (24072/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (25265/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (26465/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (27652/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (28849/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (30039/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (31235/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (32446/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (33662/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (34868/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (36061/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (37256/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (38467/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (39654/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (40841/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (42037/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (43222/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (44408/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (45580/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (46740/50000)
# TEST : Loss: (0.4654) | Acc: (86.00%) (8647/10000)
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5762, 0.4238], device='cuda:0')
percent tensor([0.5765, 0.4235], device='cuda:0')
percent tensor([0.7616, 0.2384], device='cuda:0')
percent tensor([0.6560, 0.3440], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (2480/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (91.00%) (3649/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (4846/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (6033/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (7201/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (8368/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (91.00%) (9535/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (10721/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (91.00%) (11885/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (91.00%) (13066/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (14255/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (15432/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (16607/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (18967/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (91.00%) (20135/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (21324/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (91.00%) (22488/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (91.00%) (23663/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (24843/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (91.00%) (26021/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (91.00%) (27200/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (28381/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (91.00%) (29545/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (91.00%) (30721/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (91.00%) (31912/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (33093/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (34282/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (35460/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (36657/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (37840/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (39032/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (40210/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (41379/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (42567/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (43762/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (44947/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (46085/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4459) | Acc: (86.00%) (8635/10000)
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5622, 0.4378], device='cuda:0')
percent tensor([0.5962, 0.4038], device='cuda:0')
percent tensor([0.7979, 0.2021], device='cuda:0')
percent tensor([0.6346, 0.3654], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (93.00%) (3693/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (4882/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (6089/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (7280/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (93.00%) (8458/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (9648/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (93.00%) (10834/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (12012/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (13199/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (14379/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (15567/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (16752/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (17956/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (19164/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (20346/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (21525/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (22715/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (23912/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (25104/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (26292/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (93.00%) (27501/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (28685/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (93.00%) (29886/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (93.00%) (31073/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (32258/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (33441/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (34642/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (35836/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (37012/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (38199/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (39392/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (40583/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (41783/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (42975/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (93.00%) (44172/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (45370/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (93.00%) (46518/50000)
# TEST : Loss: (0.4277) | Acc: (87.00%) (8701/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.8032, 0.1968], device='cuda:0')
percent tensor([0.6386, 0.3614], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (2513/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (92.00%) (3688/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (92.00%) (4873/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (6073/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (7267/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (8457/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (9663/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (10855/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (12039/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (13244/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (14438/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (15615/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (93.00%) (16804/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (17993/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (19176/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (20367/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (21555/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (22754/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (23952/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (25147/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (26340/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (27535/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (28739/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (29933/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (31136/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (32330/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (33531/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (34708/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (35911/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (37117/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (38317/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (39521/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (40709/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (41908/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (43099/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (44291/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (45490/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (46634/50000)
# TEST : Loss: (0.4195) | Acc: (87.00%) (8720/10000)
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5660, 0.4340], device='cuda:0')
percent tensor([0.5953, 0.4047], device='cuda:0')
percent tensor([0.8046, 0.1954], device='cuda:0')
percent tensor([0.6419, 0.3581], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (3693/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (4886/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (6083/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (7292/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (8478/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (9685/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (10882/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (12079/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (13275/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (14484/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (15671/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (16867/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (18082/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (19278/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (20493/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (21686/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (22898/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (24105/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (25296/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (26486/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (27691/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (28890/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (30090/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (31287/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (32484/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (33678/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (34878/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (36076/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (37271/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (38466/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (39648/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (40843/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (42037/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (43235/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (44438/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (45634/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (46782/50000)
# TEST : Loss: (0.4120) | Acc: (87.00%) (8739/10000)
percent tensor([0.5539, 0.4461], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5043, 0.4957], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5973, 0.4027], device='cuda:0')
percent tensor([0.7988, 0.2012], device='cuda:0')
percent tensor([0.6453, 0.3547], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (3726/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (4934/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (6135/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (7337/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (8550/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (9736/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (10933/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (12142/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (13330/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (14532/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (15735/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (16934/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (18109/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (19309/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (20515/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (21713/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (22905/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (24109/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (25304/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (26512/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (27705/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (28920/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (30117/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (31308/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (32504/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (33706/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (34906/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (36117/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (37315/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (38504/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (39714/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (40915/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (42109/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (43304/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (44502/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (45709/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (46850/50000)
# TEST : Loss: (0.4103) | Acc: (87.00%) (8729/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.7959, 0.2041], device='cuda:0')
percent tensor([0.6519, 0.3481], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (92.00%) (6061/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (92.00%) (7256/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (8459/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (9643/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (10833/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (12043/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (13220/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (14404/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (16802/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (17996/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (19188/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (20375/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (21564/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (22745/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (23936/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (25127/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (26318/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (27494/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (28680/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (29872/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (31076/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (92.00%) (32258/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (33451/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (34626/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (35824/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (92.00%) (37007/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (38209/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (39406/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (40603/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (92.00%) (41772/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (42962/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (44139/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (45327/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (92.00%) (46477/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4817) | Acc: (85.00%) (8542/10000)
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.6007, 0.3993], device='cuda:0')
percent tensor([0.7954, 0.2046], device='cuda:0')
percent tensor([0.6513, 0.3487], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.1305, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.7451, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.0057, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1522.0332, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.4399, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2268.2036, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4264.5293, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1391.8661, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6177.4077, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11787.1025, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3897.6213, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16508.7324, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (6099/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (7296/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (8494/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (9685/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (10890/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (12077/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (13267/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (14464/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (15661/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (16866/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (18058/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (19266/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (20471/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (21666/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (22869/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (24072/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (25276/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (26480/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (27672/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (28858/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (30052/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (31255/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (32438/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (33629/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (34824/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (36017/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (37213/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (38419/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (39602/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (40811/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (42005/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (43192/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (44389/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (45591/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (46743/50000)
# TEST : Loss: (0.4378) | Acc: (86.00%) (8645/10000)
percent tensor([0.5565, 0.4435], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.5722, 0.4278], device='cuda:0')
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.7946, 0.2054], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (2511/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (3721/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (7312/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (8517/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (9717/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (10919/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (12112/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (13301/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (14500/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (15690/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (16891/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (18105/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (19307/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (20517/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (21717/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (22920/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (24110/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (25304/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (26490/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (27678/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (28889/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (30094/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (31288/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (32495/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (33686/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (34868/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (36068/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (37266/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (38453/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (39651/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (40837/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (42049/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (43254/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (44439/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (45639/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (46796/50000)
# TEST : Loss: (0.4367) | Acc: (87.00%) (8709/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.6006, 0.3994], device='cuda:0')
percent tensor([0.7954, 0.2046], device='cuda:0')
percent tensor([0.6519, 0.3481], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (94.00%) (2533/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (4942/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (94.00%) (6143/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (8542/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (9757/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (10976/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (12173/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (14565/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (15777/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (16962/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (18157/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (19363/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (20569/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (21754/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (22955/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (24147/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (25348/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (26562/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (27769/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (28965/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (30164/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (31371/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (32566/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (33773/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (34963/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (36158/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (37350/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (38561/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (39766/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (40970/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (42176/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (43383/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (44577/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (45780/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (46932/50000)
# TEST : Loss: (0.4761) | Acc: (86.00%) (8615/10000)
percent tensor([0.5565, 0.4435], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.5731, 0.4269], device='cuda:0')
percent tensor([0.6019, 0.3981], device='cuda:0')
percent tensor([0.7967, 0.2033], device='cuda:0')
percent tensor([0.6572, 0.3428], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (4958/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (6172/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (7364/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (9765/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (10977/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (12172/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (94.00%) (13356/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (94.00%) (14569/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (15777/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (16967/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (18154/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (19347/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (20554/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (21757/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (22959/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (24162/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (25370/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (26576/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (27762/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (28949/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (30130/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (31332/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (32541/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (33741/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (34940/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (36134/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (37335/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (38534/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (39732/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (40937/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (42142/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (43353/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (44544/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (45747/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (46909/50000)
# TEST : Loss: (0.4289) | Acc: (87.00%) (8724/10000)
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.6002, 0.3998], device='cuda:0')
percent tensor([0.7940, 0.2060], device='cuda:0')
percent tensor([0.6506, 0.3494], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (3689/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (4846/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (6025/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (7198/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (8380/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (9563/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (10760/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (11947/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (13125/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (14318/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (15491/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (16685/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (17874/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (19050/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (20230/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (21416/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (22582/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (23750/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (24947/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (26134/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (27324/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (28515/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (29699/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (30884/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (32095/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (33281/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (34464/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (35648/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (36842/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (38023/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (39220/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (40412/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (41592/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (42776/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (43967/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (45170/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (46323/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4383) | Acc: (87.00%) (8705/10000)
percent tensor([0.5488, 0.4512], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.6023, 0.3977], device='cuda:0')
percent tensor([0.7638, 0.2362], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (2509/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (93.00%) (3691/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (4884/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.2046) |  Loss2: (0.0000) | Acc: (92.00%) (6058/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (7253/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (8439/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (9630/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (10846/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (12024/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (13206/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (14403/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (15612/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (16809/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (18014/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (19221/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (20424/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (21614/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (22810/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (24001/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (25187/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (26392/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (27604/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (28801/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (29997/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (31196/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (32402/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (33599/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (34802/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (36021/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (37224/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (38418/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (39613/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (40816/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (42031/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (43222/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (44427/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (45639/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (46795/50000)
# TEST : Loss: (0.4209) | Acc: (87.00%) (8715/10000)
percent tensor([0.5519, 0.4481], device='cuda:0')
percent tensor([0.4979, 0.5021], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5558, 0.4442], device='cuda:0')
percent tensor([0.6013, 0.3987], device='cuda:0')
percent tensor([0.7626, 0.2374], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (3714/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (93.00%) (4921/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (7310/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (9722/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (10919/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (12114/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (13318/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (14533/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (15738/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (16936/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (18137/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (19328/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (20520/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (93.00%) (21754/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (22957/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (24162/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (25377/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (26580/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (27770/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (28970/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (30176/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (31359/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (32566/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (33755/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (34956/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (36147/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (37363/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (38578/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (39783/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (40980/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (42179/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (43363/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (44575/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (45779/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (46944/50000)
# TEST : Loss: (0.4150) | Acc: (87.00%) (8748/10000)
percent tensor([0.5502, 0.4498], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5574, 0.4426], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.7644, 0.2356], device='cuda:0')
percent tensor([0.6460, 0.3540], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (2534/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (4965/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (6167/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (7376/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (8579/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (9792/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (10999/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (12210/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (13421/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (14622/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (15836/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (17040/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (18242/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (19455/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (20655/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (21845/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (23033/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (24229/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (25428/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (26633/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (27832/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (29035/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (94.00%) (30234/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (31448/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (32650/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (33866/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (35075/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (36266/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (37461/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (38672/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (39875/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (41077/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (42291/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (43498/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (44701/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (45903/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (47058/50000)
# TEST : Loss: (0.4121) | Acc: (87.00%) (8760/10000)
percent tensor([0.5509, 0.4491], device='cuda:0')
percent tensor([0.4979, 0.5021], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.7713, 0.2287], device='cuda:0')
percent tensor([0.6547, 0.3453], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (3753/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (4967/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (6145/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (93.00%) (8539/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (9735/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (10930/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (12140/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (13347/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (14546/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (15767/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (16971/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (93.00%) (18167/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (19376/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (93.00%) (20573/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (21796/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (23003/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (24220/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (25427/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (26633/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (27836/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (29049/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (30257/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (31451/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (32657/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (33868/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (35077/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (36294/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (37493/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (38712/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (39901/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (41105/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (42311/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (43518/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (44728/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (45939/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (47096/50000)
# TEST : Loss: (0.4089) | Acc: (87.00%) (8765/10000)
percent tensor([0.5535, 0.4465], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6057, 0.3943], device='cuda:0')
percent tensor([0.7655, 0.2345], device='cuda:0')
percent tensor([0.6601, 0.3399], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (4973/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (6175/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (7369/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (8566/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (9771/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (10969/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (12171/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (13388/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (14602/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (15796/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (16983/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (18189/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (19374/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (20573/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (21788/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (22995/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (24196/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (25400/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (26608/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (27780/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (28994/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (30191/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (31388/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (32586/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (33790/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (35000/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (36212/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (37410/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (38605/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (39812/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (41017/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (42196/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (43396/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (44605/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (45816/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (46967/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.5422) | Acc: (84.00%) (8490/10000)
percent tensor([0.5530, 0.4470], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.6083, 0.3917], device='cuda:0')
percent tensor([0.7689, 0.2311], device='cuda:0')
percent tensor([0.6688, 0.3312], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.0348, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.0591, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.4407, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1520.4474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.1556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2274.7036, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4261.9419, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1386.8317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6191.6094, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11753.1699, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3882.6909, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16442.5801, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (2547/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (3758/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (4957/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (6160/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (7376/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (9821/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (11031/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (12240/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (13459/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (15854/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (17051/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (18251/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (19454/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (20660/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (21864/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (23075/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (24268/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (25477/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (26683/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (27878/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (29090/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (30295/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (31499/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (32702/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (33885/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (35086/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (36284/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (37483/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (38701/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (39916/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (41120/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (42326/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (43524/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (44727/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (45943/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (47096/50000)
# TEST : Loss: (0.4557) | Acc: (86.00%) (8663/10000)
percent tensor([0.5539, 0.4461], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5619, 0.4381], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.7656, 0.2344], device='cuda:0')
percent tensor([0.6595, 0.3405], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (3751/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (4974/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (6197/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (7396/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (8608/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (9822/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (11029/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (12217/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (13432/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (14638/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (15849/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (17062/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (18272/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (19476/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (20696/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (21898/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (23103/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (24318/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (25521/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (26730/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (27927/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (29135/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (30334/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (31536/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (32731/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (33931/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (35145/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (36346/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (37550/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (38751/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (39943/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (41167/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (42365/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (43572/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (44765/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (45974/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (47144/50000)
# TEST : Loss: (0.4296) | Acc: (87.00%) (8748/10000)
percent tensor([0.5536, 0.4464], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.6029, 0.3971], device='cuda:0')
percent tensor([0.7611, 0.2389], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (3745/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (4949/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (7382/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (8603/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (9812/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (12227/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (13442/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (14653/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (15871/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (17080/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (18287/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (19494/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (20710/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (21918/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (23133/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (24344/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (25551/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (26760/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (27972/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (29183/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (30397/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (31599/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (32790/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (34002/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (35204/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (36398/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (37597/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (38805/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (40010/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (41227/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (42425/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (43627/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (44841/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (46055/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (47223/50000)
# TEST : Loss: (0.4389) | Acc: (87.00%) (8709/10000)
percent tensor([0.5531, 0.4469], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5618, 0.4382], device='cuda:0')
percent tensor([0.6047, 0.3953], device='cuda:0')
percent tensor([0.7605, 0.2395], device='cuda:0')
percent tensor([0.6611, 0.3389], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (95.00%) (4989/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (95.00%) (6205/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (95.00%) (7422/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (95.00%) (8646/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (9852/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (95.00%) (11074/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (12287/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (13489/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (14700/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (15911/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (17118/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (18330/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (19551/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (20770/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (21988/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (23210/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (24419/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (25627/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (26835/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (28038/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (29236/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (30452/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (31659/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (32862/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (34065/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (35280/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (36483/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (37698/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (38915/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (40118/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (41324/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (42524/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (43724/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (44934/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (46143/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (47304/50000)
# TEST : Loss: (0.4390) | Acc: (86.00%) (8695/10000)
percent tensor([0.5538, 0.4462], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5613, 0.4387], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.7630, 0.2370], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (3678/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (4856/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (6042/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (7216/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (8394/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (9570/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (10745/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (11921/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (13103/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (14277/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (15457/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (16655/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (17836/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (19032/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (20218/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (21402/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (22585/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (92.00%) (23784/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (24994/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (26203/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (27395/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (28593/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (29779/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (30966/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (32150/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (92.00%) (33338/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (34526/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (35719/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (36913/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (38102/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (92.00%) (39299/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (92.00%) (40495/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (92.00%) (41692/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (92.00%) (42905/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (92.00%) (44101/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (92.00%) (45296/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (46458/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4415) | Acc: (86.00%) (8695/10000)
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.6209, 0.3791], device='cuda:0')
percent tensor([0.7319, 0.2681], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (2519/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (3725/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (6135/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (8540/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (9747/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (10961/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (12164/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (13386/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (14590/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (15781/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (16995/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (19411/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (20622/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (21827/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (23040/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (24241/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (25443/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (26657/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (27862/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (29072/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (30286/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (31497/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (32700/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (33906/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (35117/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (36328/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (37528/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (38733/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (39939/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (41141/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (42358/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (43569/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (44758/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (45958/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (47129/50000)
# TEST : Loss: (0.4193) | Acc: (87.00%) (8739/10000)
percent tensor([0.5529, 0.4471], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5065, 0.4935], device='cuda:0')
percent tensor([0.5762, 0.4238], device='cuda:0')
percent tensor([0.6126, 0.3874], device='cuda:0')
percent tensor([0.7309, 0.2691], device='cuda:0')
percent tensor([0.6832, 0.3168], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (3735/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (4938/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (6150/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (7360/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (8579/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (9780/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (10989/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (12207/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (13416/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (14631/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (15845/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (17053/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (18255/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (19472/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (20685/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (21896/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (23095/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (24300/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (25526/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (26756/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (27979/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (29198/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (30400/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (31597/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (32799/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (34020/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (35222/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (36441/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (37655/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (38858/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (40068/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (41273/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (42488/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (43705/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (44915/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (46131/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (47301/50000)
# TEST : Loss: (0.4101) | Acc: (87.00%) (8782/10000)
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5079, 0.4921], device='cuda:0')
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.7364, 0.2636], device='cuda:0')
percent tensor([0.6897, 0.3103], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (3749/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (4972/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (6182/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (7395/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (8612/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (9842/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (11054/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (12269/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (13476/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (14700/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (15917/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (17139/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (18354/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (95.00%) (19588/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (95.00%) (20795/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (95.00%) (22018/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (23222/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (95.00%) (24444/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (25650/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (95.00%) (26879/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (28083/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (29301/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (95.00%) (30531/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (95.00%) (31742/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (32953/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (34183/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (35397/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (95.00%) (36606/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (37804/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (39015/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (40234/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (41453/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (42678/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (43901/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (45114/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (46326/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (47484/50000)
# TEST : Loss: (0.4076) | Acc: (87.00%) (8794/10000)
percent tensor([0.5528, 0.4472], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.6134, 0.3866], device='cuda:0')
percent tensor([0.7426, 0.2574], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (6212/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (7443/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (8660/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (9890/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (11099/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (12316/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (13530/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (14749/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (15974/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (17195/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (18414/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (19632/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (20848/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (22059/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (23283/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (24495/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (25715/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (26933/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (28135/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (29353/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (30563/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (31779/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (32994/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (34197/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (35417/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (36632/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (37853/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (39076/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (40293/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (41524/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (42737/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (43950/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (45166/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (46370/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (47548/50000)
# TEST : Loss: (0.4015) | Acc: (88.00%) (8808/10000)
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5096, 0.4904], device='cuda:0')
percent tensor([0.5666, 0.4334], device='cuda:0')
percent tensor([0.6133, 0.3867], device='cuda:0')
percent tensor([0.7482, 0.2518], device='cuda:0')
percent tensor([0.7039, 0.2961], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (5009/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (6215/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (7428/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8629/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (9835/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (11037/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (12248/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (13471/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (14658/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (17076/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (18304/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (19520/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (20738/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (21956/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (23161/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (24374/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (25593/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (26782/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (27984/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (29193/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (30413/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (31623/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (32840/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (34046/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (35260/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (36477/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (37696/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (38912/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (40124/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (41337/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (42551/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (43750/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (44946/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (46159/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (47332/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4344) | Acc: (86.00%) (8676/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5096, 0.4904], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.7473, 0.2527], device='cuda:0')
percent tensor([0.7015, 0.2985], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.8185, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.9388, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.0292, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1518.8047, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(486.6616, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2280.3442, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4259.2266, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1381.9280, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6204.7471, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11719.6816, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3867.7371, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16376.6367, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (4991/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (6218/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (7447/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (8676/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (9907/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (11111/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (12326/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (13542/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (14747/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (15959/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (17158/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (18368/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (19573/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (20767/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (21980/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (23191/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (24396/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (25609/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (26829/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (28035/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (29257/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (30467/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (31671/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (32885/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (34093/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (35315/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (36530/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (37742/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (38970/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (40165/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (41368/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (43794/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (45005/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (46210/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (47374/50000)
# TEST : Loss: (0.4695) | Acc: (86.00%) (8664/10000)
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5665, 0.4335], device='cuda:0')
percent tensor([0.6142, 0.3858], device='cuda:0')
percent tensor([0.7471, 0.2529], device='cuda:0')
percent tensor([0.7041, 0.2959], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (6257/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (7474/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (8707/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (9925/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (11146/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (12361/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (13586/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (14807/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (16013/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (17241/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (18450/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (19656/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (20875/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (22105/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (23303/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (24524/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (25737/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (26952/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (28169/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (29391/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (30599/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (31831/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (33035/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (34244/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (35442/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (36647/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (37858/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (39073/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (40299/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (41507/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (42706/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (43913/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (45118/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (46329/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (47496/50000)
# TEST : Loss: (0.4203) | Acc: (87.00%) (8792/10000)
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5095, 0.4905], device='cuda:0')
percent tensor([0.5663, 0.4337], device='cuda:0')
percent tensor([0.6141, 0.3859], device='cuda:0')
percent tensor([0.7471, 0.2529], device='cuda:0')
percent tensor([0.6995, 0.3005], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (3804/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (5029/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (6245/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (7468/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (8693/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (9914/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (11146/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (12357/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (13578/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (14782/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (15984/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (17178/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (18387/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (19607/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (20801/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (22007/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (94.00%) (23221/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (94.00%) (24437/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (25662/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (26879/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (28100/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (94.00%) (29301/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (30515/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (31731/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (32946/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (34167/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (35381/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (36582/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (94.00%) (37812/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (39022/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (94.00%) (40233/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (94.00%) (41433/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (42642/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (43860/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (45076/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (46285/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (47462/50000)
# TEST : Loss: (0.4220) | Acc: (87.00%) (8779/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5094, 0.4906], device='cuda:0')
percent tensor([0.5661, 0.4339], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.7513, 0.2487], device='cuda:0')
percent tensor([0.7038, 0.2962], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (2561/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (3782/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (5005/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (6224/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (7438/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (8670/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (9890/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (11105/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (12318/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (13543/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (14758/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (15974/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (17206/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (18412/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (19624/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (20839/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (22061/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (23278/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (24494/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (25717/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (26928/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (28154/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (29366/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (30574/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (31774/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (32978/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (34199/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (35424/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (36633/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (37854/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (39057/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (40277/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (41488/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (42703/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (43918/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (45139/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (46370/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (47533/50000)
# TEST : Loss: (0.4750) | Acc: (86.00%) (8638/10000)
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5095, 0.4905], device='cuda:0')
percent tensor([0.5661, 0.4339], device='cuda:0')
percent tensor([0.6160, 0.3840], device='cuda:0')
percent tensor([0.7485, 0.2515], device='cuda:0')
percent tensor([0.7032, 0.2968], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (95.00%) (1343/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (2534/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (4884/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (92.00%) (6067/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (7251/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (8434/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (10780/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (11985/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (13164/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (14351/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (15548/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (16746/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (17943/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (19135/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (92.00%) (20327/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (92.00%) (21521/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (92.00%) (22723/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (92.00%) (23918/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (92.00%) (25108/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (26321/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (27519/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (28710/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (29904/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (31103/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (32311/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (33508/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (34710/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (35927/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (37137/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (38342/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (39541/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (40740/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (41950/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (43159/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (44379/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (45578/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (46728/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'


Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (2527/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (4943/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (6145/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (8562/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (9767/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (10962/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (12176/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (13392/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (14596/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (15792/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (17004/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (18224/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (19442/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (20645/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (21845/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (23062/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (24267/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (25483/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (26702/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (27911/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (29117/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (30319/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (31531/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (32749/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (33959/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (35161/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (36372/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (37586/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (38807/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (40021/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (41248/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (42455/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (43662/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (44883/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (46101/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (47267/50000)
# TEST : Loss: (0.4294) | Acc: (87.00%) (8750/10000)
percent tensor([0.5579, 0.4421], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5059, 0.4941], device='cuda:0')
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.6151, 0.3849], device='cuda:0')
percent tensor([0.7206, 0.2794], device='cuda:0')
percent tensor([0.6997, 0.3003], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (6193/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (8624/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (9841/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (11054/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (12273/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (13485/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (14689/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (15900/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (17115/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (18326/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (19538/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (20748/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (21961/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (23178/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (24390/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (25607/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (26826/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (28035/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (29260/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (30479/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (31691/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (32906/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (34131/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (35350/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (36567/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (37791/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (39005/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (40221/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (41438/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (42652/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (43854/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (45080/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (46303/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (47481/50000)
# TEST : Loss: (0.4208) | Acc: (87.00%) (8787/10000)
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.5050, 0.4950], device='cuda:0')
percent tensor([0.5819, 0.4181], device='cuda:0')
percent tensor([0.6158, 0.3842], device='cuda:0')
percent tensor([0.7275, 0.2725], device='cuda:0')
percent tensor([0.7041, 0.2959], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (2569/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (3796/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (6254/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (7479/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (8679/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (9895/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (11110/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (12334/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (13557/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (14777/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (15992/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (17209/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (18424/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (19623/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (20842/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (22054/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (23268/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (24491/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (25698/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (26933/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (28161/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (29371/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (30589/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (31800/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (33016/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (34244/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (35466/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (36691/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (37908/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (39128/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (40356/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (41594/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (42821/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (44042/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (45254/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (46471/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (47640/50000)
# TEST : Loss: (0.4122) | Acc: (88.00%) (8803/10000)
percent tensor([0.5570, 0.4430], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.6128, 0.3872], device='cuda:0')
percent tensor([0.7300, 0.2700], device='cuda:0')
percent tensor([0.7006, 0.2994], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (3790/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (5000/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (6220/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (7423/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (8645/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (9873/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (11087/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (12298/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (13524/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (14745/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (15968/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (17194/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (18419/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (19648/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (20871/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (22090/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (23320/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (24537/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (25756/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (26976/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (28207/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (29416/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (30616/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (31826/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (33051/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (34273/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (35499/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (36736/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (37965/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (39168/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (40395/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (41611/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (42845/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (44079/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (45294/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (46509/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (47684/50000)
# TEST : Loss: (0.4115) | Acc: (87.00%) (8792/10000)
percent tensor([0.5585, 0.4415], device='cuda:0')
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6134, 0.3866], device='cuda:0')
percent tensor([0.7355, 0.2645], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (4999/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (6208/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (7424/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (8637/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (9854/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (11068/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (12285/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (13516/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (14712/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (15917/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (17114/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (18321/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (19539/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (20749/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (21949/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (23158/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (24356/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (25563/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (26769/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (28006/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (29228/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (30440/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (31652/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (32847/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (34056/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (35257/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (36477/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (37702/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (38906/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (40120/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (41322/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (42542/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (43758/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (44991/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (46209/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (47378/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
# TEST : Loss: (0.4438) | Acc: (87.00%) (8745/10000)
percent tensor([0.5579, 0.4421], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.6121, 0.3879], device='cuda:0')
percent tensor([0.7318, 0.2682], device='cuda:0')
percent tensor([0.7033, 0.2967], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.8209, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.9047, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.4156, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.6072, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.8419, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2290.1885, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4263.2456, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.1859, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6228.4785, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11706.1670, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3858.8577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16336.8896, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 151 | Batch_idx: 0 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (2567/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (5029/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (6255/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (7481/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (8709/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (9933/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (12359/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (13586/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (14795/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (16018/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (17242/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (18471/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (19695/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (20906/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (22127/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (23355/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (24574/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (25797/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (27017/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (28243/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (29475/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (30702/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (31914/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (33126/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (34347/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (35561/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (36773/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (37974/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (39190/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (40402/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (41611/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (42822/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (44028/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (45239/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (46454/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (47620/50000)
# TEST : Loss: (0.4312) | Acc: (87.00%) (8783/10000)
percent tensor([0.5580, 0.4420], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.6135, 0.3865], device='cuda:0')
percent tensor([0.7347, 0.2653], device='cuda:0')
percent tensor([0.7059, 0.2941], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (2574/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (3795/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (6240/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (7471/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (8696/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (9916/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (11148/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (12378/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (13605/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (14830/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (16051/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (17284/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (18497/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (19720/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (20928/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (22152/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (23371/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (24598/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (25817/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (27028/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (28242/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (29459/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (30684/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (31904/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (33107/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (34324/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (35546/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (36783/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (38012/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (39240/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (40462/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (41676/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (42890/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (44102/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (45317/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (46545/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (47722/50000)
# TEST : Loss: (0.5031) | Acc: (86.00%) (8644/10000)
percent tensor([0.5578, 0.4422], device='cuda:0')
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.6139, 0.3861], device='cuda:0')
percent tensor([0.7331, 0.2669], device='cuda:0')
percent tensor([0.7007, 0.2993], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (5009/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (6237/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (7472/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (8702/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (9922/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (11145/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (12360/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (13579/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (14793/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (16022/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (17243/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (18469/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (19709/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (20938/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (22157/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (23380/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (24605/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (25823/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (27046/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (28267/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (29479/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (30693/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (31911/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (33146/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (34370/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (35587/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (36814/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (38048/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (39269/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (40480/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (41690/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (42921/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (44156/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (45381/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (46603/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (47778/50000)
# TEST : Loss: (0.4551) | Acc: (87.00%) (8729/10000)
percent tensor([0.5578, 0.4422], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.7320, 0.2680], device='cuda:0')
percent tensor([0.7056, 0.2944], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (3797/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (5022/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (6249/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (7479/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (8707/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (9934/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (11162/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (12387/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (13597/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (14829/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (16046/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (17272/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (18499/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (19712/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (20936/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (22153/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (23369/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (24589/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (25813/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (27045/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (28257/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (29463/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (30694/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (31925/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (33159/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (34393/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (35607/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (36827/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (38038/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (39254/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (40483/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (41713/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (42945/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (44154/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (45365/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (46588/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (47752/50000)
# TEST : Loss: (0.4674) | Acc: (87.00%) (8733/10000)
percent tensor([0.5582, 0.4418], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6132, 0.3868], device='cuda:0')
percent tensor([0.7335, 0.2665], device='cuda:0')
percent tensor([0.7022, 0.2978], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (95.00%) (3770/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (4974/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (6184/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (7400/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (8613/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (9809/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (11003/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (12213/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (13425/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (14633/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (15828/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1659) |  Loss2: (0.0000) | Acc: (94.00%) (17025/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (18225/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (19435/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (20649/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (21847/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (23042/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (24259/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (25467/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (26671/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (27882/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (29093/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (30297/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (31516/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (32738/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (33944/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (35153/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (36371/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (37596/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (38810/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (40023/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (41231/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (42428/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (43631/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (44836/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (46063/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (47227/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_155.pth.tar'
# TEST : Loss: (0.4423) | Acc: (87.00%) (8756/10000)
percent tensor([0.5513, 0.4487], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.5740, 0.4260], device='cuda:0')
percent tensor([0.6163, 0.3837], device='cuda:0')
percent tensor([0.7443, 0.2557], device='cuda:0')
percent tensor([0.7139, 0.2861], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (2543/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (3755/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (4962/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (6180/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (7400/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (9834/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (94.00%) (11057/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (12279/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (13506/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (14723/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (15941/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (17174/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (18389/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (19603/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (20825/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (22035/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (23256/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (24468/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (25679/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (26911/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (28121/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (29350/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (30571/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (31781/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (32978/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (34204/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (35422/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (36657/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (37879/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (39109/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (40332/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (41555/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (42781/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (44009/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (45238/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (46452/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (47628/50000)
# TEST : Loss: (0.4370) | Acc: (87.00%) (8761/10000)
percent tensor([0.5506, 0.4494], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5670, 0.4330], device='cuda:0')
percent tensor([0.6154, 0.3846], device='cuda:0')
percent tensor([0.7503, 0.2497], device='cuda:0')
percent tensor([0.7206, 0.2794], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (3777/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (4992/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (6211/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (7438/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (8662/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (9885/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (11105/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (12327/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (13554/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (14775/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (15990/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (17212/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (18427/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (19641/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (20863/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (22087/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (23314/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (24540/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (25756/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (26983/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (28201/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (29431/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (30651/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (31875/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (33093/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (34321/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (35530/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (36741/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (37978/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (39209/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (40424/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (41648/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (42866/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (44101/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (45323/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (46543/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (47722/50000)
# TEST : Loss: (0.4258) | Acc: (87.00%) (8792/10000)
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5114, 0.4886], device='cuda:0')
percent tensor([0.5685, 0.4315], device='cuda:0')
percent tensor([0.6197, 0.3803], device='cuda:0')
percent tensor([0.7507, 0.2493], device='cuda:0')
percent tensor([0.7301, 0.2699], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (2591/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (5029/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (6261/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (7480/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8707/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (9931/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (11162/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (12394/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (13622/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (14843/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (16073/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (17305/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (18528/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (19763/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (20967/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (22203/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (23428/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (24638/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (25862/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (27085/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (28318/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (29538/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (30758/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (31982/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (33205/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (34414/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (35652/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (36880/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (38106/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (39332/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (40553/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (41791/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (43007/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (44226/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (45438/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (46674/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (47857/50000)
# TEST : Loss: (0.4161) | Acc: (88.00%) (8801/10000)
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.6177, 0.3823], device='cuda:0')
percent tensor([0.7521, 0.2479], device='cuda:0')
percent tensor([0.7303, 0.2697], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (2571/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (6228/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (7457/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (8677/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (9901/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (11123/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (12356/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (13573/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (14809/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (16025/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (17247/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (18469/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (19693/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (20929/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (22159/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (23402/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (24632/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (25869/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (27086/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (28304/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (29545/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (30764/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (31994/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (33210/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (34448/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (35668/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (36902/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (38134/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (39358/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (40602/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (41838/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (43074/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (44311/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (45548/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (46769/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (47946/50000)
# TEST : Loss: (0.4074) | Acc: (88.00%) (8811/10000)
percent tensor([0.5518, 0.4482], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.6236, 0.3764], device='cuda:0')
percent tensor([0.7494, 0.2506], device='cuda:0')
percent tensor([0.7301, 0.2699], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (95.00%) (3809/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (5032/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (6269/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (8711/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (9939/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (11155/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (12383/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (13618/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (14833/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (16044/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (17268/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (18496/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (19724/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (20954/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (22179/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (23393/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (24609/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (25834/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (27038/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (28277/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (29493/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (30732/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (31954/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (33183/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (34407/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (35641/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (36860/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (38085/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (39324/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (40540/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (41769/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (42994/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (44222/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (45456/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (46680/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (47855/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_160.pth.tar'
# TEST : Loss: (0.4801) | Acc: (86.00%) (8671/10000)
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.5731, 0.4269], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.7488, 0.2512], device='cuda:0')
percent tensor([0.7274, 0.2726], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.3250, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.9017, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.4818, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.5132, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(484.4617, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2295.6196, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4260.1484, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.3499, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6240.8916, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11673.7471, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3844.0376, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16271.3701, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (5042/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (6278/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (7499/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (96.00%) (8728/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (9953/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (11169/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (12403/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (13631/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (14851/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (16081/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (17312/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (18536/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (19769/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (20988/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (22213/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (23446/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (24655/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (25870/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (27091/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (28314/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (29530/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (30745/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (31977/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (33203/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (34417/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (35639/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (36862/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (38074/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (39308/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (40533/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (41760/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (42979/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (44202/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (45426/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (46655/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (47820/50000)
# TEST : Loss: (0.4495) | Acc: (87.00%) (8746/10000)
percent tensor([0.5508, 0.4492], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.5727, 0.4273], device='cuda:0')
percent tensor([0.6258, 0.3742], device='cuda:0')
percent tensor([0.7478, 0.2522], device='cuda:0')
percent tensor([0.7257, 0.2743], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (7497/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (95.00%) (8723/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (9942/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (95.00%) (11177/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (95.00%) (12410/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (13653/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (14880/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (16107/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (17321/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (18538/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (19746/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (20966/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (22170/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (23395/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (24622/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (25837/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (27074/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (28299/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (29508/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (30741/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (31969/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (33199/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (34421/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (35643/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (36869/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (38082/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (39310/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (40533/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (41758/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (42992/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (44222/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (45446/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (46689/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (47848/50000)
# TEST : Loss: (0.5104) | Acc: (86.00%) (8636/10000)
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.5722, 0.4278], device='cuda:0')
percent tensor([0.6233, 0.3767], device='cuda:0')
percent tensor([0.7453, 0.2547], device='cuda:0')
percent tensor([0.7303, 0.2697], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (2586/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (3820/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (6245/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (7483/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (9936/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (11163/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (12394/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (13631/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (14864/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (16097/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (17314/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (18545/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (19774/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (21000/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (22221/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (23451/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (24692/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (25918/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (27133/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (28348/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (29573/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (30799/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (32030/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (33256/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (34493/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (35715/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (36951/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (38187/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (39421/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (40653/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (41902/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (43122/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (44354/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (45584/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (46805/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (47971/50000)
# TEST : Loss: (0.5363) | Acc: (85.00%) (8584/10000)
percent tensor([0.5518, 0.4482], device='cuda:0')
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.6247, 0.3753], device='cuda:0')
percent tensor([0.7476, 0.2524], device='cuda:0')
percent tensor([0.7342, 0.2658], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (5044/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (6275/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (7508/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (8749/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (9977/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (11204/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (12441/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (13669/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (14903/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (16142/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (17376/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (18606/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (19833/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (21067/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (22299/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (23526/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (24761/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (25995/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (27230/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (28464/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (29706/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (30939/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (32167/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (33400/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (34633/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (35869/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (37100/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (38317/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (39542/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (40765/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (41991/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (43224/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (44450/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (45673/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (46893/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (48072/50000)
# TEST : Loss: (0.4690) | Acc: (87.00%) (8740/10000)
percent tensor([0.5509, 0.4491], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5144, 0.4856], device='cuda:0')
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.7481, 0.2519], device='cuda:0')
percent tensor([0.7347, 0.2653], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (3798/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (5011/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (6222/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (7454/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (8660/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (9890/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (11108/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (12327/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (13548/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (14768/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (15974/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (17200/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (18406/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (19628/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (20836/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (22068/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (23287/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (24501/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (25710/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (26933/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (28163/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (29374/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (30585/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (31807/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (33013/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (34249/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (35465/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (36680/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (37905/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (39130/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (40358/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (41571/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (42783/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (44008/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (45229/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (46453/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (47634/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_165.pth.tar'
# TEST : Loss: (0.4391) | Acc: (87.00%) (8779/10000)
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.6355, 0.3645], device='cuda:0')
percent tensor([0.7784, 0.2216], device='cuda:0')
percent tensor([0.7472, 0.2528], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 166 | Batch_idx: 0 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (6257/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (95.00%) (7486/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (8695/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (9917/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (11139/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (12366/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (13591/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (14808/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (16034/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (17261/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (18498/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (19730/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (20961/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (22186/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23414/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (24638/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (25852/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (27076/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (28296/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (29523/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (30758/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (31984/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (33207/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (34437/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (35671/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (36898/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (38123/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (39368/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (40577/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (41817/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (43054/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (44279/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (45517/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (46753/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (47949/50000)
# TEST : Loss: (0.4219) | Acc: (88.00%) (8800/10000)
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5067, 0.4933], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5666, 0.4334], device='cuda:0')
percent tensor([0.6386, 0.3614], device='cuda:0')
percent tensor([0.7802, 0.2198], device='cuda:0')
percent tensor([0.7404, 0.2596], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (2568/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (5047/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (6280/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (7513/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (8745/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (9973/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (11204/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (12424/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (13646/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (14883/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (16109/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (17336/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (18570/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (19816/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (21050/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (22268/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (23499/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (24721/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (25953/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (27183/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (28414/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (29645/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (30868/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (32090/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (33305/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (34537/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (35777/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (37013/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (38241/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (39474/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (40703/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (41943/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (43181/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (44408/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (45635/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (46868/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (48042/50000)
# TEST : Loss: (0.4136) | Acc: (88.00%) (8820/10000)
percent tensor([0.5528, 0.4472], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.6353, 0.3647], device='cuda:0')
percent tensor([0.7719, 0.2281], device='cuda:0')
percent tensor([0.7344, 0.2656], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 168 | Batch_idx: 0 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (5046/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (6284/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (7515/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (8757/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (9977/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (11216/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (12445/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (13669/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (14909/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (16144/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (17383/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (18608/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (19837/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (21060/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (22299/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (23527/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (24761/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (25989/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (27223/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (28465/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (29707/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (30941/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (32186/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (33419/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (34654/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (35897/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (37132/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (38355/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (39594/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (40838/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (42090/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (43315/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (44560/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (45790/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (47018/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (48199/50000)
# TEST : Loss: (0.4098) | Acc: (88.00%) (8840/10000)
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.5087, 0.4913], device='cuda:0')
percent tensor([0.5205, 0.4795], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.7675, 0.2325], device='cuda:0')
percent tensor([0.7344, 0.2656], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 169 | Batch_idx: 0 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (3801/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (5027/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (6260/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (95.00%) (7488/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (8725/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (9967/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (11203/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (12436/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (13669/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (14901/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (16127/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (17360/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (18584/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (19825/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (21069/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (22307/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (23551/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (24788/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (26020/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (27253/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (28493/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (29719/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (30958/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (32183/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (33421/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (34661/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (35890/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (37126/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (38357/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (39585/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (40820/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (42065/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (43314/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (44550/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (45791/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (47013/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (48199/50000)
# TEST : Loss: (0.4052) | Acc: (88.00%) (8862/10000)
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6341, 0.3659], device='cuda:0')
percent tensor([0.7658, 0.2342], device='cuda:0')
percent tensor([0.7364, 0.2636], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (5052/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (6284/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (7511/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (95.00%) (8723/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (95.00%) (9951/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (95.00%) (11179/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (12406/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (13645/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (14883/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (16111/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (17342/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (18581/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (19816/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (21044/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (22276/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (23495/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (24735/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (25973/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (27194/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (28423/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (29648/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (30885/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (32115/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (33335/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (34558/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (35797/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (37029/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (38265/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (39493/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (40728/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (41955/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (43183/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (44406/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (45619/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (46842/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (48018/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_170.pth.tar'
# TEST : Loss: (0.4563) | Acc: (87.00%) (8757/10000)
percent tensor([0.5543, 0.4457], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5211, 0.4789], device='cuda:0')
percent tensor([0.5702, 0.4298], device='cuda:0')
percent tensor([0.6332, 0.3668], device='cuda:0')
percent tensor([0.7668, 0.2332], device='cuda:0')
percent tensor([0.7375, 0.2625], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.8649, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(833.9225, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.0899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.6746, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(482.9383, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2300.8420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4256.6987, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1369.4996, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6253.3975, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11641.2803, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3829.2222, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16206.6660, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 171 | Batch_idx: 0 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (3829/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (5055/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (6276/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (7514/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (8752/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (9993/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (11223/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (12461/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (13705/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (14944/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (16173/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (17398/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (18628/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (19867/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (21111/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (22359/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (23600/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (24830/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (26053/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (27281/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (28512/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (29743/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (30975/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (32207/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (33440/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (34662/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (35883/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (37114/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (38347/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (39580/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (40811/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (42045/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (43273/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (44484/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (45703/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (46921/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (48096/50000)
# TEST : Loss: (0.5194) | Acc: (85.00%) (8587/10000)
percent tensor([0.5540, 0.4460], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.6327, 0.3673], device='cuda:0')
percent tensor([0.7674, 0.2326], device='cuda:0')
percent tensor([0.7369, 0.2631], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 172 | Batch_idx: 0 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (5057/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (6287/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (7524/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (8747/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (9975/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (11197/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (12433/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (13662/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (14902/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (16127/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (17356/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (18602/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (19829/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (21062/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (22290/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (23519/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (24754/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (25987/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (27219/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (28455/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (29689/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (30923/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (32158/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (33377/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (34607/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (35845/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (37068/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (38297/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (39525/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (40753/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (41972/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (43206/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (44437/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (45663/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (46889/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (48061/50000)
# TEST : Loss: (0.4694) | Acc: (87.00%) (8746/10000)
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5212, 0.4788], device='cuda:0')
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.6341, 0.3659], device='cuda:0')
percent tensor([0.7679, 0.2321], device='cuda:0')
percent tensor([0.7364, 0.2636], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 173 | Batch_idx: 0 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (5062/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (6288/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (7521/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (8741/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (9980/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (11199/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (12427/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (13664/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (14909/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (16143/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (17379/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (18614/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (19849/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (21081/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (22326/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (23560/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (24808/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (26048/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (27280/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (28509/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (29740/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (30980/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (32223/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (33469/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (34707/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (35946/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (37180/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (38411/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (39649/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (40878/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (42108/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (43340/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (44556/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (45758/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (46989/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (48184/50000)
# TEST : Loss: (0.5096) | Acc: (86.00%) (8620/10000)
percent tensor([0.5541, 0.4459], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5209, 0.4791], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.7623, 0.2377], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 174 | Batch_idx: 0 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (5042/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (6281/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (7518/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (8754/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (9996/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (11234/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (12464/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (13708/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (14949/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (16200/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (17438/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (18672/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (19914/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (21140/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (22377/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (23612/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (24834/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (26065/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (27307/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (28547/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (29783/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (31019/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (32257/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (33477/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (34708/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (35947/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (37176/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (38416/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (39642/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (40871/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (42113/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (43324/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (44549/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (45780/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (47008/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (48194/50000)
# TEST : Loss: (0.4370) | Acc: (87.00%) (8799/10000)
percent tensor([0.5539, 0.4461], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.6284, 0.3716], device='cuda:0')
percent tensor([0.7617, 0.2383], device='cuda:0')
percent tensor([0.7326, 0.2674], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (3791/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (5007/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (6227/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (7430/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (8643/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (9853/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (11073/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (12296/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (13516/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (14740/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (15965/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (17188/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (18409/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (19627/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (20864/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (22090/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (23301/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (24517/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (25741/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (26947/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (28173/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (29398/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (30623/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (31851/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (33066/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (34282/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (35505/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (36735/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (37941/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (39168/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (40386/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (41613/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (42834/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (44067/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (45288/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (46516/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (47714/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_175.pth.tar'
# TEST : Loss: (0.4668) | Acc: (87.00%) (8765/10000)
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5125, 0.4875], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5836, 0.4164], device='cuda:0')
percent tensor([0.6167, 0.3833], device='cuda:0')
percent tensor([0.7416, 0.2584], device='cuda:0')
percent tensor([0.7263, 0.2737], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 176 | Batch_idx: 0 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (96.00%) (6270/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (96.00%) (7502/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (8730/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (9960/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (11181/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (12402/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (13632/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (14853/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (16086/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (17321/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (18553/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (19780/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (21004/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (22249/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (23477/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (24710/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (25946/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (27173/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (28402/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (29634/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (30867/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (32100/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (33329/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (34556/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (35784/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (37021/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (38258/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (39503/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (40742/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (41974/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (43202/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (44433/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (45679/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (46920/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (48100/50000)
# TEST : Loss: (0.4533) | Acc: (87.00%) (8776/10000)
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.5177, 0.4823], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.7462, 0.2538], device='cuda:0')
percent tensor([0.7385, 0.2615], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 177 | Batch_idx: 0 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (5070/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (6309/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (7533/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (8760/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (9997/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (11215/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (12451/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (13696/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (14937/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (16174/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (17406/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (18641/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (19882/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (21114/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (22347/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (23581/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (24814/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (26050/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (27267/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (28506/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (29734/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (30972/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (32205/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (33438/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (34673/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (35896/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (37131/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (38363/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (39600/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (40841/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (42074/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (43321/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (44565/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (45807/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (47046/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (48229/50000)
# TEST : Loss: (0.4416) | Acc: (87.00%) (8799/10000)
percent tensor([0.5436, 0.4564], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.5798, 0.4202], device='cuda:0')
percent tensor([0.6229, 0.3771], device='cuda:0')
percent tensor([0.7478, 0.2522], device='cuda:0')
percent tensor([0.7340, 0.2660], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 178 | Batch_idx: 0 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (2598/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (5062/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (6306/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (7539/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (8770/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (10008/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (11250/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (12491/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (13725/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (14958/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (16199/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (17444/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (18681/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (19928/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (21166/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (22407/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (23646/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (24884/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (26126/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (27358/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (28590/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (29830/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (31071/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (32315/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (33544/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (34779/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (36005/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (37241/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (38477/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (39700/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (40941/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (42172/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (43409/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (44647/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (45894/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (47131/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (48314/50000)
# TEST : Loss: (0.4304) | Acc: (88.00%) (8815/10000)
percent tensor([0.5451, 0.4549], device='cuda:0')
percent tensor([0.5158, 0.4842], device='cuda:0')
percent tensor([0.5171, 0.4829], device='cuda:0')
percent tensor([0.5818, 0.4182], device='cuda:0')
percent tensor([0.6225, 0.3775], device='cuda:0')
percent tensor([0.7464, 0.2536], device='cuda:0')
percent tensor([0.7407, 0.2593], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 179 | Batch_idx: 0 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (6329/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (7577/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (8818/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (11306/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (12541/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (13780/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (15012/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (16249/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (17494/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (18748/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (19987/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (21223/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (22460/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (23694/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (24925/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (26166/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (27409/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (28641/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (29883/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (31133/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (32379/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (33625/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (34875/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (36118/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (37350/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (38587/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (39826/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (41065/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (42298/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (43550/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (44800/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (46032/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (47265/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (48456/50000)
# TEST : Loss: (0.4292) | Acc: (88.00%) (8823/10000)
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5161, 0.4839], device='cuda:0')
percent tensor([0.5796, 0.4204], device='cuda:0')
percent tensor([0.6245, 0.3755], device='cuda:0')
percent tensor([0.7459, 0.2541], device='cuda:0')
percent tensor([0.7422, 0.2578], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (5079/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (6312/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (7555/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (8800/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (10045/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (11277/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (12513/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (13750/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (14992/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (16217/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (17458/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (18682/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (19921/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (21164/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (22389/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (23619/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (24862/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (26094/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (27318/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (28544/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (29762/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (30988/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (32207/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (33442/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (34669/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (35899/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (37147/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (38380/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (39601/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (40831/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (42063/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (43297/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (44530/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (45770/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (46999/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (48179/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_180.pth.tar'
# TEST : Loss: (0.4751) | Acc: (87.00%) (8718/10000)
percent tensor([0.5443, 0.4557], device='cuda:0')
percent tensor([0.5153, 0.4847], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5796, 0.4204], device='cuda:0')
percent tensor([0.6272, 0.3728], device='cuda:0')
percent tensor([0.7487, 0.2513], device='cuda:0')
percent tensor([0.7473, 0.2527], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.2763, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(835.1578, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.8624, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.7078, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(481.3008, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2304.7856, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4253.1235, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1364.6573, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6265.1094, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11608.7373, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3814.4556, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16142.2734, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (3832/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (6315/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (7549/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (8796/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (10042/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (11293/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (12530/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (13761/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (15004/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (16250/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (17485/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (18711/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (19943/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (21169/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (22392/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (23623/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (24859/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (26089/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (27318/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (28560/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (29801/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (31040/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (32284/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (33512/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (34751/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (35987/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (37226/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (38462/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (39703/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (40929/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (42168/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (43399/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (44636/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (45868/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (47098/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (48271/50000)
# TEST : Loss: (0.4691) | Acc: (87.00%) (8755/10000)
percent tensor([0.5442, 0.4558], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.6278, 0.3722], device='cuda:0')
percent tensor([0.7500, 0.2500], device='cuda:0')
percent tensor([0.7500, 0.2500], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 182 | Batch_idx: 0 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (5076/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (6308/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (7528/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (8766/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (10013/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (11249/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (12499/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (13740/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (14975/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (16206/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (17440/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (18673/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (19915/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (21147/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (22380/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (23622/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (24866/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (26098/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (27332/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (28564/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (29801/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (31028/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (32260/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (33496/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (34720/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (35963/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (37197/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (38432/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (39663/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (40903/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (42135/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (43361/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (44597/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (45828/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (47071/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (48268/50000)
# TEST : Loss: (0.4745) | Acc: (87.00%) (8760/10000)
percent tensor([0.5441, 0.4559], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.5788, 0.4212], device='cuda:0')
percent tensor([0.6266, 0.3734], device='cuda:0')
percent tensor([0.7478, 0.2522], device='cuda:0')
percent tensor([0.7471, 0.2529], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 183 | Batch_idx: 0 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (3868/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (5103/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (97.00%) (6337/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (97.00%) (7578/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (8811/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (10053/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (11305/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (12537/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (13768/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (15011/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (16242/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (17470/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (18703/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (19940/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (21181/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (22408/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (23651/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (24901/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (26147/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (27382/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (28622/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (29850/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (31087/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (32321/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (33566/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (34808/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (36029/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (37253/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (38487/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (39725/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (40958/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (42188/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (43433/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (44671/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (45914/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (47152/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (48335/50000)
# TEST : Loss: (0.4630) | Acc: (87.00%) (8745/10000)
percent tensor([0.5440, 0.4560], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.5161, 0.4839], device='cuda:0')
percent tensor([0.5788, 0.4212], device='cuda:0')
percent tensor([0.6247, 0.3753], device='cuda:0')
percent tensor([0.7442, 0.2558], device='cuda:0')
percent tensor([0.7418, 0.2582], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (7607/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (8848/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (10086/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (11332/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (12573/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (13823/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (15058/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (16297/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (17533/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (18766/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (20012/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (21248/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (22486/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (23724/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (24952/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (26192/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (27431/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (28667/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (29904/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (31141/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (32369/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (33603/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (34850/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (36094/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (37337/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (38577/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (39823/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (41069/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (42305/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (43538/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (44767/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (46006/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (47237/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (48428/50000)
# TEST : Loss: (0.4721) | Acc: (87.00%) (8726/10000)
percent tensor([0.5441, 0.4559], device='cuda:0')
percent tensor([0.5157, 0.4843], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5790, 0.4210], device='cuda:0')
percent tensor([0.6237, 0.3763], device='cuda:0')
percent tensor([0.7447, 0.2553], device='cuda:0')
percent tensor([0.7415, 0.2585], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (2598/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (3832/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (5062/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (6283/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (7516/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (8753/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (9975/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (11200/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (12423/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (13645/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (14873/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (16116/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (17342/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (18579/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (19805/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (21032/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (22260/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (23494/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (24719/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (25952/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (27176/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (28409/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (29635/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (30877/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (32122/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (33339/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (34571/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (35805/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (37035/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (38256/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (39489/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (40723/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (41961/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (43188/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (44424/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (45650/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (46887/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (48071/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_185.pth.tar'
# TEST : Loss: (0.4589) | Acc: (87.00%) (8777/10000)
percent tensor([0.5532, 0.4468], device='cuda:0')
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5977, 0.4023], device='cuda:0')
percent tensor([0.6371, 0.3629], device='cuda:0')
percent tensor([0.7401, 0.2599], device='cuda:0')
percent tensor([0.7242, 0.2758], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 186 | Batch_idx: 0 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (5055/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (6278/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (7516/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (8754/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (9978/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (11216/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (12448/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (13672/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (14899/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (16136/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (17373/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (18613/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (19846/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (21066/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (22312/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (23543/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (24783/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (26029/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (27276/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (28514/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (29739/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (30991/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (32231/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (33466/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (34703/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (35938/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (37168/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (38407/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (39647/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (40876/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (42106/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (43343/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (44578/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (45812/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (47059/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (48247/50000)
# TEST : Loss: (0.4412) | Acc: (88.00%) (8823/10000)
percent tensor([0.5529, 0.4471], device='cuda:0')
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5166, 0.4834], device='cuda:0')
percent tensor([0.5952, 0.4048], device='cuda:0')
percent tensor([0.6406, 0.3594], device='cuda:0')
percent tensor([0.7480, 0.2520], device='cuda:0')
percent tensor([0.7230, 0.2770], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (5090/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (6332/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (7568/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (8809/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (10053/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (11292/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (12529/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (13757/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (14997/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (16226/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (17464/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (18716/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (19966/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (21202/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (22437/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (23687/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (24901/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (26143/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (27389/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (28620/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (29865/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (31109/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (32350/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (33584/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (34815/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (36053/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (37300/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (38546/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (39795/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (41036/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (42275/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (43523/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (44757/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (45996/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (47239/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (48435/50000)
# TEST : Loss: (0.4358) | Acc: (88.00%) (8834/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.5966, 0.4034], device='cuda:0')
percent tensor([0.6318, 0.3682], device='cuda:0')
percent tensor([0.7444, 0.2556], device='cuda:0')
percent tensor([0.7223, 0.2777], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (5090/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (6332/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (7561/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (8801/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (10035/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (11271/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (12515/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (13753/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (15008/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (16246/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (17502/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (18755/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (20004/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (21247/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (22493/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (23740/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (24987/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (26230/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (27474/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (28711/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (29956/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (31207/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (32453/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (33688/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (34937/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (36177/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (37404/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (38648/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (39888/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (41134/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (42378/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (43626/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (44868/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (46109/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (47354/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (48553/50000)
# TEST : Loss: (0.4301) | Acc: (88.00%) (8842/10000)
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6340, 0.3660], device='cuda:0')
percent tensor([0.7464, 0.2536], device='cuda:0')
percent tensor([0.7255, 0.2745], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 189 | Batch_idx: 0 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (6370/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (7618/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (10113/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (11354/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (12591/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (13837/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (15086/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (16316/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (17559/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (18801/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (20039/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (21293/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (22539/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (23778/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (25023/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (26269/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (27510/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (28752/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (29995/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (31239/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (32487/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (33730/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (34978/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (36210/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (37449/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (38695/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (39941/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (41186/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (42422/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (43662/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (44900/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (46141/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (47385/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (48590/50000)
# TEST : Loss: (0.4288) | Acc: (88.00%) (8847/10000)
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.5973, 0.4027], device='cuda:0')
percent tensor([0.6304, 0.3696], device='cuda:0')
percent tensor([0.7489, 0.2511], device='cuda:0')
percent tensor([0.7305, 0.2695], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (5088/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (6324/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (7544/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (8792/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (10039/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (11283/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (12526/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (13771/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (15014/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (16255/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (17481/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (18726/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (19966/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (21212/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (22461/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (23709/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (24955/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (26194/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (27432/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (28665/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (29907/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (31141/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (32379/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (33624/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (34862/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (36105/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (37339/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (38581/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (39815/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (41061/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (42290/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (43513/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (44760/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (45992/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (47222/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (48413/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_190.pth.tar'
# TEST : Loss: (0.4576) | Acc: (88.00%) (8836/10000)
percent tensor([0.5485, 0.4515], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6322, 0.3678], device='cuda:0')
percent tensor([0.7496, 0.2504], device='cuda:0')
percent tensor([0.7301, 0.2699], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.6893, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(835.7896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.4697, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1511.4070, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(479.8007, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2308.2590, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4248.5586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1359.6674, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6274.5391, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11576.2441, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3799.7539, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16077.9141, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (3851/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (5089/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (6326/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (7576/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (8805/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (10035/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (11272/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (12513/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (13753/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (14990/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (16237/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (17465/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (18703/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (19942/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (21178/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (22401/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (23634/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (24869/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (26117/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (27362/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (28610/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (29855/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (31091/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (32328/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (33556/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (34787/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (36023/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (37256/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (38496/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (39732/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (40972/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (42214/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (43450/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (44691/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (45937/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (47179/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (48379/50000)
# TEST : Loss: (0.4666) | Acc: (87.00%) (8796/10000)
percent tensor([0.5483, 0.4517], device='cuda:0')
percent tensor([0.5165, 0.4835], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.6303, 0.3697], device='cuda:0')
percent tensor([0.7477, 0.2523], device='cuda:0')
percent tensor([0.7294, 0.2706], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (5113/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (6360/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (7609/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (8853/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (10097/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (11342/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (12587/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (13822/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (15074/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (16316/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (17559/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (18803/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (20038/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (21276/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (22522/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (23757/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (24991/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (26228/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (27481/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (28733/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (29977/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (31222/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (32467/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (33702/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (34937/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (36174/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (37414/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (38659/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (39885/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (41125/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (42366/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (43614/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (44853/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (46091/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (47323/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (48516/50000)
# TEST : Loss: (0.4955) | Acc: (87.00%) (8762/10000)
percent tensor([0.5488, 0.4512], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.5966, 0.4034], device='cuda:0')
percent tensor([0.6304, 0.3696], device='cuda:0')
percent tensor([0.7461, 0.2539], device='cuda:0')
percent tensor([0.7325, 0.2675], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 193 | Batch_idx: 0 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (3857/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (5105/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (6350/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (7582/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (8827/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (10080/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (11328/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (12577/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (13820/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (15071/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (16321/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (17554/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (18800/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (20039/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (21282/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (22519/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (23761/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (25008/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (26245/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (27487/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (28717/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (29962/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (31214/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (32450/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (33690/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (34928/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (36177/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (37419/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (38669/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (39905/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (41147/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (42385/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (43624/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (44871/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (46102/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (47347/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (48540/50000)
# TEST : Loss: (0.4417) | Acc: (88.00%) (8830/10000)
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.5161, 0.4839], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5969, 0.4031], device='cuda:0')
percent tensor([0.6305, 0.3695], device='cuda:0')
percent tensor([0.7446, 0.2554], device='cuda:0')
percent tensor([0.7258, 0.2742], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 194 | Batch_idx: 0 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (5105/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (6354/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (7600/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (8847/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (10092/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (11338/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (12588/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (13815/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (15069/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (16319/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (17571/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (18823/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (20067/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (21304/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (22556/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (23797/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (25041/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (26296/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (27536/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (28788/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (30034/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (31268/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (32516/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (33764/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (35004/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (36242/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (37475/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (38716/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (39957/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (41201/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (42447/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (43693/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (44929/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (46168/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (47407/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (48604/50000)
# TEST : Loss: (0.5382) | Acc: (86.00%) (8645/10000)
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.6307, 0.3693], device='cuda:0')
percent tensor([0.7483, 0.2517], device='cuda:0')
percent tensor([0.7307, 0.2693], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (6249/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (7468/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8684/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (9912/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (11135/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (12362/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (13584/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (14808/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (16045/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (17264/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (18506/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (19730/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (20956/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (22181/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23400/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (24633/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (25840/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (27065/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (28294/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (29518/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (30741/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (31967/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (33187/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (34417/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (35646/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (36877/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (38116/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (39352/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (40581/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (41818/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (43046/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (95.00%) (44284/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (95.00%) (45524/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (95.00%) (46762/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (95.00%) (47948/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_195.pth.tar'
# TEST : Loss: (0.4670) | Acc: (87.00%) (8798/10000)
percent tensor([0.5573, 0.4427], device='cuda:0')
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.5125, 0.4875], device='cuda:0')
percent tensor([0.5900, 0.4100], device='cuda:0')
percent tensor([0.6325, 0.3675], device='cuda:0')
percent tensor([0.7498, 0.2502], device='cuda:0')
percent tensor([0.7316, 0.2684], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 196 | Batch_idx: 0 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (2604/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (3853/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (5079/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (6311/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (7538/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (8782/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (10015/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (11258/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (12494/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (13722/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (14959/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (16194/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (17426/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (18651/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (19898/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (21135/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (22372/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (23599/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (24829/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (26057/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (27290/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (28532/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (29775/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (30997/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (32236/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (33470/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (34716/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (35944/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (37174/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (38421/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (39666/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (40895/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (42132/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (43365/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (44611/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (45847/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (47093/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (48277/50000)
# TEST : Loss: (0.4521) | Acc: (88.00%) (8844/10000)
percent tensor([0.5571, 0.4429], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.7534, 0.2466], device='cuda:0')
percent tensor([0.7335, 0.2665], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (5089/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (6319/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (7561/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (8806/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (10049/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (11294/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (96.00%) (12538/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (13769/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (15019/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (16258/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (17494/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (18729/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (19956/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (21204/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (22449/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (23688/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (24934/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (26177/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (96.00%) (27429/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (28668/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (29902/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (31145/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (32384/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (33627/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (34858/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (36096/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (37335/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (38573/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (39816/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (41061/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (42300/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (43549/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (44792/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (46036/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (47281/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (48475/50000)
# TEST : Loss: (0.4445) | Acc: (88.00%) (8851/10000)
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5101, 0.4899], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.6324, 0.3676], device='cuda:0')
percent tensor([0.7529, 0.2471], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 198 | Batch_idx: 0 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (2604/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (5081/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (6326/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (7581/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (8825/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (11313/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (12549/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (13793/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (15035/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (16274/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (17507/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (18749/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (19991/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (21236/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (22466/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (23721/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (24951/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (26192/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (27430/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (28667/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (29915/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (31161/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (32401/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (33647/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (34879/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (36112/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (37356/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (38599/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (39841/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (41083/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (42317/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (43559/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (44807/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (46054/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (47295/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (48487/50000)
# TEST : Loss: (0.4388) | Acc: (88.00%) (8862/10000)
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.5024, 0.4976], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5783, 0.4217], device='cuda:0')
percent tensor([0.6333, 0.3667], device='cuda:0')
percent tensor([0.7569, 0.2431], device='cuda:0')
percent tensor([0.7408, 0.2592], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 199 | Batch_idx: 0 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (2624/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (3867/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (5114/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (7596/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (8836/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (10085/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (11337/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (12585/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (13829/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (15061/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (16305/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (17550/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (18790/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (20034/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (21282/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (22528/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (23773/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (25021/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (26270/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (27523/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (28775/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (30018/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (31253/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (32501/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (33744/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (34989/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (36229/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (37477/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (38725/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (39959/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (41201/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (42438/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (43683/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (44937/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (46183/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (47431/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (48641/50000)
# TEST : Loss: (0.4349) | Acc: (88.00%) (8869/10000)
percent tensor([0.5573, 0.4427], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.7625, 0.2375], device='cuda:0')
percent tensor([0.7424, 0.2576], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (5115/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (6351/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (7582/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (8822/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (10058/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (11298/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (12535/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (13785/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (15026/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (16273/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (17516/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (18765/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (20008/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (21267/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (22508/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (23758/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (24996/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (26238/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (27485/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (28722/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (29957/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (31194/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (32437/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (33672/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (34919/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (36161/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (37405/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (38650/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (39898/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (41132/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (42378/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (43610/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (44854/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (46093/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (47338/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (48534/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_200.pth.tar'
# TEST : Loss: (0.4641) | Acc: (87.00%) (8797/10000)
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.5888, 0.4112], device='cuda:0')
percent tensor([0.6336, 0.3664], device='cuda:0')
percent tensor([0.7649, 0.2351], device='cuda:0')
percent tensor([0.7315, 0.2685], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(188.0444, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(836.2303, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(833.8304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1508.9166, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(478.2907, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2311.3611, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4243.7607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1354.7926, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6283.2705, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11542.7832, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3785.0647, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16014.0039, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (2626/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (3872/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (5118/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (6367/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (7626/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (8878/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (10131/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (11375/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (12623/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (13871/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (15119/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (16367/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (17602/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (18844/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (20093/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (21329/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (22578/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (23830/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (25083/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (26338/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (27582/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (28822/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (30074/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (31318/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (32559/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (33807/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (35064/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (36315/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (37558/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (38811/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (40056/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (41301/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (42548/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (43793/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (45041/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (46294/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (47527/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (48716/50000)
# TEST : Loss: (0.5105) | Acc: (87.00%) (8730/10000)
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.5927, 0.4073], device='cuda:0')
percent tensor([0.6414, 0.3586], device='cuda:0')
percent tensor([0.7686, 0.2314], device='cuda:0')
percent tensor([0.7410, 0.2590], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (3858/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (7598/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (8850/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (10090/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (11340/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (12586/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (13832/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (15081/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (16327/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (17581/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (18824/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (20062/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (21301/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (22539/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (23779/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (25030/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (26271/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (27515/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (28765/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (30018/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (31270/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (32524/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (33760/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (35005/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (36250/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (37487/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (38731/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (39974/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (41217/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (42467/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (43712/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (44957/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (46197/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (47434/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (48624/50000)
# TEST : Loss: (0.4851) | Acc: (87.00%) (8755/10000)
percent tensor([0.5573, 0.4427], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.6390, 0.3610], device='cuda:0')
percent tensor([0.7746, 0.2254], device='cuda:0')
percent tensor([0.7417, 0.2583], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 203 | Batch_idx: 0 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (3881/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (5132/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (6378/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (7612/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (8865/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (10112/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (11358/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (12602/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (13842/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (15086/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (16342/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (17588/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (18838/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (20087/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (21338/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (22592/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (23842/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (25098/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (26348/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (27592/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (28844/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (30084/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (31336/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (32591/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (33840/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (35090/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (36341/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (37593/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (38849/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (40088/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (41338/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (42581/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (43829/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (45065/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (46313/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (47559/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (48758/50000)
# TEST : Loss: (0.4767) | Acc: (87.00%) (8753/10000)
percent tensor([0.5589, 0.4411], device='cuda:0')
percent tensor([0.5044, 0.4956], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.6411, 0.3589], device='cuda:0')
percent tensor([0.7786, 0.2214], device='cuda:0')
percent tensor([0.7394, 0.2606], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 204 | Batch_idx: 0 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (5105/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (6345/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (7599/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (8843/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (10093/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (11345/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (12591/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (13843/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (15090/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (16332/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (17578/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (18823/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (20065/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (21315/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (22559/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (23809/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (25063/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (26305/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (27548/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (28790/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (30024/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (31266/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (32509/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (33759/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (35005/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (36242/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (37500/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (38749/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (39992/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (41246/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (42483/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (43721/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (44957/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (46205/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (47454/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (48651/50000)
# TEST : Loss: (0.4576) | Acc: (88.00%) (8814/10000)
percent tensor([0.5582, 0.4418], device='cuda:0')
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5979, 0.4021], device='cuda:0')
percent tensor([0.6460, 0.3540], device='cuda:0')
percent tensor([0.7810, 0.2190], device='cuda:0')
percent tensor([0.7351, 0.2649], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 205 | Batch_idx: 0 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (3880/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (5126/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (6372/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (7622/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (8867/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (10113/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (11365/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (12622/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (13863/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (15122/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (16370/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (17622/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (18881/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (20132/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (21382/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (22639/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (23889/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (25133/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (26382/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (27617/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (28860/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (30102/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (31353/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (32608/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (33847/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (35093/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (36325/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (37567/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (38807/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (40050/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (41298/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (42534/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (43790/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (45030/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (46268/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (47513/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (48712/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_205.pth.tar'
# TEST : Loss: (0.4615) | Acc: (88.00%) (8836/10000)
percent tensor([0.5592, 0.4408], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5977, 0.4023], device='cuda:0')
percent tensor([0.6453, 0.3547], device='cuda:0')
percent tensor([0.7810, 0.2190], device='cuda:0')
percent tensor([0.7311, 0.2689], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0313) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (2626/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (7618/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (10116/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (11355/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (12601/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (13849/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (15088/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (16334/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (17582/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (18831/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (20081/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (21329/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (22570/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (23813/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (25060/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (26316/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (27569/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (28822/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (30075/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (31318/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (32565/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (33811/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (35062/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (36314/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (37565/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (38807/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (40059/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (41294/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (42535/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (43789/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (45031/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (46276/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (47530/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (48725/50000)
# TEST : Loss: (0.5177) | Acc: (87.00%) (8733/10000)
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.5052, 0.4948], device='cuda:0')
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.6447, 0.3553], device='cuda:0')
percent tensor([0.7846, 0.2154], device='cuda:0')
percent tensor([0.7294, 0.2706], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 207 | Batch_idx: 0 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (3886/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (5133/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (6381/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (7632/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (8874/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (10120/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (11368/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (12610/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (13871/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (15127/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (16366/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (17617/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (18872/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (20118/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (21364/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (22617/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (23870/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (25121/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (26355/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (27600/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (28838/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (30088/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (31327/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (32573/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (33818/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (35070/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (36323/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (37573/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (38825/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (40069/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (41316/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (42558/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (43805/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (45056/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (46299/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (47545/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (48742/50000)
# TEST : Loss: (0.5363) | Acc: (87.00%) (8703/10000)
percent tensor([0.5616, 0.4384], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.6472, 0.3528], device='cuda:0')
percent tensor([0.7850, 0.2150], device='cuda:0')
percent tensor([0.7401, 0.2599], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (2632/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (5135/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (7634/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (8886/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (10130/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (11376/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (12625/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (13880/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (15124/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (16382/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (17631/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (18876/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (20122/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (21379/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (22623/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (23877/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (25127/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (26366/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (27614/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (28855/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (30104/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (31349/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (32601/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (33838/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (35095/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (36343/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (37588/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (38840/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (40089/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (41343/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (42595/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (43841/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (45087/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (46332/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (47587/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (48798/50000)
# TEST : Loss: (0.4639) | Acc: (88.00%) (8824/10000)
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.5058, 0.4942], device='cuda:0')
percent tensor([0.5185, 0.4815], device='cuda:0')
percent tensor([0.6038, 0.3962], device='cuda:0')
percent tensor([0.6488, 0.3512], device='cuda:0')
percent tensor([0.7882, 0.2118], device='cuda:0')
percent tensor([0.7445, 0.2555], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (3885/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (5138/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (6392/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (7645/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (8900/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (10155/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (11402/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (12658/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (13902/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (15163/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (16412/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (17662/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (18910/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (20159/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (21412/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (22651/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (23904/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (25155/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (26404/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (27653/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (28907/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (30168/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (31415/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (32664/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (33913/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (35159/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (36409/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (37656/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (38904/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (40156/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (41406/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (42660/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (43909/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (45155/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (46408/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (47660/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (48873/50000)
# TEST : Loss: (0.5024) | Acc: (87.00%) (8774/10000)
percent tensor([0.5637, 0.4363], device='cuda:0')
percent tensor([0.5059, 0.4941], device='cuda:0')
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6496, 0.3504], device='cuda:0')
percent tensor([0.7909, 0.2091], device='cuda:0')
percent tensor([0.7442, 0.2558], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 210 | Batch_idx: 0 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (2623/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (3878/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (5125/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (6371/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (7614/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (8855/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (10116/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (11372/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (12627/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (13883/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (15142/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (16398/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (17649/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (18903/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (20163/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (21422/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (22681/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (23925/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (25182/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (26441/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (27685/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (28941/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (30198/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (31448/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (32694/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (33946/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (97.00%) (35192/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36432/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (37678/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (38928/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (40175/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (41416/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (42672/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (43920/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (45169/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (46410/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (47663/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (48863/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_210.pth.tar'
# TEST : Loss: (0.4729) | Acc: (88.00%) (8842/10000)
percent tensor([0.5631, 0.4369], device='cuda:0')
percent tensor([0.5065, 0.4935], device='cuda:0')
percent tensor([0.5209, 0.4791], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.6505, 0.3495], device='cuda:0')
percent tensor([0.7889, 0.2111], device='cuda:0')
percent tensor([0.7321, 0.2679], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(189.3921, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(840.4191, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(838.5203, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.7542, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(477.0123, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2325.2659, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4250.0913, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1350.2316, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6322.9951, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11520.8281, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3770.4624, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15949.1777, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (2621/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (3883/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (5138/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (6383/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (7633/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (8885/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (10136/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (11383/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (12636/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (13877/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (15130/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (16388/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (17648/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (18897/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (20150/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (21414/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (22674/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (23927/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (25180/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (26431/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (27684/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (28930/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (30170/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (31427/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (32674/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (33930/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (35183/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (36438/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (37696/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (38948/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (40198/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (41447/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (42692/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (43940/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (45193/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (46446/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (47700/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (48904/50000)
# TEST : Loss: (0.4736) | Acc: (87.00%) (8799/10000)
percent tensor([0.5638, 0.4362], device='cuda:0')
percent tensor([0.5068, 0.4932], device='cuda:0')
percent tensor([0.5211, 0.4789], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6484, 0.3516], device='cuda:0')
percent tensor([0.7920, 0.2080], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (3890/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (98.00%) (7661/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (98.00%) (8914/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (98.00%) (10168/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (98.00%) (11423/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (98.00%) (12675/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (98.00%) (13933/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (15196/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (16444/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (98.00%) (17692/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (18946/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (20195/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (21445/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (22702/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (23953/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (25199/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (26453/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (27704/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (28959/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (30214/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (31472/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (32728/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (33970/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (35229/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (36475/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (37731/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (38984/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (40232/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (41492/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (42746/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (43997/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (45251/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (46504/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (47755/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (48953/50000)
# TEST : Loss: (0.4526) | Acc: (88.00%) (8840/10000)
percent tensor([0.5635, 0.4365], device='cuda:0')
percent tensor([0.5075, 0.4925], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.6040, 0.3960], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.7916, 0.2084], device='cuda:0')
percent tensor([0.7377, 0.2623], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (5152/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (6410/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (7668/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (8918/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (10175/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (11421/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (12673/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (13921/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (15168/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (16422/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (17680/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (18934/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (20186/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (21440/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (22685/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (23941/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (25189/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (26446/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (27691/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (28945/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (30205/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (31454/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (32713/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (33962/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (35217/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (36466/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (37720/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (38978/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (40226/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (41479/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (42734/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (43981/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (45233/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (46485/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (47737/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (48949/50000)
# TEST : Loss: (0.5163) | Acc: (87.00%) (8751/10000)
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.6050, 0.3950], device='cuda:0')
percent tensor([0.6506, 0.3494], device='cuda:0')
percent tensor([0.7937, 0.2063], device='cuda:0')
percent tensor([0.7464, 0.2536], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (3892/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (6405/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (7658/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (8906/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (10168/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (11426/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (12677/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (13933/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (15192/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (16451/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (17707/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (18966/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (20224/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (21483/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (22736/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (23993/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (25241/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (26492/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (27738/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (28993/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (30247/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (31501/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (32752/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (34009/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (35273/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (36523/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (37781/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (39029/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (40279/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (41530/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (42771/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (44010/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (97.00%) (45262/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (46520/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (47773/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (48987/50000)
# TEST : Loss: (0.5007) | Acc: (87.00%) (8789/10000)
percent tensor([0.5658, 0.4342], device='cuda:0')
percent tensor([0.5080, 0.4920], device='cuda:0')
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.6079, 0.3921], device='cuda:0')
percent tensor([0.6527, 0.3473], device='cuda:0')
percent tensor([0.7986, 0.2014], device='cuda:0')
percent tensor([0.7455, 0.2545], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (3882/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (5132/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (6387/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (7646/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (8904/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (10150/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (11395/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (12654/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (13911/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (15159/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (16412/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (17674/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (18929/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (20179/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (21445/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (22691/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (23942/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (25196/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (26455/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (27712/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (28970/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0650) |  Loss2: (0.0000) | Acc: (97.00%) (30229/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (98.00%) (31490/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (98.00%) (32743/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (98.00%) (34001/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (98.00%) (35256/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (98.00%) (36508/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (98.00%) (37765/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (39008/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (40260/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (98.00%) (41526/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (98.00%) (42791/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (98.00%) (44045/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (98.00%) (45298/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (98.00%) (46543/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (98.00%) (47796/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (98.00%) (49002/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_215.pth.tar'
# TEST : Loss: (0.5336) | Acc: (87.00%) (8720/10000)
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6103, 0.3897], device='cuda:0')
percent tensor([0.6498, 0.3502], device='cuda:0')
percent tensor([0.8009, 0.1991], device='cuda:0')
percent tensor([0.7487, 0.2513], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (6408/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (7666/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (8923/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (10187/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (11439/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (12697/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (13953/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (15211/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (16466/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (17721/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (18981/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (20237/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (21497/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (22750/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (24007/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (25257/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (26509/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (27769/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (29012/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (30269/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (31513/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (32767/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (34018/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (35258/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (36512/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (37758/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (39011/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (40272/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (41525/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (42771/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (44026/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (45280/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (46525/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (47776/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (97.00%) (48984/50000)
# TEST : Loss: (0.5037) | Acc: (87.00%) (8780/10000)
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.6106, 0.3894], device='cuda:0')
percent tensor([0.6566, 0.3434], device='cuda:0')
percent tensor([0.7963, 0.2037], device='cuda:0')
percent tensor([0.7411, 0.2589], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (97.00%) (2634/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (3897/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (5157/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (6414/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (7674/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (8933/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (10192/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (11447/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (12703/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (13961/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (15223/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (16462/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (17720/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (18977/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (98.00%) (20232/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (21479/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (22733/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (23988/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (25235/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (26494/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (27753/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (29006/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (30265/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (31517/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (32768/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (34013/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (35271/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (36530/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (37792/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (39045/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (40299/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (41564/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (42819/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (44081/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (45340/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (46591/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (47849/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (49055/50000)
# TEST : Loss: (0.5001) | Acc: (87.00%) (8798/10000)
percent tensor([0.5661, 0.4339], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5259, 0.4741], device='cuda:0')
percent tensor([0.6106, 0.3894], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.7993, 0.2007], device='cuda:0')
percent tensor([0.7431, 0.2569], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 218 | Batch_idx: 0 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (5159/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (6412/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (7670/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (8924/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (10180/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (11450/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (12712/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (13969/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (15231/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (16488/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (17748/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (18997/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (20257/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (21509/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (22774/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (24029/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (25288/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (26545/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (27792/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (29044/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (30302/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (31567/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (32826/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (34085/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (35340/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (36586/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (37838/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (39083/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (40337/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (41595/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (42843/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (44093/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (45349/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (46600/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (47857/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (49066/50000)
# TEST : Loss: (0.4761) | Acc: (88.00%) (8841/10000)
percent tensor([0.5676, 0.4324], device='cuda:0')
percent tensor([0.5085, 0.4915], device='cuda:0')
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.6117, 0.3883], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.8028, 0.1972], device='cuda:0')
percent tensor([0.7487, 0.2513], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (5156/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (6414/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (7672/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (8931/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (10184/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (11441/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (12700/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (13962/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (15221/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (16478/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (17740/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (19007/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (20253/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (21504/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (22759/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (24018/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (25276/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (26534/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (27790/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (29054/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (30304/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (31560/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (32817/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (34074/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (35331/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (36593/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (37847/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (39106/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (40357/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (41621/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (42886/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (44146/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (45394/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (46650/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (47908/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (49113/50000)
# TEST : Loss: (0.4845) | Acc: (87.00%) (8799/10000)
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5089, 0.4911], device='cuda:0')
percent tensor([0.5266, 0.4734], device='cuda:0')
percent tensor([0.6129, 0.3871], device='cuda:0')
percent tensor([0.6526, 0.3474], device='cuda:0')
percent tensor([0.8039, 0.1961], device='cuda:0')
percent tensor([0.7397, 0.2603], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (3901/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (7673/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (8937/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (10198/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (11457/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (12694/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (13950/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (15211/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (16468/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (17730/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (18978/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (20227/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (21484/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (22749/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (24012/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (25264/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (26519/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (27777/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (29038/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (30301/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (31559/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (32825/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (34087/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (35344/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (36603/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (37856/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (39112/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (40374/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (41639/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (42899/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (44150/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (45403/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (46650/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (47893/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (49092/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_220.pth.tar'
# TEST : Loss: (0.5300) | Acc: (87.00%) (8760/10000)
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5092, 0.4908], device='cuda:0')
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.6107, 0.3893], device='cuda:0')
percent tensor([0.6564, 0.3436], device='cuda:0')
percent tensor([0.8037, 0.1963], device='cuda:0')
percent tensor([0.7465, 0.2535], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(190.3645, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(843.1945, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(842.3310, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1510.2998, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(475.7344, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2337.2820, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4254.5420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1345.5785, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6356.8501, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11495.4902, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3755.9844, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15884.7891, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (6419/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (7678/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (8941/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (10199/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (11458/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (12707/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (13952/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (15209/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (16470/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (17726/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (18982/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (20237/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (21484/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (22739/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (23989/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (25242/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (26501/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (27763/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (29024/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (30276/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (31535/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (32789/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (34053/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (35314/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (36571/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (37833/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (39099/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (40357/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (41608/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (42870/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (44119/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (45373/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (46629/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (47883/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (49098/50000)
# TEST : Loss: (0.4989) | Acc: (88.00%) (8800/10000)
percent tensor([0.5696, 0.4304], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5284, 0.4716], device='cuda:0')
percent tensor([0.6095, 0.3905], device='cuda:0')
percent tensor([0.6583, 0.3417], device='cuda:0')
percent tensor([0.8096, 0.1904], device='cuda:0')
percent tensor([0.7505, 0.2495], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0196) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (5161/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (6421/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (7671/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (8928/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (10186/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (11447/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (12700/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (13953/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (15219/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (16478/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (17732/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (18988/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (20248/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (21511/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (22771/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (24031/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (25282/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (26544/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (27807/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (29068/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (30328/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (31590/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (32854/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (34106/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (35361/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (36610/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (37858/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (39111/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (40359/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (41625/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (42890/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (44142/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (45393/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (46650/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (47904/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (49110/50000)
# TEST : Loss: (0.4737) | Acc: (88.00%) (8847/10000)
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.6121, 0.3879], device='cuda:0')
percent tensor([0.6554, 0.3446], device='cuda:0')
percent tensor([0.8057, 0.1943], device='cuda:0')
percent tensor([0.7470, 0.2530], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (99.00%) (2664/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (3923/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (5177/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (7686/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (8942/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (10200/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (11461/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (12716/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (13967/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (15227/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (16477/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (17733/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (18992/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (20244/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (21502/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (22760/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (24015/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (25283/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (26549/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (27819/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (29075/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (30335/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (31592/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (32851/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (34104/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (35355/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (36612/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (37877/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (39143/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (40401/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (41662/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (42921/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (44183/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (45440/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (46702/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (47963/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (49176/50000)
# TEST : Loss: (0.4889) | Acc: (88.00%) (8830/10000)
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.5095, 0.4905], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.6139, 0.3861], device='cuda:0')
percent tensor([0.6611, 0.3389], device='cuda:0')
percent tensor([0.8087, 0.1913], device='cuda:0')
percent tensor([0.7477, 0.2523], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (3906/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (5166/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (6423/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (7675/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (8943/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (10204/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (11461/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (12720/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (13985/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (15242/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (16503/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (17768/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (19031/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (20292/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (21553/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (22814/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (24072/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (25331/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (26584/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (27834/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (29093/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (30357/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (31614/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (32875/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (34131/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (35399/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (36665/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (37932/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (39188/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (40437/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (41689/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (42939/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (44194/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (45457/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (46709/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (47965/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (49180/50000)
# TEST : Loss: (0.4636) | Acc: (88.00%) (8856/10000)
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.6123, 0.3877], device='cuda:0')
percent tensor([0.6625, 0.3375], device='cuda:0')
percent tensor([0.8066, 0.1934], device='cuda:0')
percent tensor([0.7471, 0.2529], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (3913/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (6427/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (7692/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (8950/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (10208/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (11465/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (12720/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (13979/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (15231/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (16494/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (17751/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (19009/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (20269/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (21518/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (22778/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (24037/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (25295/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (26548/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (27807/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (29066/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (30326/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (31583/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (32840/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (34093/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (35351/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (36609/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (37864/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (39115/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (40375/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (41635/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (42900/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (44162/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (45418/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (46669/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (47928/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (49146/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_225.pth.tar'
# TEST : Loss: (0.5564) | Acc: (87.00%) (8707/10000)
percent tensor([0.5714, 0.4286], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.6137, 0.3863], device='cuda:0')
percent tensor([0.6644, 0.3356], device='cuda:0')
percent tensor([0.8071, 0.1929], device='cuda:0')
percent tensor([0.7451, 0.2549], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (2659/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (3920/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (5177/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (6435/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (7692/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (8954/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (10213/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (11470/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (12732/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (13993/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (15248/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (16506/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (17769/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (19028/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (20292/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (21550/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (22804/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (24062/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (25324/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (26578/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (27837/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (29098/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (30352/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (31609/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (32872/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (34132/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (35396/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (36644/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (37906/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (39169/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (40424/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (41685/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (42939/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (44206/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (45461/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (46719/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (47982/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (49197/50000)
# TEST : Loss: (0.4849) | Acc: (88.00%) (8854/10000)
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.5300, 0.4700], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.8092, 0.1908], device='cuda:0')
percent tensor([0.7425, 0.2575], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0291) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0502) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (3906/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (5173/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (6432/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (7685/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (8938/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (11443/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (12703/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (13961/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (15222/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (16486/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (17745/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (19005/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (20260/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (21520/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (22772/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (24036/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (25298/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (26561/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (27827/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (29093/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (30358/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (31611/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (32877/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (34139/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (35393/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (36649/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (37911/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (39168/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (40421/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (41679/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (42937/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (44190/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (45449/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (46713/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (47971/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (49186/50000)
# TEST : Loss: (0.5071) | Acc: (88.00%) (8808/10000)
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6159, 0.3841], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.8136, 0.1864], device='cuda:0')
percent tensor([0.7451, 0.2549], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (3905/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (5166/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (6430/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (7687/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (8949/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (10213/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (11475/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (12738/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (14000/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (15266/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (16520/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (17788/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (19046/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (20315/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (21573/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (22829/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (24086/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (25350/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (26616/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (27871/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (29124/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (30377/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (31644/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (32909/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (34168/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (35431/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (36691/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (37948/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (39209/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (40473/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (41732/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (42993/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (44260/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (45516/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (46774/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (48027/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (49244/50000)
# TEST : Loss: (0.5086) | Acc: (88.00%) (8819/10000)
percent tensor([0.5706, 0.4294], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5311, 0.4689], device='cuda:0')
percent tensor([0.6128, 0.3872], device='cuda:0')
percent tensor([0.6602, 0.3398], device='cuda:0')
percent tensor([0.8148, 0.1852], device='cuda:0')
percent tensor([0.7525, 0.2475], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (2659/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (3925/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (5191/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (6449/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (7715/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (8968/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (10234/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (11493/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (12753/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (14016/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (16547/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (17810/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (19073/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (20339/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (21598/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (22853/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (24118/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (25380/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (26642/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (27901/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (29163/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (30426/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (31687/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (32943/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (34205/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (35469/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (36727/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (37986/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (39244/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (40497/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (41756/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (43013/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (44267/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (45524/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (46785/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (48046/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (49262/50000)
# TEST : Loss: (0.4748) | Acc: (88.00%) (8862/10000)
percent tensor([0.5706, 0.4294], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5315, 0.4685], device='cuda:0')
percent tensor([0.6147, 0.3853], device='cuda:0')
percent tensor([0.6544, 0.3456], device='cuda:0')
percent tensor([0.8128, 0.1872], device='cuda:0')
percent tensor([0.7474, 0.2526], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (3909/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (6432/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (7685/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (8955/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (10209/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (11471/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (12743/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (14011/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (15270/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (16535/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (17801/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (19064/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (20327/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (21591/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (22856/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (24120/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (25382/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (26642/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (27905/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (29163/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (30429/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (31688/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (32945/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (34205/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (35467/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (36729/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (37991/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (39252/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (40513/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (41781/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (43042/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (44308/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (45568/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (46821/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (48090/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (49298/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_230.pth.tar'
# TEST : Loss: (0.5305) | Acc: (87.00%) (8758/10000)
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.6159, 0.3841], device='cuda:0')
percent tensor([0.6572, 0.3428], device='cuda:0')
percent tensor([0.8103, 0.1897], device='cuda:0')
percent tensor([0.7510, 0.2490], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(191.1151, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(845.5648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(845.4753, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.9564, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(474.3676, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2346.3279, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4256.9204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1340.8992, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6385.2769, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11469.7939, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3741.5459, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15820.8594, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (1376/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (3891/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (5150/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (6411/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (7668/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (8931/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (10190/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (11450/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (12704/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (13964/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (15225/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (16490/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (17750/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (19003/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (20272/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (21536/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (22803/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (24060/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (25321/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (26574/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (27828/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (29085/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (30342/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (31601/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (32864/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (34126/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (35384/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (36644/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (37906/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (39158/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (40412/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (41674/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (42928/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (44192/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0491) |  Loss2: (0.0000) | Acc: (98.00%) (45447/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (46704/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (47969/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (49176/50000)
# TEST : Loss: (0.5062) | Acc: (87.00%) (8789/10000)
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.5105, 0.4895], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.6168, 0.3832], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.8088, 0.1912], device='cuda:0')
percent tensor([0.7498, 0.2502], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0320) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (3901/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (5165/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (6431/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (7696/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (8958/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (10217/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (11484/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (12747/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (15272/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (16534/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (17793/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (19051/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (20303/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (21566/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (22828/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (24081/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (25343/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (26604/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (27860/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (29119/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (30378/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (31630/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (32888/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (34148/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (35414/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (36671/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (37927/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (39188/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (40453/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (41711/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (42975/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (44237/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (45490/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (46743/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (48012/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (49227/50000)
# TEST : Loss: (0.4762) | Acc: (88.00%) (8867/10000)
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.6146, 0.3854], device='cuda:0')
percent tensor([0.6613, 0.3387], device='cuda:0')
percent tensor([0.8106, 0.1894], device='cuda:0')
percent tensor([0.7503, 0.2497], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (3901/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (5167/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (98.00%) (6429/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (7681/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (8942/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (10202/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (11459/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (12717/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (13982/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (15247/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (16501/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (17762/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (19023/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (20281/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (21544/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (22795/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (24042/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (25303/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (26569/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (27836/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (29098/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (30361/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (31617/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (32883/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (34140/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (35405/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (36660/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (37922/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (39185/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (40449/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (41711/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (42971/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (44229/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (45480/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (46744/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (48006/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (49219/50000)
# TEST : Loss: (0.6091) | Acc: (86.00%) (8689/10000)
percent tensor([0.5731, 0.4269], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6629, 0.3371], device='cuda:0')
percent tensor([0.8163, 0.1837], device='cuda:0')
percent tensor([0.7544, 0.2456], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (2645/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (3899/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (6434/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (7701/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (8965/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (10232/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (11496/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (12753/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (14015/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (15281/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (16549/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (17809/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (19069/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (20334/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (21597/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (22864/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (24121/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (25388/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (26653/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (27922/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (29178/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (30445/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (31707/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (32968/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (34231/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (35489/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (36752/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (38019/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (39278/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (40534/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (41791/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (43051/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (44311/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (45568/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (46819/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (48075/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (49282/50000)
# TEST : Loss: (0.5106) | Acc: (88.00%) (8864/10000)
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6619, 0.3381], device='cuda:0')
percent tensor([0.8136, 0.1864], device='cuda:0')
percent tensor([0.7533, 0.2467], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (2645/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (3913/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (5183/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (6444/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (7707/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (8973/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (10237/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (11502/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (12768/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (14033/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (15301/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (16564/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (17822/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (19079/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (20334/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (21599/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (22856/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (24108/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (25361/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (26614/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (27876/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (29138/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (30399/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (31659/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (32922/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (34173/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (35435/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (36694/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (37954/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (39217/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (40478/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (41740/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (43001/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (44261/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (45525/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (46795/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (48060/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (49273/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_235.pth.tar'
# TEST : Loss: (0.5135) | Acc: (88.00%) (8842/10000)
percent tensor([0.5722, 0.4278], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.6162, 0.3838], device='cuda:0')
percent tensor([0.6596, 0.3404], device='cuda:0')
percent tensor([0.8162, 0.1838], device='cuda:0')
percent tensor([0.7509, 0.2491], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (3911/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (6432/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (7695/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (8955/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (10216/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (11475/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (12731/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (13993/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (15255/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (16518/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (17790/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (19056/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (20316/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0432) |  Loss2: (0.0000) | Acc: (98.00%) (21579/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (22852/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (24119/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (25389/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (26651/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (27917/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (29181/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (30448/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (31707/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (32967/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (34222/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (35478/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (36740/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (38002/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (39257/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (40520/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (41780/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (43048/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (44311/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (45565/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (46822/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (48081/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (49298/50000)
# TEST : Loss: (0.5751) | Acc: (87.00%) (8751/10000)
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.6174, 0.3826], device='cuda:0')
percent tensor([0.6590, 0.3410], device='cuda:0')
percent tensor([0.8135, 0.1865], device='cuda:0')
percent tensor([0.7519, 0.2481], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0165) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0311) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (99.00%) (2662/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (99.00%) (3930/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (99.00%) (5199/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (99.00%) (6466/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (99.00%) (7737/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (8994/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (10255/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (11519/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (12784/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (14048/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (15312/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (16580/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (17845/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (19107/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (20368/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (21629/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (22893/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (24154/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (25418/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (26681/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (27943/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (29205/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (30470/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (31731/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (32994/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (34259/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (35530/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (36797/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (38059/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (39327/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (40588/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (41853/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (43115/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (44376/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (45637/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (46905/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (48164/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (49382/50000)
# TEST : Loss: (0.5055) | Acc: (88.00%) (8863/10000)
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.6175, 0.3825], device='cuda:0')
percent tensor([0.6543, 0.3457], device='cuda:0')
percent tensor([0.8107, 0.1893], device='cuda:0')
percent tensor([0.7514, 0.2486], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (2658/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (5177/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (6450/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0390) |  Loss2: (0.0000) | Acc: (98.00%) (7714/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (8979/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (10248/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (11515/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (12782/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (14053/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (15310/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (16579/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (17845/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (19118/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (20384/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (21639/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (22905/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (24163/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (25431/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (26690/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (27952/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (29214/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (30471/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (31734/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (33001/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (34265/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (35524/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (36794/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (38059/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (39321/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (40583/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (41849/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (43112/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (44368/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (45626/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (46894/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (48154/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (49376/50000)
# TEST : Loss: (0.5023) | Acc: (88.00%) (8897/10000)
percent tensor([0.5745, 0.4255], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.6184, 0.3816], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.8158, 0.1842], device='cuda:0')
percent tensor([0.7513, 0.2487], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (2649/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (3908/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (6433/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (7684/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (8950/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (10216/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (11476/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (12738/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (14003/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (15264/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (16524/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (17784/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (19051/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (20323/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (21590/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (22856/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (24123/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (25392/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (26660/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (27921/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (29180/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (30447/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (31708/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (32980/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (34253/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (35522/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (36787/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (38053/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (39317/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (40581/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (41849/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (43116/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (44376/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (45633/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (46897/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (48160/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (49374/50000)
# TEST : Loss: (0.4817) | Acc: (88.00%) (8891/10000)
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.5116, 0.4884], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6208, 0.3792], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.8188, 0.1812], device='cuda:0')
percent tensor([0.7497, 0.2503], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(191.7651, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(847.2844, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(847.7007, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1509.5737, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(473.1333, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2352.9448, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4258.0640, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1336.5569, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6408.5205, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11445.5303, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3728.6125, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15763.9639, device='cuda:0', grad_fn=<NormBackward0>)
2 hours 33 mins 26 secs for training