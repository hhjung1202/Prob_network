Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3252) |  Loss2: (0.0000) | Acc: (6.00%) (8/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3081) |  Loss2: (0.0000) | Acc: (8.00%) (116/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.3007) |  Loss2: (0.0000) | Acc: (10.00%) (288/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2936) |  Loss2: (0.0000) | Acc: (12.00%) (504/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2850) |  Loss2: (0.0000) | Acc: (13.00%) (723/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2773) |  Loss2: (0.0000) | Acc: (14.00%) (949/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2685) |  Loss2: (0.0000) | Acc: (15.00%) (1194/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2607) |  Loss2: (0.0000) | Acc: (16.00%) (1468/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2518) |  Loss2: (0.0000) | Acc: (16.00%) (1732/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2432) |  Loss2: (0.0000) | Acc: (17.00%) (2034/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2346) |  Loss2: (0.0000) | Acc: (18.00%) (2340/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2274) |  Loss2: (0.0000) | Acc: (18.00%) (2596/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2193) |  Loss2: (0.0000) | Acc: (18.00%) (2899/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2104) |  Loss2: (0.0000) | Acc: (19.00%) (3199/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.2018) |  Loss2: (0.0000) | Acc: (19.00%) (3526/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1943) |  Loss2: (0.0000) | Acc: (19.00%) (3845/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1865) |  Loss2: (0.0000) | Acc: (20.00%) (4170/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1782) |  Loss2: (0.0000) | Acc: (20.00%) (4477/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1707) |  Loss2: (0.0000) | Acc: (20.00%) (4803/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1631) |  Loss2: (0.0000) | Acc: (20.00%) (5134/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1558) |  Loss2: (0.0000) | Acc: (21.00%) (5471/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1488) |  Loss2: (0.0000) | Acc: (21.00%) (5837/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1409) |  Loss2: (0.0000) | Acc: (21.00%) (6191/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1342) |  Loss2: (0.0000) | Acc: (22.00%) (6561/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1274) |  Loss2: (0.0000) | Acc: (22.00%) (6887/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1203) |  Loss2: (0.0000) | Acc: (22.00%) (7275/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1135) |  Loss2: (0.0000) | Acc: (22.00%) (7661/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.1069) |  Loss2: (0.0000) | Acc: (23.00%) (8042/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0996) |  Loss2: (0.0000) | Acc: (23.00%) (8430/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0924) |  Loss2: (0.0000) | Acc: (23.00%) (8815/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0858) |  Loss2: (0.0000) | Acc: (23.00%) (9219/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0797) |  Loss2: (0.0000) | Acc: (24.00%) (9594/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0734) |  Loss2: (0.0000) | Acc: (24.00%) (9982/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0678) |  Loss2: (0.0000) | Acc: (24.00%) (10356/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0617) |  Loss2: (0.0000) | Acc: (24.00%) (10770/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0552) |  Loss2: (0.0000) | Acc: (24.00%) (11213/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0499) |  Loss2: (0.0000) | Acc: (25.00%) (11609/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0441) |  Loss2: (0.0000) | Acc: (25.00%) (12018/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0378) |  Loss2: (0.0000) | Acc: (25.00%) (12442/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0320) |  Loss2: (0.0000) | Acc: (25.00%) (12857/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_000.pth.tar'
# TEST : Loss: (1.8058) | Acc: (33.00%) (3313/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(169.6978, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(769.8872, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(767.3859, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1537.7188, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(512.2092, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2170.6055, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4338.5439, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1447.6373, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6133.5688, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12277.7334, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4086.9187, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17370.7441, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8515) |  Loss2: (0.0000) | Acc: (28.00%) (36/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8130) |  Loss2: (0.0000) | Acc: (34.00%) (480/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.7948) |  Loss2: (0.0000) | Acc: (35.00%) (944/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8079) |  Loss2: (0.0000) | Acc: (33.00%) (1348/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8039) |  Loss2: (0.0000) | Acc: (34.00%) (1807/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7992) |  Loss2: (0.0000) | Acc: (34.00%) (2271/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7965) |  Loss2: (0.0000) | Acc: (34.00%) (2722/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7908) |  Loss2: (0.0000) | Acc: (35.00%) (3181/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7866) |  Loss2: (0.0000) | Acc: (35.00%) (3630/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7826) |  Loss2: (0.0000) | Acc: (35.00%) (4080/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7786) |  Loss2: (0.0000) | Acc: (35.00%) (4535/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7766) |  Loss2: (0.0000) | Acc: (35.00%) (5010/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7745) |  Loss2: (0.0000) | Acc: (35.00%) (5478/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7739) |  Loss2: (0.0000) | Acc: (35.00%) (5914/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7708) |  Loss2: (0.0000) | Acc: (35.00%) (6376/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7670) |  Loss2: (0.0000) | Acc: (35.00%) (6872/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7636) |  Loss2: (0.0000) | Acc: (35.00%) (7327/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7604) |  Loss2: (0.0000) | Acc: (35.00%) (7791/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7586) |  Loss2: (0.0000) | Acc: (35.00%) (8247/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7547) |  Loss2: (0.0000) | Acc: (35.00%) (8715/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7513) |  Loss2: (0.0000) | Acc: (35.00%) (9220/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7498) |  Loss2: (0.0000) | Acc: (35.00%) (9702/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7465) |  Loss2: (0.0000) | Acc: (36.00%) (10190/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7438) |  Loss2: (0.0000) | Acc: (36.00%) (10692/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7427) |  Loss2: (0.0000) | Acc: (36.00%) (11161/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7413) |  Loss2: (0.0000) | Acc: (36.00%) (11612/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7393) |  Loss2: (0.0000) | Acc: (36.00%) (12087/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7375) |  Loss2: (0.0000) | Acc: (36.00%) (12570/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7339) |  Loss2: (0.0000) | Acc: (36.00%) (13085/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7315) |  Loss2: (0.0000) | Acc: (36.00%) (13571/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7293) |  Loss2: (0.0000) | Acc: (36.00%) (14071/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7263) |  Loss2: (0.0000) | Acc: (36.00%) (14611/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7230) |  Loss2: (0.0000) | Acc: (36.00%) (15123/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7208) |  Loss2: (0.0000) | Acc: (36.00%) (15614/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7182) |  Loss2: (0.0000) | Acc: (36.00%) (16102/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7165) |  Loss2: (0.0000) | Acc: (36.00%) (16583/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7137) |  Loss2: (0.0000) | Acc: (36.00%) (17085/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7115) |  Loss2: (0.0000) | Acc: (37.00%) (17596/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7096) |  Loss2: (0.0000) | Acc: (37.00%) (18076/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7068) |  Loss2: (0.0000) | Acc: (37.00%) (18589/50000)
# TEST : Loss: (1.5795) | Acc: (41.00%) (4123/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.6352) |  Loss2: (0.0000) | Acc: (42.00%) (55/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.6149) |  Loss2: (0.0000) | Acc: (39.00%) (557/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.6063) |  Loss2: (0.0000) | Acc: (40.00%) (1095/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5972) |  Loss2: (0.0000) | Acc: (40.00%) (1617/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5997) |  Loss2: (0.0000) | Acc: (41.00%) (2155/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5931) |  Loss2: (0.0000) | Acc: (41.00%) (2699/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5906) |  Loss2: (0.0000) | Acc: (41.00%) (3218/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5808) |  Loss2: (0.0000) | Acc: (41.00%) (3787/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5800) |  Loss2: (0.0000) | Acc: (41.00%) (4301/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5799) |  Loss2: (0.0000) | Acc: (41.00%) (4824/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5791) |  Loss2: (0.0000) | Acc: (41.00%) (5362/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5786) |  Loss2: (0.0000) | Acc: (41.00%) (5878/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5797) |  Loss2: (0.0000) | Acc: (41.00%) (6385/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5789) |  Loss2: (0.0000) | Acc: (41.00%) (6933/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5773) |  Loss2: (0.0000) | Acc: (41.00%) (7489/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5764) |  Loss2: (0.0000) | Acc: (41.00%) (8020/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5745) |  Loss2: (0.0000) | Acc: (41.00%) (8539/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5718) |  Loss2: (0.0000) | Acc: (41.00%) (9107/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5688) |  Loss2: (0.0000) | Acc: (41.00%) (9664/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5662) |  Loss2: (0.0000) | Acc: (41.00%) (10224/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5645) |  Loss2: (0.0000) | Acc: (41.00%) (10779/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5633) |  Loss2: (0.0000) | Acc: (41.00%) (11316/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5608) |  Loss2: (0.0000) | Acc: (42.00%) (11891/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5583) |  Loss2: (0.0000) | Acc: (42.00%) (12438/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5570) |  Loss2: (0.0000) | Acc: (42.00%) (13007/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5556) |  Loss2: (0.0000) | Acc: (42.00%) (13552/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5535) |  Loss2: (0.0000) | Acc: (42.00%) (14134/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5522) |  Loss2: (0.0000) | Acc: (42.00%) (14704/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5497) |  Loss2: (0.0000) | Acc: (42.00%) (15290/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5474) |  Loss2: (0.0000) | Acc: (42.00%) (15861/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5459) |  Loss2: (0.0000) | Acc: (42.00%) (16447/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5434) |  Loss2: (0.0000) | Acc: (42.00%) (17032/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5415) |  Loss2: (0.0000) | Acc: (42.00%) (17595/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5410) |  Loss2: (0.0000) | Acc: (42.00%) (18155/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5396) |  Loss2: (0.0000) | Acc: (42.00%) (18713/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5374) |  Loss2: (0.0000) | Acc: (42.00%) (19302/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5353) |  Loss2: (0.0000) | Acc: (43.00%) (19895/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5329) |  Loss2: (0.0000) | Acc: (43.00%) (20481/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5328) |  Loss2: (0.0000) | Acc: (43.00%) (21022/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5322) |  Loss2: (0.0000) | Acc: (43.00%) (21556/50000)
# TEST : Loss: (1.4744) | Acc: (45.00%) (4539/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.4202) |  Loss2: (0.0000) | Acc: (47.00%) (61/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4618) |  Loss2: (0.0000) | Acc: (45.00%) (646/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4475) |  Loss2: (0.0000) | Acc: (46.00%) (1263/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4587) |  Loss2: (0.0000) | Acc: (46.00%) (1848/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4588) |  Loss2: (0.0000) | Acc: (46.00%) (2445/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4516) |  Loss2: (0.0000) | Acc: (46.00%) (3063/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4496) |  Loss2: (0.0000) | Acc: (47.00%) (3689/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4496) |  Loss2: (0.0000) | Acc: (47.00%) (4309/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4496) |  Loss2: (0.0000) | Acc: (47.00%) (4881/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4455) |  Loss2: (0.0000) | Acc: (47.00%) (5490/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4446) |  Loss2: (0.0000) | Acc: (47.00%) (6112/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4416) |  Loss2: (0.0000) | Acc: (47.00%) (6732/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4389) |  Loss2: (0.0000) | Acc: (47.00%) (7352/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4366) |  Loss2: (0.0000) | Acc: (47.00%) (7993/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4342) |  Loss2: (0.0000) | Acc: (47.00%) (8622/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4329) |  Loss2: (0.0000) | Acc: (47.00%) (9232/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.4311) |  Loss2: (0.0000) | Acc: (47.00%) (9849/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.4282) |  Loss2: (0.0000) | Acc: (47.00%) (10487/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.4232) |  Loss2: (0.0000) | Acc: (48.00%) (11153/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.4199) |  Loss2: (0.0000) | Acc: (48.00%) (11817/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.4183) |  Loss2: (0.0000) | Acc: (48.00%) (12437/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.4168) |  Loss2: (0.0000) | Acc: (48.00%) (13092/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.4152) |  Loss2: (0.0000) | Acc: (48.00%) (13742/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.4113) |  Loss2: (0.0000) | Acc: (48.00%) (14402/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.4104) |  Loss2: (0.0000) | Acc: (48.00%) (15039/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.4088) |  Loss2: (0.0000) | Acc: (48.00%) (15703/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.4072) |  Loss2: (0.0000) | Acc: (48.00%) (16356/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.4042) |  Loss2: (0.0000) | Acc: (49.00%) (17035/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.4022) |  Loss2: (0.0000) | Acc: (49.00%) (17671/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.4003) |  Loss2: (0.0000) | Acc: (49.00%) (18328/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3978) |  Loss2: (0.0000) | Acc: (49.00%) (19009/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3974) |  Loss2: (0.0000) | Acc: (49.00%) (19658/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3964) |  Loss2: (0.0000) | Acc: (49.00%) (20309/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3942) |  Loss2: (0.0000) | Acc: (49.00%) (20988/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3927) |  Loss2: (0.0000) | Acc: (49.00%) (21662/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3903) |  Loss2: (0.0000) | Acc: (49.00%) (22337/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3885) |  Loss2: (0.0000) | Acc: (49.00%) (23025/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3862) |  Loss2: (0.0000) | Acc: (49.00%) (23712/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3844) |  Loss2: (0.0000) | Acc: (50.00%) (24391/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3830) |  Loss2: (0.0000) | Acc: (50.00%) (25034/50000)
# TEST : Loss: (1.2850) | Acc: (53.00%) (5342/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.2829) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.2967) |  Loss2: (0.0000) | Acc: (53.00%) (760/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (54.00%) (1467/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.2764) |  Loss2: (0.0000) | Acc: (54.00%) (2177/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.2820) |  Loss2: (0.0000) | Acc: (55.00%) (2888/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.2838) |  Loss2: (0.0000) | Acc: (54.00%) (3586/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2782) |  Loss2: (0.0000) | Acc: (54.00%) (4291/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2716) |  Loss2: (0.0000) | Acc: (55.00%) (5010/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2742) |  Loss2: (0.0000) | Acc: (54.00%) (5683/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2732) |  Loss2: (0.0000) | Acc: (54.00%) (6384/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2733) |  Loss2: (0.0000) | Acc: (54.00%) (7088/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2744) |  Loss2: (0.0000) | Acc: (54.00%) (7783/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2723) |  Loss2: (0.0000) | Acc: (54.00%) (8495/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2734) |  Loss2: (0.0000) | Acc: (54.00%) (9160/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2709) |  Loss2: (0.0000) | Acc: (54.00%) (9892/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2707) |  Loss2: (0.0000) | Acc: (54.00%) (10599/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2702) |  Loss2: (0.0000) | Acc: (54.00%) (11295/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2685) |  Loss2: (0.0000) | Acc: (54.00%) (12016/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2678) |  Loss2: (0.0000) | Acc: (54.00%) (12736/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2667) |  Loss2: (0.0000) | Acc: (55.00%) (13448/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2646) |  Loss2: (0.0000) | Acc: (55.00%) (14185/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2634) |  Loss2: (0.0000) | Acc: (55.00%) (14914/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2596) |  Loss2: (0.0000) | Acc: (55.00%) (15673/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2590) |  Loss2: (0.0000) | Acc: (55.00%) (16374/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2580) |  Loss2: (0.0000) | Acc: (55.00%) (17087/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2560) |  Loss2: (0.0000) | Acc: (55.00%) (17832/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2545) |  Loss2: (0.0000) | Acc: (55.00%) (18548/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2536) |  Loss2: (0.0000) | Acc: (55.00%) (19283/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2523) |  Loss2: (0.0000) | Acc: (55.00%) (19990/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2492) |  Loss2: (0.0000) | Acc: (55.00%) (20770/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2493) |  Loss2: (0.0000) | Acc: (55.00%) (21449/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2483) |  Loss2: (0.0000) | Acc: (55.00%) (22157/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2475) |  Loss2: (0.0000) | Acc: (55.00%) (22891/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2464) |  Loss2: (0.0000) | Acc: (55.00%) (23621/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2444) |  Loss2: (0.0000) | Acc: (55.00%) (24374/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2424) |  Loss2: (0.0000) | Acc: (55.00%) (25096/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2409) |  Loss2: (0.0000) | Acc: (55.00%) (25837/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2385) |  Loss2: (0.0000) | Acc: (56.00%) (26597/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2372) |  Loss2: (0.0000) | Acc: (56.00%) (27331/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2352) |  Loss2: (0.0000) | Acc: (56.00%) (28072/50000)
# TEST : Loss: (1.2584) | Acc: (54.00%) (5478/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.1252) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2797) |  Loss2: (0.0000) | Acc: (51.00%) (730/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3131) |  Loss2: (0.0000) | Acc: (51.00%) (1382/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3537) |  Loss2: (0.0000) | Acc: (49.00%) (1980/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.3568) |  Loss2: (0.0000) | Acc: (50.00%) (2627/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.3634) |  Loss2: (0.0000) | Acc: (50.00%) (3267/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.3626) |  Loss2: (0.0000) | Acc: (50.00%) (3910/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.3612) |  Loss2: (0.0000) | Acc: (50.00%) (4569/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.3590) |  Loss2: (0.0000) | Acc: (50.00%) (5223/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.3570) |  Loss2: (0.0000) | Acc: (50.00%) (5888/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.3532) |  Loss2: (0.0000) | Acc: (50.00%) (6565/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.3516) |  Loss2: (0.0000) | Acc: (50.00%) (7236/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.3478) |  Loss2: (0.0000) | Acc: (51.00%) (7905/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.3499) |  Loss2: (0.0000) | Acc: (50.00%) (8528/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.3428) |  Loss2: (0.0000) | Acc: (51.00%) (9244/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.3361) |  Loss2: (0.0000) | Acc: (51.00%) (9951/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.3341) |  Loss2: (0.0000) | Acc: (51.00%) (10600/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.3356) |  Loss2: (0.0000) | Acc: (51.00%) (11263/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.3302) |  Loss2: (0.0000) | Acc: (51.00%) (11982/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.3285) |  Loss2: (0.0000) | Acc: (51.00%) (12651/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.3253) |  Loss2: (0.0000) | Acc: (51.00%) (13347/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.3217) |  Loss2: (0.0000) | Acc: (51.00%) (14035/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.3200) |  Loss2: (0.0000) | Acc: (52.00%) (14731/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.3176) |  Loss2: (0.0000) | Acc: (52.00%) (15426/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.3152) |  Loss2: (0.0000) | Acc: (52.00%) (16110/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.3127) |  Loss2: (0.0000) | Acc: (52.00%) (16828/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.3105) |  Loss2: (0.0000) | Acc: (52.00%) (17523/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.3082) |  Loss2: (0.0000) | Acc: (52.00%) (18226/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3064) |  Loss2: (0.0000) | Acc: (52.00%) (18929/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3048) |  Loss2: (0.0000) | Acc: (52.00%) (19615/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3015) |  Loss2: (0.0000) | Acc: (52.00%) (20337/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.2996) |  Loss2: (0.0000) | Acc: (52.00%) (21056/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.2972) |  Loss2: (0.0000) | Acc: (53.00%) (21796/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.2948) |  Loss2: (0.0000) | Acc: (53.00%) (22522/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.2929) |  Loss2: (0.0000) | Acc: (53.00%) (23238/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.2904) |  Loss2: (0.0000) | Acc: (53.00%) (23950/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.2888) |  Loss2: (0.0000) | Acc: (53.00%) (24659/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.2875) |  Loss2: (0.0000) | Acc: (53.00%) (25349/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.2864) |  Loss2: (0.0000) | Acc: (53.00%) (26047/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2843) |  Loss2: (0.0000) | Acc: (53.00%) (26759/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_005.pth.tar'
# TEST : Loss: (1.1945) | Acc: (56.00%) (5643/10000)
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5102, 0.4898], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.6133, 0.3867], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.1773) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.1860) |  Loss2: (0.0000) | Acc: (58.00%) (819/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.2092) |  Loss2: (0.0000) | Acc: (56.00%) (1528/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.1968) |  Loss2: (0.0000) | Acc: (57.00%) (2269/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (56.00%) (2981/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.2016) |  Loss2: (0.0000) | Acc: (56.00%) (3691/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2022) |  Loss2: (0.0000) | Acc: (56.00%) (4426/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2019) |  Loss2: (0.0000) | Acc: (56.00%) (5145/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2054) |  Loss2: (0.0000) | Acc: (56.00%) (5865/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.1987) |  Loss2: (0.0000) | Acc: (56.00%) (6625/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2057) |  Loss2: (0.0000) | Acc: (56.00%) (7316/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2044) |  Loss2: (0.0000) | Acc: (56.00%) (8040/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2078) |  Loss2: (0.0000) | Acc: (56.00%) (8749/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2060) |  Loss2: (0.0000) | Acc: (56.00%) (9470/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2092) |  Loss2: (0.0000) | Acc: (56.00%) (10175/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2099) |  Loss2: (0.0000) | Acc: (56.00%) (10874/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2129) |  Loss2: (0.0000) | Acc: (56.00%) (11576/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2097) |  Loss2: (0.0000) | Acc: (56.00%) (12314/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2090) |  Loss2: (0.0000) | Acc: (56.00%) (13019/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2078) |  Loss2: (0.0000) | Acc: (56.00%) (13745/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2067) |  Loss2: (0.0000) | Acc: (56.00%) (14475/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2046) |  Loss2: (0.0000) | Acc: (56.00%) (15219/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2047) |  Loss2: (0.0000) | Acc: (56.00%) (15962/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (56.00%) (16683/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2060) |  Loss2: (0.0000) | Acc: (56.00%) (17395/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2028) |  Loss2: (0.0000) | Acc: (56.00%) (18146/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2036) |  Loss2: (0.0000) | Acc: (56.00%) (18859/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2036) |  Loss2: (0.0000) | Acc: (56.00%) (19563/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2026) |  Loss2: (0.0000) | Acc: (56.00%) (20309/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2023) |  Loss2: (0.0000) | Acc: (56.00%) (21068/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2004) |  Loss2: (0.0000) | Acc: (56.00%) (21832/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (56.00%) (22575/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.1988) |  Loss2: (0.0000) | Acc: (56.00%) (23306/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.1991) |  Loss2: (0.0000) | Acc: (56.00%) (24023/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.1994) |  Loss2: (0.0000) | Acc: (56.00%) (24737/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.1984) |  Loss2: (0.0000) | Acc: (56.00%) (25473/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.1977) |  Loss2: (0.0000) | Acc: (56.00%) (26208/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.1976) |  Loss2: (0.0000) | Acc: (56.00%) (26923/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.1957) |  Loss2: (0.0000) | Acc: (56.00%) (27660/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.1949) |  Loss2: (0.0000) | Acc: (56.00%) (28374/50000)
# TEST : Loss: (1.1619) | Acc: (57.00%) (5740/10000)
percent tensor([0.4973, 0.5027], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.5182, 0.4818], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.7391, 0.2609], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.0162) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.1857) |  Loss2: (0.0000) | Acc: (55.00%) (788/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2027) |  Loss2: (0.0000) | Acc: (54.00%) (1476/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.1846) |  Loss2: (0.0000) | Acc: (55.00%) (2216/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.1933) |  Loss2: (0.0000) | Acc: (55.00%) (2926/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1842) |  Loss2: (0.0000) | Acc: (56.00%) (3672/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.1746) |  Loss2: (0.0000) | Acc: (56.00%) (4436/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.1782) |  Loss2: (0.0000) | Acc: (57.00%) (5193/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1721) |  Loss2: (0.0000) | Acc: (57.00%) (5953/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1672) |  Loss2: (0.0000) | Acc: (57.00%) (6734/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1682) |  Loss2: (0.0000) | Acc: (57.00%) (7460/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1671) |  Loss2: (0.0000) | Acc: (57.00%) (8216/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (57.00%) (8968/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.1679) |  Loss2: (0.0000) | Acc: (57.00%) (9683/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.1670) |  Loss2: (0.0000) | Acc: (57.00%) (10416/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.1673) |  Loss2: (0.0000) | Acc: (57.00%) (11173/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.1664) |  Loss2: (0.0000) | Acc: (57.00%) (11910/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.1645) |  Loss2: (0.0000) | Acc: (57.00%) (12666/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1645) |  Loss2: (0.0000) | Acc: (57.00%) (13415/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1657) |  Loss2: (0.0000) | Acc: (57.00%) (14132/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1636) |  Loss2: (0.0000) | Acc: (57.00%) (14889/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1656) |  Loss2: (0.0000) | Acc: (57.00%) (15615/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1650) |  Loss2: (0.0000) | Acc: (57.00%) (16362/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1636) |  Loss2: (0.0000) | Acc: (57.00%) (17123/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1627) |  Loss2: (0.0000) | Acc: (57.00%) (17879/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1640) |  Loss2: (0.0000) | Acc: (57.00%) (18604/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1639) |  Loss2: (0.0000) | Acc: (57.00%) (19335/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1630) |  Loss2: (0.0000) | Acc: (57.00%) (20096/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1623) |  Loss2: (0.0000) | Acc: (57.00%) (20842/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1622) |  Loss2: (0.0000) | Acc: (57.00%) (21582/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1620) |  Loss2: (0.0000) | Acc: (57.00%) (22319/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1635) |  Loss2: (0.0000) | Acc: (57.00%) (23055/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1637) |  Loss2: (0.0000) | Acc: (57.00%) (23793/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1627) |  Loss2: (0.0000) | Acc: (57.00%) (24532/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1640) |  Loss2: (0.0000) | Acc: (57.00%) (25228/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1631) |  Loss2: (0.0000) | Acc: (57.00%) (25975/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1624) |  Loss2: (0.0000) | Acc: (57.00%) (26716/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1624) |  Loss2: (0.0000) | Acc: (57.00%) (27463/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1643) |  Loss2: (0.0000) | Acc: (57.00%) (28180/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1635) |  Loss2: (0.0000) | Acc: (57.00%) (28907/50000)
# TEST : Loss: (1.1507) | Acc: (57.00%) (5785/10000)
percent tensor([0.5004, 0.4996], device='cuda:0')
percent tensor([0.4988, 0.5012], device='cuda:0')
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.5289, 0.4711], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5152, 0.4848], device='cuda:0')
percent tensor([0.5241, 0.4759], device='cuda:0')
percent tensor([0.7870, 0.2130], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1832) |  Loss2: (0.0000) | Acc: (52.00%) (67/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (54.00%) (770/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1747) |  Loss2: (0.0000) | Acc: (55.00%) (1495/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1581) |  Loss2: (0.0000) | Acc: (57.00%) (2262/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1576) |  Loss2: (0.0000) | Acc: (57.00%) (3022/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1497) |  Loss2: (0.0000) | Acc: (58.00%) (3795/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1542) |  Loss2: (0.0000) | Acc: (57.00%) (4521/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1502) |  Loss2: (0.0000) | Acc: (58.00%) (5291/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1440) |  Loss2: (0.0000) | Acc: (58.00%) (6052/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1396) |  Loss2: (0.0000) | Acc: (58.00%) (6805/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1449) |  Loss2: (0.0000) | Acc: (58.00%) (7510/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1402) |  Loss2: (0.0000) | Acc: (58.00%) (8271/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1389) |  Loss2: (0.0000) | Acc: (58.00%) (9014/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1376) |  Loss2: (0.0000) | Acc: (58.00%) (9815/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1380) |  Loss2: (0.0000) | Acc: (58.00%) (10572/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1383) |  Loss2: (0.0000) | Acc: (58.00%) (11339/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (12069/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1412) |  Loss2: (0.0000) | Acc: (58.00%) (12839/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1411) |  Loss2: (0.0000) | Acc: (58.00%) (13603/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1405) |  Loss2: (0.0000) | Acc: (58.00%) (14366/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1426) |  Loss2: (0.0000) | Acc: (58.00%) (15125/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1410) |  Loss2: (0.0000) | Acc: (58.00%) (15904/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1421) |  Loss2: (0.0000) | Acc: (58.00%) (16648/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1409) |  Loss2: (0.0000) | Acc: (58.00%) (17416/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1414) |  Loss2: (0.0000) | Acc: (58.00%) (18184/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1410) |  Loss2: (0.0000) | Acc: (58.00%) (18924/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1409) |  Loss2: (0.0000) | Acc: (58.00%) (19657/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (58.00%) (20407/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (58.00%) (21140/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1425) |  Loss2: (0.0000) | Acc: (58.00%) (21910/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1439) |  Loss2: (0.0000) | Acc: (58.00%) (22637/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1450) |  Loss2: (0.0000) | Acc: (58.00%) (23364/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1454) |  Loss2: (0.0000) | Acc: (58.00%) (24097/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1467) |  Loss2: (0.0000) | Acc: (58.00%) (24832/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1475) |  Loss2: (0.0000) | Acc: (58.00%) (25558/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1477) |  Loss2: (0.0000) | Acc: (58.00%) (26311/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1492) |  Loss2: (0.0000) | Acc: (58.00%) (27018/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1501) |  Loss2: (0.0000) | Acc: (58.00%) (27756/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1500) |  Loss2: (0.0000) | Acc: (58.00%) (28509/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1498) |  Loss2: (0.0000) | Acc: (58.00%) (29231/50000)
# TEST : Loss: (1.1376) | Acc: (58.00%) (5874/10000)
percent tensor([0.5023, 0.4977], device='cuda:0')
percent tensor([0.5008, 0.4992], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.5265, 0.4735], device='cuda:0')
percent tensor([0.5179, 0.4821], device='cuda:0')
percent tensor([0.5280, 0.4720], device='cuda:0')
percent tensor([0.8061, 0.1939], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1591) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1279) |  Loss2: (0.0000) | Acc: (59.00%) (840/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1165) |  Loss2: (0.0000) | Acc: (59.00%) (1602/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1196) |  Loss2: (0.0000) | Acc: (59.00%) (2374/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1331) |  Loss2: (0.0000) | Acc: (59.00%) (3110/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1410) |  Loss2: (0.0000) | Acc: (58.00%) (3833/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1400) |  Loss2: (0.0000) | Acc: (58.00%) (4583/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1347) |  Loss2: (0.0000) | Acc: (58.00%) (5355/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1321) |  Loss2: (0.0000) | Acc: (58.00%) (6101/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1310) |  Loss2: (0.0000) | Acc: (59.00%) (6884/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1350) |  Loss2: (0.0000) | Acc: (58.00%) (7626/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1367) |  Loss2: (0.0000) | Acc: (59.00%) (8387/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1398) |  Loss2: (0.0000) | Acc: (58.00%) (9129/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (9866/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1419) |  Loss2: (0.0000) | Acc: (58.00%) (10603/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (11343/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (58.00%) (12090/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1417) |  Loss2: (0.0000) | Acc: (58.00%) (12840/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1406) |  Loss2: (0.0000) | Acc: (58.00%) (13602/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1398) |  Loss2: (0.0000) | Acc: (58.00%) (14355/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (15078/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1456) |  Loss2: (0.0000) | Acc: (58.00%) (15818/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1444) |  Loss2: (0.0000) | Acc: (58.00%) (16581/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1450) |  Loss2: (0.0000) | Acc: (58.00%) (17323/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1442) |  Loss2: (0.0000) | Acc: (58.00%) (18088/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1457) |  Loss2: (0.0000) | Acc: (58.00%) (18822/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1468) |  Loss2: (0.0000) | Acc: (58.00%) (19531/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1448) |  Loss2: (0.0000) | Acc: (58.00%) (20291/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1450) |  Loss2: (0.0000) | Acc: (58.00%) (21060/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1445) |  Loss2: (0.0000) | Acc: (58.00%) (21811/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1449) |  Loss2: (0.0000) | Acc: (58.00%) (22557/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1461) |  Loss2: (0.0000) | Acc: (58.00%) (23286/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1460) |  Loss2: (0.0000) | Acc: (58.00%) (24035/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1451) |  Loss2: (0.0000) | Acc: (58.00%) (24795/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1448) |  Loss2: (0.0000) | Acc: (58.00%) (25542/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1459) |  Loss2: (0.0000) | Acc: (58.00%) (26281/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1447) |  Loss2: (0.0000) | Acc: (58.00%) (27055/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1441) |  Loss2: (0.0000) | Acc: (58.00%) (27807/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1434) |  Loss2: (0.0000) | Acc: (58.00%) (28550/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1433) |  Loss2: (0.0000) | Acc: (58.00%) (29244/50000)
# TEST : Loss: (1.1302) | Acc: (59.00%) (5901/10000)
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5118, 0.4882], device='cuda:0')
percent tensor([0.5404, 0.4596], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.8225, 0.1775], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.0250) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1429) |  Loss2: (0.0000) | Acc: (56.00%) (795/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1545) |  Loss2: (0.0000) | Acc: (56.00%) (1531/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1418) |  Loss2: (0.0000) | Acc: (57.00%) (2288/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1511) |  Loss2: (0.0000) | Acc: (57.00%) (3033/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1649) |  Loss2: (0.0000) | Acc: (57.00%) (3761/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1598) |  Loss2: (0.0000) | Acc: (58.00%) (4530/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1530) |  Loss2: (0.0000) | Acc: (58.00%) (5289/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1525) |  Loss2: (0.0000) | Acc: (58.00%) (6038/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1490) |  Loss2: (0.0000) | Acc: (58.00%) (6788/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1413) |  Loss2: (0.0000) | Acc: (58.00%) (7600/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1397) |  Loss2: (0.0000) | Acc: (58.00%) (8377/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1387) |  Loss2: (0.0000) | Acc: (59.00%) (9145/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1404) |  Loss2: (0.0000) | Acc: (58.00%) (9885/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1356) |  Loss2: (0.0000) | Acc: (59.00%) (10662/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1326) |  Loss2: (0.0000) | Acc: (59.00%) (11459/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1294) |  Loss2: (0.0000) | Acc: (59.00%) (12249/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1249) |  Loss2: (0.0000) | Acc: (59.00%) (13034/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1224) |  Loss2: (0.0000) | Acc: (59.00%) (13799/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1202) |  Loss2: (0.0000) | Acc: (59.00%) (14581/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1201) |  Loss2: (0.0000) | Acc: (59.00%) (15343/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1178) |  Loss2: (0.0000) | Acc: (59.00%) (16131/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1153) |  Loss2: (0.0000) | Acc: (59.00%) (16921/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1151) |  Loss2: (0.0000) | Acc: (59.00%) (17673/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1151) |  Loss2: (0.0000) | Acc: (59.00%) (18446/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1146) |  Loss2: (0.0000) | Acc: (59.00%) (19241/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1130) |  Loss2: (0.0000) | Acc: (59.00%) (20043/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1106) |  Loss2: (0.0000) | Acc: (60.00%) (20861/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1105) |  Loss2: (0.0000) | Acc: (60.00%) (21634/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1093) |  Loss2: (0.0000) | Acc: (60.00%) (22425/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1062) |  Loss2: (0.0000) | Acc: (60.00%) (23230/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1058) |  Loss2: (0.0000) | Acc: (60.00%) (23994/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1036) |  Loss2: (0.0000) | Acc: (60.00%) (24780/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1020) |  Loss2: (0.0000) | Acc: (60.00%) (25572/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1015) |  Loss2: (0.0000) | Acc: (60.00%) (26371/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1001) |  Loss2: (0.0000) | Acc: (60.00%) (27156/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.0987) |  Loss2: (0.0000) | Acc: (60.00%) (27953/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.0964) |  Loss2: (0.0000) | Acc: (60.00%) (28765/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.0953) |  Loss2: (0.0000) | Acc: (60.00%) (29561/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.0934) |  Loss2: (0.0000) | Acc: (60.00%) (30351/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_010.pth.tar'
# TEST : Loss: (1.2037) | Acc: (58.00%) (5853/10000)
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5108, 0.4892], device='cuda:0')
percent tensor([0.5398, 0.4602], device='cuda:0')
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.8271, 0.1729], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.4153, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(775.5703, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(771.5829, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1534.2938, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(510.2527, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2165.7239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4321.0068, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1441.7596, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6104.4160, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12223.5586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4070.1523, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17282.7070, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (1.1168) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (0.9960) |  Loss2: (0.0000) | Acc: (65.00%) (924/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0391) |  Loss2: (0.0000) | Acc: (63.00%) (1719/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0296) |  Loss2: (0.0000) | Acc: (64.00%) (2550/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0271) |  Loss2: (0.0000) | Acc: (63.00%) (3350/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0181) |  Loss2: (0.0000) | Acc: (63.00%) (4158/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0182) |  Loss2: (0.0000) | Acc: (63.00%) (4966/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0242) |  Loss2: (0.0000) | Acc: (63.00%) (5760/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0242) |  Loss2: (0.0000) | Acc: (63.00%) (6561/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0280) |  Loss2: (0.0000) | Acc: (63.00%) (7352/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0279) |  Loss2: (0.0000) | Acc: (63.00%) (8169/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0251) |  Loss2: (0.0000) | Acc: (63.00%) (8994/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0239) |  Loss2: (0.0000) | Acc: (63.00%) (9798/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0215) |  Loss2: (0.0000) | Acc: (63.00%) (10647/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0217) |  Loss2: (0.0000) | Acc: (63.00%) (11448/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0224) |  Loss2: (0.0000) | Acc: (63.00%) (12253/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0216) |  Loss2: (0.0000) | Acc: (63.00%) (13075/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0188) |  Loss2: (0.0000) | Acc: (63.00%) (13926/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0182) |  Loss2: (0.0000) | Acc: (63.00%) (14750/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0166) |  Loss2: (0.0000) | Acc: (63.00%) (15576/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0166) |  Loss2: (0.0000) | Acc: (63.00%) (16395/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0162) |  Loss2: (0.0000) | Acc: (63.00%) (17199/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0120) |  Loss2: (0.0000) | Acc: (63.00%) (18037/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0099) |  Loss2: (0.0000) | Acc: (63.00%) (18857/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0081) |  Loss2: (0.0000) | Acc: (63.00%) (19701/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0068) |  Loss2: (0.0000) | Acc: (63.00%) (20532/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0045) |  Loss2: (0.0000) | Acc: (63.00%) (21381/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0039) |  Loss2: (0.0000) | Acc: (64.00%) (22221/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0014) |  Loss2: (0.0000) | Acc: (64.00%) (23083/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0003) |  Loss2: (0.0000) | Acc: (64.00%) (23909/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (0.9998) |  Loss2: (0.0000) | Acc: (64.00%) (24742/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (0.9988) |  Loss2: (0.0000) | Acc: (64.00%) (25586/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (0.9977) |  Loss2: (0.0000) | Acc: (64.00%) (26427/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (0.9970) |  Loss2: (0.0000) | Acc: (64.00%) (27249/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (0.9953) |  Loss2: (0.0000) | Acc: (64.00%) (28098/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (0.9926) |  Loss2: (0.0000) | Acc: (64.00%) (28969/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (0.9916) |  Loss2: (0.0000) | Acc: (64.00%) (29812/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (0.9909) |  Loss2: (0.0000) | Acc: (64.00%) (30661/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (0.9907) |  Loss2: (0.0000) | Acc: (64.00%) (31496/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (0.9894) |  Loss2: (0.0000) | Acc: (64.00%) (32310/50000)
# TEST : Loss: (1.0538) | Acc: (62.00%) (6293/10000)
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5124, 0.4876], device='cuda:0')
percent tensor([0.5397, 0.4603], device='cuda:0')
percent tensor([0.5295, 0.4705], device='cuda:0')
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.8185, 0.1815], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9299) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9527) |  Loss2: (0.0000) | Acc: (66.00%) (938/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9500) |  Loss2: (0.0000) | Acc: (66.00%) (1788/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9498) |  Loss2: (0.0000) | Acc: (65.00%) (2616/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9429) |  Loss2: (0.0000) | Acc: (66.00%) (3478/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9455) |  Loss2: (0.0000) | Acc: (66.00%) (4340/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9385) |  Loss2: (0.0000) | Acc: (66.00%) (5206/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9345) |  Loss2: (0.0000) | Acc: (66.00%) (6063/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9433) |  Loss2: (0.0000) | Acc: (66.00%) (6870/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9436) |  Loss2: (0.0000) | Acc: (66.00%) (7715/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9375) |  Loss2: (0.0000) | Acc: (66.00%) (8605/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9348) |  Loss2: (0.0000) | Acc: (66.00%) (9469/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9368) |  Loss2: (0.0000) | Acc: (66.00%) (10314/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9343) |  Loss2: (0.0000) | Acc: (66.00%) (11177/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9331) |  Loss2: (0.0000) | Acc: (66.00%) (12029/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9320) |  Loss2: (0.0000) | Acc: (66.00%) (12895/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9316) |  Loss2: (0.0000) | Acc: (66.00%) (13755/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9331) |  Loss2: (0.0000) | Acc: (66.00%) (14596/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9335) |  Loss2: (0.0000) | Acc: (66.00%) (15430/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9325) |  Loss2: (0.0000) | Acc: (66.00%) (16269/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9306) |  Loss2: (0.0000) | Acc: (66.00%) (17169/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9292) |  Loss2: (0.0000) | Acc: (66.00%) (18054/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9313) |  Loss2: (0.0000) | Acc: (66.00%) (18894/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9300) |  Loss2: (0.0000) | Acc: (66.00%) (19749/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9278) |  Loss2: (0.0000) | Acc: (66.00%) (20633/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9284) |  Loss2: (0.0000) | Acc: (66.00%) (21482/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9273) |  Loss2: (0.0000) | Acc: (66.00%) (22354/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9249) |  Loss2: (0.0000) | Acc: (67.00%) (23258/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9240) |  Loss2: (0.0000) | Acc: (67.00%) (24119/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9227) |  Loss2: (0.0000) | Acc: (67.00%) (24993/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (25846/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9216) |  Loss2: (0.0000) | Acc: (67.00%) (26720/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9209) |  Loss2: (0.0000) | Acc: (67.00%) (27601/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9190) |  Loss2: (0.0000) | Acc: (67.00%) (28491/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9176) |  Loss2: (0.0000) | Acc: (67.00%) (29368/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9168) |  Loss2: (0.0000) | Acc: (67.00%) (30256/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9156) |  Loss2: (0.0000) | Acc: (67.00%) (31130/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9148) |  Loss2: (0.0000) | Acc: (67.00%) (32020/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9144) |  Loss2: (0.0000) | Acc: (67.00%) (32896/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9136) |  Loss2: (0.0000) | Acc: (67.00%) (33741/50000)
# TEST : Loss: (0.9041) | Acc: (67.00%) (6774/10000)
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5130, 0.4870], device='cuda:0')
percent tensor([0.5410, 0.4590], device='cuda:0')
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.8185, 0.1815], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.7416) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.8671) |  Loss2: (0.0000) | Acc: (69.00%) (983/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.8702) |  Loss2: (0.0000) | Acc: (69.00%) (1861/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.8644) |  Loss2: (0.0000) | Acc: (69.00%) (2738/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.8809) |  Loss2: (0.0000) | Acc: (68.00%) (3610/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.8867) |  Loss2: (0.0000) | Acc: (68.00%) (4478/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.8819) |  Loss2: (0.0000) | Acc: (68.00%) (5362/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.8730) |  Loss2: (0.0000) | Acc: (69.00%) (6279/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.8726) |  Loss2: (0.0000) | Acc: (69.00%) (7168/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.8695) |  Loss2: (0.0000) | Acc: (69.00%) (8074/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8671) |  Loss2: (0.0000) | Acc: (69.00%) (8966/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8658) |  Loss2: (0.0000) | Acc: (69.00%) (9829/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8650) |  Loss2: (0.0000) | Acc: (69.00%) (10726/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8631) |  Loss2: (0.0000) | Acc: (69.00%) (11640/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8658) |  Loss2: (0.0000) | Acc: (69.00%) (12486/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8631) |  Loss2: (0.0000) | Acc: (69.00%) (13399/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8625) |  Loss2: (0.0000) | Acc: (69.00%) (14290/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (15175/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.8618) |  Loss2: (0.0000) | Acc: (69.00%) (16063/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.8635) |  Loss2: (0.0000) | Acc: (69.00%) (16931/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (17817/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8621) |  Loss2: (0.0000) | Acc: (69.00%) (18712/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8596) |  Loss2: (0.0000) | Acc: (69.00%) (19634/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8587) |  Loss2: (0.0000) | Acc: (69.00%) (20525/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (21400/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8597) |  Loss2: (0.0000) | Acc: (69.00%) (22264/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8604) |  Loss2: (0.0000) | Acc: (69.00%) (23164/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8582) |  Loss2: (0.0000) | Acc: (69.00%) (24082/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8592) |  Loss2: (0.0000) | Acc: (69.00%) (24949/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8580) |  Loss2: (0.0000) | Acc: (69.00%) (25872/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8571) |  Loss2: (0.0000) | Acc: (69.00%) (26779/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8572) |  Loss2: (0.0000) | Acc: (69.00%) (27684/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8572) |  Loss2: (0.0000) | Acc: (69.00%) (28597/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8549) |  Loss2: (0.0000) | Acc: (69.00%) (29531/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8547) |  Loss2: (0.0000) | Acc: (69.00%) (30432/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8549) |  Loss2: (0.0000) | Acc: (69.00%) (31329/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8550) |  Loss2: (0.0000) | Acc: (69.00%) (32205/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8539) |  Loss2: (0.0000) | Acc: (69.00%) (33117/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (34026/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8514) |  Loss2: (0.0000) | Acc: (69.00%) (34901/50000)
# TEST : Loss: (0.8399) | Acc: (70.00%) (7002/10000)
percent tensor([0.5049, 0.4951], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.5398, 0.4602], device='cuda:0')
percent tensor([0.5302, 0.4698], device='cuda:0')
percent tensor([0.5205, 0.4795], device='cuda:0')
percent tensor([0.5325, 0.4675], device='cuda:0')
percent tensor([0.8205, 0.1795], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.6448) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.7849) |  Loss2: (0.0000) | Acc: (73.00%) (1030/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8037) |  Loss2: (0.0000) | Acc: (71.00%) (1928/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8131) |  Loss2: (0.0000) | Acc: (71.00%) (2828/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (71.00%) (3733/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8205) |  Loss2: (0.0000) | Acc: (70.00%) (4634/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8225) |  Loss2: (0.0000) | Acc: (70.00%) (5525/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8188) |  Loss2: (0.0000) | Acc: (71.00%) (6461/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (71.00%) (7370/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (8295/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8158) |  Loss2: (0.0000) | Acc: (71.00%) (9202/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8177) |  Loss2: (0.0000) | Acc: (71.00%) (10106/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8210) |  Loss2: (0.0000) | Acc: (71.00%) (11007/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8218) |  Loss2: (0.0000) | Acc: (71.00%) (11917/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8206) |  Loss2: (0.0000) | Acc: (71.00%) (12826/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8200) |  Loss2: (0.0000) | Acc: (71.00%) (13735/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8209) |  Loss2: (0.0000) | Acc: (71.00%) (14644/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8198) |  Loss2: (0.0000) | Acc: (71.00%) (15547/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8192) |  Loss2: (0.0000) | Acc: (71.00%) (16458/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (17383/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (18277/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8153) |  Loss2: (0.0000) | Acc: (71.00%) (19190/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8153) |  Loss2: (0.0000) | Acc: (71.00%) (20105/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8145) |  Loss2: (0.0000) | Acc: (71.00%) (21020/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8129) |  Loss2: (0.0000) | Acc: (71.00%) (21961/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8115) |  Loss2: (0.0000) | Acc: (71.00%) (22890/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8111) |  Loss2: (0.0000) | Acc: (71.00%) (23815/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8093) |  Loss2: (0.0000) | Acc: (71.00%) (24742/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (25679/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8060) |  Loss2: (0.0000) | Acc: (71.00%) (26579/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8060) |  Loss2: (0.0000) | Acc: (71.00%) (27483/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8059) |  Loss2: (0.0000) | Acc: (71.00%) (28383/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (71.00%) (29339/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8031) |  Loss2: (0.0000) | Acc: (71.00%) (30265/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8023) |  Loss2: (0.0000) | Acc: (71.00%) (31189/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (71.00%) (32117/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (33072/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8005) |  Loss2: (0.0000) | Acc: (71.00%) (34009/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8006) |  Loss2: (0.0000) | Acc: (71.00%) (34934/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.7997) |  Loss2: (0.0000) | Acc: (71.00%) (35840/50000)
# TEST : Loss: (0.8177) | Acc: (71.00%) (7142/10000)
percent tensor([0.5048, 0.4952], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5119, 0.4881], device='cuda:0')
percent tensor([0.5402, 0.4598], device='cuda:0')
percent tensor([0.5286, 0.4714], device='cuda:0')
percent tensor([0.5208, 0.4792], device='cuda:0')
percent tensor([0.5334, 0.4666], device='cuda:0')
percent tensor([0.8240, 0.1760], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.6873) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.7989) |  Loss2: (0.0000) | Acc: (71.00%) (1010/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.8668) |  Loss2: (0.0000) | Acc: (68.00%) (1854/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.8823) |  Loss2: (0.0000) | Acc: (68.00%) (2716/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9065) |  Loss2: (0.0000) | Acc: (67.00%) (3544/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9090) |  Loss2: (0.0000) | Acc: (67.00%) (4410/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9083) |  Loss2: (0.0000) | Acc: (67.00%) (5282/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9084) |  Loss2: (0.0000) | Acc: (67.00%) (6131/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9243) |  Loss2: (0.0000) | Acc: (66.00%) (6940/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9278) |  Loss2: (0.0000) | Acc: (66.00%) (7785/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9184) |  Loss2: (0.0000) | Acc: (67.00%) (8694/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9118) |  Loss2: (0.0000) | Acc: (67.00%) (9583/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9085) |  Loss2: (0.0000) | Acc: (67.00%) (10473/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9076) |  Loss2: (0.0000) | Acc: (67.00%) (11357/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9034) |  Loss2: (0.0000) | Acc: (67.00%) (12255/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.8998) |  Loss2: (0.0000) | Acc: (68.00%) (13147/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.8948) |  Loss2: (0.0000) | Acc: (68.00%) (14068/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (14967/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.8925) |  Loss2: (0.0000) | Acc: (68.00%) (15848/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.8903) |  Loss2: (0.0000) | Acc: (68.00%) (16747/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.8900) |  Loss2: (0.0000) | Acc: (68.00%) (17625/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.8887) |  Loss2: (0.0000) | Acc: (68.00%) (18506/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.8866) |  Loss2: (0.0000) | Acc: (68.00%) (19382/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.8874) |  Loss2: (0.0000) | Acc: (68.00%) (20248/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.8856) |  Loss2: (0.0000) | Acc: (68.00%) (21137/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.8846) |  Loss2: (0.0000) | Acc: (68.00%) (22021/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.8815) |  Loss2: (0.0000) | Acc: (68.00%) (22937/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.8790) |  Loss2: (0.0000) | Acc: (68.00%) (23840/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.8779) |  Loss2: (0.0000) | Acc: (68.00%) (24730/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.8766) |  Loss2: (0.0000) | Acc: (68.00%) (25623/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.8771) |  Loss2: (0.0000) | Acc: (68.00%) (26490/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.8746) |  Loss2: (0.0000) | Acc: (68.00%) (27401/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.8743) |  Loss2: (0.0000) | Acc: (68.00%) (28286/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.8719) |  Loss2: (0.0000) | Acc: (68.00%) (29201/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.8706) |  Loss2: (0.0000) | Acc: (68.00%) (30109/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.8697) |  Loss2: (0.0000) | Acc: (69.00%) (31003/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8712) |  Loss2: (0.0000) | Acc: (68.00%) (31852/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8707) |  Loss2: (0.0000) | Acc: (68.00%) (32747/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8700) |  Loss2: (0.0000) | Acc: (68.00%) (33644/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8692) |  Loss2: (0.0000) | Acc: (69.00%) (34522/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_015.pth.tar'
# TEST : Loss: (0.8561) | Acc: (70.00%) (7002/10000)
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.4970, 0.5030], device='cuda:0')
percent tensor([0.5124, 0.4876], device='cuda:0')
percent tensor([0.5240, 0.4760], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.5337, 0.4663], device='cuda:0')
percent tensor([0.5402, 0.4598], device='cuda:0')
percent tensor([0.8764, 0.1236], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (0.8579) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (70.00%) (998/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8256) |  Loss2: (0.0000) | Acc: (70.00%) (1895/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (2800/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8389) |  Loss2: (0.0000) | Acc: (70.00%) (3675/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8297) |  Loss2: (0.0000) | Acc: (70.00%) (4615/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8199) |  Loss2: (0.0000) | Acc: (71.00%) (5551/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8206) |  Loss2: (0.0000) | Acc: (71.00%) (6462/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8149) |  Loss2: (0.0000) | Acc: (71.00%) (7407/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8183) |  Loss2: (0.0000) | Acc: (71.00%) (8293/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (71.00%) (9226/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8162) |  Loss2: (0.0000) | Acc: (71.00%) (10143/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8150) |  Loss2: (0.0000) | Acc: (71.00%) (11034/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8150) |  Loss2: (0.0000) | Acc: (71.00%) (11945/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8153) |  Loss2: (0.0000) | Acc: (71.00%) (12852/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8185) |  Loss2: (0.0000) | Acc: (71.00%) (13734/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8178) |  Loss2: (0.0000) | Acc: (71.00%) (14663/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8176) |  Loss2: (0.0000) | Acc: (71.00%) (15577/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8174) |  Loss2: (0.0000) | Acc: (71.00%) (16477/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8181) |  Loss2: (0.0000) | Acc: (70.00%) (17350/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8155) |  Loss2: (0.0000) | Acc: (71.00%) (18293/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (19203/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8133) |  Loss2: (0.0000) | Acc: (71.00%) (20111/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8122) |  Loss2: (0.0000) | Acc: (71.00%) (21015/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8115) |  Loss2: (0.0000) | Acc: (71.00%) (21926/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8097) |  Loss2: (0.0000) | Acc: (71.00%) (22852/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (71.00%) (23770/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (24666/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8106) |  Loss2: (0.0000) | Acc: (71.00%) (25603/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8113) |  Loss2: (0.0000) | Acc: (71.00%) (26497/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8090) |  Loss2: (0.0000) | Acc: (71.00%) (27449/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8073) |  Loss2: (0.0000) | Acc: (71.00%) (28378/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8055) |  Loss2: (0.0000) | Acc: (71.00%) (29307/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8057) |  Loss2: (0.0000) | Acc: (71.00%) (30196/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8064) |  Loss2: (0.0000) | Acc: (71.00%) (31086/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8054) |  Loss2: (0.0000) | Acc: (71.00%) (32001/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8058) |  Loss2: (0.0000) | Acc: (71.00%) (32900/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8048) |  Loss2: (0.0000) | Acc: (71.00%) (33836/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8051) |  Loss2: (0.0000) | Acc: (71.00%) (34761/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8053) |  Loss2: (0.0000) | Acc: (71.00%) (35626/50000)
# TEST : Loss: (0.8212) | Acc: (71.00%) (7144/10000)
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.5388, 0.4612], device='cuda:0')
percent tensor([0.5487, 0.4513], device='cuda:0')
percent tensor([0.5539, 0.4461], device='cuda:0')
percent tensor([0.9141, 0.0859], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.8683) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.7823) |  Loss2: (0.0000) | Acc: (71.00%) (1008/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (1920/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8042) |  Loss2: (0.0000) | Acc: (71.00%) (2829/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.7961) |  Loss2: (0.0000) | Acc: (71.00%) (3765/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (4666/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.7948) |  Loss2: (0.0000) | Acc: (71.00%) (5588/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.7928) |  Loss2: (0.0000) | Acc: (71.00%) (6523/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.7902) |  Loss2: (0.0000) | Acc: (71.00%) (7459/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.7949) |  Loss2: (0.0000) | Acc: (71.00%) (8372/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (9269/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (10189/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.7959) |  Loss2: (0.0000) | Acc: (71.00%) (11103/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.7943) |  Loss2: (0.0000) | Acc: (71.00%) (12034/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.7921) |  Loss2: (0.0000) | Acc: (71.00%) (12957/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.7903) |  Loss2: (0.0000) | Acc: (71.00%) (13882/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (14797/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (15719/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.7902) |  Loss2: (0.0000) | Acc: (71.00%) (16638/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (71.00%) (17560/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (71.00%) (18474/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.7913) |  Loss2: (0.0000) | Acc: (71.00%) (19393/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.7918) |  Loss2: (0.0000) | Acc: (71.00%) (20309/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.7909) |  Loss2: (0.0000) | Acc: (71.00%) (21228/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.7908) |  Loss2: (0.0000) | Acc: (71.00%) (22148/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (71.00%) (23044/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.7906) |  Loss2: (0.0000) | Acc: (71.00%) (23975/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (24885/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (71.00%) (25824/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.7886) |  Loss2: (0.0000) | Acc: (71.00%) (26750/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.7874) |  Loss2: (0.0000) | Acc: (71.00%) (27693/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.7895) |  Loss2: (0.0000) | Acc: (71.00%) (28593/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.7897) |  Loss2: (0.0000) | Acc: (71.00%) (29522/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.7902) |  Loss2: (0.0000) | Acc: (71.00%) (30437/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (31378/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (32293/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.7893) |  Loss2: (0.0000) | Acc: (71.00%) (33222/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.7873) |  Loss2: (0.0000) | Acc: (71.00%) (34181/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (71.00%) (35101/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.7863) |  Loss2: (0.0000) | Acc: (72.00%) (36002/50000)
# TEST : Loss: (0.8003) | Acc: (71.00%) (7186/10000)
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.4926, 0.5074], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.5253, 0.4747], device='cuda:0')
percent tensor([0.5406, 0.4594], device='cuda:0')
percent tensor([0.5623, 0.4377], device='cuda:0')
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.9327, 0.0673], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.6773) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (73.00%) (1038/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.7788) |  Loss2: (0.0000) | Acc: (72.00%) (1944/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.7704) |  Loss2: (0.0000) | Acc: (72.00%) (2870/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7788) |  Loss2: (0.0000) | Acc: (71.00%) (3776/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7740) |  Loss2: (0.0000) | Acc: (72.00%) (4717/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.7835) |  Loss2: (0.0000) | Acc: (71.00%) (5608/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.7885) |  Loss2: (0.0000) | Acc: (71.00%) (6521/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.7844) |  Loss2: (0.0000) | Acc: (72.00%) (7466/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.7834) |  Loss2: (0.0000) | Acc: (72.00%) (8391/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.7881) |  Loss2: (0.0000) | Acc: (71.00%) (9306/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.7900) |  Loss2: (0.0000) | Acc: (71.00%) (10221/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.7883) |  Loss2: (0.0000) | Acc: (72.00%) (11168/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (72.00%) (12075/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (72.00%) (13006/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (72.00%) (13930/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.7846) |  Loss2: (0.0000) | Acc: (72.00%) (14882/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.7832) |  Loss2: (0.0000) | Acc: (72.00%) (15818/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.7824) |  Loss2: (0.0000) | Acc: (72.00%) (16743/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.7826) |  Loss2: (0.0000) | Acc: (72.00%) (17642/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.7800) |  Loss2: (0.0000) | Acc: (72.00%) (18581/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.7784) |  Loss2: (0.0000) | Acc: (72.00%) (19516/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.7768) |  Loss2: (0.0000) | Acc: (72.00%) (20465/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.7770) |  Loss2: (0.0000) | Acc: (72.00%) (21389/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.7770) |  Loss2: (0.0000) | Acc: (72.00%) (22316/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.7764) |  Loss2: (0.0000) | Acc: (72.00%) (23238/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.7732) |  Loss2: (0.0000) | Acc: (72.00%) (24199/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.7722) |  Loss2: (0.0000) | Acc: (72.00%) (25132/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.7718) |  Loss2: (0.0000) | Acc: (72.00%) (26097/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.7716) |  Loss2: (0.0000) | Acc: (72.00%) (27030/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.7726) |  Loss2: (0.0000) | Acc: (72.00%) (27931/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (28866/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.7733) |  Loss2: (0.0000) | Acc: (72.00%) (29780/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.7720) |  Loss2: (0.0000) | Acc: (72.00%) (30731/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.7719) |  Loss2: (0.0000) | Acc: (72.00%) (31673/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.7723) |  Loss2: (0.0000) | Acc: (72.00%) (32603/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.7726) |  Loss2: (0.0000) | Acc: (72.00%) (33532/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.7726) |  Loss2: (0.0000) | Acc: (72.00%) (34459/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.7727) |  Loss2: (0.0000) | Acc: (72.00%) (35383/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.7722) |  Loss2: (0.0000) | Acc: (72.00%) (36292/50000)
# TEST : Loss: (0.7936) | Acc: (72.00%) (7248/10000)
percent tensor([0.5044, 0.4956], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.5301, 0.4699], device='cuda:0')
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.5823, 0.4177], device='cuda:0')
percent tensor([0.9457, 0.0543], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7408) |  Loss2: (0.0000) | Acc: (73.00%) (1032/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.7649) |  Loss2: (0.0000) | Acc: (72.00%) (1956/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7599) |  Loss2: (0.0000) | Acc: (72.00%) (2895/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7679) |  Loss2: (0.0000) | Acc: (72.00%) (3819/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7713) |  Loss2: (0.0000) | Acc: (72.00%) (4750/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7725) |  Loss2: (0.0000) | Acc: (72.00%) (5668/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7649) |  Loss2: (0.0000) | Acc: (73.00%) (6643/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7642) |  Loss2: (0.0000) | Acc: (73.00%) (7580/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7648) |  Loss2: (0.0000) | Acc: (72.00%) (8491/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7637) |  Loss2: (0.0000) | Acc: (72.00%) (9417/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7732) |  Loss2: (0.0000) | Acc: (72.00%) (10301/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7742) |  Loss2: (0.0000) | Acc: (72.00%) (11210/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7791) |  Loss2: (0.0000) | Acc: (72.00%) (12112/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7744) |  Loss2: (0.0000) | Acc: (72.00%) (13077/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.7730) |  Loss2: (0.0000) | Acc: (72.00%) (14020/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.7742) |  Loss2: (0.0000) | Acc: (72.00%) (14945/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.7736) |  Loss2: (0.0000) | Acc: (72.00%) (15877/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.7727) |  Loss2: (0.0000) | Acc: (72.00%) (16810/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.7726) |  Loss2: (0.0000) | Acc: (72.00%) (17739/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7711) |  Loss2: (0.0000) | Acc: (72.00%) (18674/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7730) |  Loss2: (0.0000) | Acc: (72.00%) (19579/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7734) |  Loss2: (0.0000) | Acc: (72.00%) (20507/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7749) |  Loss2: (0.0000) | Acc: (72.00%) (21434/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.7736) |  Loss2: (0.0000) | Acc: (72.00%) (22387/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7726) |  Loss2: (0.0000) | Acc: (72.00%) (23339/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.7737) |  Loss2: (0.0000) | Acc: (72.00%) (24255/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.7732) |  Loss2: (0.0000) | Acc: (72.00%) (25170/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.7730) |  Loss2: (0.0000) | Acc: (72.00%) (26109/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (27050/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.7709) |  Loss2: (0.0000) | Acc: (72.00%) (28002/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.7707) |  Loss2: (0.0000) | Acc: (72.00%) (28914/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.7708) |  Loss2: (0.0000) | Acc: (72.00%) (29840/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.7696) |  Loss2: (0.0000) | Acc: (72.00%) (30808/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.7692) |  Loss2: (0.0000) | Acc: (72.00%) (31759/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.7681) |  Loss2: (0.0000) | Acc: (72.00%) (32710/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.7682) |  Loss2: (0.0000) | Acc: (72.00%) (33647/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7677) |  Loss2: (0.0000) | Acc: (72.00%) (34587/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7682) |  Loss2: (0.0000) | Acc: (72.00%) (35501/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7686) |  Loss2: (0.0000) | Acc: (72.00%) (36386/50000)
# TEST : Loss: (0.7900) | Acc: (72.00%) (7267/10000)
percent tensor([0.5055, 0.4945], device='cuda:0')
percent tensor([0.4922, 0.5078], device='cuda:0')
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5415, 0.4585], device='cuda:0')
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.9536, 0.0464], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7545) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.7879) |  Loss2: (0.0000) | Acc: (71.00%) (1001/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.7715) |  Loss2: (0.0000) | Acc: (71.00%) (1921/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.7788) |  Loss2: (0.0000) | Acc: (71.00%) (2836/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.7881) |  Loss2: (0.0000) | Acc: (71.00%) (3743/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.7896) |  Loss2: (0.0000) | Acc: (71.00%) (4664/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.7857) |  Loss2: (0.0000) | Acc: (71.00%) (5599/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7857) |  Loss2: (0.0000) | Acc: (71.00%) (6520/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.7797) |  Loss2: (0.0000) | Acc: (72.00%) (7484/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.7775) |  Loss2: (0.0000) | Acc: (72.00%) (8422/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.7740) |  Loss2: (0.0000) | Acc: (72.00%) (9366/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.7718) |  Loss2: (0.0000) | Acc: (72.00%) (10305/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.7737) |  Loss2: (0.0000) | Acc: (72.00%) (11229/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (12149/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.7743) |  Loss2: (0.0000) | Acc: (72.00%) (13078/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (14017/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.7743) |  Loss2: (0.0000) | Acc: (72.00%) (14952/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.7744) |  Loss2: (0.0000) | Acc: (72.00%) (15879/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.7717) |  Loss2: (0.0000) | Acc: (72.00%) (16820/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.7702) |  Loss2: (0.0000) | Acc: (72.00%) (17746/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.7701) |  Loss2: (0.0000) | Acc: (72.00%) (18682/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.7695) |  Loss2: (0.0000) | Acc: (72.00%) (19622/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.7686) |  Loss2: (0.0000) | Acc: (72.00%) (20569/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7692) |  Loss2: (0.0000) | Acc: (72.00%) (21502/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7682) |  Loss2: (0.0000) | Acc: (72.00%) (22436/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7661) |  Loss2: (0.0000) | Acc: (72.00%) (23394/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (72.00%) (24334/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7636) |  Loss2: (0.0000) | Acc: (72.00%) (25273/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7621) |  Loss2: (0.0000) | Acc: (72.00%) (26210/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7612) |  Loss2: (0.0000) | Acc: (72.00%) (27167/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7622) |  Loss2: (0.0000) | Acc: (72.00%) (28078/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7607) |  Loss2: (0.0000) | Acc: (72.00%) (29036/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7595) |  Loss2: (0.0000) | Acc: (73.00%) (30002/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7566) |  Loss2: (0.0000) | Acc: (73.00%) (30986/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7557) |  Loss2: (0.0000) | Acc: (73.00%) (31939/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7538) |  Loss2: (0.0000) | Acc: (73.00%) (32923/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7529) |  Loss2: (0.0000) | Acc: (73.00%) (33878/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7534) |  Loss2: (0.0000) | Acc: (73.00%) (34801/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7522) |  Loss2: (0.0000) | Acc: (73.00%) (35779/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7509) |  Loss2: (0.0000) | Acc: (73.00%) (36716/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_020.pth.tar'
# TEST : Loss: (0.7662) | Acc: (72.00%) (7283/10000)
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5148, 0.4852], device='cuda:0')
percent tensor([0.5368, 0.4632], device='cuda:0')
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5881, 0.4119], device='cuda:0')
percent tensor([0.5927, 0.4073], device='cuda:0')
percent tensor([0.9569, 0.0431], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(172.4919, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.7136, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(779.7403, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1533.7775, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(508.5111, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2171.6072, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4309.6836, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1436.3192, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6089.4858, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12173.4199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4054.0930, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17201.2891, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7023) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7547) |  Loss2: (0.0000) | Acc: (74.00%) (1046/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7257) |  Loss2: (0.0000) | Acc: (75.00%) (2017/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7113) |  Loss2: (0.0000) | Acc: (75.00%) (2981/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7134) |  Loss2: (0.0000) | Acc: (74.00%) (3927/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7126) |  Loss2: (0.0000) | Acc: (74.00%) (4889/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7062) |  Loss2: (0.0000) | Acc: (75.00%) (5870/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (75.00%) (6821/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7145) |  Loss2: (0.0000) | Acc: (74.00%) (7760/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7146) |  Loss2: (0.0000) | Acc: (74.00%) (8715/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7105) |  Loss2: (0.0000) | Acc: (75.00%) (9712/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7048) |  Loss2: (0.0000) | Acc: (75.00%) (10705/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7120) |  Loss2: (0.0000) | Acc: (74.00%) (11610/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7115) |  Loss2: (0.0000) | Acc: (74.00%) (12567/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7143) |  Loss2: (0.0000) | Acc: (74.00%) (13507/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7123) |  Loss2: (0.0000) | Acc: (74.00%) (14480/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7090) |  Loss2: (0.0000) | Acc: (74.00%) (15449/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7066) |  Loss2: (0.0000) | Acc: (75.00%) (16431/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7075) |  Loss2: (0.0000) | Acc: (75.00%) (17386/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7097) |  Loss2: (0.0000) | Acc: (75.00%) (18341/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (74.00%) (19283/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7104) |  Loss2: (0.0000) | Acc: (74.00%) (20252/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7105) |  Loss2: (0.0000) | Acc: (75.00%) (21217/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (75.00%) (22176/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7094) |  Loss2: (0.0000) | Acc: (75.00%) (23138/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7088) |  Loss2: (0.0000) | Acc: (75.00%) (24102/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7079) |  Loss2: (0.0000) | Acc: (75.00%) (25093/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7072) |  Loss2: (0.0000) | Acc: (75.00%) (26072/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7075) |  Loss2: (0.0000) | Acc: (75.00%) (27025/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7076) |  Loss2: (0.0000) | Acc: (75.00%) (27990/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7061) |  Loss2: (0.0000) | Acc: (75.00%) (28977/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7054) |  Loss2: (0.0000) | Acc: (75.00%) (29946/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7073) |  Loss2: (0.0000) | Acc: (75.00%) (30891/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7067) |  Loss2: (0.0000) | Acc: (75.00%) (31855/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7055) |  Loss2: (0.0000) | Acc: (75.00%) (32833/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7045) |  Loss2: (0.0000) | Acc: (75.00%) (33808/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7037) |  Loss2: (0.0000) | Acc: (75.00%) (34778/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7040) |  Loss2: (0.0000) | Acc: (75.00%) (35735/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7037) |  Loss2: (0.0000) | Acc: (75.00%) (36712/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7037) |  Loss2: (0.0000) | Acc: (75.00%) (37639/50000)
# TEST : Loss: (0.9529) | Acc: (68.00%) (6860/10000)
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.4922, 0.5078], device='cuda:0')
percent tensor([0.5145, 0.4855], device='cuda:0')
percent tensor([0.5360, 0.4640], device='cuda:0')
percent tensor([0.5447, 0.4553], device='cuda:0')
percent tensor([0.5881, 0.4119], device='cuda:0')
percent tensor([0.5943, 0.4057], device='cuda:0')
percent tensor([0.9568, 0.0432], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6798) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (1075/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.6762) |  Loss2: (0.0000) | Acc: (76.00%) (2050/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6831) |  Loss2: (0.0000) | Acc: (75.00%) (3007/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6843) |  Loss2: (0.0000) | Acc: (75.00%) (3973/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6786) |  Loss2: (0.0000) | Acc: (75.00%) (4960/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.6874) |  Loss2: (0.0000) | Acc: (75.00%) (5884/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.6856) |  Loss2: (0.0000) | Acc: (75.00%) (6870/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6831) |  Loss2: (0.0000) | Acc: (75.00%) (7852/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6815) |  Loss2: (0.0000) | Acc: (75.00%) (8814/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6792) |  Loss2: (0.0000) | Acc: (75.00%) (9806/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6817) |  Loss2: (0.0000) | Acc: (75.00%) (10767/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6809) |  Loss2: (0.0000) | Acc: (75.00%) (11739/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.6772) |  Loss2: (0.0000) | Acc: (76.00%) (12746/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.6782) |  Loss2: (0.0000) | Acc: (75.00%) (13706/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.6788) |  Loss2: (0.0000) | Acc: (75.00%) (14676/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.6762) |  Loss2: (0.0000) | Acc: (76.00%) (15666/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.6754) |  Loss2: (0.0000) | Acc: (76.00%) (16657/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.6762) |  Loss2: (0.0000) | Acc: (76.00%) (17622/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.6743) |  Loss2: (0.0000) | Acc: (76.00%) (18607/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.6728) |  Loss2: (0.0000) | Acc: (76.00%) (19594/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.6717) |  Loss2: (0.0000) | Acc: (76.00%) (20589/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.6711) |  Loss2: (0.0000) | Acc: (76.00%) (21579/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.6716) |  Loss2: (0.0000) | Acc: (76.00%) (22567/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.6712) |  Loss2: (0.0000) | Acc: (76.00%) (23540/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6699) |  Loss2: (0.0000) | Acc: (76.00%) (24530/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6686) |  Loss2: (0.0000) | Acc: (76.00%) (25534/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6671) |  Loss2: (0.0000) | Acc: (76.00%) (26529/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6660) |  Loss2: (0.0000) | Acc: (76.00%) (27510/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.6674) |  Loss2: (0.0000) | Acc: (76.00%) (28483/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6677) |  Loss2: (0.0000) | Acc: (76.00%) (29461/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6676) |  Loss2: (0.0000) | Acc: (76.00%) (30455/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6663) |  Loss2: (0.0000) | Acc: (76.00%) (31447/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6665) |  Loss2: (0.0000) | Acc: (76.00%) (32441/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6655) |  Loss2: (0.0000) | Acc: (76.00%) (33449/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6644) |  Loss2: (0.0000) | Acc: (76.00%) (34445/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6633) |  Loss2: (0.0000) | Acc: (76.00%) (35440/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6627) |  Loss2: (0.0000) | Acc: (76.00%) (36420/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6625) |  Loss2: (0.0000) | Acc: (76.00%) (37416/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6620) |  Loss2: (0.0000) | Acc: (76.00%) (38378/50000)
# TEST : Loss: (0.7304) | Acc: (74.00%) (7470/10000)
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.4921, 0.5079], device='cuda:0')
percent tensor([0.5152, 0.4848], device='cuda:0')
percent tensor([0.5369, 0.4631], device='cuda:0')
percent tensor([0.5420, 0.4580], device='cuda:0')
percent tensor([0.5855, 0.4145], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.9564, 0.0436], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.6123) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6371) |  Loss2: (0.0000) | Acc: (76.00%) (1079/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (2103/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6139) |  Loss2: (0.0000) | Acc: (78.00%) (3118/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6280) |  Loss2: (0.0000) | Acc: (77.00%) (4093/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (5080/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (6061/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (7062/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6380) |  Loss2: (0.0000) | Acc: (77.00%) (8049/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6370) |  Loss2: (0.0000) | Acc: (77.00%) (9016/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (10017/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (77.00%) (11043/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6370) |  Loss2: (0.0000) | Acc: (77.00%) (12019/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6359) |  Loss2: (0.0000) | Acc: (77.00%) (13028/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (77.00%) (14025/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6386) |  Loss2: (0.0000) | Acc: (77.00%) (15000/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6388) |  Loss2: (0.0000) | Acc: (77.00%) (15994/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (77.00%) (16969/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (17984/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6365) |  Loss2: (0.0000) | Acc: (77.00%) (19005/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (20025/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6349) |  Loss2: (0.0000) | Acc: (77.00%) (21022/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6337) |  Loss2: (0.0000) | Acc: (77.00%) (22033/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6335) |  Loss2: (0.0000) | Acc: (77.00%) (23032/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (24051/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6307) |  Loss2: (0.0000) | Acc: (77.00%) (25058/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (26076/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (27095/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6287) |  Loss2: (0.0000) | Acc: (78.00%) (28086/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6305) |  Loss2: (0.0000) | Acc: (77.00%) (29050/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6298) |  Loss2: (0.0000) | Acc: (78.00%) (30068/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (31046/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6297) |  Loss2: (0.0000) | Acc: (78.00%) (32062/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (78.00%) (33062/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6293) |  Loss2: (0.0000) | Acc: (78.00%) (34060/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (35076/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6275) |  Loss2: (0.0000) | Acc: (78.00%) (36083/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (37093/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (38103/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6266) |  Loss2: (0.0000) | Acc: (78.00%) (39066/50000)
# TEST : Loss: (0.7074) | Acc: (76.00%) (7610/10000)
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.4920, 0.5080], device='cuda:0')
percent tensor([0.5147, 0.4853], device='cuda:0')
percent tensor([0.5359, 0.4641], device='cuda:0')
percent tensor([0.5432, 0.4568], device='cuda:0')
percent tensor([0.5854, 0.4146], device='cuda:0')
percent tensor([0.5944, 0.4056], device='cuda:0')
percent tensor([0.9544, 0.0456], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.5830) |  Loss2: (0.0000) | Acc: (80.00%) (1130/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.5891) |  Loss2: (0.0000) | Acc: (79.00%) (2133/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6041) |  Loss2: (0.0000) | Acc: (78.00%) (3132/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.5977) |  Loss2: (0.0000) | Acc: (79.00%) (4147/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (78.00%) (5123/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6162) |  Loss2: (0.0000) | Acc: (78.00%) (6139/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6120) |  Loss2: (0.0000) | Acc: (78.00%) (7150/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6110) |  Loss2: (0.0000) | Acc: (78.00%) (8165/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6068) |  Loss2: (0.0000) | Acc: (78.00%) (9182/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6029) |  Loss2: (0.0000) | Acc: (78.00%) (10206/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (79.00%) (11231/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.5997) |  Loss2: (0.0000) | Acc: (79.00%) (12236/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.5990) |  Loss2: (0.0000) | Acc: (79.00%) (13247/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.5987) |  Loss2: (0.0000) | Acc: (78.00%) (14246/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.5955) |  Loss2: (0.0000) | Acc: (79.00%) (15281/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.5937) |  Loss2: (0.0000) | Acc: (79.00%) (16314/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (17344/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.5950) |  Loss2: (0.0000) | Acc: (79.00%) (18348/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.5924) |  Loss2: (0.0000) | Acc: (79.00%) (19384/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.5942) |  Loss2: (0.0000) | Acc: (79.00%) (20390/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.5954) |  Loss2: (0.0000) | Acc: (79.00%) (21386/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.5955) |  Loss2: (0.0000) | Acc: (79.00%) (22392/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.5961) |  Loss2: (0.0000) | Acc: (79.00%) (23411/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (24436/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.5966) |  Loss2: (0.0000) | Acc: (79.00%) (25453/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.5975) |  Loss2: (0.0000) | Acc: (79.00%) (26449/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.5988) |  Loss2: (0.0000) | Acc: (79.00%) (27454/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.5973) |  Loss2: (0.0000) | Acc: (79.00%) (28486/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.5971) |  Loss2: (0.0000) | Acc: (79.00%) (29508/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.5982) |  Loss2: (0.0000) | Acc: (79.00%) (30510/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (79.00%) (31506/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.5973) |  Loss2: (0.0000) | Acc: (79.00%) (32530/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (33565/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.5966) |  Loss2: (0.0000) | Acc: (79.00%) (34586/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.5964) |  Loss2: (0.0000) | Acc: (79.00%) (35615/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.5960) |  Loss2: (0.0000) | Acc: (79.00%) (36654/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (37663/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.5955) |  Loss2: (0.0000) | Acc: (79.00%) (38667/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.5960) |  Loss2: (0.0000) | Acc: (79.00%) (39639/50000)
# TEST : Loss: (0.6462) | Acc: (77.00%) (7781/10000)
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.4923, 0.5077], device='cuda:0')
percent tensor([0.5150, 0.4850], device='cuda:0')
percent tensor([0.5372, 0.4628], device='cuda:0')
percent tensor([0.5414, 0.4586], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.5970, 0.4030], device='cuda:0')
percent tensor([0.9607, 0.0393], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6079) |  Loss2: (0.0000) | Acc: (78.00%) (1109/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (2115/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (78.00%) (3121/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.6222) |  Loss2: (0.0000) | Acc: (78.00%) (4101/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (5082/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.6323) |  Loss2: (0.0000) | Acc: (77.00%) (6052/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.6341) |  Loss2: (0.0000) | Acc: (77.00%) (7031/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.6459) |  Loss2: (0.0000) | Acc: (76.00%) (7981/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.6465) |  Loss2: (0.0000) | Acc: (77.00%) (8976/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.6511) |  Loss2: (0.0000) | Acc: (76.00%) (9947/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.6498) |  Loss2: (0.0000) | Acc: (77.00%) (10944/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.6508) |  Loss2: (0.0000) | Acc: (77.00%) (11929/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.6481) |  Loss2: (0.0000) | Acc: (77.00%) (12918/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.6470) |  Loss2: (0.0000) | Acc: (77.00%) (13905/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.6495) |  Loss2: (0.0000) | Acc: (76.00%) (14874/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.6479) |  Loss2: (0.0000) | Acc: (77.00%) (15871/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.6479) |  Loss2: (0.0000) | Acc: (77.00%) (16862/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.6462) |  Loss2: (0.0000) | Acc: (77.00%) (17872/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.6449) |  Loss2: (0.0000) | Acc: (77.00%) (18879/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (19904/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.6414) |  Loss2: (0.0000) | Acc: (77.00%) (20888/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.6415) |  Loss2: (0.0000) | Acc: (77.00%) (21889/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6398) |  Loss2: (0.0000) | Acc: (77.00%) (22900/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6391) |  Loss2: (0.0000) | Acc: (77.00%) (23903/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6384) |  Loss2: (0.0000) | Acc: (77.00%) (24898/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6394) |  Loss2: (0.0000) | Acc: (77.00%) (25873/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (26869/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6375) |  Loss2: (0.0000) | Acc: (77.00%) (27875/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6363) |  Loss2: (0.0000) | Acc: (77.00%) (28884/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (77.00%) (29910/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6339) |  Loss2: (0.0000) | Acc: (77.00%) (30931/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (77.00%) (31949/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6334) |  Loss2: (0.0000) | Acc: (77.00%) (32923/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (33946/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6307) |  Loss2: (0.0000) | Acc: (77.00%) (34950/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (35926/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (36934/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6295) |  Loss2: (0.0000) | Acc: (77.00%) (37953/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6288) |  Loss2: (0.0000) | Acc: (77.00%) (38932/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_025.pth.tar'
# TEST : Loss: (0.6348) | Acc: (78.00%) (7839/10000)
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.4951, 0.5049], device='cuda:0')
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5839, 0.4161], device='cuda:0')
percent tensor([0.9715, 0.0285], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.7214) |  Loss2: (0.0000) | Acc: (75.00%) (96/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.6020) |  Loss2: (0.0000) | Acc: (78.00%) (1103/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.5926) |  Loss2: (0.0000) | Acc: (78.00%) (2113/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (78.00%) (3103/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6117) |  Loss2: (0.0000) | Acc: (78.00%) (4094/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6070) |  Loss2: (0.0000) | Acc: (78.00%) (5119/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6065) |  Loss2: (0.0000) | Acc: (78.00%) (6129/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6042) |  Loss2: (0.0000) | Acc: (78.00%) (7152/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6075) |  Loss2: (0.0000) | Acc: (78.00%) (8150/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6077) |  Loss2: (0.0000) | Acc: (78.00%) (9170/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (78.00%) (10203/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6030) |  Loss2: (0.0000) | Acc: (78.00%) (11214/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6031) |  Loss2: (0.0000) | Acc: (78.00%) (12216/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6010) |  Loss2: (0.0000) | Acc: (78.00%) (13233/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.5974) |  Loss2: (0.0000) | Acc: (79.00%) (14266/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.5952) |  Loss2: (0.0000) | Acc: (79.00%) (15294/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.5943) |  Loss2: (0.0000) | Acc: (79.00%) (16321/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.5934) |  Loss2: (0.0000) | Acc: (79.00%) (17345/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.5925) |  Loss2: (0.0000) | Acc: (79.00%) (18370/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.5925) |  Loss2: (0.0000) | Acc: (79.00%) (19394/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.5942) |  Loss2: (0.0000) | Acc: (79.00%) (20400/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.5942) |  Loss2: (0.0000) | Acc: (79.00%) (21436/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.5933) |  Loss2: (0.0000) | Acc: (79.00%) (22440/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.5942) |  Loss2: (0.0000) | Acc: (79.00%) (23440/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.5960) |  Loss2: (0.0000) | Acc: (79.00%) (24425/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (25436/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.5944) |  Loss2: (0.0000) | Acc: (79.00%) (26466/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.5928) |  Loss2: (0.0000) | Acc: (79.00%) (27503/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.5927) |  Loss2: (0.0000) | Acc: (79.00%) (28506/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.5927) |  Loss2: (0.0000) | Acc: (79.00%) (29524/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.5926) |  Loss2: (0.0000) | Acc: (79.00%) (30563/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.5928) |  Loss2: (0.0000) | Acc: (79.00%) (31566/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (32595/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (33618/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.5906) |  Loss2: (0.0000) | Acc: (79.00%) (34638/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.5895) |  Loss2: (0.0000) | Acc: (79.00%) (35675/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.5904) |  Loss2: (0.0000) | Acc: (79.00%) (36669/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.5900) |  Loss2: (0.0000) | Acc: (79.00%) (37696/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.5908) |  Loss2: (0.0000) | Acc: (79.00%) (38703/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.5904) |  Loss2: (0.0000) | Acc: (79.00%) (39697/50000)
# TEST : Loss: (0.6077) | Acc: (79.00%) (7929/10000)
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.4968, 0.5032], device='cuda:0')
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.5574, 0.4426], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5892, 0.4108], device='cuda:0')
percent tensor([0.5839, 0.4161], device='cuda:0')
percent tensor([0.9812, 0.0188], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.5276) |  Loss2: (0.0000) | Acc: (81.00%) (1154/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.5496) |  Loss2: (0.0000) | Acc: (81.00%) (2180/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (81.00%) (3227/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.5669) |  Loss2: (0.0000) | Acc: (80.00%) (4227/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.5690) |  Loss2: (0.0000) | Acc: (80.00%) (5236/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.5671) |  Loss2: (0.0000) | Acc: (80.00%) (6288/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (80.00%) (7304/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (8343/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.5659) |  Loss2: (0.0000) | Acc: (80.00%) (9372/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.5677) |  Loss2: (0.0000) | Acc: (80.00%) (10410/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.5671) |  Loss2: (0.0000) | Acc: (80.00%) (11428/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (12450/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (13494/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.5702) |  Loss2: (0.0000) | Acc: (80.00%) (14515/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.5737) |  Loss2: (0.0000) | Acc: (80.00%) (15525/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.5753) |  Loss2: (0.0000) | Acc: (80.00%) (16544/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.5769) |  Loss2: (0.0000) | Acc: (80.00%) (17556/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.5781) |  Loss2: (0.0000) | Acc: (80.00%) (18553/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.5778) |  Loss2: (0.0000) | Acc: (80.00%) (19572/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.5783) |  Loss2: (0.0000) | Acc: (80.00%) (20583/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.5762) |  Loss2: (0.0000) | Acc: (80.00%) (21631/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.5766) |  Loss2: (0.0000) | Acc: (80.00%) (22650/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.5761) |  Loss2: (0.0000) | Acc: (80.00%) (23679/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.5762) |  Loss2: (0.0000) | Acc: (80.00%) (24693/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.5748) |  Loss2: (0.0000) | Acc: (80.00%) (25734/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.5736) |  Loss2: (0.0000) | Acc: (80.00%) (26770/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (80.00%) (27805/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (80.00%) (28836/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.5730) |  Loss2: (0.0000) | Acc: (80.00%) (29862/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.5738) |  Loss2: (0.0000) | Acc: (80.00%) (30864/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.5738) |  Loss2: (0.0000) | Acc: (80.00%) (31903/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.5720) |  Loss2: (0.0000) | Acc: (80.00%) (32964/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.5731) |  Loss2: (0.0000) | Acc: (80.00%) (33963/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.5738) |  Loss2: (0.0000) | Acc: (80.00%) (34961/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.5741) |  Loss2: (0.0000) | Acc: (80.00%) (35986/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.5740) |  Loss2: (0.0000) | Acc: (80.00%) (37004/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (80.00%) (38022/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.5741) |  Loss2: (0.0000) | Acc: (80.00%) (39051/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.5751) |  Loss2: (0.0000) | Acc: (80.00%) (40020/50000)
# TEST : Loss: (0.5981) | Acc: (79.00%) (7954/10000)
percent tensor([0.5170, 0.4830], device='cuda:0')
percent tensor([0.4974, 0.5026], device='cuda:0')
percent tensor([0.5225, 0.4775], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5995, 0.4005], device='cuda:0')
percent tensor([0.5891, 0.4109], device='cuda:0')
percent tensor([0.9868, 0.0132], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.6111) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.5523) |  Loss2: (0.0000) | Acc: (81.00%) (1143/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.5768) |  Loss2: (0.0000) | Acc: (80.00%) (2156/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.5649) |  Loss2: (0.0000) | Acc: (80.00%) (3179/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.5558) |  Loss2: (0.0000) | Acc: (80.00%) (4218/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (5242/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (79.00%) (6227/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (79.00%) (7266/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.5668) |  Loss2: (0.0000) | Acc: (80.00%) (8301/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.5608) |  Loss2: (0.0000) | Acc: (80.00%) (9361/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (10410/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (11405/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.5601) |  Loss2: (0.0000) | Acc: (80.00%) (12461/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (13478/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (14520/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (15580/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (80.00%) (16603/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.5596) |  Loss2: (0.0000) | Acc: (80.00%) (17609/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.5588) |  Loss2: (0.0000) | Acc: (80.00%) (18635/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.5590) |  Loss2: (0.0000) | Acc: (80.00%) (19663/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.5584) |  Loss2: (0.0000) | Acc: (80.00%) (20712/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (21741/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.5592) |  Loss2: (0.0000) | Acc: (80.00%) (22784/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.5601) |  Loss2: (0.0000) | Acc: (80.00%) (23802/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.5610) |  Loss2: (0.0000) | Acc: (80.00%) (24820/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (25855/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (26877/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (27894/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (28942/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (29966/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (30962/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.5632) |  Loss2: (0.0000) | Acc: (80.00%) (31994/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (33049/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (34083/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (35120/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (36123/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.5650) |  Loss2: (0.0000) | Acc: (80.00%) (37155/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (38197/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.5652) |  Loss2: (0.0000) | Acc: (80.00%) (39207/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.5649) |  Loss2: (0.0000) | Acc: (80.00%) (40206/50000)
# TEST : Loss: (0.5899) | Acc: (79.00%) (7981/10000)
percent tensor([0.5193, 0.4807], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5214, 0.4786], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.6086, 0.3914], device='cuda:0')
percent tensor([0.5938, 0.4062], device='cuda:0')
percent tensor([0.9898, 0.0102], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.5571) |  Loss2: (0.0000) | Acc: (80.00%) (1140/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.5378) |  Loss2: (0.0000) | Acc: (81.00%) (2185/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (80.00%) (3209/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (4244/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (5274/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (6282/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.5680) |  Loss2: (0.0000) | Acc: (80.00%) (7332/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.5682) |  Loss2: (0.0000) | Acc: (80.00%) (8358/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.5694) |  Loss2: (0.0000) | Acc: (80.00%) (9383/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.5689) |  Loss2: (0.0000) | Acc: (80.00%) (10412/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.5691) |  Loss2: (0.0000) | Acc: (80.00%) (11431/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.5665) |  Loss2: (0.0000) | Acc: (80.00%) (12479/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.5646) |  Loss2: (0.0000) | Acc: (80.00%) (13511/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (14538/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.5679) |  Loss2: (0.0000) | Acc: (80.00%) (15554/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.5691) |  Loss2: (0.0000) | Acc: (80.00%) (16570/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.5714) |  Loss2: (0.0000) | Acc: (80.00%) (17568/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.5702) |  Loss2: (0.0000) | Acc: (80.00%) (18598/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.5687) |  Loss2: (0.0000) | Acc: (80.00%) (19623/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (20643/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (21670/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (22680/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (23698/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.5703) |  Loss2: (0.0000) | Acc: (80.00%) (24720/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.5689) |  Loss2: (0.0000) | Acc: (80.00%) (25764/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.5682) |  Loss2: (0.0000) | Acc: (80.00%) (26798/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.5671) |  Loss2: (0.0000) | Acc: (80.00%) (27845/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.5666) |  Loss2: (0.0000) | Acc: (80.00%) (28862/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.5660) |  Loss2: (0.0000) | Acc: (80.00%) (29909/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (30935/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (31958/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (80.00%) (32980/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.5664) |  Loss2: (0.0000) | Acc: (80.00%) (34025/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.5661) |  Loss2: (0.0000) | Acc: (80.00%) (35053/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.5660) |  Loss2: (0.0000) | Acc: (80.00%) (36092/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.5661) |  Loss2: (0.0000) | Acc: (80.00%) (37125/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (38168/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.5649) |  Loss2: (0.0000) | Acc: (80.00%) (39203/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (80.00%) (40195/50000)
# TEST : Loss: (0.5876) | Acc: (80.00%) (8014/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5214, 0.4786], device='cuda:0')
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.5586, 0.4414], device='cuda:0')
percent tensor([0.6154, 0.3846], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.9920, 0.0080], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.5592) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.6099) |  Loss2: (0.0000) | Acc: (79.00%) (1115/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.5790) |  Loss2: (0.0000) | Acc: (80.00%) (2154/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.5845) |  Loss2: (0.0000) | Acc: (79.00%) (3166/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (4208/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.5797) |  Loss2: (0.0000) | Acc: (79.00%) (5214/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.5803) |  Loss2: (0.0000) | Acc: (79.00%) (6232/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.5826) |  Loss2: (0.0000) | Acc: (79.00%) (7238/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.5842) |  Loss2: (0.0000) | Acc: (79.00%) (8251/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.5817) |  Loss2: (0.0000) | Acc: (79.00%) (9278/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.5804) |  Loss2: (0.0000) | Acc: (79.00%) (10309/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.5807) |  Loss2: (0.0000) | Acc: (79.00%) (11327/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.5815) |  Loss2: (0.0000) | Acc: (79.00%) (12328/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.5805) |  Loss2: (0.0000) | Acc: (79.00%) (13368/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.5795) |  Loss2: (0.0000) | Acc: (79.00%) (14400/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.5809) |  Loss2: (0.0000) | Acc: (79.00%) (15421/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.5778) |  Loss2: (0.0000) | Acc: (79.00%) (16453/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.5770) |  Loss2: (0.0000) | Acc: (79.00%) (17480/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.5768) |  Loss2: (0.0000) | Acc: (79.00%) (18509/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.5756) |  Loss2: (0.0000) | Acc: (79.00%) (19529/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.5750) |  Loss2: (0.0000) | Acc: (79.00%) (20570/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (79.00%) (21581/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.5736) |  Loss2: (0.0000) | Acc: (79.00%) (22609/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.5749) |  Loss2: (0.0000) | Acc: (79.00%) (23614/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.5748) |  Loss2: (0.0000) | Acc: (79.00%) (24638/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.5748) |  Loss2: (0.0000) | Acc: (79.00%) (25675/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.5742) |  Loss2: (0.0000) | Acc: (79.00%) (26700/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.5724) |  Loss2: (0.0000) | Acc: (79.00%) (27743/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (28781/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.5737) |  Loss2: (0.0000) | Acc: (79.00%) (29785/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.5739) |  Loss2: (0.0000) | Acc: (79.00%) (30809/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.5736) |  Loss2: (0.0000) | Acc: (79.00%) (31841/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.5732) |  Loss2: (0.0000) | Acc: (80.00%) (32883/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.5727) |  Loss2: (0.0000) | Acc: (80.00%) (33922/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.5728) |  Loss2: (0.0000) | Acc: (80.00%) (34940/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.5716) |  Loss2: (0.0000) | Acc: (80.00%) (36001/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.5719) |  Loss2: (0.0000) | Acc: (80.00%) (37017/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.5714) |  Loss2: (0.0000) | Acc: (80.00%) (38047/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.5713) |  Loss2: (0.0000) | Acc: (80.00%) (39065/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.5712) |  Loss2: (0.0000) | Acc: (80.00%) (40057/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_030.pth.tar'
# TEST : Loss: (0.6299) | Acc: (78.00%) (7851/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5215, 0.4785], device='cuda:0')
percent tensor([0.5678, 0.4322], device='cuda:0')
percent tensor([0.5576, 0.4424], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6020, 0.3980], device='cuda:0')
percent tensor([0.9919, 0.0081], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.6707, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(789.9906, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(785.9171, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1533.5609, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.9686, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2180.3201, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4302.1348, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1431.2389, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6089.6362, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12129.0107, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4038.2876, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17125.9746, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.5666) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5618) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5482) |  Loss2: (0.0000) | Acc: (80.00%) (2166/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (81.00%) (3227/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (4266/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5330) |  Loss2: (0.0000) | Acc: (81.00%) (5315/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (6360/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5276) |  Loss2: (0.0000) | Acc: (81.00%) (7413/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5237) |  Loss2: (0.0000) | Acc: (81.00%) (8478/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (81.00%) (9528/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (10538/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5302) |  Loss2: (0.0000) | Acc: (81.00%) (11581/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (12603/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5340) |  Loss2: (0.0000) | Acc: (81.00%) (13640/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (14661/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5388) |  Loss2: (0.0000) | Acc: (81.00%) (15706/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (81.00%) (16743/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (17769/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5415) |  Loss2: (0.0000) | Acc: (81.00%) (18781/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (19822/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5436) |  Loss2: (0.0000) | Acc: (81.00%) (20842/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5444) |  Loss2: (0.0000) | Acc: (81.00%) (21878/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (22922/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (81.00%) (23972/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5426) |  Loss2: (0.0000) | Acc: (81.00%) (25021/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (26046/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (81.00%) (27075/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (28115/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (29155/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (30184/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (80.00%) (31206/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5470) |  Loss2: (0.0000) | Acc: (80.00%) (32244/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5474) |  Loss2: (0.0000) | Acc: (80.00%) (33272/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5477) |  Loss2: (0.0000) | Acc: (80.00%) (34304/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5470) |  Loss2: (0.0000) | Acc: (80.00%) (35351/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (80.00%) (36387/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (37440/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (38484/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (39524/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5457) |  Loss2: (0.0000) | Acc: (81.00%) (40535/50000)
# TEST : Loss: (0.6202) | Acc: (78.00%) (7887/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5218, 0.4782], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.6149, 0.3851], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.9926, 0.0074], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.5504) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5140) |  Loss2: (0.0000) | Acc: (82.00%) (1161/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5135) |  Loss2: (0.0000) | Acc: (82.00%) (2210/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (82.00%) (3272/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (4340/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (5403/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (83.00%) (6498/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (83.00%) (7551/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (8591/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5127) |  Loss2: (0.0000) | Acc: (82.00%) (9634/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5154) |  Loss2: (0.0000) | Acc: (82.00%) (10671/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (11706/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (82.00%) (12761/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (13820/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (14866/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5167) |  Loss2: (0.0000) | Acc: (82.00%) (15893/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (16929/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (17967/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (82.00%) (19002/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (20043/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (82.00%) (21098/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (22137/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (23205/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (82.00%) (24263/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (25323/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (26372/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (27430/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (28485/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (29533/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (30586/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (31627/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (82.00%) (32659/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (33704/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (82.00%) (34774/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (82.00%) (35819/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (36874/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (82.00%) (37904/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (38972/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (40039/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (41044/50000)
# TEST : Loss: (0.5802) | Acc: (79.00%) (7976/10000)
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5227, 0.4773], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.6081, 0.3919], device='cuda:0')
percent tensor([0.9928, 0.0072], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.5729) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (82.00%) (1166/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (82.00%) (2214/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5057) |  Loss2: (0.0000) | Acc: (82.00%) (3277/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.4966) |  Loss2: (0.0000) | Acc: (82.00%) (4353/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (5398/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (6468/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.4999) |  Loss2: (0.0000) | Acc: (82.00%) (7519/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5001) |  Loss2: (0.0000) | Acc: (82.00%) (8575/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (9634/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (10711/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.4997) |  Loss2: (0.0000) | Acc: (82.00%) (11758/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (12818/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.4996) |  Loss2: (0.0000) | Acc: (82.00%) (13900/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (82.00%) (14941/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (16000/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (17062/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5044) |  Loss2: (0.0000) | Acc: (82.00%) (18083/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (19157/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (20201/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5057) |  Loss2: (0.0000) | Acc: (82.00%) (21243/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (22291/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (23335/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (24383/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (25464/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (26551/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (27633/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5036) |  Loss2: (0.0000) | Acc: (82.00%) (28689/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (29728/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (30769/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (82.00%) (31835/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (32910/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (33975/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (35024/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5054) |  Loss2: (0.0000) | Acc: (82.00%) (36081/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (37163/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (38224/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (39314/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (40385/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (41422/50000)
# TEST : Loss: (0.5967) | Acc: (80.00%) (8017/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.6126, 0.3874], device='cuda:0')
percent tensor([0.6027, 0.3973], device='cuda:0')
percent tensor([0.9916, 0.0084], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (1162/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.4820) |  Loss2: (0.0000) | Acc: (83.00%) (2238/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.4676) |  Loss2: (0.0000) | Acc: (83.00%) (3322/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (4380/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.4704) |  Loss2: (0.0000) | Acc: (83.00%) (5459/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.4763) |  Loss2: (0.0000) | Acc: (83.00%) (6508/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (7578/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (8642/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (9703/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.4782) |  Loss2: (0.0000) | Acc: (83.00%) (10772/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.4774) |  Loss2: (0.0000) | Acc: (83.00%) (11845/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.4764) |  Loss2: (0.0000) | Acc: (83.00%) (12922/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (13983/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.4776) |  Loss2: (0.0000) | Acc: (83.00%) (15046/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (16105/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (17155/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.4800) |  Loss2: (0.0000) | Acc: (83.00%) (18219/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.4796) |  Loss2: (0.0000) | Acc: (83.00%) (19287/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.4782) |  Loss2: (0.0000) | Acc: (83.00%) (20365/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (21432/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (22515/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (23593/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (24651/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.4793) |  Loss2: (0.0000) | Acc: (83.00%) (25710/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.4785) |  Loss2: (0.0000) | Acc: (83.00%) (26783/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.4804) |  Loss2: (0.0000) | Acc: (83.00%) (27834/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.4830) |  Loss2: (0.0000) | Acc: (83.00%) (28878/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (29951/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.4838) |  Loss2: (0.0000) | Acc: (83.00%) (31009/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (32079/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.4836) |  Loss2: (0.0000) | Acc: (83.00%) (33144/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (34209/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (35280/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.4834) |  Loss2: (0.0000) | Acc: (83.00%) (36355/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (37433/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (38484/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (39553/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.4831) |  Loss2: (0.0000) | Acc: (83.00%) (40605/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.4831) |  Loss2: (0.0000) | Acc: (83.00%) (41625/50000)
# TEST : Loss: (0.5623) | Acc: (80.00%) (8097/10000)
percent tensor([0.5204, 0.4796], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.6108, 0.3892], device='cuda:0')
percent tensor([0.5988, 0.4012], device='cuda:0')
percent tensor([0.9906, 0.0094], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (2219/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5301) |  Loss2: (0.0000) | Acc: (81.00%) (3242/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.5443) |  Loss2: (0.0000) | Acc: (81.00%) (4261/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.5570) |  Loss2: (0.0000) | Acc: (80.00%) (5264/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.5597) |  Loss2: (0.0000) | Acc: (80.00%) (6279/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (7309/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.5607) |  Loss2: (0.0000) | Acc: (80.00%) (8330/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (9359/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.5589) |  Loss2: (0.0000) | Acc: (80.00%) (10391/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.5567) |  Loss2: (0.0000) | Acc: (80.00%) (11422/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.5562) |  Loss2: (0.0000) | Acc: (80.00%) (12469/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.5559) |  Loss2: (0.0000) | Acc: (80.00%) (13490/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.5519) |  Loss2: (0.0000) | Acc: (80.00%) (14549/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (15554/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.5525) |  Loss2: (0.0000) | Acc: (80.00%) (16596/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (17639/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.5509) |  Loss2: (0.0000) | Acc: (80.00%) (18676/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.5531) |  Loss2: (0.0000) | Acc: (80.00%) (19671/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.5522) |  Loss2: (0.0000) | Acc: (80.00%) (20729/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.5491) |  Loss2: (0.0000) | Acc: (80.00%) (21790/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (80.00%) (22849/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.5458) |  Loss2: (0.0000) | Acc: (80.00%) (23892/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.5463) |  Loss2: (0.0000) | Acc: (80.00%) (24915/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (80.00%) (25971/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (80.00%) (27019/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.5443) |  Loss2: (0.0000) | Acc: (80.00%) (28062/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (80.00%) (29117/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.5419) |  Loss2: (0.0000) | Acc: (81.00%) (30173/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (31200/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (32224/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (80.00%) (33241/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (34282/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.5426) |  Loss2: (0.0000) | Acc: (80.00%) (35308/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.5429) |  Loss2: (0.0000) | Acc: (80.00%) (36339/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.5426) |  Loss2: (0.0000) | Acc: (80.00%) (37376/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.5412) |  Loss2: (0.0000) | Acc: (80.00%) (38427/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.5404) |  Loss2: (0.0000) | Acc: (80.00%) (39493/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.5405) |  Loss2: (0.0000) | Acc: (80.00%) (40478/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_035.pth.tar'
# TEST : Loss: (0.5824) | Acc: (80.00%) (8046/10000)
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5428, 0.4572], device='cuda:0')
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5874, 0.4126], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.9930, 0.0070], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5138) |  Loss2: (0.0000) | Acc: (82.00%) (1157/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5327) |  Loss2: (0.0000) | Acc: (81.00%) (2180/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (3232/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (81.00%) (4290/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (5329/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (81.00%) (6385/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5107) |  Loss2: (0.0000) | Acc: (82.00%) (7468/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5108) |  Loss2: (0.0000) | Acc: (82.00%) (8516/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5154) |  Loss2: (0.0000) | Acc: (81.00%) (9550/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (81.00%) (10592/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (81.00%) (11627/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (81.00%) (12683/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5131) |  Loss2: (0.0000) | Acc: (81.00%) (13743/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5131) |  Loss2: (0.0000) | Acc: (81.00%) (14785/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (81.00%) (15812/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (81.00%) (16867/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5140) |  Loss2: (0.0000) | Acc: (81.00%) (17916/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (81.00%) (18972/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (20049/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (21105/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (22177/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5100) |  Loss2: (0.0000) | Acc: (82.00%) (23237/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (24314/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (25382/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (26450/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (27527/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (28574/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5055) |  Loss2: (0.0000) | Acc: (82.00%) (29638/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (30690/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (31724/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (32781/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (33839/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5067) |  Loss2: (0.0000) | Acc: (82.00%) (34881/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (35918/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (36986/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (38036/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (39107/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (40154/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (41183/50000)
# TEST : Loss: (0.5514) | Acc: (81.00%) (8115/10000)
percent tensor([0.5245, 0.4755], device='cuda:0')
percent tensor([0.4992, 0.5008], device='cuda:0')
percent tensor([0.4999, 0.5001], device='cuda:0')
percent tensor([0.5408, 0.4592], device='cuda:0')
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.9939, 0.0061], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (81.00%) (1144/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (2189/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5124) |  Loss2: (0.0000) | Acc: (81.00%) (3249/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (4313/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (5360/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5048) |  Loss2: (0.0000) | Acc: (82.00%) (6421/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (7503/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (82.00%) (8559/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (9617/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (10667/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.4971) |  Loss2: (0.0000) | Acc: (82.00%) (11757/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.4930) |  Loss2: (0.0000) | Acc: (82.00%) (12827/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (82.00%) (13901/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (14981/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.4917) |  Loss2: (0.0000) | Acc: (82.00%) (16041/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (82.00%) (17096/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (82.00%) (18165/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (19243/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (83.00%) (20321/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (21389/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (22455/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (23493/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (83.00%) (24558/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.4906) |  Loss2: (0.0000) | Acc: (83.00%) (25633/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (26683/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (83.00%) (27738/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (83.00%) (28792/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (82.00%) (29850/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (82.00%) (30906/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.4904) |  Loss2: (0.0000) | Acc: (82.00%) (31972/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (33039/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (82.00%) (34103/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.4910) |  Loss2: (0.0000) | Acc: (82.00%) (35163/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.4903) |  Loss2: (0.0000) | Acc: (83.00%) (36249/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.4893) |  Loss2: (0.0000) | Acc: (83.00%) (37322/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (38373/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.4884) |  Loss2: (0.0000) | Acc: (83.00%) (39445/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.4878) |  Loss2: (0.0000) | Acc: (83.00%) (40511/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.4879) |  Loss2: (0.0000) | Acc: (83.00%) (41541/50000)
# TEST : Loss: (0.5444) | Acc: (81.00%) (8143/10000)
percent tensor([0.5216, 0.4784], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5440, 0.4560], device='cuda:0')
percent tensor([0.5257, 0.4743], device='cuda:0')
percent tensor([0.5962, 0.4038], device='cuda:0')
percent tensor([0.5888, 0.4112], device='cuda:0')
percent tensor([0.9948, 0.0052], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4962) |  Loss2: (0.0000) | Acc: (82.00%) (1160/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (82.00%) (2229/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.4832) |  Loss2: (0.0000) | Acc: (82.00%) (3289/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.4817) |  Loss2: (0.0000) | Acc: (82.00%) (4347/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.4803) |  Loss2: (0.0000) | Acc: (82.00%) (5416/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.4772) |  Loss2: (0.0000) | Acc: (82.00%) (6477/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (82.00%) (7521/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (82.00%) (8568/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.4872) |  Loss2: (0.0000) | Acc: (82.00%) (9645/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (82.00%) (10715/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.4814) |  Loss2: (0.0000) | Acc: (83.00%) (11805/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.4801) |  Loss2: (0.0000) | Acc: (83.00%) (12869/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.4836) |  Loss2: (0.0000) | Acc: (83.00%) (13918/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.4850) |  Loss2: (0.0000) | Acc: (82.00%) (14976/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (82.00%) (16037/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (82.00%) (17101/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (82.00%) (18161/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (83.00%) (19238/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.4852) |  Loss2: (0.0000) | Acc: (83.00%) (20302/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (21382/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (22438/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (23506/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.4831) |  Loss2: (0.0000) | Acc: (83.00%) (24586/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (25655/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.4818) |  Loss2: (0.0000) | Acc: (83.00%) (26732/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.4825) |  Loss2: (0.0000) | Acc: (83.00%) (27783/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (28836/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.4855) |  Loss2: (0.0000) | Acc: (83.00%) (29874/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (83.00%) (30949/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (83.00%) (32022/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (33070/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (83.00%) (34132/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (35204/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.4838) |  Loss2: (0.0000) | Acc: (83.00%) (36281/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (37353/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.4834) |  Loss2: (0.0000) | Acc: (83.00%) (38424/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (83.00%) (39487/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (83.00%) (40559/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.4834) |  Loss2: (0.0000) | Acc: (83.00%) (41582/50000)
# TEST : Loss: (0.5350) | Acc: (81.00%) (8175/10000)
percent tensor([0.5203, 0.4797], device='cuda:0')
percent tensor([0.4990, 0.5010], device='cuda:0')
percent tensor([0.4959, 0.5041], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.5277, 0.4723], device='cuda:0')
percent tensor([0.6010, 0.3990], device='cuda:0')
percent tensor([0.5952, 0.4048], device='cuda:0')
percent tensor([0.9955, 0.0045], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.4572) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.4819) |  Loss2: (0.0000) | Acc: (83.00%) (2249/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (83.00%) (3306/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.4898) |  Loss2: (0.0000) | Acc: (83.00%) (4361/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (83.00%) (5436/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.4901) |  Loss2: (0.0000) | Acc: (83.00%) (6500/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (83.00%) (7571/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.4866) |  Loss2: (0.0000) | Acc: (83.00%) (8656/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.4855) |  Loss2: (0.0000) | Acc: (83.00%) (9726/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (83.00%) (10762/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (11832/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (83.00%) (12903/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.4879) |  Loss2: (0.0000) | Acc: (83.00%) (13969/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.4896) |  Loss2: (0.0000) | Acc: (83.00%) (15038/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (16094/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (17181/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (18255/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (19330/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (20410/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (21497/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.4823) |  Loss2: (0.0000) | Acc: (83.00%) (22558/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.4802) |  Loss2: (0.0000) | Acc: (83.00%) (23648/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (24723/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.4785) |  Loss2: (0.0000) | Acc: (83.00%) (25784/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (26860/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.4781) |  Loss2: (0.0000) | Acc: (83.00%) (27935/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.4780) |  Loss2: (0.0000) | Acc: (83.00%) (29008/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (30070/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (31145/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (32223/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (33310/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.4754) |  Loss2: (0.0000) | Acc: (83.00%) (34378/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (35439/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (36505/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (37578/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (38648/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (39713/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (83.00%) (40811/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (83.00%) (41851/50000)
# TEST : Loss: (0.5306) | Acc: (81.00%) (8187/10000)
percent tensor([0.5195, 0.4805], device='cuda:0')
percent tensor([0.4997, 0.5003], device='cuda:0')
percent tensor([0.4955, 0.5045], device='cuda:0')
percent tensor([0.5457, 0.4543], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.9961, 0.0039], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.5749) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.4557) |  Loss2: (0.0000) | Acc: (84.00%) (2259/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (3309/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.4694) |  Loss2: (0.0000) | Acc: (83.00%) (4388/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (5463/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.4659) |  Loss2: (0.0000) | Acc: (83.00%) (6547/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4747) |  Loss2: (0.0000) | Acc: (83.00%) (7599/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4797) |  Loss2: (0.0000) | Acc: (83.00%) (8669/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4817) |  Loss2: (0.0000) | Acc: (83.00%) (9729/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4842) |  Loss2: (0.0000) | Acc: (83.00%) (10780/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4820) |  Loss2: (0.0000) | Acc: (83.00%) (11855/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (12923/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4824) |  Loss2: (0.0000) | Acc: (83.00%) (13976/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4813) |  Loss2: (0.0000) | Acc: (83.00%) (15049/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4791) |  Loss2: (0.0000) | Acc: (83.00%) (16133/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4797) |  Loss2: (0.0000) | Acc: (83.00%) (17197/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (18280/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (19354/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (20420/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4792) |  Loss2: (0.0000) | Acc: (83.00%) (21476/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (22552/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (23627/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (24684/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (25758/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4781) |  Loss2: (0.0000) | Acc: (83.00%) (26839/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (27895/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4769) |  Loss2: (0.0000) | Acc: (83.00%) (28985/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (30059/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (31139/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4775) |  Loss2: (0.0000) | Acc: (83.00%) (32193/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (33243/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4790) |  Loss2: (0.0000) | Acc: (83.00%) (34308/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4790) |  Loss2: (0.0000) | Acc: (83.00%) (35384/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (36447/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (37520/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (38579/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (39649/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (83.00%) (40708/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4776) |  Loss2: (0.0000) | Acc: (83.00%) (41742/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_040.pth.tar'
# TEST : Loss: (0.6051) | Acc: (80.00%) (8036/10000)
percent tensor([0.5190, 0.4810], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.6049, 0.3951], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.4901, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(794.9619, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(791.4003, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1532.2493, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(505.4293, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2188.9355, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4296.0298, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1426.1865, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6095.5586, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12087.2910, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4022.6860, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17054.2148, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (1191/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (2273/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4526) |  Loss2: (0.0000) | Acc: (84.00%) (3351/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (4413/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (5502/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (6598/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (7656/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (8711/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (9800/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4606) |  Loss2: (0.0000) | Acc: (83.00%) (10856/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4600) |  Loss2: (0.0000) | Acc: (84.00%) (11943/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4609) |  Loss2: (0.0000) | Acc: (84.00%) (13025/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (14102/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (84.00%) (15177/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4636) |  Loss2: (0.0000) | Acc: (84.00%) (16236/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4640) |  Loss2: (0.0000) | Acc: (83.00%) (17296/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4638) |  Loss2: (0.0000) | Acc: (83.00%) (18363/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4636) |  Loss2: (0.0000) | Acc: (83.00%) (19418/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4633) |  Loss2: (0.0000) | Acc: (83.00%) (20494/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (21557/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (22636/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (83.00%) (23726/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4643) |  Loss2: (0.0000) | Acc: (83.00%) (24809/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4650) |  Loss2: (0.0000) | Acc: (83.00%) (25872/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (83.00%) (26956/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4638) |  Loss2: (0.0000) | Acc: (83.00%) (28051/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (83.00%) (29128/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4634) |  Loss2: (0.0000) | Acc: (83.00%) (30203/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (84.00%) (31293/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4656) |  Loss2: (0.0000) | Acc: (83.00%) (32335/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (33411/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4649) |  Loss2: (0.0000) | Acc: (83.00%) (34503/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4637) |  Loss2: (0.0000) | Acc: (83.00%) (35586/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (83.00%) (36663/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (84.00%) (37743/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4618) |  Loss2: (0.0000) | Acc: (84.00%) (38831/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4605) |  Loss2: (0.0000) | Acc: (84.00%) (39924/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4606) |  Loss2: (0.0000) | Acc: (84.00%) (41000/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4605) |  Loss2: (0.0000) | Acc: (84.00%) (42026/50000)
# TEST : Loss: (0.5408) | Acc: (82.00%) (8210/10000)
percent tensor([0.5185, 0.4815], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.4958, 0.5042], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.6050, 0.3950], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.9965, 0.0035], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5885) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (84.00%) (1186/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4615) |  Loss2: (0.0000) | Acc: (85.00%) (2289/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (3349/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (4448/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (85.00%) (5553/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4540) |  Loss2: (0.0000) | Acc: (84.00%) (6618/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4548) |  Loss2: (0.0000) | Acc: (84.00%) (7681/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (8779/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (84.00%) (9878/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4499) |  Loss2: (0.0000) | Acc: (84.00%) (10948/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (12014/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4515) |  Loss2: (0.0000) | Acc: (84.00%) (13101/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4500) |  Loss2: (0.0000) | Acc: (84.00%) (14187/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4460) |  Loss2: (0.0000) | Acc: (84.00%) (15286/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (16383/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (17467/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (18557/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (19640/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (20720/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (21793/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (22883/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (23970/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (25059/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (26162/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (27251/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (28341/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4418) |  Loss2: (0.0000) | Acc: (84.00%) (29411/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4418) |  Loss2: (0.0000) | Acc: (84.00%) (30503/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (31593/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4418) |  Loss2: (0.0000) | Acc: (84.00%) (32685/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (33791/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (34887/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (35966/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (37051/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (38115/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (39194/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (40264/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (41347/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (42397/50000)
# TEST : Loss: (0.5145) | Acc: (82.00%) (8282/10000)
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.4997, 0.5003], device='cuda:0')
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.5464, 0.4536], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.9962, 0.0038], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4865) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (1208/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (85.00%) (2293/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (4501/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (85.00%) (5598/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (6707/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (7788/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (8876/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (9962/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (11054/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (12169/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (13271/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (14352/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (15430/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (16531/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4204) |  Loss2: (0.0000) | Acc: (85.00%) (17617/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (18705/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (19807/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4203) |  Loss2: (0.0000) | Acc: (85.00%) (20897/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (22003/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (23091/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4210) |  Loss2: (0.0000) | Acc: (85.00%) (24161/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4214) |  Loss2: (0.0000) | Acc: (85.00%) (25244/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4224) |  Loss2: (0.0000) | Acc: (85.00%) (26330/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (27416/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (28499/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4245) |  Loss2: (0.0000) | Acc: (85.00%) (29596/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (30681/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (31790/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (32895/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4238) |  Loss2: (0.0000) | Acc: (85.00%) (33982/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (35088/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4235) |  Loss2: (0.0000) | Acc: (85.00%) (36176/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (37281/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (38353/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4235) |  Loss2: (0.0000) | Acc: (85.00%) (39431/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4238) |  Loss2: (0.0000) | Acc: (85.00%) (40518/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4240) |  Loss2: (0.0000) | Acc: (85.00%) (41611/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (42665/50000)
# TEST : Loss: (0.5531) | Acc: (81.00%) (8130/10000)
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.5969, 0.4031], device='cuda:0')
percent tensor([0.9961, 0.0039], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (87.00%) (1232/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.3992) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (3409/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (4492/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (5585/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (6700/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (7798/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (86.00%) (8922/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (10017/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (86.00%) (11120/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (86.00%) (12222/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (13317/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (85.00%) (14411/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (86.00%) (15535/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (86.00%) (16645/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (17711/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (85.00%) (18801/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (19881/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (20957/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (22066/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (23151/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (24240/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (25335/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (26430/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (27521/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (28619/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (29737/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (30835/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (31927/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (33016/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (34114/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (35192/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (36293/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (37385/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (38471/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (85.00%) (39561/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (40675/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (41764/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (42828/50000)
# TEST : Loss: (0.5877) | Acc: (80.00%) (8070/10000)
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.4998, 0.5002], device='cuda:0')
percent tensor([0.4956, 0.5044], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6038, 0.3962], device='cuda:0')
percent tensor([0.6013, 0.3987], device='cuda:0')
percent tensor([0.9963, 0.0037], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4592) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (85.00%) (1198/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (2254/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (82.00%) (3283/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4906) |  Loss2: (0.0000) | Acc: (82.00%) (4350/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (82.00%) (5411/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (6456/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (7500/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (8545/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (9603/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (10647/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (11685/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5065) |  Loss2: (0.0000) | Acc: (82.00%) (12750/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (13799/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5035) |  Loss2: (0.0000) | Acc: (82.00%) (14884/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (15955/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (17012/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (18061/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (19121/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.5010) |  Loss2: (0.0000) | Acc: (82.00%) (20193/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (21279/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4959) |  Loss2: (0.0000) | Acc: (82.00%) (22352/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4948) |  Loss2: (0.0000) | Acc: (82.00%) (23421/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (82.00%) (24497/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4906) |  Loss2: (0.0000) | Acc: (82.00%) (25595/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (82.00%) (26652/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (82.00%) (27717/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (82.00%) (28789/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4906) |  Loss2: (0.0000) | Acc: (83.00%) (29865/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (30942/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4877) |  Loss2: (0.0000) | Acc: (83.00%) (32029/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4866) |  Loss2: (0.0000) | Acc: (83.00%) (33100/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4855) |  Loss2: (0.0000) | Acc: (83.00%) (34184/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (35275/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4836) |  Loss2: (0.0000) | Acc: (83.00%) (36356/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4831) |  Loss2: (0.0000) | Acc: (83.00%) (37431/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (38496/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (83.00%) (39578/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4817) |  Loss2: (0.0000) | Acc: (83.00%) (40642/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4798) |  Loss2: (0.0000) | Acc: (83.00%) (41705/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_045.pth.tar'
# TEST : Loss: (0.5243) | Acc: (82.00%) (8200/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.4979, 0.5021], device='cuda:0')
percent tensor([0.5547, 0.4453], device='cuda:0')
percent tensor([0.5435, 0.4565], device='cuda:0')
percent tensor([0.6170, 0.3830], device='cuda:0')
percent tensor([0.5713, 0.4287], device='cuda:0')
percent tensor([0.9960, 0.0040], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4693) |  Loss2: (0.0000) | Acc: (83.00%) (1174/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (2233/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4522) |  Loss2: (0.0000) | Acc: (83.00%) (3314/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4593) |  Loss2: (0.0000) | Acc: (83.00%) (4373/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4508) |  Loss2: (0.0000) | Acc: (83.00%) (5473/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (83.00%) (6546/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4519) |  Loss2: (0.0000) | Acc: (83.00%) (7612/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (83.00%) (8700/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4504) |  Loss2: (0.0000) | Acc: (83.00%) (9778/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (10871/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4480) |  Loss2: (0.0000) | Acc: (84.00%) (11954/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4462) |  Loss2: (0.0000) | Acc: (84.00%) (13034/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (14141/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (15234/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (16310/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (17396/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (18501/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (19586/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (20673/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (21771/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (22868/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (23947/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (25046/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4381) |  Loss2: (0.0000) | Acc: (84.00%) (26144/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (27230/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (84.00%) (28305/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (29382/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (30459/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (84.00%) (31547/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (32627/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (33728/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (34812/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (35905/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (84.00%) (36970/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (38078/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (39180/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4341) |  Loss2: (0.0000) | Acc: (84.00%) (40298/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (84.00%) (41381/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (84.00%) (42428/50000)
# TEST : Loss: (0.4979) | Acc: (82.00%) (8270/10000)
percent tensor([0.5154, 0.4846], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.4982, 0.5018], device='cuda:0')
percent tensor([0.5585, 0.4415], device='cuda:0')
percent tensor([0.5489, 0.4511], device='cuda:0')
percent tensor([0.6267, 0.3733], device='cuda:0')
percent tensor([0.5798, 0.4202], device='cuda:0')
percent tensor([0.9964, 0.0036], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.3521) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (1214/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (2296/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4434) |  Loss2: (0.0000) | Acc: (84.00%) (3349/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (4423/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4319) |  Loss2: (0.0000) | Acc: (84.00%) (5517/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (84.00%) (6608/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (84.00%) (7705/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (84.00%) (8798/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (84.00%) (9881/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (84.00%) (10968/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (84.00%) (12052/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (13134/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (84.00%) (14229/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (84.00%) (15321/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (84.00%) (16409/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (84.00%) (17510/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (84.00%) (18580/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4328) |  Loss2: (0.0000) | Acc: (84.00%) (19668/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (84.00%) (20763/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (84.00%) (21847/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4322) |  Loss2: (0.0000) | Acc: (84.00%) (22938/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (84.00%) (24030/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (84.00%) (25127/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4322) |  Loss2: (0.0000) | Acc: (84.00%) (26195/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (84.00%) (27295/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (28407/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (29506/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (30603/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (31710/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4281) |  Loss2: (0.0000) | Acc: (85.00%) (32809/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4270) |  Loss2: (0.0000) | Acc: (85.00%) (33907/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4271) |  Loss2: (0.0000) | Acc: (85.00%) (34989/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4269) |  Loss2: (0.0000) | Acc: (85.00%) (36086/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4269) |  Loss2: (0.0000) | Acc: (85.00%) (37182/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (38270/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (39363/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4253) |  Loss2: (0.0000) | Acc: (85.00%) (40468/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (41585/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (42634/50000)
# TEST : Loss: (0.4843) | Acc: (83.00%) (8320/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.4984, 0.5016], device='cuda:0')
percent tensor([0.5612, 0.4388], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.6304, 0.3696], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.9966, 0.0034], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (86.00%) (2323/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (86.00%) (3422/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (86.00%) (4527/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4169) |  Loss2: (0.0000) | Acc: (85.00%) (5603/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (6671/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (7756/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4199) |  Loss2: (0.0000) | Acc: (85.00%) (8846/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4195) |  Loss2: (0.0000) | Acc: (85.00%) (9940/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4228) |  Loss2: (0.0000) | Acc: (85.00%) (11019/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (12128/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (13222/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (14336/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4174) |  Loss2: (0.0000) | Acc: (85.00%) (15419/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (16535/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (17629/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (18726/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (19832/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (20930/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (22030/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (23146/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (24240/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (25336/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (26454/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (27535/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (28640/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (29723/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (30808/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (31905/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (32983/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (34086/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (35172/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (36260/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (37358/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (38462/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (39546/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (40660/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (41730/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (42776/50000)
# TEST : Loss: (0.4793) | Acc: (83.00%) (8339/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.4981, 0.5019], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.6361, 0.3639], device='cuda:0')
percent tensor([0.5939, 0.4061], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (84.00%) (1194/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (84.00%) (3359/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4246) |  Loss2: (0.0000) | Acc: (85.00%) (4462/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (5549/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (6664/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (7780/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (8874/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (9970/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (11043/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (12144/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (13252/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (14360/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (15469/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (85.00%) (16582/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (85.00%) (17697/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (18794/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (19891/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (20973/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (22076/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (23147/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (24268/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (25371/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4124) |  Loss2: (0.0000) | Acc: (85.00%) (26466/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4112) |  Loss2: (0.0000) | Acc: (85.00%) (27574/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (28686/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (29789/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4099) |  Loss2: (0.0000) | Acc: (85.00%) (30889/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (31973/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (33080/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4096) |  Loss2: (0.0000) | Acc: (85.00%) (34177/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (35286/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4102) |  Loss2: (0.0000) | Acc: (85.00%) (36374/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (37478/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (38561/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (39669/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (40779/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (41875/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (42927/50000)
# TEST : Loss: (0.4757) | Acc: (83.00%) (8354/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4978, 0.5022], device='cuda:0')
percent tensor([0.5652, 0.4348], device='cuda:0')
percent tensor([0.5486, 0.4514], device='cuda:0')
percent tensor([0.6417, 0.3583], device='cuda:0')
percent tensor([0.6014, 0.3986], device='cuda:0')
percent tensor([0.9972, 0.0028], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4783) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (85.00%) (1197/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (2296/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (3392/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (85.00%) (4506/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (86.00%) (5618/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (6711/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (7794/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (8886/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (10000/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (86.00%) (11119/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (12212/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (85.00%) (13295/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (14401/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (15494/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (86.00%) (16628/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (17719/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (86.00%) (18828/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (19918/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (20998/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4098) |  Loss2: (0.0000) | Acc: (85.00%) (22105/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (23187/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4113) |  Loss2: (0.0000) | Acc: (85.00%) (24270/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (25353/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (26462/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (27560/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (28670/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (29785/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (85.00%) (30879/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (85.00%) (31947/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (33055/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4098) |  Loss2: (0.0000) | Acc: (85.00%) (34173/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (35277/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (36386/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (37489/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (38590/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (85.00%) (39680/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4094) |  Loss2: (0.0000) | Acc: (85.00%) (40784/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (41887/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (42970/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_050.pth.tar'
# TEST : Loss: (0.5264) | Acc: (82.00%) (8217/10000)
percent tensor([0.5174, 0.4826], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.6431, 0.3569], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.7987, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(799.2943, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(796.6058, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1531.1168, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.9630, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2197.2559, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4290.2886, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1421.1370, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6104.2090, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12047.6914, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4007.1143, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16984.3770, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.3927) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4047) |  Loss2: (0.0000) | Acc: (85.00%) (1207/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (2341/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (3440/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (4558/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (86.00%) (5662/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (6756/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (7849/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.3833) |  Loss2: (0.0000) | Acc: (86.00%) (8948/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (10038/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (11148/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (12257/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (13380/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (14481/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (15579/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (16690/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (17788/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (18894/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (20001/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (21108/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (22194/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (23292/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (24395/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (25511/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (26613/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (27743/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (28848/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (29957/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (31051/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (32156/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (86.00%) (33238/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (34334/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (35451/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (36569/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (37671/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (38763/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (39886/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (40979/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (42085/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (43146/50000)
# TEST : Loss: (0.4663) | Acc: (84.00%) (8468/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4978, 0.5022], device='cuda:0')
percent tensor([0.5650, 0.4350], device='cuda:0')
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.6417, 0.3583], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.9972, 0.0028], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (87.00%) (1232/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.3743) |  Loss2: (0.0000) | Acc: (87.00%) (3473/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (4598/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (87.00%) (5700/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (6811/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (7926/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (9039/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3769) |  Loss2: (0.0000) | Acc: (86.00%) (10126/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (86.00%) (11242/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (12338/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (86.00%) (13440/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (14550/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (15666/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (16781/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (17885/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (19026/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (20144/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (21276/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3766) |  Loss2: (0.0000) | Acc: (87.00%) (22403/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (87.00%) (23500/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (24611/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (87.00%) (25732/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3783) |  Loss2: (0.0000) | Acc: (87.00%) (26841/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (87.00%) (27962/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (86.00%) (29062/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (30152/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (31271/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (32388/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (33477/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (34596/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (35709/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (86.00%) (36824/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (37939/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (86.00%) (39063/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (40174/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (86.00%) (41286/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (86.00%) (42383/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (43474/50000)
# TEST : Loss: (0.5398) | Acc: (82.00%) (8212/10000)
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4977, 0.5023], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.5505, 0.4495], device='cuda:0')
percent tensor([0.6409, 0.3591], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.9970, 0.0030], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (3487/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (4604/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (5724/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (6835/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (7954/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3720) |  Loss2: (0.0000) | Acc: (87.00%) (9067/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (10176/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (11291/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (12396/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (13519/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3738) |  Loss2: (0.0000) | Acc: (87.00%) (14615/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3739) |  Loss2: (0.0000) | Acc: (87.00%) (15729/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (16862/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (17998/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (19128/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (20234/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (21354/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3700) |  Loss2: (0.0000) | Acc: (87.00%) (22473/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (23558/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (24659/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (25784/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (26900/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (28009/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (29125/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3729) |  Loss2: (0.0000) | Acc: (87.00%) (30248/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (31373/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (32491/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (33615/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (34731/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (35859/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (36959/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (38078/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (39190/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (40302/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (41432/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (42520/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (43589/50000)
# TEST : Loss: (0.5337) | Acc: (82.00%) (8221/10000)
percent tensor([0.5176, 0.4824], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5655, 0.4345], device='cuda:0')
percent tensor([0.5522, 0.4478], device='cuda:0')
percent tensor([0.6465, 0.3535], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3755) |  Loss2: (0.0000) | Acc: (86.00%) (1223/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (2354/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (3470/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (4595/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (5708/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (6823/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (7953/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (9059/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (10174/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (11307/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (12438/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (13557/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (14696/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (15806/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (16910/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (18031/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (19149/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (20266/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (21372/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (22494/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (23615/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (24753/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (25884/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (27018/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (28128/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (29245/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (30364/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (31488/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (32590/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (33709/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (34821/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (35926/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (37051/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (38178/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (39316/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (40425/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (41549/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (42670/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (43762/50000)
# TEST : Loss: (0.4757) | Acc: (84.00%) (8443/10000)
percent tensor([0.5176, 0.4824], device='cuda:0')
percent tensor([0.5039, 0.4961], device='cuda:0')
percent tensor([0.4978, 0.5022], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.5469, 0.4531], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (88.00%) (2371/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (88.00%) (3510/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.3707) |  Loss2: (0.0000) | Acc: (87.00%) (4609/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (5698/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (6816/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (7924/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (87.00%) (9046/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (10129/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (11237/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (87.00%) (12364/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (87.00%) (13482/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (14586/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (15684/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (16788/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (17897/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (19002/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (20104/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (21226/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (22348/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (23476/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (24603/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (25709/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (87.00%) (26839/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (87.00%) (27968/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (29078/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (87.00%) (30184/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (87.00%) (31300/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (87.00%) (32422/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (87.00%) (33521/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (34656/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (35786/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.3758) |  Loss2: (0.0000) | Acc: (87.00%) (36889/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (87.00%) (38009/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (87.00%) (39134/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (40246/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.3752) |  Loss2: (0.0000) | Acc: (87.00%) (41354/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.3747) |  Loss2: (0.0000) | Acc: (87.00%) (42461/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.3748) |  Loss2: (0.0000) | Acc: (87.00%) (43528/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_055.pth.tar'
# TEST : Loss: (0.4696) | Acc: (84.00%) (8457/10000)
percent tensor([0.5213, 0.4787], device='cuda:0')
percent tensor([0.5057, 0.4943], device='cuda:0')
percent tensor([0.4949, 0.5051], device='cuda:0')
percent tensor([0.5735, 0.4265], device='cuda:0')
percent tensor([0.5678, 0.4322], device='cuda:0')
percent tensor([0.6546, 0.3454], device='cuda:0')
percent tensor([0.6282, 0.3718], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.3540) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.3551) |  Loss2: (0.0000) | Acc: (87.00%) (3466/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.3581) |  Loss2: (0.0000) | Acc: (87.00%) (4587/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (5718/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.3539) |  Loss2: (0.0000) | Acc: (87.00%) (6860/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (7976/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (9092/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (10214/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (11346/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (87.00%) (12442/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (13564/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (14665/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (15777/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (16916/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (18041/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (19169/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (20266/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (21404/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (22524/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (23654/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (24773/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (25896/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (27027/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (28144/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (29263/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (30410/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (31547/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3551) |  Loss2: (0.0000) | Acc: (87.00%) (32665/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (33784/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (34898/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (36011/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (37162/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (38289/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (39430/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (40555/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (41668/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (42789/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (43849/50000)
# TEST : Loss: (0.4547) | Acc: (85.00%) (8507/10000)
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.5073, 0.4927], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5759, 0.4241], device='cuda:0')
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.6513, 0.3487], device='cuda:0')
percent tensor([0.6372, 0.3628], device='cuda:0')
percent tensor([0.9976, 0.0024], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.3201) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (3529/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (4630/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (5760/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (87.00%) (6866/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (87.00%) (7993/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (87.00%) (9119/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (87.00%) (10240/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (87.00%) (11364/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (12509/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (13634/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (88.00%) (14762/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (88.00%) (15884/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (17027/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (87.00%) (18132/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (19250/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (20372/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (87.00%) (21507/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (22657/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (88.00%) (23795/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (24906/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (26028/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (87.00%) (27143/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (28289/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (29421/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3438) |  Loss2: (0.0000) | Acc: (88.00%) (30564/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (31707/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (88.00%) (32822/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3443) |  Loss2: (0.0000) | Acc: (88.00%) (33938/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (35052/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (36177/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (37306/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (38434/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3455) |  Loss2: (0.0000) | Acc: (88.00%) (39542/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (87.00%) (40651/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (41806/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (42948/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (44027/50000)
# TEST : Loss: (0.4469) | Acc: (85.00%) (8522/10000)
percent tensor([0.5210, 0.4790], device='cuda:0')
percent tensor([0.5078, 0.4922], device='cuda:0')
percent tensor([0.4936, 0.5064], device='cuda:0')
percent tensor([0.5775, 0.4225], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.6481, 0.3519], device='cuda:0')
percent tensor([0.6473, 0.3527], device='cuda:0')
percent tensor([0.9978, 0.0022], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (1251/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (89.00%) (2406/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (89.00%) (3546/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (4656/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (5777/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3439) |  Loss2: (0.0000) | Acc: (88.00%) (6897/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (8019/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (9158/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (10299/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (11433/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (12567/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (13687/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (14835/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (15990/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (17122/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (18249/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (19368/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (20503/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (21635/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (22759/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (23876/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (24983/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (26108/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (27215/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (28347/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (29470/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3421) |  Loss2: (0.0000) | Acc: (88.00%) (30584/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (31716/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3421) |  Loss2: (0.0000) | Acc: (88.00%) (32851/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (33981/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (35129/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (36258/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (37396/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (38523/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (39647/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (40783/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (41920/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (43023/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (44121/50000)
# TEST : Loss: (0.4418) | Acc: (85.00%) (8550/10000)
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.4947, 0.5053], device='cuda:0')
percent tensor([0.5767, 0.4233], device='cuda:0')
percent tensor([0.5672, 0.4328], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.6486, 0.3514], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (87.00%) (3484/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (88.00%) (4620/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (87.00%) (5740/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (6886/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (8029/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (9172/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (10303/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (11440/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (12570/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3335) |  Loss2: (0.0000) | Acc: (88.00%) (13693/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (14828/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (15957/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (17088/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (18208/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (19327/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (20459/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (21588/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (22716/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (23820/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (24958/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (26086/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (27205/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (28338/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (29482/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (30615/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (31767/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (32907/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (34024/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (35149/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (36265/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (37388/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (38514/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (39649/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (40776/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (41892/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (43026/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (44102/50000)
# TEST : Loss: (0.4368) | Acc: (85.00%) (8560/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5787, 0.4213], device='cuda:0')
percent tensor([0.5672, 0.4328], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.6489, 0.3511], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3211) |  Loss2: (0.0000) | Acc: (87.00%) (1238/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (87.00%) (2350/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (3493/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (5757/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (6899/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (88.00%) (8012/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3438) |  Loss2: (0.0000) | Acc: (88.00%) (9139/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (10265/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (11393/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (12520/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (88.00%) (13632/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (14772/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3448) |  Loss2: (0.0000) | Acc: (88.00%) (15924/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (17045/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (88.00%) (18172/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (88.00%) (19280/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (20399/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (21529/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (88.00%) (22642/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (88.00%) (23769/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (88.00%) (24901/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (26009/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (87.00%) (27136/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (28241/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (29356/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3540) |  Loss2: (0.0000) | Acc: (87.00%) (30461/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (31591/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (32709/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (33822/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (34939/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (36057/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3551) |  Loss2: (0.0000) | Acc: (87.00%) (37195/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (38308/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (39423/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (40551/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (87.00%) (41667/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (42784/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (43861/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_060.pth.tar'
# TEST : Loss: (0.5313) | Acc: (83.00%) (8338/10000)
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5081, 0.4919], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.6549, 0.3451], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.9465, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.9835, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.2504, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.9352, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(502.4104, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2205.2395, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4285.6431, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1416.0559, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6115.1216, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12009.5010, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3991.5605, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16915.5137, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (2366/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (3499/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (4636/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (5779/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (6934/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (8069/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (9186/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (10317/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (11436/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (12574/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (13702/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (14834/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (15957/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (17103/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (18232/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (19362/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (20483/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (21626/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (22772/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (23899/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (25011/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (26118/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (27252/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (28401/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (29545/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (30670/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (31784/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (32880/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (34004/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (35127/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (36248/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3452) |  Loss2: (0.0000) | Acc: (88.00%) (37350/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (38477/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (39605/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3458) |  Loss2: (0.0000) | Acc: (88.00%) (40734/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (41865/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (88.00%) (42984/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (44065/50000)
# TEST : Loss: (0.4733) | Acc: (84.00%) (8450/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.4972, 0.5028], device='cuda:0')
percent tensor([0.5795, 0.4205], device='cuda:0')
percent tensor([0.5654, 0.4346], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.6512, 0.3488], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (2400/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (89.00%) (3533/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (4644/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (5775/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (6899/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (8035/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (9169/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (10286/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (11409/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (12557/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (13686/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (14834/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (88.00%) (15973/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (17113/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (88.00%) (18250/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (19386/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (88.00%) (20520/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (21656/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (22787/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (23942/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (25101/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (26239/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (88.00%) (27377/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (28496/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (29628/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (30759/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (31885/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (33028/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (34155/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (35309/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (36426/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (37549/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (38679/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (39798/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (40928/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (42055/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (43172/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (44268/50000)
# TEST : Loss: (0.5105) | Acc: (83.00%) (8352/10000)
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.6577, 0.3423], device='cuda:0')
percent tensor([0.6562, 0.3438], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (2390/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (88.00%) (3526/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (88.00%) (4670/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (88.00%) (5807/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (6961/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (8111/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (9249/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (10380/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3165) |  Loss2: (0.0000) | Acc: (89.00%) (11521/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (12686/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (13826/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (14965/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (16091/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (17208/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (18334/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (19462/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (20611/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (88.00%) (21752/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (22886/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (24013/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (25125/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (26264/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (27400/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (28559/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (29675/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (30792/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (88.00%) (31938/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (33069/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (34215/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (35345/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (36495/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (37648/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (38795/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (39905/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (88.00%) (41023/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (42155/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (43286/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3255) |  Loss2: (0.0000) | Acc: (88.00%) (44362/50000)
# TEST : Loss: (0.4651) | Acc: (84.00%) (8484/10000)
percent tensor([0.5196, 0.4804], device='cuda:0')
percent tensor([0.5083, 0.4917], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.5781, 0.4219], device='cuda:0')
percent tensor([0.5690, 0.4310], device='cuda:0')
percent tensor([0.6571, 0.3429], device='cuda:0')
percent tensor([0.6524, 0.3476], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (1240/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (3536/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (4671/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (5817/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (6959/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (8093/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (88.00%) (9223/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (88.00%) (10366/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (11507/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (12667/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (13802/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (14935/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (88.00%) (16060/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (17212/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3170) |  Loss2: (0.0000) | Acc: (89.00%) (18348/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (19499/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3168) |  Loss2: (0.0000) | Acc: (89.00%) (20630/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (21782/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (22928/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (24067/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (25199/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (26329/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (27477/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (28616/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (29750/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (30893/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (32020/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (33159/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (34320/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (35456/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (36593/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3161) |  Loss2: (0.0000) | Acc: (89.00%) (37719/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (38853/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (39996/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (41136/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (42271/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (43414/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (89.00%) (44504/50000)
# TEST : Loss: (0.4527) | Acc: (85.00%) (8526/10000)
percent tensor([0.5200, 0.4800], device='cuda:0')
percent tensor([0.5082, 0.4918], device='cuda:0')
percent tensor([0.4971, 0.5029], device='cuda:0')
percent tensor([0.5780, 0.4220], device='cuda:0')
percent tensor([0.5669, 0.4331], device='cuda:0')
percent tensor([0.6549, 0.3451], device='cuda:0')
percent tensor([0.6475, 0.3525], device='cuda:0')
percent tensor([0.9981, 0.0019], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (2367/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (3496/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (4626/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (5750/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (6874/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (8034/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (9163/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (10288/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (11399/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (12539/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (13677/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (14809/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (15959/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (17088/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (18224/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (19371/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3334) |  Loss2: (0.0000) | Acc: (88.00%) (20500/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (21642/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (22752/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (23875/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (25001/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (26130/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (27263/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (28382/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (29507/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (30630/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (31767/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (32886/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (34023/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (35143/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (36272/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (37405/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (38534/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (39673/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (40793/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (41920/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (43043/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (44136/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_065.pth.tar'
# TEST : Loss: (0.4565) | Acc: (85.00%) (8505/10000)
percent tensor([0.5226, 0.4774], device='cuda:0')
percent tensor([0.5060, 0.4940], device='cuda:0')
percent tensor([0.5004, 0.4996], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.6404, 0.3596], device='cuda:0')
percent tensor([0.9983, 0.0017], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (2370/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (3496/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3374) |  Loss2: (0.0000) | Acc: (88.00%) (4633/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (5781/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (6923/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (8058/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (9183/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (10325/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (11449/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (12579/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3263) |  Loss2: (0.0000) | Acc: (88.00%) (13733/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (14874/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (15993/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (17115/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (18248/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (19388/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (20527/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (21656/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (22801/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (23956/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (88.00%) (25115/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (26250/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (27383/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (88.00%) (28522/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (29689/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (30828/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (31981/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (33099/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (34238/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (88.00%) (35362/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (36508/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (37644/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (38794/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (39936/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (41065/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (42209/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (43349/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (44446/50000)
# TEST : Loss: (0.4386) | Acc: (85.00%) (8576/10000)
percent tensor([0.5235, 0.4765], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5018, 0.4982], device='cuda:0')
percent tensor([0.5806, 0.4194], device='cuda:0')
percent tensor([0.5535, 0.4465], device='cuda:0')
percent tensor([0.6845, 0.3155], device='cuda:0')
percent tensor([0.6512, 0.3488], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (1234/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (3502/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (4641/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (5756/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (6889/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (8029/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (9174/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (10323/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (11459/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (12611/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (13753/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (14909/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (88.00%) (16052/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (17194/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (88.00%) (18340/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (19488/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (20648/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (21790/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (89.00%) (22932/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (24084/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (25230/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (26354/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (27476/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (28633/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (29778/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (30922/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (32080/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (33213/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (89.00%) (34341/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (35497/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (36657/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (37806/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (38960/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (40107/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (41249/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (42392/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (43540/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (44642/50000)
# TEST : Loss: (0.4295) | Acc: (85.00%) (8590/10000)
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.6863, 0.3137], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.3103) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3140) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (3540/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (4695/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (5857/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (6987/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (8128/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (9272/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (10419/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (11564/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (12695/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (13858/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (14996/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (16140/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (17294/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (18447/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (19599/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (20730/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (21865/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (23011/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (24143/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (25284/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (26430/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (27581/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (28756/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (29902/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (31057/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (32225/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (33358/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (34520/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (35649/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (36774/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (37912/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (39070/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (40214/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (41351/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (42514/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (43669/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (44770/50000)
# TEST : Loss: (0.4264) | Acc: (85.00%) (8582/10000)
percent tensor([0.5237, 0.4763], device='cuda:0')
percent tensor([0.5043, 0.4957], device='cuda:0')
percent tensor([0.5041, 0.4959], device='cuda:0')
percent tensor([0.5875, 0.4125], device='cuda:0')
percent tensor([0.5590, 0.4410], device='cuda:0')
percent tensor([0.6945, 0.3055], device='cuda:0')
percent tensor([0.6646, 0.3354], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (1266/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (2415/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (3558/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (4706/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (5865/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (7008/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (8153/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (9289/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (10438/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (11572/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (12709/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (13847/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (15003/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (16142/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (17273/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (18433/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (19589/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (20716/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3082) |  Loss2: (0.0000) | Acc: (89.00%) (21846/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (22994/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3083) |  Loss2: (0.0000) | Acc: (89.00%) (24142/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (89.00%) (25303/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (26466/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (27622/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (28774/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (29932/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (31069/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (32215/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (33358/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (34513/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (35672/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (36820/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (37979/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (39135/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (40291/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (41438/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (42606/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (43754/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (44852/50000)
# TEST : Loss: (0.4221) | Acc: (86.00%) (8621/10000)
percent tensor([0.5235, 0.4765], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.5541, 0.4459], device='cuda:0')
percent tensor([0.6900, 0.3100], device='cuda:0')
percent tensor([0.6619, 0.3381], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (90.00%) (1268/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (3557/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (4708/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (5818/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (6971/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (8132/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (9257/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (10409/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (11545/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (12689/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (13824/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (14973/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (16104/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3087) |  Loss2: (0.0000) | Acc: (89.00%) (17242/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (18375/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (19532/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (20644/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (21793/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (22929/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (24090/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (25253/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (26392/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (27528/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (28670/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (29805/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (30950/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3142) |  Loss2: (0.0000) | Acc: (89.00%) (32069/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (33216/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (34375/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (35515/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (36657/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (37794/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (38925/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (40065/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (41215/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (42353/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (43502/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (44611/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_070.pth.tar'
# TEST : Loss: (0.4286) | Acc: (85.00%) (8576/10000)
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.5531, 0.4469], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.6592, 0.3408], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.0627, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.6630, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(805.1624, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.0126, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.7733, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2212.5039, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.6069, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1411.1742, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6127.6055, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11973.5127, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3976.0747, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16847.4922, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (2426/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (3581/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (4742/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (5863/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (7001/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (8128/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (9278/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (10433/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (11573/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (12716/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (13863/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (14985/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (16132/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (17272/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (18405/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (19549/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (20678/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (21825/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (22969/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (24117/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (25274/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (26419/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (27573/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (28725/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (29872/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (31003/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (32154/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (33311/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (34468/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (35611/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (36738/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (37893/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (39037/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (40190/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (41341/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (42495/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (43639/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (44754/50000)
# TEST : Loss: (0.4608) | Acc: (85.00%) (8543/10000)
percent tensor([0.5235, 0.4765], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5839, 0.4161], device='cuda:0')
percent tensor([0.5558, 0.4442], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.6669, 0.3331], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (5896/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (7049/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (8208/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (9357/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (10492/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (89.00%) (11624/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (12773/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (13925/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (15084/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (89.00%) (16240/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (17355/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (18501/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (19649/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (20791/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (21917/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (23051/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (24191/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (25328/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (26485/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (27614/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (28762/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (29906/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (31082/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (32206/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (33375/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (34512/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (35640/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (36777/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (37929/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (39084/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (40236/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (41396/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (42549/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (43705/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (44822/50000)
# TEST : Loss: (0.4766) | Acc: (84.00%) (8476/10000)
percent tensor([0.5234, 0.4766], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5054, 0.4946], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.5550, 0.4450], device='cuda:0')
percent tensor([0.6908, 0.3092], device='cuda:0')
percent tensor([0.6602, 0.3398], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (1281/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (2442/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (91.00%) (3612/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (91.00%) (4778/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (91.00%) (5942/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (91.00%) (7113/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (91.00%) (9443/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (10589/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (11719/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (12873/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (14029/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (15203/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (16363/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (17512/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (18655/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (19811/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (20955/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (22107/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (23243/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (24412/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (25558/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (26699/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (27862/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (29038/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (30201/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (31345/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2824) |  Loss2: (0.0000) | Acc: (90.00%) (32486/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (33634/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2824) |  Loss2: (0.0000) | Acc: (90.00%) (34777/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (35906/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (37069/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (38202/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (39345/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (40496/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (41650/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (42783/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (43933/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (45025/50000)
# TEST : Loss: (0.4578) | Acc: (85.00%) (8531/10000)
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5845, 0.4155], device='cuda:0')
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.6890, 0.3110], device='cuda:0')
percent tensor([0.6647, 0.3353], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (89.00%) (2415/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2853) |  Loss2: (0.0000) | Acc: (90.00%) (3576/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (4730/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (5897/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (7056/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (8212/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (9367/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (10525/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (11675/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (12834/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (13970/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (15129/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (16281/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (17437/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (18592/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (19745/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (20911/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (90.00%) (22071/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (23230/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (24399/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (25561/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (26721/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (27883/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (29056/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (30221/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (31371/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (32514/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (33669/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (34812/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (35976/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (37127/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (90.00%) (38278/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (39425/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (40579/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (41740/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (42887/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (44063/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (45168/50000)
# TEST : Loss: (0.4510) | Acc: (85.00%) (8554/10000)
percent tensor([0.5236, 0.4764], device='cuda:0')
percent tensor([0.5046, 0.4954], device='cuda:0')
percent tensor([0.5053, 0.4947], device='cuda:0')
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.5548, 0.4452], device='cuda:0')
percent tensor([0.6881, 0.3119], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.9986, 0.0014], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (2413/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (3537/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (4636/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (5767/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (6911/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (8049/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (9187/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (88.00%) (10335/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (11454/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (12588/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (88.00%) (13728/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3201) |  Loss2: (0.0000) | Acc: (88.00%) (14864/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (15993/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (17118/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (88.00%) (18253/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (88.00%) (19416/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (88.00%) (20555/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (21686/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (22808/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (23955/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (88.00%) (25101/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (26229/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (27376/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (88.00%) (28516/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (88.00%) (29674/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (30804/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (88.00%) (31944/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (88.00%) (33087/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (34221/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (88.00%) (35364/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (36500/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (88.00%) (37644/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (88.00%) (38799/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (88.00%) (39952/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (88.00%) (41082/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (42217/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (88.00%) (43342/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (44442/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_075.pth.tar'
# TEST : Loss: (0.4582) | Acc: (85.00%) (8513/10000)
percent tensor([0.5256, 0.4744], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5056, 0.4944], device='cuda:0')
percent tensor([0.6039, 0.3961], device='cuda:0')
percent tensor([0.5682, 0.4318], device='cuda:0')
percent tensor([0.6972, 0.3028], device='cuda:0')
percent tensor([0.6709, 0.3291], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.3263) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (87.00%) (1239/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (89.00%) (2396/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (3552/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (4705/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (5845/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (6995/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (89.00%) (8149/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (89.00%) (9298/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (89.00%) (10445/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (11602/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (12761/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (13895/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.2951) |  Loss2: (0.0000) | Acc: (89.00%) (15047/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (16186/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (17349/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (18501/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (19658/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (20808/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (21969/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (23118/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (24268/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (25408/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (26552/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (27700/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (28844/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (30002/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (31159/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (32302/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (33443/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (34592/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (35738/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (36887/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (38041/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (39189/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (40329/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (41477/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (42609/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (43746/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (44859/50000)
# TEST : Loss: (0.4363) | Acc: (85.00%) (8591/10000)
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.6107, 0.3893], device='cuda:0')
percent tensor([0.5772, 0.4228], device='cuda:0')
percent tensor([0.7016, 0.2984], device='cuda:0')
percent tensor([0.6736, 0.3264], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (2400/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (3560/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (90.00%) (4724/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (7029/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (8187/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (89.00%) (9331/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (10496/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2850) |  Loss2: (0.0000) | Acc: (89.00%) (11631/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (89.00%) (12786/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (13959/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (15109/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (16246/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (17404/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (18575/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (19734/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (20914/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2804) |  Loss2: (0.0000) | Acc: (90.00%) (22062/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (23208/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (24373/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (25519/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (26682/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (90.00%) (27834/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (28974/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (30129/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (31273/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (32433/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (33585/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (34742/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (35894/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (37038/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (38200/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (39353/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (40515/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (41664/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (42823/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (43993/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (45111/50000)
# TEST : Loss: (0.4230) | Acc: (86.00%) (8631/10000)
percent tensor([0.5225, 0.4775], device='cuda:0')
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.5096, 0.4904], device='cuda:0')
percent tensor([0.6135, 0.3865], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.7066, 0.2934], device='cuda:0')
percent tensor([0.6812, 0.3188], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (2437/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2824) |  Loss2: (0.0000) | Acc: (90.00%) (3585/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (4754/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (5902/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (7066/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (8214/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (9374/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (10556/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (11726/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (12881/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (14043/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (15231/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (16399/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (17561/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (18722/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (19883/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (21051/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (22207/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (23373/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (24538/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (25699/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (26869/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (28023/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (29177/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (30311/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (31472/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (32644/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (33801/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (34952/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (36118/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (37271/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (38400/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (39564/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (40719/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2726) |  Loss2: (0.0000) | Acc: (90.00%) (41865/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (43013/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (44180/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (45280/50000)
# TEST : Loss: (0.4164) | Acc: (86.00%) (8653/10000)
percent tensor([0.5222, 0.4778], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.6141, 0.3859], device='cuda:0')
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.7089, 0.2911], device='cuda:0')
percent tensor([0.6885, 0.3115], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (2430/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (3570/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (4734/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (5894/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (7062/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (8234/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (9396/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (10561/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (11741/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (12895/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (14048/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (15215/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (16377/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (17539/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (18697/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (19850/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (21018/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (22180/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (23352/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (24506/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (25671/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (26818/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (27971/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (29141/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (30313/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (31475/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (32634/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (33813/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (34972/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (36127/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37283/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (38430/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (39586/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (40760/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (41909/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (43078/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (44215/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (45324/50000)
# TEST : Loss: (0.4117) | Acc: (86.00%) (8671/10000)
percent tensor([0.5226, 0.4774], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.6135, 0.3865], device='cuda:0')
percent tensor([0.5784, 0.4216], device='cuda:0')
percent tensor([0.7144, 0.2856], device='cuda:0')
percent tensor([0.6906, 0.3094], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (3617/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (5937/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (91.00%) (7116/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (8281/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (9429/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (10585/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (11749/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (12917/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (14089/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (15252/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (16416/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (17579/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (18743/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (91.00%) (19920/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (21066/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (22219/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (23391/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (24571/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (25728/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (26885/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (28049/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (29203/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (30353/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (31498/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (32644/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (33795/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (34938/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2749) |  Loss2: (0.0000) | Acc: (90.00%) (36090/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (37238/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (38384/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (39548/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (40696/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (41868/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (43037/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (44192/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (45297/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_080.pth.tar'
# TEST : Loss: (0.4400) | Acc: (85.00%) (8568/10000)
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5115, 0.4885], device='cuda:0')
percent tensor([0.6138, 0.3862], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.7155, 0.2845], device='cuda:0')
percent tensor([0.6895, 0.3105], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.7837, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.6365, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.1215, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.1251, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(499.0656, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2220.3154, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4277.4810, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1406.2512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6140.7173, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11938.0381, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3960.7368, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16779.7363, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (1289/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (92.00%) (2480/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (3629/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (4795/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (91.00%) (5967/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (7135/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (8308/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (9469/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (10619/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (91.00%) (11780/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (91.00%) (12952/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (91.00%) (14112/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (91.00%) (15266/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (16421/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (17579/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (18734/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (19899/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (21072/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (22228/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (23379/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (24545/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (25709/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (26876/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (28050/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (29196/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (30358/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (31503/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (32648/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (33798/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (34956/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (36118/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (37273/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (38429/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (39586/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (40752/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (41906/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (43075/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (44228/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (45342/50000)
# TEST : Loss: (0.4281) | Acc: (86.00%) (8608/10000)
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.6140, 0.3860], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.7154, 0.2846], device='cuda:0')
percent tensor([0.6874, 0.3126], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (2465/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (3639/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (4814/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (5972/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (7129/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (8279/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (9425/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (90.00%) (10586/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (11755/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (12923/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (14090/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (15254/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2594) |  Loss2: (0.0000) | Acc: (91.00%) (16430/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2599) |  Loss2: (0.0000) | Acc: (90.00%) (17586/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (18755/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (19912/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (21071/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (22238/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (90.00%) (23405/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (24569/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (25737/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (26880/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (28027/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (29183/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (30342/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (31517/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (32669/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (33831/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (34986/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (36144/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (37322/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (38470/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (39628/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (40790/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (41962/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (43123/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (44283/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (45413/50000)
# TEST : Loss: (0.4304) | Acc: (86.00%) (8613/10000)
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5116, 0.4884], device='cuda:0')
percent tensor([0.6131, 0.3869], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.7167, 0.2833], device='cuda:0')
percent tensor([0.6920, 0.3080], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (3622/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (4782/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (5960/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (7145/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (8313/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (9483/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (10648/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (11818/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (12990/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (14163/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (15332/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (16501/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (17684/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (18857/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (20024/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2516) |  Loss2: (0.0000) | Acc: (91.00%) (21188/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (22337/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (23517/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (24675/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (25854/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (27023/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (28184/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (29372/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (30522/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (31684/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (32849/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (34026/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (35198/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (36368/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (37531/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (38700/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (39852/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (41020/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (42167/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (43340/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (44492/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (45596/50000)
# TEST : Loss: (0.4165) | Acc: (86.00%) (8638/10000)
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5112, 0.4888], device='cuda:0')
percent tensor([0.6140, 0.3860], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.7147, 0.2853], device='cuda:0')
percent tensor([0.6894, 0.3106], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (2458/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (3624/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (4801/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (5990/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (7168/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (8341/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (9516/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (10684/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (11853/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (13009/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (14176/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (15379/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (16557/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (17734/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (18904/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (20071/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (21232/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (22383/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (23548/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (24712/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (25876/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (27049/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (28205/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (29388/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (30544/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (31710/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (32885/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (34032/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (35197/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (36378/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (37548/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (38722/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (39878/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (41042/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (42196/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (43344/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (91.00%) (44500/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (45640/50000)
# TEST : Loss: (0.4618) | Acc: (85.00%) (8560/10000)
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5117, 0.4883], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.7146, 0.2854], device='cuda:0')
percent tensor([0.6921, 0.3079], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (1279/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (2441/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (3609/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (4753/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (5923/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (90.00%) (7046/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (8203/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (90.00%) (9338/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (10482/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (11616/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (12750/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (13901/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (15066/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (89.00%) (16228/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (17356/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (18500/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2947) |  Loss2: (0.0000) | Acc: (89.00%) (19624/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (20778/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (21918/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (23078/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (24239/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (25392/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (26545/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (27696/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (28845/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (30008/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (89.00%) (31168/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (32333/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (89.00%) (33504/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (34655/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (35804/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (89.00%) (36972/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (38144/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (39299/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (40467/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (41640/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (42819/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (43975/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (45092/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_085.pth.tar'
# TEST : Loss: (0.4198) | Acc: (86.00%) (8661/10000)
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.6306, 0.3694], device='cuda:0')
percent tensor([0.5889, 0.4111], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.6471, 0.3529], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (3623/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (4791/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (5954/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (90.00%) (7105/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (8272/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (90.00%) (9414/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (90.00%) (10581/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (11732/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (12906/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (14070/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (90.00%) (15240/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (16382/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (17550/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (18703/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (19861/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (21031/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (22193/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (23339/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (24506/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (90.00%) (26866/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (28036/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (29200/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (90.00%) (30381/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (31562/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (90.00%) (32725/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (33891/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (35053/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (90.00%) (36225/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (37403/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2587) |  Loss2: (0.0000) | Acc: (91.00%) (38565/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (39752/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (40914/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (42089/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (43262/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (44428/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (45547/50000)
# TEST : Loss: (0.4020) | Acc: (87.00%) (8731/10000)
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5149, 0.4851], device='cuda:0')
percent tensor([0.6314, 0.3686], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.6984, 0.3016], device='cuda:0')
percent tensor([0.6517, 0.3483], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (1270/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (2426/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (90.00%) (4771/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (5940/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (7110/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (8264/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (9437/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (90.00%) (10597/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2524) |  Loss2: (0.0000) | Acc: (90.00%) (11755/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (12933/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (14107/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (15274/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (16442/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (17621/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (18806/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (19981/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (21148/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (22320/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (23505/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (24682/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (25866/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (27028/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (28221/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (29398/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (30575/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (31752/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (32917/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (34100/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (35262/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (36443/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (37624/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (38819/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (39997/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (41165/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (42339/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (43512/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (44695/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (45817/50000)
# TEST : Loss: (0.3942) | Acc: (87.00%) (8735/10000)
percent tensor([0.5282, 0.4718], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.6321, 0.3679], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.7004, 0.2996], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (2444/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (5961/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (7146/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (8328/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (9495/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (10669/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (11836/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (13024/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (14193/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (15368/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (16544/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (17714/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (18877/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (20055/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (21246/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (22430/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (23602/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (24774/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (25962/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (27126/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (28303/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (29487/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (30676/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (31872/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (33058/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (34241/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (35412/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (36577/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (37773/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (38942/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (40134/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (41310/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (42491/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (43682/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (44860/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (46008/50000)
# TEST : Loss: (0.3885) | Acc: (87.00%) (8755/10000)
percent tensor([0.5280, 0.4720], device='cuda:0')
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.6296, 0.3704], device='cuda:0')
percent tensor([0.5947, 0.4053], device='cuda:0')
percent tensor([0.7035, 0.2965], device='cuda:0')
percent tensor([0.6640, 0.3360], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (3659/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (91.00%) (4821/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (5994/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (7166/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (8338/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (9505/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (10680/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (11850/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (13037/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (14218/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (15393/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (16577/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (17737/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (18919/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (20096/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (21281/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (22444/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (23599/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (24792/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (25954/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (27124/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (28322/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (29501/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (30688/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (31872/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (33045/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (34224/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (35390/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (36559/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (37745/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (38926/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (40095/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (41276/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (42458/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (43637/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (44823/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (45956/50000)
# TEST : Loss: (0.3844) | Acc: (87.00%) (8752/10000)
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5141, 0.4859], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.7071, 0.2929], device='cuda:0')
percent tensor([0.6698, 0.3302], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (2468/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (3650/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (4828/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (6005/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (92.00%) (7201/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (92.00%) (8377/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (9548/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (10713/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (11885/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (13052/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (14227/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (15392/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (16552/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (17722/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (18900/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (20070/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (21236/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (22396/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (23548/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (24725/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (25900/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (27069/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (28225/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (29405/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (30594/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (31753/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (32917/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (34104/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (35263/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (36454/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (37630/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (38804/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (39963/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (41144/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (42322/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (43489/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (44657/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (45780/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_090.pth.tar'
# TEST : Loss: (0.4780) | Acc: (85.00%) (8505/10000)
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.6285, 0.3715], device='cuda:0')
percent tensor([0.5949, 0.4051], device='cuda:0')
percent tensor([0.7065, 0.2935], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.5599, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.0495, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.1286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.9277, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(497.4951, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2228.3281, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4273.9478, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1401.4420, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6154.6909, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11903.2188, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3945.5186, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16712.5371, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (3641/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (4843/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (6018/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (7183/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (8330/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (9495/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (10662/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (11813/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (12980/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (14169/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (15344/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (16514/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (17686/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (18856/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (20015/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (21204/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (22382/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (23564/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (24739/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (25917/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (27088/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (28252/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (29425/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (30601/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (31784/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (32953/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (34135/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (35313/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (36505/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (37677/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (38843/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (40017/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (41206/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (42379/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (43572/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (44747/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (45869/50000)
# TEST : Loss: (0.4001) | Acc: (87.00%) (8708/10000)
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5136, 0.4864], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.5938, 0.4062], device='cuda:0')
percent tensor([0.7063, 0.2937], device='cuda:0')
percent tensor([0.6678, 0.3322], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (2473/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (6023/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (7211/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (8385/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (9563/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (10725/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (11898/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (13083/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (14264/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (16610/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (91.00%) (17766/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (18941/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (20113/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (21286/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (22466/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (23650/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (24836/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (26024/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (27203/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (28366/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (29545/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (30720/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (31884/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (33067/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (34253/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (35429/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (36605/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (37766/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (38934/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (40119/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (41306/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (42488/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (43662/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (44844/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (45991/50000)
# TEST : Loss: (0.4185) | Acc: (86.00%) (8695/10000)
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.6262, 0.3738], device='cuda:0')
percent tensor([0.5945, 0.4055], device='cuda:0')
percent tensor([0.7071, 0.2929], device='cuda:0')
percent tensor([0.6700, 0.3300], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (3674/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (4857/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (6050/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (7228/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (8399/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (9576/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (10759/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (11935/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (13126/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (14311/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (15491/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (16658/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (17843/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (19009/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (20173/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (21364/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (22559/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (23730/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (24910/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (26099/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (27261/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (28456/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (29623/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (30785/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (31972/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (33146/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (34329/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (35489/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (36674/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (37846/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (39016/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (40186/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (41359/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (42538/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (92.00%) (43712/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (44878/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (46013/50000)
# TEST : Loss: (0.4278) | Acc: (86.00%) (8651/10000)
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.5015, 0.4985], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.6281, 0.3719], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.7063, 0.2937], device='cuda:0')
percent tensor([0.6738, 0.3262], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (1299/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (3670/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (4858/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (6034/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (7230/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (8418/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (9595/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (10783/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (11978/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (13163/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (14346/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (15514/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (16703/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (17862/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (19044/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (20238/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (21405/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (22584/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (23773/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (24972/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (26159/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (27334/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (28492/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (29660/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (30844/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (32021/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (33193/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (34376/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (35548/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (36720/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (37907/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (39092/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (40262/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (41459/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (42635/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (43816/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (44996/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (46148/50000)
# TEST : Loss: (0.3982) | Acc: (87.00%) (8744/10000)
percent tensor([0.5286, 0.4714], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.5958, 0.4042], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.6757, 0.3243], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (2465/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (3611/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (90.00%) (4769/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (90.00%) (5924/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (7069/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (8220/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (9372/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (10515/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (11653/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (12813/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (13986/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (15140/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (16303/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (17455/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (18626/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (19772/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (20932/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (22102/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (23258/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (24412/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (25589/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (26757/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (27931/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (29089/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (30240/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (31409/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (32584/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (33748/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (34899/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (36063/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (37218/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (38384/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (39540/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (40695/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (41850/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (43007/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (44167/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (45292/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_095.pth.tar'
# TEST : Loss: (0.4216) | Acc: (86.00%) (8651/10000)
percent tensor([0.5247, 0.4753], device='cuda:0')
percent tensor([0.5024, 0.4976], device='cuda:0')
percent tensor([0.5206, 0.4794], device='cuda:0')
percent tensor([0.6433, 0.3567], device='cuda:0')
percent tensor([0.6136, 0.3864], device='cuda:0')
percent tensor([0.7340, 0.2660], device='cuda:0')
percent tensor([0.6495, 0.3505], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (3634/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (4800/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (5970/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (7142/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (8320/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (9477/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (10641/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (11816/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (12980/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (14147/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (15302/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (16437/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (17604/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (18780/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (19955/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (21122/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (22290/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (23462/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (24634/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (25815/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (26983/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (28151/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (29340/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (30516/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (31687/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (32845/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (34010/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (35207/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (36390/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (37560/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (38734/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (39898/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41092/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (42273/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (43458/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (44630/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (45752/50000)
# TEST : Loss: (0.4027) | Acc: (87.00%) (8700/10000)
percent tensor([0.5255, 0.4745], device='cuda:0')
percent tensor([0.5029, 0.4971], device='cuda:0')
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.6483, 0.3517], device='cuda:0')
percent tensor([0.6159, 0.3841], device='cuda:0')
percent tensor([0.7405, 0.2595], device='cuda:0')
percent tensor([0.6459, 0.3541], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (2473/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (3645/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (4827/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (5997/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (7172/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (8339/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (9518/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (91.00%) (10707/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (91.00%) (11874/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (13051/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (91.00%) (14243/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (15444/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (16617/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (17797/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (18993/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (20174/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (21358/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (22524/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (23700/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (24881/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (26058/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (27219/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (28406/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (29604/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (30793/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (31996/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (33165/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (34350/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (35531/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (36707/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (37884/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (39069/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (40260/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (41428/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (42604/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (43783/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (44977/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (46102/50000)
# TEST : Loss: (0.3960) | Acc: (87.00%) (8727/10000)
percent tensor([0.5277, 0.4723], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5283, 0.4717], device='cuda:0')
percent tensor([0.6478, 0.3522], device='cuda:0')
percent tensor([0.6144, 0.3856], device='cuda:0')
percent tensor([0.7446, 0.2554], device='cuda:0')
percent tensor([0.6511, 0.3489], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (4873/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (93.00%) (6073/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (93.00%) (7270/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (93.00%) (8463/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (93.00%) (9660/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (93.00%) (10842/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (12001/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (13184/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (14368/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (15528/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (16716/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (17890/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (19075/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (20257/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (21439/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (22633/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (23806/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (24973/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (26172/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (27349/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (28527/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (29703/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (30884/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (32071/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (33257/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (34447/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (35614/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (36806/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (37986/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (39179/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (40371/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (41555/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (42749/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (43945/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (45137/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (46262/50000)
# TEST : Loss: (0.3866) | Acc: (87.00%) (8770/10000)
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5304, 0.4696], device='cuda:0')
percent tensor([0.6460, 0.3540], device='cuda:0')
percent tensor([0.6110, 0.3890], device='cuda:0')
percent tensor([0.7472, 0.2528], device='cuda:0')
percent tensor([0.6611, 0.3389], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (4889/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (93.00%) (6073/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (7258/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (8429/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (9597/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (10785/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (11982/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (13164/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (14344/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (15543/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (16730/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (17913/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (19081/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (20289/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (21495/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (22682/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (23866/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (25056/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (26229/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (27420/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (28613/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (29801/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (30990/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (32189/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (33360/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (34533/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (35697/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (36878/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (38053/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (39227/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (40404/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (41601/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (42795/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (43974/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (45172/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (46330/50000)
# TEST : Loss: (0.3857) | Acc: (87.00%) (8769/10000)
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.6423, 0.3577], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.7450, 0.2550], device='cuda:0')
percent tensor([0.6619, 0.3381], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (3690/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (6044/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (7242/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (8411/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (9595/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (10776/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (11962/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (13136/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (14313/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (15501/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (16688/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (17877/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (19069/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (20254/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (21437/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (22623/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (23817/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (25006/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (26195/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (27384/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (28571/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (29767/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (30934/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (32112/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (33286/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (34448/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (35638/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (36824/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (38001/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (39179/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (40352/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (41532/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (42713/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (43892/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (45080/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (46216/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_100.pth.tar'
# TEST : Loss: (0.4011) | Acc: (87.00%) (8727/10000)
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5299, 0.4701], device='cuda:0')
percent tensor([0.6429, 0.3571], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.7398, 0.2602], device='cuda:0')
percent tensor([0.6573, 0.3427], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.3056, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(813.1550, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.3818, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.5978, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.7510, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2235.7661, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4271.7959, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1396.5530, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6168.3506, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11868.4727, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3930.2065, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16645.5898, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (2491/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (4869/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (93.00%) (6072/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (7271/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (8459/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (9645/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (10843/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (12037/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (93.00%) (13222/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (93.00%) (14413/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (93.00%) (15599/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (93.00%) (16798/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (17971/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (19160/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (20338/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (21515/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (22695/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (23886/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (25069/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (26242/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (27425/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (28625/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (29812/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (30989/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (32175/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (33370/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (34560/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (35759/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (36942/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (38139/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (39328/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (40508/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (41680/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (42869/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (44039/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (45222/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (46360/50000)
# TEST : Loss: (0.4135) | Acc: (86.00%) (8666/10000)
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.6425, 0.3575], device='cuda:0')
percent tensor([0.6101, 0.3899], device='cuda:0')
percent tensor([0.7398, 0.2602], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (2505/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (3700/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (4892/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (6089/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (7291/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (8497/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (9690/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (10870/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (12045/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (93.00%) (13224/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (14393/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (15589/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (93.00%) (16788/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (93.00%) (17987/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (93.00%) (19171/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (93.00%) (20357/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (21553/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (22734/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (23918/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (25096/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (26296/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (27484/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (28665/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (29852/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (31039/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (32221/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (33404/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (34585/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (35766/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (36960/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (38146/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (39339/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (40529/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (41719/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (42916/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (44110/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (45299/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (46450/50000)
# TEST : Loss: (0.4038) | Acc: (87.00%) (8738/10000)
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.6424, 0.3576], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.7434, 0.2566], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (2531/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (3725/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (6106/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (7309/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (8493/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (9683/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (10876/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (12068/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (13260/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (14467/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (15666/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (16855/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (18048/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (19236/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (20423/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (21615/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (22804/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (23987/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (25185/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (26377/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (27575/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (28782/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (29961/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (31152/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (32327/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (33520/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (34711/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (35902/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (37109/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (38292/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (39474/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (40646/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (41810/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (43006/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (44197/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (45403/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (46541/50000)
# TEST : Loss: (0.4142) | Acc: (87.00%) (8715/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.6424, 0.3576], device='cuda:0')
percent tensor([0.6134, 0.3866], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
percent tensor([0.6613, 0.3387], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (2522/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (4913/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (6120/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (7306/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (8508/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (9707/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (10901/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (12080/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (13279/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (14476/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (15661/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (16862/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (18050/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (19242/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (20439/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (21637/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (22826/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (24009/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (25206/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (26402/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (27602/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (28787/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (29972/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (31163/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (32342/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (33519/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (34720/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (35908/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (37100/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (38288/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (39481/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (40678/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (41851/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (43055/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (44224/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (45416/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (46544/50000)
# TEST : Loss: (0.4076) | Acc: (87.00%) (8751/10000)
percent tensor([0.5296, 0.4704], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5289, 0.4711], device='cuda:0')
percent tensor([0.6405, 0.3595], device='cuda:0')
percent tensor([0.6148, 0.3852], device='cuda:0')
percent tensor([0.7440, 0.2560], device='cuda:0')
percent tensor([0.6616, 0.3384], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (3652/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (91.00%) (4818/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (91.00%) (5994/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (91.00%) (7162/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (8328/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (9503/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (10672/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (11842/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (13022/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (14199/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (15368/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (16546/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (17713/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (18889/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (20071/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (21254/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (22414/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (23588/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (24767/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (25941/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (27118/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (91.00%) (28319/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (29485/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (91.00%) (30664/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (31818/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (33000/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (34172/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (35354/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (36547/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (37717/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (38894/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (40071/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (91.00%) (41260/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (91.00%) (42450/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (43632/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (91.00%) (44820/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (91.00%) (45941/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_105.pth.tar'
# TEST : Loss: (0.4258) | Acc: (86.00%) (8690/10000)
percent tensor([0.5408, 0.4592], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5270, 0.4730], device='cuda:0')
percent tensor([0.6403, 0.3597], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.7331, 0.2669], device='cuda:0')
percent tensor([0.6344, 0.3656], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (3689/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2077) |  Loss2: (0.0000) | Acc: (93.00%) (4884/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (7269/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (8434/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (9624/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (10809/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (11984/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (13173/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (14356/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (15546/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (16741/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (17924/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (19102/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (20298/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (21481/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (22663/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (23843/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (25042/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (26229/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (27421/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (28611/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (29792/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (30977/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (32172/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (33364/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (34553/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (35737/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (36926/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (38094/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (39281/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (40472/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (41666/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (42852/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (44039/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (45221/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (46345/50000)
# TEST : Loss: (0.4060) | Acc: (87.00%) (8755/10000)
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5278, 0.4722], device='cuda:0')
percent tensor([0.6419, 0.3581], device='cuda:0')
percent tensor([0.5953, 0.4047], device='cuda:0')
percent tensor([0.7395, 0.2605], device='cuda:0')
percent tensor([0.6403, 0.3597], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (4892/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (6076/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (7264/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (8454/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (9639/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (10835/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (12028/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (13226/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (14415/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (15593/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (16777/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (17966/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (19164/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (20365/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (21560/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (22744/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (23930/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (25118/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (93.00%) (26310/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (27492/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (28699/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (29892/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (31097/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (32303/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (33494/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (34684/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (35863/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (37053/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (38270/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (39450/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (40648/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (41841/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (43045/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (44238/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (45451/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (46603/50000)
# TEST : Loss: (0.3963) | Acc: (87.00%) (8782/10000)
percent tensor([0.5368, 0.4632], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5287, 0.4713], device='cuda:0')
percent tensor([0.6507, 0.3493], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.7496, 0.2504], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (4896/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (6104/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (7302/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (8497/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (9691/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (10880/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (12076/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (13261/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (14452/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (15634/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (16829/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (18034/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (19230/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (20430/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (21623/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (22834/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (24035/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (25232/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (26422/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (27625/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (28835/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (30023/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (31218/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (32418/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (33626/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (34829/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (36025/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (37235/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (38431/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (39628/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (40822/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (42014/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (43220/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (44421/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (45614/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (46754/50000)
# TEST : Loss: (0.3888) | Acc: (88.00%) (8818/10000)
percent tensor([0.5372, 0.4628], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6500, 0.3500], device='cuda:0')
percent tensor([0.6038, 0.3962], device='cuda:0')
percent tensor([0.7533, 0.2467], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (4891/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (6075/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (7262/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (8461/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (9658/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (10853/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (12058/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (13239/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (14440/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (15647/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (16842/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (18039/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (19253/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (20448/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (21659/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (22863/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (24060/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (25253/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (26453/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (27645/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (28835/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (30019/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (31231/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (32427/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (33623/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (34827/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (36032/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (37215/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (38416/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (39607/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (40811/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (42014/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (43215/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (44418/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (45625/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (46786/50000)
# TEST : Loss: (0.3832) | Acc: (88.00%) (8816/10000)
percent tensor([0.5378, 0.4622], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.6508, 0.3492], device='cuda:0')
percent tensor([0.6021, 0.3979], device='cuda:0')
percent tensor([0.7607, 0.2393], device='cuda:0')
percent tensor([0.6734, 0.3266], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (4891/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (92.00%) (6070/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (7243/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (8433/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (9605/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (92.00%) (12006/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (92.00%) (13204/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (92.00%) (14398/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (15602/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (16798/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (17997/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (19195/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (20379/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (21583/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (22765/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (23952/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (25148/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (26333/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (27538/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (28745/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (29939/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (31126/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (32309/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (33497/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (34701/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (35884/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (37072/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (38262/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (39449/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (40652/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (41838/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (43032/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (44229/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (45421/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (46580/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_110.pth.tar'
# TEST : Loss: (0.4047) | Acc: (87.00%) (8772/10000)
percent tensor([0.5378, 0.4622], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6514, 0.3486], device='cuda:0')
percent tensor([0.5995, 0.4005], device='cuda:0')
percent tensor([0.7616, 0.2384], device='cuda:0')
percent tensor([0.6712, 0.3288], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.9425, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(815.2234, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.4657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.4454, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(494.1717, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2243.3728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4268.9448, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1391.5807, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6182.9263, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11834.0098, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3915.1248, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16578.8086, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (3729/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (4930/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1779) |  Loss2: (0.0000) | Acc: (93.00%) (6126/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (7325/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (8520/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (9729/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (10942/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (94.00%) (12154/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (13339/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (14536/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (15738/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (16922/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (18131/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (19328/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (20514/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (21703/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (22894/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (24090/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (25285/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (26462/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (27670/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (28863/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (30056/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (31246/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (32447/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (33639/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (34830/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (36036/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (37237/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (38426/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (39605/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (40792/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (41992/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (43179/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (44370/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (45567/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (46696/50000)
# TEST : Loss: (0.4365) | Acc: (86.00%) (8671/10000)
percent tensor([0.5376, 0.4624], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.6506, 0.3494], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.7571, 0.2429], device='cuda:0')
percent tensor([0.6746, 0.3254], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (2529/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (3727/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (4924/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (6134/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (7333/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (8538/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (9734/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (10946/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (94.00%) (12158/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (94.00%) (13358/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (14555/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (15742/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (16933/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (18115/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (19313/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (20527/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (21724/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (22925/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (24131/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (25326/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (26519/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (27719/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (28911/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (30113/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (31305/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (32499/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (33684/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (34875/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (36051/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (37235/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (38427/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (39625/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (40809/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (41998/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (43188/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (44388/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (45580/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (46746/50000)
# TEST : Loss: (0.4131) | Acc: (87.00%) (8783/10000)
percent tensor([0.5376, 0.4624], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.6506, 0.3494], device='cuda:0')
percent tensor([0.6027, 0.3973], device='cuda:0')
percent tensor([0.7598, 0.2402], device='cuda:0')
percent tensor([0.6726, 0.3274], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (4949/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (6132/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (7327/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (8529/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (9738/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (10948/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (94.00%) (12157/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (13366/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (94.00%) (14565/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (15779/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (16986/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (18188/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (94.00%) (19373/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (20572/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (94.00%) (21788/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (22981/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (24184/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (94.00%) (25404/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (94.00%) (26613/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (94.00%) (27815/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (94.00%) (29018/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (94.00%) (30214/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (94.00%) (31409/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1797) |  Loss2: (0.0000) | Acc: (93.00%) (32602/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (33799/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34996/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (36192/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (37398/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (38594/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (39792/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (41000/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (42200/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (43384/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (44574/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (45767/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (46939/50000)
# TEST : Loss: (0.4152) | Acc: (87.00%) (8742/10000)
percent tensor([0.5378, 0.4622], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.6509, 0.3491], device='cuda:0')
percent tensor([0.6010, 0.3990], device='cuda:0')
percent tensor([0.7583, 0.2417], device='cuda:0')
percent tensor([0.6711, 0.3289], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (3737/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (4928/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (6130/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (7327/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (8519/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (9725/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (10921/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (12140/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (13331/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (15750/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (16966/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (18175/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (19374/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (20575/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (21798/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (22997/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (24202/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (25404/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (26609/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (27815/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (29018/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (30228/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (31432/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (32649/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (33852/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (94.00%) (35056/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (36252/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (37456/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (94.00%) (38655/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (39853/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (41046/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (42269/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (43471/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (44661/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (45862/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (47017/50000)
# TEST : Loss: (0.4073) | Acc: (87.00%) (8752/10000)
percent tensor([0.5379, 0.4621], device='cuda:0')
percent tensor([0.5034, 0.4966], device='cuda:0')
percent tensor([0.5303, 0.4697], device='cuda:0')
percent tensor([0.6504, 0.3496], device='cuda:0')
percent tensor([0.6004, 0.3996], device='cuda:0')
percent tensor([0.7591, 0.2409], device='cuda:0')
percent tensor([0.6694, 0.3306], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (3656/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (4796/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (5952/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (7100/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (90.00%) (8264/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (9405/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (10569/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (11725/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (12889/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (14055/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (15220/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (16394/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (17563/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (18734/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (19903/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (21085/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (90.00%) (22245/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (23418/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (24588/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (25757/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (26929/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (28093/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (29273/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (30458/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (31616/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (32804/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (33987/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (35171/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (36347/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (37518/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (38693/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (39869/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (41051/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (42242/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (43423/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (44606/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (45746/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_115.pth.tar'
# TEST : Loss: (0.4421) | Acc: (86.00%) (8668/10000)
percent tensor([0.5381, 0.4619], device='cuda:0')
percent tensor([0.5013, 0.4987], device='cuda:0')
percent tensor([0.5282, 0.4718], device='cuda:0')
percent tensor([0.6793, 0.3207], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.7679, 0.2321], device='cuda:0')
percent tensor([0.6445, 0.3555], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (4845/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (7226/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (8411/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (9591/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (10778/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (11955/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (13137/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (14328/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (15505/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (16701/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (17886/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (19085/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (20272/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (21449/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (22635/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (23840/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (25040/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (26218/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (27416/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.2083) |  Loss2: (0.0000) | Acc: (92.00%) (28598/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (29796/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (30999/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (32196/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (33397/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (34586/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (35781/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (36959/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (38156/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (39336/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (92.00%) (40510/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (41702/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (42893/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (44088/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (45272/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (46428/50000)
# TEST : Loss: (0.4204) | Acc: (87.00%) (8723/10000)
percent tensor([0.5340, 0.4660], device='cuda:0')
percent tensor([0.5003, 0.4997], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.6769, 0.3231], device='cuda:0')
percent tensor([0.6023, 0.3977], device='cuda:0')
percent tensor([0.7723, 0.2277], device='cuda:0')
percent tensor([0.6511, 0.3489], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (4912/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (6094/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (7294/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (8496/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (9696/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (10898/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (12075/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (13257/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (14454/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (15642/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (16840/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (18040/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (19232/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (20439/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (21639/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (22837/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (24037/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (25236/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (26433/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (27637/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (28833/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (30019/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (31224/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (32417/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (33622/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (34814/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (36003/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (37185/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (38368/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (39569/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (40762/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (41962/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (43136/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (44336/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (45531/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (46699/50000)
# TEST : Loss: (0.4065) | Acc: (87.00%) (8759/10000)
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5327, 0.4673], device='cuda:0')
percent tensor([0.6804, 0.3196], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.7753, 0.2247], device='cuda:0')
percent tensor([0.6653, 0.3347], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (3705/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (4910/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (6118/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (7308/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (8505/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (9691/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (10890/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (12083/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (13277/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (14461/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (15683/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (16881/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (18078/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (19275/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (20469/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (21671/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (22871/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (24083/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (25282/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (26475/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (27674/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (28867/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (30074/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (31273/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (32482/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (33688/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (34896/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (36098/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (37301/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (38515/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (39717/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (40916/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (42107/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (43298/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (44491/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (45692/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (46845/50000)
# TEST : Loss: (0.4003) | Acc: (87.00%) (8765/10000)
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6792, 0.3208], device='cuda:0')
percent tensor([0.6021, 0.3979], device='cuda:0')
percent tensor([0.7747, 0.2253], device='cuda:0')
percent tensor([0.6679, 0.3321], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (1323/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (4929/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (6134/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1857) |  Loss2: (0.0000) | Acc: (93.00%) (7329/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (8523/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (9723/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (10922/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (12120/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (13313/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (14506/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (15710/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (16902/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (18109/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (19297/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (20482/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (21686/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (22899/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (24110/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (25308/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (26501/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (27701/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (28904/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (30105/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (31286/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (32487/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (33685/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (34897/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (36099/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (37313/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (38536/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (39729/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (40926/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (42125/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (43328/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (44538/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (45745/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (46885/50000)
# TEST : Loss: (0.3952) | Acc: (87.00%) (8786/10000)
percent tensor([0.5367, 0.4633], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5318, 0.4682], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.7814, 0.2186], device='cuda:0')
percent tensor([0.6737, 0.3263], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (3718/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (6121/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (7322/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (8496/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (9693/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (10890/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (12096/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (13284/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (14469/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (15656/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (16857/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (18053/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (19267/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (20468/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (21669/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (22869/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (24068/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (25265/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (26483/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (27682/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (28870/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1830) |  Loss2: (0.0000) | Acc: (93.00%) (30080/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (31289/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (32496/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (33693/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (34895/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (36094/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (37284/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (38495/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (39707/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (40908/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (42104/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (43313/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (44515/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (45732/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (46896/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_120.pth.tar'
# TEST : Loss: (0.4225) | Acc: (87.00%) (8746/10000)
percent tensor([0.5366, 0.4634], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5327, 0.4673], device='cuda:0')
percent tensor([0.6745, 0.3255], device='cuda:0')
percent tensor([0.6060, 0.3940], device='cuda:0')
percent tensor([0.7799, 0.2201], device='cuda:0')
percent tensor([0.6752, 0.3248], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.5521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(817.1996, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.1428, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.2561, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(492.3895, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2250.1016, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4265.7256, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1386.7239, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6197.7305, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11800.3975, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3900.0374, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16512.4141, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (3744/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (4943/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (6151/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (7373/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (8573/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (9771/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (10975/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (12179/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (13381/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (14588/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (15799/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (17014/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (18211/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (19416/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (20617/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (21813/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (23028/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (24230/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (25422/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (26618/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (27830/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (29034/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (30236/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (31440/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (32642/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (33857/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (35062/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1702) |  Loss2: (0.0000) | Acc: (94.00%) (36263/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (37465/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (38661/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (39856/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (41061/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (42270/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (94.00%) (43476/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (44690/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (45889/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (47038/50000)
# TEST : Loss: (0.4114) | Acc: (87.00%) (8773/10000)
percent tensor([0.5362, 0.4638], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5314, 0.4686], device='cuda:0')
percent tensor([0.6741, 0.3259], device='cuda:0')
percent tensor([0.6043, 0.3957], device='cuda:0')
percent tensor([0.7801, 0.2199], device='cuda:0')
percent tensor([0.6745, 0.3255], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (4967/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (6177/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (7394/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (8605/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (9810/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (11020/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (12238/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (13449/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (14651/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (15864/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (17054/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (18261/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (19461/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (20670/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (21874/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (23073/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (24265/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (25481/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (26686/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (27881/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (29083/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (30279/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (31482/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (32696/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (33892/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (35099/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (36296/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (37501/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (38701/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (39895/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (41103/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (42324/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (43522/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (44712/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (45912/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (47061/50000)
# TEST : Loss: (0.4060) | Acc: (87.00%) (8799/10000)
percent tensor([0.5362, 0.4638], device='cuda:0')
percent tensor([0.4992, 0.5008], device='cuda:0')
percent tensor([0.5318, 0.4682], device='cuda:0')
percent tensor([0.6739, 0.3261], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.7804, 0.2196], device='cuda:0')
percent tensor([0.6751, 0.3249], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (2551/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (6184/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (7391/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (8600/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (9818/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (11036/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (12262/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (13489/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (15890/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (17100/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (18308/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (19536/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (20737/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (21940/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (23163/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (24361/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (25562/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (26782/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (27990/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (29201/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (30405/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (31620/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (32846/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (34054/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (35272/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (36473/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (37689/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (38897/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (40093/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (41295/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (42506/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (43717/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (44909/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (46104/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (47268/50000)
# TEST : Loss: (0.4262) | Acc: (87.00%) (8752/10000)
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.4992, 0.5008], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.6721, 0.3279], device='cuda:0')
percent tensor([0.6078, 0.3922], device='cuda:0')
percent tensor([0.7807, 0.2193], device='cuda:0')
percent tensor([0.6759, 0.3241], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (2552/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (3753/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (6158/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (7366/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (8582/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (9805/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (11007/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (12218/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (13446/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (14653/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (15861/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (17072/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (18297/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (19492/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (20689/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (21903/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (23111/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (24320/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (25526/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (26738/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (27949/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1592) |  Loss2: (0.0000) | Acc: (94.00%) (29151/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (30358/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (31570/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (32792/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (33992/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (35209/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (36408/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (37613/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (38816/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (40030/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (41225/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (42429/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (43623/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (44814/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (46015/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (47176/50000)
# TEST : Loss: (0.4436) | Acc: (87.00%) (8703/10000)
percent tensor([0.5364, 0.4636], device='cuda:0')
percent tensor([0.4991, 0.5009], device='cuda:0')
percent tensor([0.5320, 0.4680], device='cuda:0')
percent tensor([0.6728, 0.3272], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.7784, 0.2216], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (3720/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (4905/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (8477/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (9672/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (10857/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (12061/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (13232/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (14438/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (15624/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (16819/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (18008/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (19204/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (20392/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (21602/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (22795/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (23990/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (25178/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (26385/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (27596/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (28798/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (29986/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (31176/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (32378/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (33569/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (34763/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (35955/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (37158/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (38345/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (39548/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (40757/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (41947/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (43149/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (44346/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (45559/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (46705/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_125.pth.tar'
# TEST : Loss: (0.4096) | Acc: (87.00%) (8787/10000)
percent tensor([0.5429, 0.4571], device='cuda:0')
percent tensor([0.4976, 0.5024], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.7102, 0.2898], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.6996, 0.3004], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (3750/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (4942/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (6142/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (93.00%) (7336/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (8527/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (9709/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (10920/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (12130/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (13335/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (14535/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (15728/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (16935/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (18142/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (19367/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (93.00%) (20565/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (93.00%) (21761/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (93.00%) (22963/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (24192/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (25401/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (26615/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (27825/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (29041/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (30260/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (31470/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (32666/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (33876/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (35071/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (36270/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (37471/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (38673/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (39883/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (41108/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (42306/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (43520/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (44730/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (45937/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (47106/50000)
# TEST : Loss: (0.3946) | Acc: (88.00%) (8821/10000)
percent tensor([0.5424, 0.4576], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.7171, 0.2829], device='cuda:0')
percent tensor([0.6098, 0.3902], device='cuda:0')
percent tensor([0.7200, 0.2800], device='cuda:0')
percent tensor([0.7187, 0.2813], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (4972/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (6197/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (7405/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (8610/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (11022/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (12232/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (13461/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (14662/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (15884/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (17095/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (18306/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (19512/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (20712/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (21931/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (23133/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (24346/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (25570/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (26763/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (27968/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (29181/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (30376/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (31588/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (32803/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (34012/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (35212/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (36418/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (37627/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (38844/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (40056/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (41265/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (42466/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (43664/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (44876/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (46087/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (47263/50000)
# TEST : Loss: (0.3890) | Acc: (88.00%) (8827/10000)
percent tensor([0.5395, 0.4605], device='cuda:0')
percent tensor([0.4965, 0.5035], device='cuda:0')
percent tensor([0.5339, 0.4661], device='cuda:0')
percent tensor([0.7144, 0.2856], device='cuda:0')
percent tensor([0.6140, 0.3860], device='cuda:0')
percent tensor([0.7220, 0.2780], device='cuda:0')
percent tensor([0.7238, 0.2762], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (2561/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (95.00%) (3774/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (4982/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (6205/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (7413/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (8622/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (9842/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (11056/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (12264/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (13478/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (14692/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (15912/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (17127/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (18339/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (19543/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (20736/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (21947/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (23161/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (24368/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (25583/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (26797/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (28011/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (29220/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (30428/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (31650/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (32859/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (34075/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (35294/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (36500/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (37710/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (38912/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (40119/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (41339/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (42565/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (43777/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (44982/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (46201/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (47369/50000)
# TEST : Loss: (0.3844) | Acc: (88.00%) (8847/10000)
percent tensor([0.5391, 0.4609], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.7110, 0.2890], device='cuda:0')
percent tensor([0.6190, 0.3810], device='cuda:0')
percent tensor([0.7269, 0.2731], device='cuda:0')
percent tensor([0.7295, 0.2705], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (2547/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (6177/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (7386/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (8611/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (9814/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (11021/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (12241/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (13458/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (14677/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (15885/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (17086/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (18308/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (19516/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (20731/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (21957/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (23176/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (24404/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (25614/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (26822/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (28029/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (29239/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (30457/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (31660/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (32869/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (34082/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (35279/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (36511/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (37734/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (38937/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (40157/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (41364/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (42598/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (43806/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (45012/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (46221/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (47384/50000)
# TEST : Loss: (0.3833) | Acc: (88.00%) (8840/10000)
percent tensor([0.5386, 0.4614], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5342, 0.4658], device='cuda:0')
percent tensor([0.7126, 0.2874], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.7298, 0.2702], device='cuda:0')
percent tensor([0.7277, 0.2723], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (4966/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (6191/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (7396/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (8598/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (9815/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (11039/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (12258/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (13454/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (14674/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (15881/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (17075/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (18298/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (19514/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (20731/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (21943/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (23143/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1591) |  Loss2: (0.0000) | Acc: (94.00%) (24340/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (25548/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (26742/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (27950/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (29157/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (30357/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (31570/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (32764/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (33971/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (35184/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1604) |  Loss2: (0.0000) | Acc: (94.00%) (36400/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (37596/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (38804/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (40016/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (41206/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (42419/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (43630/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (44850/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (46049/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (47210/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_130.pth.tar'
# TEST : Loss: (0.5042) | Acc: (85.00%) (8530/10000)
percent tensor([0.5383, 0.4617], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5346, 0.4654], device='cuda:0')
percent tensor([0.7135, 0.2865], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.7281, 0.2719], device='cuda:0')
percent tensor([0.7229, 0.2771], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.9367, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.1784, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.7265, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.6671, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(490.6533, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2256.9438, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4262.6689, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1381.6877, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6212.6240, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11766.8301, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3884.9946, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16446.6035, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (3745/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (6164/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (8593/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (9823/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (11044/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (12249/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (13468/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (14679/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (15896/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (17107/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (18311/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (19515/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (20722/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (21918/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (23128/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (24332/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (25523/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (26744/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (27959/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (29180/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (30401/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (31612/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (32818/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (34033/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (35245/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (36443/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (37648/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (38860/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (40071/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (41280/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (42485/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (43697/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (44917/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (46135/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (47299/50000)
# TEST : Loss: (0.4117) | Acc: (87.00%) (8755/10000)
percent tensor([0.5382, 0.4618], device='cuda:0')
percent tensor([0.4968, 0.5032], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.7127, 0.2873], device='cuda:0')
percent tensor([0.6189, 0.3811], device='cuda:0')
percent tensor([0.7311, 0.2689], device='cuda:0')
percent tensor([0.7242, 0.2758], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (4971/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (6179/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (7382/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (8583/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (9797/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (11024/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (12248/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (13461/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (14664/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (15890/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (17103/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (18333/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (19552/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (20768/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (21993/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (23202/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (24421/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (25620/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (26836/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (28060/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (29272/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (30494/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (31704/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (32924/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (34134/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (35339/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (36541/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (37750/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (38949/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (40147/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (41357/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (42563/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (43783/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (45001/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (46219/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (47384/50000)
# TEST : Loss: (0.4766) | Acc: (86.00%) (8632/10000)
percent tensor([0.5384, 0.4616], device='cuda:0')
percent tensor([0.4968, 0.5032], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.7122, 0.2878], device='cuda:0')
percent tensor([0.6185, 0.3815], device='cuda:0')
percent tensor([0.7299, 0.2701], device='cuda:0')
percent tensor([0.7250, 0.2750], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (94.00%) (3768/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (6211/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (7429/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (8646/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (9865/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (11079/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (12294/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (13505/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (14720/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (15942/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (17133/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (18353/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (19564/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (20781/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (21996/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (23221/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (24427/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (25618/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (26835/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (28058/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (29270/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (30486/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (31694/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (32918/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (34154/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (35374/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (36591/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (37794/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (39012/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (40251/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (41466/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (42682/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (43889/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (45094/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (46299/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (47454/50000)
# TEST : Loss: (0.4483) | Acc: (86.00%) (8690/10000)
percent tensor([0.5385, 0.4615], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.7116, 0.2884], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.7279, 0.2721], device='cuda:0')
percent tensor([0.7234, 0.2766], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (2551/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (6198/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (7428/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (8656/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (9871/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (11095/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (12309/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (13532/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (14730/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (95.00%) (15941/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (17165/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (18392/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (19600/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (95.00%) (20809/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (22032/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (23252/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (95.00%) (24463/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (95.00%) (25669/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (26880/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (28083/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (29302/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (30513/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (31723/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (32944/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (34148/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (35370/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (36595/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (37805/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (39029/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (40238/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (41461/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (42658/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (43871/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (45101/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (46316/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (47481/50000)
# TEST : Loss: (0.4046) | Acc: (88.00%) (8848/10000)
percent tensor([0.5384, 0.4616], device='cuda:0')
percent tensor([0.4969, 0.5031], device='cuda:0')
percent tensor([0.5344, 0.4656], device='cuda:0')
percent tensor([0.7112, 0.2888], device='cuda:0')
percent tensor([0.6201, 0.3799], device='cuda:0')
percent tensor([0.7285, 0.2715], device='cuda:0')
percent tensor([0.7260, 0.2740], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (4943/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (6142/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (7342/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (8521/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (9716/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (10925/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (12136/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (13333/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (15755/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (16953/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (18148/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (19345/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (20540/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (21749/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (22955/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (24178/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (25377/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (26580/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (27788/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (28989/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (30196/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (31389/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (32581/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (93.00%) (33776/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (34994/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (36204/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (37404/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (38606/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (39814/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (41034/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (93.00%) (42227/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (43427/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (93.00%) (44626/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (93.00%) (45824/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (46991/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_135.pth.tar'
# TEST : Loss: (0.4208) | Acc: (87.00%) (8789/10000)
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.4980, 0.5020], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.6990, 0.3010], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.7385, 0.2615], device='cuda:0')
percent tensor([0.7064, 0.2936], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (1340/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (6182/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (7381/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (8596/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (11040/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (12245/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (13451/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (15866/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (17071/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (18292/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (19504/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (20723/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (21935/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (23163/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (24369/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (25590/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (26796/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (27999/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (29208/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (30434/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (31649/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (32855/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (34084/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (35308/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (36528/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (37748/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (38966/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (40184/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (41407/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (42616/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (43839/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (45024/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (46242/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (47417/50000)
# TEST : Loss: (0.4006) | Acc: (88.00%) (8837/10000)
percent tensor([0.5409, 0.4591], device='cuda:0')
percent tensor([0.4989, 0.5011], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.7032, 0.2968], device='cuda:0')
percent tensor([0.6219, 0.3781], device='cuda:0')
percent tensor([0.7463, 0.2537], device='cuda:0')
percent tensor([0.7171, 0.2829], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (2559/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (3770/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (4983/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (6192/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (7415/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (8643/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (9862/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (11084/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (12288/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (13501/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (14718/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (15932/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (17151/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (94.00%) (18360/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (19582/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (20800/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (22021/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (23232/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (24454/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (25679/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (28121/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (29331/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (30542/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (31770/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (32980/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (34201/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (35402/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (36618/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (37827/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (39047/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (40262/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (41472/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (42690/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (43929/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (45150/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (46358/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (47518/50000)
# TEST : Loss: (0.3916) | Acc: (88.00%) (8877/10000)
percent tensor([0.5434, 0.4566], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5326, 0.4674], device='cuda:0')
percent tensor([0.7053, 0.2947], device='cuda:0')
percent tensor([0.6219, 0.3781], device='cuda:0')
percent tensor([0.7534, 0.2466], device='cuda:0')
percent tensor([0.7190, 0.2810], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (94.00%) (3768/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (6212/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (94.00%) (7415/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (8636/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (9861/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (11072/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (94.00%) (12277/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (13498/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (14724/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (15952/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (17174/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (18390/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (19610/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (20831/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (22043/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (23242/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (24467/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (25683/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (26900/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (28117/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (29340/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (30553/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (31784/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (33007/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (34232/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (35452/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (36670/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (37887/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (39110/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (40334/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (41568/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (42784/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (44015/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (45225/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (46432/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (47615/50000)
# TEST : Loss: (0.3805) | Acc: (88.00%) (8899/10000)
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5007, 0.4993], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.7048, 0.2952], device='cuda:0')
percent tensor([0.6224, 0.3776], device='cuda:0')
percent tensor([0.7520, 0.2480], device='cuda:0')
percent tensor([0.7248, 0.2752], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (2560/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (3784/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (5015/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (6244/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (7462/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (8681/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (9894/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (11117/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (12330/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (13545/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (14761/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (15970/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (17188/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (18407/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (19619/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (20831/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (22050/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (23269/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (24489/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (25709/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (26933/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (28162/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (29384/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (30618/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (31847/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (33076/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (34294/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (35529/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (36758/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (37978/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (39187/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (40422/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (41645/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (42881/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (44094/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (45321/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (46553/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (47725/50000)
# TEST : Loss: (0.3796) | Acc: (89.00%) (8910/10000)
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5300, 0.4700], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.6289, 0.3711], device='cuda:0')
percent tensor([0.7543, 0.2457], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (5016/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (6238/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (7449/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (8676/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (9896/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (11114/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (12332/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (13558/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (14766/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (15994/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (17218/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (18425/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (19649/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (20874/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (22090/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (23309/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (24524/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (25738/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (26949/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (28176/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (29379/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (30611/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (31820/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (33036/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (34249/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (35469/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (36692/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (37916/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (39144/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (40359/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (41578/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (42784/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (44002/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (45222/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (46429/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (47589/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_140.pth.tar'
# TEST : Loss: (0.4464) | Acc: (87.00%) (8727/10000)
percent tensor([0.5427, 0.4573], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.7028, 0.2972], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.7536, 0.2464], device='cuda:0')
percent tensor([0.7231, 0.2769], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.4459, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.8405, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.0748, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.9574, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(489.0450, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2263.4875, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4260.0249, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1376.8384, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6225.7100, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11734.2881, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3869.9558, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16381.1514, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (3774/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (4987/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (6202/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (7421/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (94.00%) (8627/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (94.00%) (9839/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (94.00%) (11058/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (12293/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (13522/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (14737/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (15964/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (17184/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (18413/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (19629/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (20848/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (22075/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (23301/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (24530/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (25748/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (26960/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (28175/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (29388/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (30605/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (31808/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (33023/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (34245/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (35464/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (36674/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (37882/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (39103/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (40329/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (41524/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (42736/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (43954/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (45188/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (46406/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (47569/50000)
# TEST : Loss: (0.4577) | Acc: (87.00%) (8741/10000)
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.7022, 0.2978], device='cuda:0')
percent tensor([0.6295, 0.3705], device='cuda:0')
percent tensor([0.7552, 0.2448], device='cuda:0')
percent tensor([0.7328, 0.2672], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (2574/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (5005/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (6228/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (7462/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (8677/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (9893/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (11112/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (12345/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (13568/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (14796/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (16001/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (17223/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (18438/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (19662/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (20883/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (22093/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (23313/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (24541/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (25767/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (26993/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (28217/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (29440/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (30678/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (31904/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (33117/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (34339/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (35543/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (36763/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (37989/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (39212/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (40428/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (41637/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (42870/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (44095/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (45292/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (46505/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (47676/50000)
# TEST : Loss: (0.4069) | Acc: (88.00%) (8820/10000)
percent tensor([0.5421, 0.4579], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.7004, 0.2996], device='cuda:0')
percent tensor([0.6287, 0.3713], device='cuda:0')
percent tensor([0.7553, 0.2447], device='cuda:0')
percent tensor([0.7263, 0.2737], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (3806/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (6250/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (8721/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (9951/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (11167/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (12389/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (13618/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (14853/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (16081/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (17302/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (18525/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (19761/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (20990/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (22211/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (23437/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (24670/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (25892/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (27113/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (28339/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (29555/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (30779/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (31992/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (33212/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (34420/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (35636/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (36861/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (38089/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (39304/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (40529/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (41769/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (42989/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (44219/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (45437/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (46654/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (47825/50000)
# TEST : Loss: (0.3961) | Acc: (88.00%) (8860/10000)
percent tensor([0.5419, 0.4581], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.7025, 0.2975], device='cuda:0')
percent tensor([0.6258, 0.3742], device='cuda:0')
percent tensor([0.7500, 0.2500], device='cuda:0')
percent tensor([0.7260, 0.2740], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (2546/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (5013/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (6235/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (7459/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (8701/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (9922/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (11152/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (12367/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (13602/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (14827/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (16047/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (17268/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (18469/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (19701/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (20916/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (22130/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (23346/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (24566/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (25796/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (27030/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (28259/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (29468/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (30693/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (31911/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (33130/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (34348/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (35569/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (36796/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (38030/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (39248/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (40456/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (41689/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (42894/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (44104/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (45325/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (46551/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (47724/50000)
# TEST : Loss: (0.4133) | Acc: (88.00%) (8814/10000)
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5301, 0.4699], device='cuda:0')
percent tensor([0.7002, 0.2998], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.7526, 0.2474], device='cuda:0')
percent tensor([0.7320, 0.2680], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (3796/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (6235/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (7424/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8623/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (9840/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (11060/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (12266/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (13477/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (14680/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (15884/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1502) |  Loss2: (0.0000) | Acc: (94.00%) (17101/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (18309/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (19526/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (20740/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (21958/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (23179/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (24390/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (25599/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (26800/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (28010/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (29228/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (30450/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (31653/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (32871/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (34074/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (35284/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (36510/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (37727/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (38949/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (40156/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (41375/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (42597/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (43818/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (45036/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (46260/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (94.00%) (47437/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_145.pth.tar'
# TEST : Loss: (0.3941) | Acc: (88.00%) (8875/10000)
percent tensor([0.5447, 0.4553], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5307, 0.4693], device='cuda:0')
percent tensor([0.7024, 0.2976], device='cuda:0')
percent tensor([0.6362, 0.3638], device='cuda:0')
percent tensor([0.7426, 0.2574], device='cuda:0')
percent tensor([0.7299, 0.2701], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (4990/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (6214/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (7425/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (8649/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (9869/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (11090/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (12304/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (13528/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (14761/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (15980/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (17202/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (18424/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (19644/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (20865/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (22089/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (23315/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (24551/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (25776/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (26998/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (28221/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (29444/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (30665/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (31898/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (33119/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (34341/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (35562/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (36794/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (38026/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (39249/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (40486/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (41711/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (42933/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (44151/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (45381/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (46610/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (47797/50000)
# TEST : Loss: (0.3804) | Acc: (89.00%) (8907/10000)
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.5016, 0.4984], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6973, 0.3027], device='cuda:0')
percent tensor([0.6354, 0.3646], device='cuda:0')
percent tensor([0.7440, 0.2560], device='cuda:0')
percent tensor([0.7356, 0.2644], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (6263/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (7487/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (8713/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (9948/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (11181/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (12395/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (13618/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (14846/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (16074/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (17305/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (18535/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (19760/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (20985/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (22223/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (23448/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (24668/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (25903/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (27134/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (28357/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (29593/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (30813/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (32042/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (33265/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (34501/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (35729/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (36964/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (38185/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (39417/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (40654/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (41881/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (43105/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (44337/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (45553/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (46784/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (47958/50000)
# TEST : Loss: (0.3739) | Acc: (89.00%) (8930/10000)
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.7012, 0.2988], device='cuda:0')
percent tensor([0.6375, 0.3625], device='cuda:0')
percent tensor([0.7445, 0.2555], device='cuda:0')
percent tensor([0.7339, 0.2661], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (3825/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (5061/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (6292/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (7525/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (8755/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (9982/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (11212/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (12445/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (13672/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (14882/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (16114/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (17354/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (18587/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (19811/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (21044/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (22283/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (23512/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (24742/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (25970/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (27209/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (28422/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (29643/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (30876/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (32113/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (33345/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (34576/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (35809/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (37044/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (38280/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (39496/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (40728/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (41958/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (43194/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (44419/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (45637/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (46864/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (48048/50000)
# TEST : Loss: (0.3714) | Acc: (89.00%) (8929/10000)
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.7015, 0.2985], device='cuda:0')
percent tensor([0.6378, 0.3622], device='cuda:0')
percent tensor([0.7519, 0.2481], device='cuda:0')
percent tensor([0.7360, 0.2640], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (3828/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (5057/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (7524/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (8750/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (9981/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (11213/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (12449/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (13682/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (14897/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (16129/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (17360/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (18596/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (19832/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (21066/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (22294/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (23530/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (24762/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (25991/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (27223/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (28446/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (29682/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (30917/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (32145/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (33369/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (34587/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (35817/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (37064/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (38302/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (39537/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (40773/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (42009/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (43242/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (44485/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (45714/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (46949/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (48130/50000)
# TEST : Loss: (0.3633) | Acc: (89.00%) (8934/10000)
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.7013, 0.2987], device='cuda:0')
percent tensor([0.6350, 0.3650], device='cuda:0')
percent tensor([0.7546, 0.2454], device='cuda:0')
percent tensor([0.7325, 0.2675], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (5017/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (7469/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (8694/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (9923/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (11158/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (12376/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (13587/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (14807/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (16036/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (17261/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (18478/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (19706/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (20922/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (22143/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (23355/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (24586/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (25814/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (27031/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (28251/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (29467/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (30692/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (31917/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (33145/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (34358/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (35575/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (36794/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (38020/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (39253/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (40463/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (41682/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (42887/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (44109/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (45341/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (46562/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (47739/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_150.pth.tar'
# TEST : Loss: (0.4282) | Acc: (88.00%) (8812/10000)
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.5358, 0.4642], device='cuda:0')
percent tensor([0.7019, 0.2981], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.7572, 0.2428], device='cuda:0')
percent tensor([0.7384, 0.2616], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.8385, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.2549, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.4628, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1518.5597, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(487.4081, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2269.4709, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4257.3979, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1371.7886, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6239.7417, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11701.3828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3854.9773, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16315.9082, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 151 | Batch_idx: 0 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (3808/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (6240/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (7474/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (8703/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (9936/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (11166/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (12387/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (13592/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (14815/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (16047/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (17273/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (18508/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (19734/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (20960/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (22180/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (23404/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (24632/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (25855/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (27093/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (28331/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (29576/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (30802/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (32034/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (33251/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (34483/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (35703/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (36916/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (38126/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (39339/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (40570/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (41797/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (43026/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (44248/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (45468/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (46700/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (47885/50000)
# TEST : Loss: (0.4596) | Acc: (87.00%) (8728/10000)
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.6332, 0.3668], device='cuda:0')
percent tensor([0.7546, 0.2454], device='cuda:0')
percent tensor([0.7323, 0.2677], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (3815/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (96.00%) (5042/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (96.00%) (6276/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (7505/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (8736/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (9968/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (11195/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (12426/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (13652/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (14881/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (16113/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (17350/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (18574/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (19813/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (21046/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (22275/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (23490/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (24708/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (25929/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (27157/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (28373/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (29597/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (30815/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (32044/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (33257/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (34492/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (35719/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (36939/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (38170/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (39393/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (40614/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (41847/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1205) |  Loss2: (0.0000) | Acc: (95.00%) (43066/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (44273/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (45494/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (46715/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (47884/50000)
# TEST : Loss: (0.4175) | Acc: (88.00%) (8839/10000)
percent tensor([0.5469, 0.4531], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.7011, 0.2989], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.7547, 0.2453], device='cuda:0')
percent tensor([0.7312, 0.2688], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (3804/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (6262/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (7492/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (8728/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (9965/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (11200/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (12428/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (13661/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (14884/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (16116/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (17351/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (18584/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (19820/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (21046/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (22268/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (23494/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (24719/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (25953/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (27178/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (28409/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (29639/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (30878/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (32106/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (33328/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (34557/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (35780/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (37012/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (38236/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (39474/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (40703/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (41931/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (43166/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (44393/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (45624/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (46853/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (48032/50000)
# TEST : Loss: (0.4004) | Acc: (88.00%) (8857/10000)
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.6315, 0.3685], device='cuda:0')
percent tensor([0.7530, 0.2470], device='cuda:0')
percent tensor([0.7312, 0.2688], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (5078/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (6305/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (7532/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (8768/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (9996/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (11224/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (12455/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (13691/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (14905/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (16141/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (17376/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (18599/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (19821/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (21044/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (22260/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (23481/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (24703/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (95.00%) (25921/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (27147/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (28384/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (29602/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (30834/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (32060/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (33277/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (34518/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (35745/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (36969/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (38208/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (39440/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (40673/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (41891/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (43122/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (44350/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (45572/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (46795/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (47982/50000)
# TEST : Loss: (0.3975) | Acc: (88.00%) (8860/10000)
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5027, 0.4973], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6999, 0.3001], device='cuda:0')
percent tensor([0.6320, 0.3680], device='cuda:0')
percent tensor([0.7550, 0.2450], device='cuda:0')
percent tensor([0.7333, 0.2667], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (6250/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (7461/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (8673/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (9888/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (11108/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (12326/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (13542/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (14755/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (15965/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (17184/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (18398/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (19618/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (20835/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (22062/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (23272/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (24488/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (25714/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (26950/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (28167/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (29384/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (30603/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (31827/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (33056/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (34274/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (35494/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (36730/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (37951/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (39183/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (40410/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (41640/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (42855/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (44093/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (45316/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (46532/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (47715/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_155.pth.tar'
# TEST : Loss: (0.3988) | Acc: (88.00%) (8863/10000)
percent tensor([0.5522, 0.4478], device='cuda:0')
percent tensor([0.5024, 0.4976], device='cuda:0')
percent tensor([0.5383, 0.4617], device='cuda:0')
percent tensor([0.6957, 0.3043], device='cuda:0')
percent tensor([0.6391, 0.3609], device='cuda:0')
percent tensor([0.7499, 0.2501], device='cuda:0')
percent tensor([0.7122, 0.2878], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (7480/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (8704/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (9934/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (11164/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (12389/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (13614/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (14835/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (16057/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (17290/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (18517/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (19749/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (20983/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (22210/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (23444/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (24672/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (25900/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (27132/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (28362/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (29583/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (30818/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (32045/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (33260/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (34482/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (35696/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (36932/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (38147/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (39367/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (40603/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (41834/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (43053/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (44283/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (45505/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (46742/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (47917/50000)
# TEST : Loss: (0.3905) | Acc: (88.00%) (8886/10000)
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5447, 0.4553], device='cuda:0')
percent tensor([0.6947, 0.3053], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.7507, 0.2493], device='cuda:0')
percent tensor([0.7092, 0.2908], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (2588/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (3808/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (6264/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (7472/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8698/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (9934/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (11172/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (12403/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (13619/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (14851/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (16084/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (17314/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (18544/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (19773/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (21008/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (22239/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (23473/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (24705/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (25934/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (27171/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (28406/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (29634/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (30872/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (32107/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (33350/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (34597/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (35827/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (37054/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (38286/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (39512/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (40735/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (41976/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (43209/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (44443/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (45668/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (46911/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (48095/50000)
# TEST : Loss: (0.3869) | Acc: (88.00%) (8875/10000)
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5017, 0.4983], device='cuda:0')
percent tensor([0.5462, 0.4538], device='cuda:0')
percent tensor([0.6980, 0.3020], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.7518, 0.2482], device='cuda:0')
percent tensor([0.7137, 0.2863], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (2604/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (3826/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (5055/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (6288/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (7529/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (8760/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (9987/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (11223/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (12458/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (13692/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (14925/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (16160/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (17392/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (18633/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (19879/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (21111/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (22351/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (23591/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (24829/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (26059/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (27289/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (28527/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (29765/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (31002/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (32228/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (33469/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (34696/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (35923/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (37166/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (38405/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (39630/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (40857/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (42106/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (43343/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (44581/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (45823/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (47054/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (48245/50000)
# TEST : Loss: (0.3763) | Acc: (89.00%) (8930/10000)
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.5013, 0.4987], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.6415, 0.3585], device='cuda:0')
percent tensor([0.7558, 0.2442], device='cuda:0')
percent tensor([0.7139, 0.2861], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (2596/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (3836/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (5070/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (6305/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (7542/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (8773/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (10015/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (11258/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (12493/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (13726/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (14963/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (16200/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (17441/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (18684/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (19922/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (21158/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (22385/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23617/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (24861/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (26093/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (27330/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (28564/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (29806/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (31037/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (32268/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (33501/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (34748/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (35982/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (37206/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (38434/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (39678/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (40907/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (42130/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (43376/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (44616/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (45843/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (47075/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (48265/50000)
# TEST : Loss: (0.3758) | Acc: (89.00%) (8939/10000)
percent tensor([0.5519, 0.4481], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5442, 0.4558], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.6420, 0.3580], device='cuda:0')
percent tensor([0.7573, 0.2427], device='cuda:0')
percent tensor([0.7194, 0.2806], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (5059/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (6286/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (7512/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1141) |  Loss2: (0.0000) | Acc: (96.00%) (8743/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (9978/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (11220/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (12450/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (13682/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (14912/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (16149/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (17381/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (18605/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (19844/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (21065/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (22283/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (23507/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (24734/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (25963/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (27192/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (28413/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (29635/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (30867/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (32086/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (33318/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (34542/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (35773/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (36979/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (38213/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (39444/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (40678/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (41910/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (43129/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (44366/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (45587/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (46816/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (47996/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_160.pth.tar'
# TEST : Loss: (0.3947) | Acc: (88.00%) (8888/10000)
percent tensor([0.5515, 0.4485], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5446, 0.4554], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.6409, 0.3591], device='cuda:0')
percent tensor([0.7575, 0.2425], device='cuda:0')
percent tensor([0.7147, 0.2853], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.1226, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.6982, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.2842, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1516.7260, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.8406, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2274.7603, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4254.6851, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1366.6506, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6251.5820, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11668.4824, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3840.0583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16251.0430, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (5063/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (6292/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (7507/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (8746/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (9981/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (11212/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (12451/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (13676/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (14902/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (16129/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (96.00%) (17356/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (18600/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (19839/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (21069/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (22303/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (23538/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (24770/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (26006/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (27242/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (28487/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (29728/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (30959/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (32186/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (33411/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (34643/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (35863/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (37086/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (38324/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (39560/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (40798/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (42030/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (43252/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (44482/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (45713/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (46931/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (48114/50000)
# TEST : Loss: (0.4219) | Acc: (88.00%) (8800/10000)
percent tensor([0.5527, 0.4473], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.6439, 0.3561], device='cuda:0')
percent tensor([0.7569, 0.2431], device='cuda:0')
percent tensor([0.7150, 0.2850], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (3853/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (5082/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (6312/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (7552/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (8780/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (10014/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (11241/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (12485/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (13725/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (14956/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (16187/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (17425/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (18653/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (19884/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (21115/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (22347/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (23583/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (24814/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (26042/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (27275/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (28505/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (29739/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (30977/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (32208/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (33448/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (34675/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (35893/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (37127/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (38353/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (39592/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (40813/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (42038/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (43264/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (44502/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (45740/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (46974/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (48162/50000)
# TEST : Loss: (0.4097) | Acc: (88.00%) (8889/10000)
percent tensor([0.5524, 0.4476], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.6958, 0.3042], device='cuda:0')
percent tensor([0.6448, 0.3552], device='cuda:0')
percent tensor([0.7571, 0.2429], device='cuda:0')
percent tensor([0.7191, 0.2809], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1123) |  Loss2: (0.0000) | Acc: (95.00%) (2576/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (5044/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (95.00%) (6266/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (7487/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (8724/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (9965/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (11199/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (12434/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (13673/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (14903/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (16145/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (17367/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (18600/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (19832/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (21047/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (22275/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (23518/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (24761/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (25994/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (27233/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (28472/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (29693/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (30927/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (32157/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (33384/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (34607/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (35851/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (37080/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (38317/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (39557/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (40799/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (42027/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (43262/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (44493/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (45733/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (46965/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (48154/50000)
# TEST : Loss: (0.4079) | Acc: (88.00%) (8876/10000)
percent tensor([0.5523, 0.4477], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.6931, 0.3069], device='cuda:0')
percent tensor([0.6413, 0.3587], device='cuda:0')
percent tensor([0.7555, 0.2445], device='cuda:0')
percent tensor([0.7183, 0.2817], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (2619/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (97.00%) (3851/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (97.00%) (5096/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (6317/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (7550/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (8792/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (10033/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (11279/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (12507/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (13747/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (14989/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (16233/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (17473/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (18705/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (19939/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (21174/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (22409/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (23639/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (24877/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (26121/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (27361/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (28593/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (29828/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (31070/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (32298/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (33530/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (34779/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (36013/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (37265/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (38519/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (39755/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (40992/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (42222/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (43467/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (44702/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (45946/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (47188/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (48360/50000)
# TEST : Loss: (0.4285) | Acc: (88.00%) (8840/10000)
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.5012, 0.4988], device='cuda:0')
percent tensor([0.5433, 0.4567], device='cuda:0')
percent tensor([0.6941, 0.3059], device='cuda:0')
percent tensor([0.6408, 0.3592], device='cuda:0')
percent tensor([0.7566, 0.2434], device='cuda:0')
percent tensor([0.7139, 0.2861], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (6249/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (8695/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (9912/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (11127/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (12346/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (13574/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (14781/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (16004/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (17225/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (18462/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (19688/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (20912/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (22143/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (23351/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (24584/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (25817/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (27035/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (28253/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (29483/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (30698/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (31920/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (33147/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (34373/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (35608/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (36839/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (38065/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (39293/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (40516/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (41750/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (42974/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (44211/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (45433/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (46664/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (47846/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_165.pth.tar'
# TEST : Loss: (0.4118) | Acc: (88.00%) (8843/10000)
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.5008, 0.4992], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.6948, 0.3052], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.7156, 0.2844], device='cuda:0')
percent tensor([0.7157, 0.2843], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 166 | Batch_idx: 0 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (6271/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (7508/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (8746/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1114) |  Loss2: (0.0000) | Acc: (96.00%) (9975/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (11207/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (12415/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (13643/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (14874/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (96.00%) (16103/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (17335/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (18574/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (19817/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (21051/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (22282/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (23517/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (24745/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (25976/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (27208/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (28440/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (29672/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (30893/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (32130/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (33369/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (34608/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (35841/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (37074/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (38309/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (39550/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (40776/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (42007/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (43241/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (44480/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (45705/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (46934/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (48110/50000)
# TEST : Loss: (0.3937) | Acc: (89.00%) (8913/10000)
percent tensor([0.5584, 0.4416], device='cuda:0')
percent tensor([0.5011, 0.4989], device='cuda:0')
percent tensor([0.5513, 0.4487], device='cuda:0')
percent tensor([0.6958, 0.3042], device='cuda:0')
percent tensor([0.6539, 0.3461], device='cuda:0')
percent tensor([0.7202, 0.2798], device='cuda:0')
percent tensor([0.7187, 0.2813], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (3841/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (5073/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (6305/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (7530/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (8758/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (9992/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (11223/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (12456/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (13687/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (14917/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (16150/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (17381/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (18627/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (19863/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (21111/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (22349/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (23590/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (24830/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (26069/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (27310/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (28543/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (29791/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (31030/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (32277/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (33511/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (34757/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (36001/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (37234/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (38463/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (39692/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (40917/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (42148/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (43369/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (44602/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (45845/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (47083/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (48269/50000)
# TEST : Loss: (0.3887) | Acc: (89.00%) (8921/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.6996, 0.3004], device='cuda:0')
percent tensor([0.6595, 0.3405], device='cuda:0')
percent tensor([0.7206, 0.2794], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 168 | Batch_idx: 0 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (3860/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (5106/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (7581/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (97.00%) (8817/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (10049/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (11284/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (12512/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (13748/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (14981/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (16225/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (17464/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (18698/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (19922/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (21163/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (22403/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (23636/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (24882/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (26117/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (27361/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (28600/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (29826/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (31068/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (32313/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (33557/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (34784/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (36025/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (37265/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (38493/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (39737/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (40968/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (42192/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (43434/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (44672/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (45911/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (47149/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (48348/50000)
# TEST : Loss: (0.3822) | Acc: (89.00%) (8936/10000)
percent tensor([0.5545, 0.4455], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5508, 0.4492], device='cuda:0')
percent tensor([0.6993, 0.3007], device='cuda:0')
percent tensor([0.6585, 0.3415], device='cuda:0')
percent tensor([0.7223, 0.2777], device='cuda:0')
percent tensor([0.7248, 0.2752], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 169 | Batch_idx: 0 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (3835/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (5083/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (6320/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (7562/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (8797/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (10025/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (11273/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (12513/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (13748/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (14985/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (16224/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (17457/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (18694/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (19943/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (21177/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (22419/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (23649/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (24891/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (26126/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (27367/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (28607/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (29844/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (31085/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (32319/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (33549/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (34792/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (36028/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (37275/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (38501/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (39746/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (40988/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (42219/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (43457/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (44696/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (45944/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (47184/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (48384/50000)
# TEST : Loss: (0.3779) | Acc: (89.00%) (8943/10000)
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5509, 0.4491], device='cuda:0')
percent tensor([0.6978, 0.3022], device='cuda:0')
percent tensor([0.6587, 0.3413], device='cuda:0')
percent tensor([0.7256, 0.2744], device='cuda:0')
percent tensor([0.7317, 0.2683], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (3843/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (5075/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (6313/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (8770/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (10010/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (11244/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (12486/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (13728/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (14961/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (16199/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (17434/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (18657/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (19890/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (21136/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (22368/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (23607/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (24842/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (26080/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (27318/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (28546/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (29782/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (31014/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (32247/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (33473/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (34694/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (35927/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (37172/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (38403/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (39630/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (40867/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (42100/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (43328/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (44553/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (45786/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (47020/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (48184/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_170.pth.tar'
# TEST : Loss: (0.4661) | Acc: (87.00%) (8721/10000)
percent tensor([0.5551, 0.4449], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5518, 0.4482], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.6570, 0.3430], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.4512, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.0417, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.7928, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.2786, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(484.0652, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2279.0679, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4251.1191, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1361.8379, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6262.9683, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11634.8623, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3825.1570, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16186.0645, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 171 | Batch_idx: 0 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (1343/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (3800/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (6267/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (7497/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (8734/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (9973/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (11216/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (12442/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (13661/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (14898/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (16129/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (17363/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (18583/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (19818/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (21060/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (22291/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (23528/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (24755/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (25996/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (27240/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (28470/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (29711/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (30940/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (32178/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (33417/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (34643/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (35886/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (37116/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (38358/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (39600/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (40840/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (42074/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (43303/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (44535/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (45769/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (47005/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (48195/50000)
# TEST : Loss: (0.3986) | Acc: (89.00%) (8916/10000)
percent tensor([0.5548, 0.4452], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.6954, 0.3046], device='cuda:0')
percent tensor([0.6543, 0.3457], device='cuda:0')
percent tensor([0.7248, 0.2752], device='cuda:0')
percent tensor([0.7316, 0.2684], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 172 | Batch_idx: 0 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (3857/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (5100/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (7589/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (8831/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (10075/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (97.00%) (11312/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (97.00%) (12547/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (13776/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (15017/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (16243/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (17483/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (18718/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (19959/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (21184/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (22424/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (23653/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (24889/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (26133/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (27368/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (28600/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (29841/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (31082/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (32304/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (33539/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (34777/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (36015/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (37252/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (38493/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (39732/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (40956/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (42170/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (43415/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (44649/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (45886/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (47125/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (48317/50000)
# TEST : Loss: (0.4109) | Acc: (88.00%) (8862/10000)
percent tensor([0.5548, 0.4452], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5509, 0.4491], device='cuda:0')
percent tensor([0.6963, 0.3037], device='cuda:0')
percent tensor([0.6564, 0.3436], device='cuda:0')
percent tensor([0.7255, 0.2745], device='cuda:0')
percent tensor([0.7358, 0.2642], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 173 | Batch_idx: 0 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (97.00%) (3851/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (97.00%) (5091/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (6327/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (7569/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (8813/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (10044/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (11280/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (12519/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (13751/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (14991/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (16230/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (17467/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (18713/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (19956/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (21198/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (22448/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (23697/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (24939/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (26171/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (27407/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (28648/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (29888/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (31119/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (32359/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (33593/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (34832/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (36060/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (37299/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (38543/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (39781/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (41027/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (42269/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (43499/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (44735/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (45964/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (47200/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (48400/50000)
# TEST : Loss: (0.4054) | Acc: (88.00%) (8897/10000)
percent tensor([0.5548, 0.4452], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.6957, 0.3043], device='cuda:0')
percent tensor([0.6534, 0.3466], device='cuda:0')
percent tensor([0.7232, 0.2768], device='cuda:0')
percent tensor([0.7333, 0.2667], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 174 | Batch_idx: 0 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (2615/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (5088/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (6328/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (7567/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (8813/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (10048/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (11288/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (12517/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (13762/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (15005/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (16254/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (17502/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (18757/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (20006/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (21253/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (22497/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (23735/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (24976/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (26222/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (27453/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (28689/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (97.00%) (29926/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (31159/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (32399/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (33635/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (34879/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (36115/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (37358/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (38597/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (39837/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (41064/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (42307/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (43550/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (44790/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (46036/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (47275/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (48482/50000)
# TEST : Loss: (0.4491) | Acc: (87.00%) (8777/10000)
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5508, 0.4492], device='cuda:0')
percent tensor([0.6951, 0.3049], device='cuda:0')
percent tensor([0.6563, 0.3437], device='cuda:0')
percent tensor([0.7240, 0.2760], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1031) |  Loss2: (0.0000) | Acc: (96.00%) (5055/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (6293/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (7533/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (8758/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (9998/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (11225/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (12462/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (13684/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (14911/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (16158/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (17395/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (18628/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (19863/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (21093/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (22338/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (23566/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (24811/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (26054/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (27283/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (28511/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (29751/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (30979/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (32214/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (33456/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (34699/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (35932/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (37177/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (38410/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (39639/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (40879/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (42115/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (43346/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (44583/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (45815/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (47066/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (48252/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_175.pth.tar'
# TEST : Loss: (0.4065) | Acc: (88.00%) (8892/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5020, 0.4980], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6877, 0.3123], device='cuda:0')
percent tensor([0.6645, 0.3355], device='cuda:0')
percent tensor([0.7386, 0.2614], device='cuda:0')
percent tensor([0.7335, 0.2665], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 176 | Batch_idx: 0 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (2600/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (5081/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (6323/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (7567/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (8802/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (10040/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (11281/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (12522/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (13771/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (15015/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (16254/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (17492/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (18741/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (19979/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (21221/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (22463/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (23696/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (24925/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (26160/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (27394/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (28643/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (29863/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (31111/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (32354/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (33593/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (34829/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (36074/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (37320/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (38555/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (39806/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (41055/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (42292/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (43535/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (44777/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (46031/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (47263/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (48458/50000)
# TEST : Loss: (0.3970) | Acc: (88.00%) (8896/10000)
percent tensor([0.5475, 0.4525], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.6824, 0.3176], device='cuda:0')
percent tensor([0.6694, 0.3306], device='cuda:0')
percent tensor([0.7434, 0.2566], device='cuda:0')
percent tensor([0.7318, 0.2682], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 177 | Batch_idx: 0 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (3836/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (5075/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (6325/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (7553/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (8783/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (10031/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (11282/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (12532/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (13781/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (15027/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (16260/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (17489/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (18729/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (19968/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (21218/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (22453/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (23709/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (24958/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (26212/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (27438/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (28671/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (29913/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (31158/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (32400/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (33643/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (34891/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (36138/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (37392/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (38635/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (39882/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (41138/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (42382/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (43622/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (44868/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (46115/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (47357/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (48556/50000)
# TEST : Loss: (0.3893) | Acc: (89.00%) (8917/10000)
percent tensor([0.5486, 0.4514], device='cuda:0')
percent tensor([0.5009, 0.4991], device='cuda:0')
percent tensor([0.5462, 0.4538], device='cuda:0')
percent tensor([0.6787, 0.3213], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.7412, 0.2588], device='cuda:0')
percent tensor([0.7307, 0.2693], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 178 | Batch_idx: 0 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.1019) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.1016) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (3845/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (6329/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (7570/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (97.00%) (8822/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (97.00%) (10066/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (97.00%) (11311/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (97.00%) (12550/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (97.00%) (13794/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (97.00%) (15036/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (16283/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (97.00%) (17528/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (97.00%) (18776/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (20031/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (21278/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (22524/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (23775/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (25019/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (26264/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (27501/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (28749/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (29997/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (31246/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (32491/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (33741/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (34986/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (36230/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (37475/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (38717/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (39966/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (41216/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (42452/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (43706/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (44956/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (46196/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (47442/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (48636/50000)
# TEST : Loss: (0.3891) | Acc: (89.00%) (8919/10000)
percent tensor([0.5489, 0.4511], device='cuda:0')
percent tensor([0.5004, 0.4996], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6859, 0.3141], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.7389, 0.2611], device='cuda:0')
percent tensor([0.7305, 0.2695], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 179 | Batch_idx: 0 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (2615/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (6350/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (7596/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (8847/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (11341/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (12580/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (13826/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (15078/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (16335/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (17582/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (18813/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (20068/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (21310/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (22562/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (23804/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (25040/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (26289/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (27535/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (28765/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (30017/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (31261/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (32508/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (33756/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (34999/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (36236/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (37483/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (38723/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (39968/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (41212/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (42451/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (43701/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (44945/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (46206/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (47441/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (48647/50000)
# TEST : Loss: (0.3870) | Acc: (89.00%) (8934/10000)
percent tensor([0.5487, 0.4513], device='cuda:0')
percent tensor([0.5002, 0.4998], device='cuda:0')
percent tensor([0.5468, 0.4532], device='cuda:0')
percent tensor([0.6835, 0.3165], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.7393, 0.2607], device='cuda:0')
percent tensor([0.7321, 0.2679], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (3839/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (5075/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (6306/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (7544/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (8780/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (10025/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (11268/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (12491/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (13737/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (14987/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (16227/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (17454/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (18692/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (19923/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (21158/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (22392/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (23639/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (24892/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (26136/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (27368/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (28595/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (29840/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (31087/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (32321/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (33570/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (34803/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (36052/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (37293/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (38538/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (39779/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (41020/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (42247/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (43476/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (44718/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (45954/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (47197/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (48391/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_180.pth.tar'
# TEST : Loss: (0.4614) | Acc: (87.00%) (8793/10000)
percent tensor([0.5514, 0.4486], device='cuda:0')
percent tensor([0.5006, 0.4994], device='cuda:0')
percent tensor([0.5513, 0.4487], device='cuda:0')
percent tensor([0.6896, 0.3104], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.7451, 0.2549], device='cuda:0')
percent tensor([0.7368, 0.2632], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.7151, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.1648, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(834.1873, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1513.5400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(482.3854, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2283.3538, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4247.4917, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1356.7111, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6273.0835, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11601.6172, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3810.3701, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16121.6543, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (3861/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (5102/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (6349/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (7596/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (8836/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (97.00%) (11308/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (12559/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (13800/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (15032/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (16283/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (17525/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (18768/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (20005/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (21243/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (22480/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (23731/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (24973/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (26224/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (27469/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (28710/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (29962/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (31209/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (32449/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (33693/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (34932/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (36175/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (37420/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (38679/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (39924/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (41166/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (42406/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (97.00%) (43640/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (44878/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (46121/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (47366/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (97.00%) (48562/50000)
# TEST : Loss: (0.4124) | Acc: (88.00%) (8873/10000)
percent tensor([0.5520, 0.4480], device='cuda:0')
percent tensor([0.5010, 0.4990], device='cuda:0')
percent tensor([0.5520, 0.4480], device='cuda:0')
percent tensor([0.6929, 0.3071], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.7532, 0.2468], device='cuda:0')
percent tensor([0.7347, 0.2653], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 182 | Batch_idx: 0 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (5106/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (6351/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (7584/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (8826/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (10074/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (11309/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (12558/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (13803/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (15036/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (16266/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (96.00%) (17505/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (18749/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (20009/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (21253/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (22494/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (23737/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (24983/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (26214/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (27459/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (28716/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (29943/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (31170/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (32418/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (33670/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (34907/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (36134/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (37371/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (96.00%) (38609/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (96.00%) (39854/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (41101/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (42336/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (43575/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (44800/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (46024/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (47259/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (48450/50000)
# TEST : Loss: (0.4304) | Acc: (88.00%) (8858/10000)
percent tensor([0.5542, 0.4458], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.5541, 0.4459], device='cuda:0')
percent tensor([0.6993, 0.3007], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.7563, 0.2437], device='cuda:0')
percent tensor([0.7350, 0.2650], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 183 | Batch_idx: 0 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (5111/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (7597/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (8838/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (10076/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (11325/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (12566/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (13802/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (15040/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (16287/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (17527/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (18776/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (20029/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (21280/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (22527/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (23774/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (25013/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (26254/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (27495/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (28742/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (29989/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (31235/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (97.00%) (32471/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (33710/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (34957/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (36205/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (37440/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (38691/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (39928/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (41164/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (42390/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (43633/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (44883/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (46127/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (47376/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (48560/50000)
# TEST : Loss: (0.4353) | Acc: (88.00%) (8841/10000)
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5019, 0.4981], device='cuda:0')
percent tensor([0.5551, 0.4449], device='cuda:0')
percent tensor([0.7025, 0.2975], device='cuda:0')
percent tensor([0.6598, 0.3402], device='cuda:0')
percent tensor([0.7608, 0.2392], device='cuda:0')
percent tensor([0.7356, 0.2644], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 184 | Batch_idx: 0 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (3865/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (5113/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (6356/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (7599/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (8843/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (10089/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (11338/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (12586/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (13825/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (15074/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (16326/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (17571/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (18801/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (20048/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (21285/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (22528/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (23785/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (25023/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (26270/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (27511/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (28748/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (29983/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (31230/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (32469/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (33715/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (34957/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (36202/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (37446/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (38700/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (39934/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (41179/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (42407/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (43653/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (44896/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (46133/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (47371/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (48566/50000)
# TEST : Loss: (0.4140) | Acc: (88.00%) (8894/10000)
percent tensor([0.5562, 0.4438], device='cuda:0')
percent tensor([0.5022, 0.4978], device='cuda:0')
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.7012, 0.2988], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.7597, 0.2403], device='cuda:0')
percent tensor([0.7314, 0.2686], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 185 | Batch_idx: 0 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (3854/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (5093/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (6342/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (7584/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (8830/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (10078/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (11332/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (12573/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (13820/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (15076/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (16314/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (17566/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (18817/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (20070/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (21320/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (22560/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (23808/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (25046/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (26289/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (27528/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (28766/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (30016/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (31262/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (32507/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (33749/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (34984/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (36226/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (37466/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (38711/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (39954/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (41200/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (42446/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (43674/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (44918/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (46163/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (47395/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (48593/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_185.pth.tar'
# TEST : Loss: (0.4168) | Acc: (88.00%) (8897/10000)
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5581, 0.4419], device='cuda:0')
percent tensor([0.7036, 0.2964], device='cuda:0')
percent tensor([0.6577, 0.3423], device='cuda:0')
percent tensor([0.7689, 0.2311], device='cuda:0')
percent tensor([0.7355, 0.2645], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 186 | Batch_idx: 0 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (5113/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (7600/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (8845/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (10086/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (11336/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (12582/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (13829/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (15064/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (16306/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (17551/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (18800/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (20041/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (21280/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (22532/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (23776/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (25012/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (26266/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (27521/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (28762/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (29995/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (31229/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (32475/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (33720/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (34961/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (36210/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (37459/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (38692/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (39933/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (41181/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (42420/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (43679/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (44932/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (46177/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (47419/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (48620/50000)
# TEST : Loss: (0.4087) | Acc: (89.00%) (8914/10000)
percent tensor([0.5573, 0.4427], device='cuda:0')
percent tensor([0.5021, 0.4979], device='cuda:0')
percent tensor([0.5579, 0.4421], device='cuda:0')
percent tensor([0.7033, 0.2967], device='cuda:0')
percent tensor([0.6552, 0.3448], device='cuda:0')
percent tensor([0.7677, 0.2323], device='cuda:0')
percent tensor([0.7316, 0.2684], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (2611/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (3865/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (5109/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (7605/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (8856/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (10104/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (11359/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (12597/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (13843/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (15087/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (16338/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (17575/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (18823/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (20072/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (21329/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (22578/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (23822/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (25067/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (26322/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (27571/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (28817/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (30064/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (31305/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (32555/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (33794/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (35029/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (36275/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (37525/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (38774/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (40019/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (41267/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (42520/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (43763/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (45002/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (46251/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (47491/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (48680/50000)
# TEST : Loss: (0.4064) | Acc: (89.00%) (8929/10000)
percent tensor([0.5578, 0.4422], device='cuda:0')
percent tensor([0.5025, 0.4975], device='cuda:0')
percent tensor([0.5574, 0.4426], device='cuda:0')
percent tensor([0.7069, 0.2931], device='cuda:0')
percent tensor([0.6650, 0.3350], device='cuda:0')
percent tensor([0.7718, 0.2282], device='cuda:0')
percent tensor([0.7383, 0.2617], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (5110/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (6368/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (7620/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (8864/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (10099/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (11344/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (12582/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (13836/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (15085/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (16332/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (17582/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (18830/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (20075/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (21322/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (22569/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (23822/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (25070/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (26323/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (27564/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (28815/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (30053/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (31302/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (32551/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (33808/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (35056/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (36301/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (37540/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (38788/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (40036/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (41289/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (42534/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (43784/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (45023/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (46270/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (47515/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (48711/50000)
# TEST : Loss: (0.4475) | Acc: (88.00%) (8865/10000)
percent tensor([0.5577, 0.4423], device='cuda:0')
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.5615, 0.4385], device='cuda:0')
percent tensor([0.7097, 0.2903], device='cuda:0')
percent tensor([0.6610, 0.3390], device='cuda:0')
percent tensor([0.7709, 0.2291], device='cuda:0')
percent tensor([0.7350, 0.2650], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 189 | Batch_idx: 0 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (1367/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (5115/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (6361/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (7616/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (10121/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (11361/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (12601/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (13838/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (15082/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (16330/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (17574/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (18818/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (20075/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (21321/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (22585/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (23837/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (25085/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (26326/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (27570/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (28823/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (30071/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (31317/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (32565/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (33824/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (35067/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (36322/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (37563/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (38811/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (40065/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (41304/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (42551/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (43799/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (45040/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (46285/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (47524/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (48730/50000)
# TEST : Loss: (0.4714) | Acc: (88.00%) (8802/10000)
percent tensor([0.5596, 0.4404], device='cuda:0')
percent tensor([0.5029, 0.4971], device='cuda:0')
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.7097, 0.2903], device='cuda:0')
percent tensor([0.6657, 0.3343], device='cuda:0')
percent tensor([0.7771, 0.2229], device='cuda:0')
percent tensor([0.7376, 0.2624], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0721) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (3873/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (5131/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (6385/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (7639/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (8895/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (10149/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (11395/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (12648/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (13901/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (15157/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (16413/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (17661/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (18916/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (20165/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (21411/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (22675/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (23932/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (25176/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (26437/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (27698/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0673) |  Loss2: (0.0000) | Acc: (97.00%) (28949/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (30195/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (31450/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (32698/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (33955/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (35201/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (36450/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (37699/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (38957/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (40205/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (41459/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (42705/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (43946/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (45194/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (46427/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (47669/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (48869/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_190.pth.tar'
# TEST : Loss: (0.4809) | Acc: (87.00%) (8758/10000)
percent tensor([0.5610, 0.4390], device='cuda:0')
percent tensor([0.5030, 0.4970], device='cuda:0')
percent tensor([0.5601, 0.4399], device='cuda:0')
percent tensor([0.7117, 0.2883], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.7799, 0.2201], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(186.6570, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.7042, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(839.3979, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.7625, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(480.8259, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2298.3374, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4255.4268, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1352.1720, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6314.1572, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11578.6846, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3795.7148, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16056.1816, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (3884/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (5130/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (6379/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (7629/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (8877/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (10124/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (11371/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (12628/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (13871/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (15117/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (16360/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (17608/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (18862/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (20105/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (21345/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (22597/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (23844/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (25098/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (26352/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (27600/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (28860/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (30107/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (31360/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (32618/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (33872/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (35119/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (36361/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (37609/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (38857/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (40104/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (41347/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (42591/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (43836/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (45080/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (46330/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (47571/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (48768/50000)
# TEST : Loss: (0.4258) | Acc: (89.00%) (8902/10000)
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.5033, 0.4967], device='cuda:0')
percent tensor([0.5585, 0.4415], device='cuda:0')
percent tensor([0.7117, 0.2883], device='cuda:0')
percent tensor([0.6651, 0.3349], device='cuda:0')
percent tensor([0.7812, 0.2188], device='cuda:0')
percent tensor([0.7437, 0.2563], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (3873/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (5126/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (6382/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (7640/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (8894/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (10151/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (11397/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (12641/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (13883/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (15137/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (16392/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0678) |  Loss2: (0.0000) | Acc: (97.00%) (17646/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (18898/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (20153/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (21397/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (22647/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (23892/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (25145/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (26391/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (27630/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (28873/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (30129/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (31385/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (32627/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (33878/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (35122/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (36372/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (37628/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (38886/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (40126/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (41382/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (42635/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (43870/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (45123/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (46379/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (47627/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (48837/50000)
# TEST : Loss: (0.4496) | Acc: (88.00%) (8899/10000)
percent tensor([0.5621, 0.4379], device='cuda:0')
percent tensor([0.5035, 0.4965], device='cuda:0')
percent tensor([0.5600, 0.4400], device='cuda:0')
percent tensor([0.7125, 0.2875], device='cuda:0')
percent tensor([0.6667, 0.3333], device='cuda:0')
percent tensor([0.7832, 0.2168], device='cuda:0')
percent tensor([0.7390, 0.2610], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 193 | Batch_idx: 0 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (3890/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (6394/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (7657/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (8914/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (10176/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (11430/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (12681/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (13933/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (98.00%) (15184/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (98.00%) (16442/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (17683/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (18939/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (98.00%) (20198/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (21450/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (98.00%) (22707/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (23955/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (25206/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (26453/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (27706/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (28958/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (30199/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (31459/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (32718/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (33970/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (35228/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (36472/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (37723/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (38969/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (40224/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0653) |  Loss2: (0.0000) | Acc: (97.00%) (41466/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (42715/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (43965/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (45210/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (46458/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (47709/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (48905/50000)
# TEST : Loss: (0.4238) | Acc: (89.00%) (8929/10000)
percent tensor([0.5637, 0.4363], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.7119, 0.2881], device='cuda:0')
percent tensor([0.6669, 0.3331], device='cuda:0')
percent tensor([0.7852, 0.2148], device='cuda:0')
percent tensor([0.7336, 0.2664], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 194 | Batch_idx: 0 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (2621/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (6372/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (7625/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (8880/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (10135/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (11383/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (12629/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (13871/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (15123/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (16378/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (17625/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (18876/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (20126/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (21387/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (22642/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (23894/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (25137/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (26396/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (27644/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (28902/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (30154/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (31399/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (32643/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (33892/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (35147/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (36384/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (37630/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (38878/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (40134/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (41382/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (42630/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (43870/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (45118/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (46378/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (47625/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (48825/50000)
# TEST : Loss: (0.4270) | Acc: (89.00%) (8936/10000)
percent tensor([0.5623, 0.4377], device='cuda:0')
percent tensor([0.5040, 0.4960], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.7119, 0.2881], device='cuda:0')
percent tensor([0.6689, 0.3311], device='cuda:0')
percent tensor([0.7849, 0.2151], device='cuda:0')
percent tensor([0.7391, 0.2609], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (2643/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (3902/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (6406/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (7672/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (8928/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (10182/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (11433/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (12694/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (13954/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (15212/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (16465/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (17708/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (18961/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (20204/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (21458/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (22710/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (23962/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (25213/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (26472/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (27712/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (28960/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (30204/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (31460/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (32718/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (33977/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (35231/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (36489/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (37735/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (38994/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (40249/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (41498/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (42751/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (44001/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (45256/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (46498/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (47754/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (48954/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate/checkpoint_195.pth.tar'
# TEST : Loss: (0.4426) | Acc: (89.00%) (8927/10000)
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.5042, 0.4958], device='cuda:0')
percent tensor([0.5633, 0.4367], device='cuda:0')
percent tensor([0.7130, 0.2870], device='cuda:0')
percent tensor([0.6711, 0.3289], device='cuda:0')
percent tensor([0.7881, 0.2119], device='cuda:0')
percent tensor([0.7331, 0.2669], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0224) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (3882/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (5132/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (6385/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (7641/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (8905/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (10156/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (11407/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (12657/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (13910/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (15159/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (16405/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (17663/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (18915/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.0646) |  Loss2: (0.0000) | Acc: (97.00%) (20165/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (21426/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (22683/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (23944/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (25195/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (26452/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (27707/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (28967/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (30218/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (31471/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (32730/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (33977/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (35238/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (36486/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (37739/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (38992/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (40246/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (41496/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (42745/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (43999/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (97.00%) (45243/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (46491/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0644) |  Loss2: (0.0000) | Acc: (97.00%) (47745/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (48945/50000)
# TEST : Loss: (0.4651) | Acc: (88.00%) (8845/10000)
percent tensor([0.5617, 0.4383], device='cuda:0')
percent tensor([0.5043, 0.4957], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.7119, 0.2881], device='cuda:0')
percent tensor([0.6658, 0.3342], device='cuda:0')
percent tensor([0.7895, 0.2105], device='cuda:0')
percent tensor([0.7381, 0.2619], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (98.00%) (2636/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (5148/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (6403/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (7653/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (8907/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (97.00%) (10160/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (11418/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (12676/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (13924/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (98.00%) (15179/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (16422/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (17679/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (18923/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (20178/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (21427/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (22676/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (23925/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (25187/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (26436/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (27687/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (28940/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (30195/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (31449/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (32704/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (33968/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (35225/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (36475/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (37724/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (38979/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (40232/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (41471/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (42721/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (43974/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (97.00%) (45225/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (46472/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (47715/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (48925/50000)
# TEST : Loss: (0.4540) | Acc: (88.00%) (8875/10000)
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5669, 0.4331], device='cuda:0')
percent tensor([0.7165, 0.2835], device='cuda:0')
percent tensor([0.6686, 0.3314], device='cuda:0')
percent tensor([0.7892, 0.2108], device='cuda:0')
percent tensor([0.7454, 0.2546], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (6412/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (7666/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (8920/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (10173/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (11431/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (12685/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0570) |  Loss2: (0.0000) | Acc: (98.00%) (13940/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (15198/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (16457/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (17710/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (18964/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (20221/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (21471/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (22722/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (23984/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (25248/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (26507/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (27749/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (29008/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (30268/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (31517/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (32773/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (34027/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (35288/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (36542/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (37797/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (39047/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (40306/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (41563/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (42807/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (44067/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (45317/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (46578/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (47833/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (49026/50000)
# TEST : Loss: (0.4366) | Acc: (88.00%) (8887/10000)
percent tensor([0.5631, 0.4369], device='cuda:0')
percent tensor([0.5047, 0.4953], device='cuda:0')
percent tensor([0.5656, 0.4344], device='cuda:0')
percent tensor([0.7179, 0.2821], device='cuda:0')
percent tensor([0.6634, 0.3366], device='cuda:0')
percent tensor([0.7923, 0.2077], device='cuda:0')
percent tensor([0.7457, 0.2543], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 199 | Batch_idx: 0 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (1386/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (3899/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (5152/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0640) |  Loss2: (0.0000) | Acc: (98.00%) (6402/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (98.00%) (7655/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (8905/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (98.00%) (10164/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (98.00%) (11416/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (98.00%) (12670/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (13922/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (15174/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (16431/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (17684/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (98.00%) (18950/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (20213/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (21471/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (22730/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (23986/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (25244/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (26492/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (27748/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (29002/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (30256/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (31515/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (32773/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (34026/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (35287/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (36547/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (37804/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (39064/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (40311/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (41569/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (42819/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (44077/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (45318/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (46580/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (47826/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (49036/50000)
# TEST : Loss: (0.4587) | Acc: (88.00%) (8845/10000)
percent tensor([0.5636, 0.4364], device='cuda:0')
percent tensor([0.5051, 0.4949], device='cuda:0')
percent tensor([0.5665, 0.4335], device='cuda:0')
percent tensor([0.7191, 0.2809], device='cuda:0')
percent tensor([0.6723, 0.3277], device='cuda:0')
percent tensor([0.7942, 0.2058], device='cuda:0')
percent tensor([0.7391, 0.2609], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(187.3883, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(833.6777, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(843.3322, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1516.7577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(479.3714, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2309.6577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4260.0029, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1347.8483, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6346.0430, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11556.5674, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3782.6401, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15997.6533, device='cuda:0', grad_fn=<NormBackward0>)
5 hours 28 mins 49 secs for training