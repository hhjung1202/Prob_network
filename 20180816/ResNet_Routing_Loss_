Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3097) |  Loss2: (0.0000) | Acc: (8.00%) (11/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3038) |  Loss2: (0.0000) | Acc: (8.00%) (126/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2970) |  Loss2: (0.0000) | Acc: (10.00%) (283/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2906) |  Loss2: (0.0000) | Acc: (11.00%) (449/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2814) |  Loss2: (0.0000) | Acc: (12.00%) (670/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2720) |  Loss2: (0.0000) | Acc: (14.00%) (931/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2618) |  Loss2: (0.0000) | Acc: (15.00%) (1194/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2526) |  Loss2: (0.0000) | Acc: (15.00%) (1426/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2432) |  Loss2: (0.0000) | Acc: (16.00%) (1740/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2334) |  Loss2: (0.0000) | Acc: (17.00%) (2039/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2256) |  Loss2: (0.0000) | Acc: (17.00%) (2308/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2161) |  Loss2: (0.0000) | Acc: (18.00%) (2617/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2082) |  Loss2: (0.0000) | Acc: (18.00%) (2912/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.1998) |  Loss2: (0.0000) | Acc: (19.00%) (3231/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1915) |  Loss2: (0.0000) | Acc: (19.00%) (3554/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1836) |  Loss2: (0.0000) | Acc: (20.00%) (3880/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1750) |  Loss2: (0.0000) | Acc: (20.00%) (4204/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1669) |  Loss2: (0.0000) | Acc: (20.00%) (4515/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1595) |  Loss2: (0.0000) | Acc: (20.00%) (4831/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1516) |  Loss2: (0.0000) | Acc: (21.00%) (5186/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1431) |  Loss2: (0.0000) | Acc: (21.00%) (5554/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1348) |  Loss2: (0.0000) | Acc: (21.00%) (5908/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1277) |  Loss2: (0.0000) | Acc: (22.00%) (6269/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1202) |  Loss2: (0.0000) | Acc: (22.00%) (6586/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1134) |  Loss2: (0.0000) | Acc: (22.00%) (6963/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1074) |  Loss2: (0.0000) | Acc: (22.00%) (7316/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1004) |  Loss2: (0.0000) | Acc: (22.00%) (7672/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0941) |  Loss2: (0.0000) | Acc: (23.00%) (8042/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0884) |  Loss2: (0.0000) | Acc: (23.00%) (8439/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (23.00%) (8808/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0757) |  Loss2: (0.0000) | Acc: (23.00%) (9190/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0699) |  Loss2: (0.0000) | Acc: (24.00%) (9589/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0640) |  Loss2: (0.0000) | Acc: (24.00%) (9999/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0589) |  Loss2: (0.0000) | Acc: (24.00%) (10411/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0530) |  Loss2: (0.0000) | Acc: (24.00%) (10833/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0461) |  Loss2: (0.0000) | Acc: (25.00%) (11277/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0403) |  Loss2: (0.0000) | Acc: (25.00%) (11730/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0348) |  Loss2: (0.0000) | Acc: (25.00%) (12133/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0289) |  Loss2: (0.0000) | Acc: (25.00%) (12579/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0234) |  Loss2: (0.0000) | Acc: (26.00%) (13017/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_000.pth.tar'
# TEST : Loss: (1.7808) | Acc: (33.00%) (3372/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1519,  0.0635, -0.0291],
          [ 0.0750, -0.1724, -0.1529],
          [ 0.1071, -0.1921, -0.0358]],

         [[ 0.0334, -0.1012,  0.1912],
          [ 0.0132,  0.0132,  0.0277],
          [-0.0073, -0.0323, -0.0325]],

         [[ 0.0551, -0.0507,  0.1164],
          [ 0.1239, -0.0736,  0.1406],
          [-0.1012,  0.0689, -0.1362]]],


        [[[ 0.1549, -0.0402,  0.0203],
          [ 0.1568,  0.1885,  0.1605],
          [ 0.0554, -0.0309,  0.1296]],

         [[-0.1585, -0.1712,  0.1620],
          [ 0.0235, -0.0398,  0.0758],
          [ 0.1584, -0.1665,  0.1436]],

         [[ 0.0899, -0.0444, -0.0307],
          [-0.0034,  0.0401, -0.1571],
          [-0.0230, -0.1247, -0.0186]]],


        [[[-0.0550,  0.1033, -0.0398],
          [-0.1034,  0.0789,  0.1259],
          [ 0.1433,  0.1839,  0.0077]],

         [[ 0.1057,  0.1303,  0.1066],
          [ 0.0864, -0.0405, -0.0418],
          [-0.0281, -0.0256,  0.1472]],

         [[-0.0522,  0.1383,  0.0637],
          [-0.0380, -0.0501, -0.0656],
          [-0.0300, -0.0061,  0.0480]]],


        ...,


        [[[ 0.1750, -0.1532, -0.1531],
          [-0.0720, -0.1066, -0.1174],
          [-0.1352,  0.0070,  0.0285]],

         [[ 0.1257,  0.1494, -0.1626],
          [ 0.1832, -0.1561, -0.1357],
          [-0.1067, -0.1012, -0.0574]],

         [[ 0.1576,  0.0749, -0.1223],
          [-0.1090, -0.1706, -0.0748],
          [ 0.0160,  0.1623, -0.1432]]],


        [[[-0.1381,  0.0679, -0.1914],
          [-0.1515, -0.1485,  0.1085],
          [-0.1649,  0.1538, -0.1319]],

         [[ 0.0830, -0.1345,  0.0612],
          [ 0.1623,  0.0192, -0.0870],
          [-0.1473,  0.1280, -0.1838]],

         [[ 0.0973, -0.0507, -0.0410],
          [ 0.1284, -0.1887, -0.0044],
          [-0.0453,  0.0098, -0.1200]]],


        [[[ 0.1058,  0.1656,  0.0764],
          [-0.0715, -0.1096,  0.1654],
          [-0.1295,  0.0505, -0.0331]],

         [[-0.0326, -0.0749,  0.1111],
          [-0.0404, -0.1886, -0.0547],
          [-0.0664,  0.0279, -0.0380]],

         [[-0.1495, -0.1641,  0.1280],
          [ 0.1340, -0.1665, -0.1270],
          [-0.0636, -0.1129,  0.1707]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0070, -0.0080, -0.0077],
          [-0.0025, -0.0043, -0.0042],
          [ 0.0112,  0.0052,  0.0012]],

         [[-0.0041, -0.0067, -0.0069],
          [-0.0005, -0.0042, -0.0037],
          [ 0.0141,  0.0066,  0.0038]],

         [[ 0.0027,  0.0006,  0.0019],
          [ 0.0032, -0.0004,  0.0021],
          [ 0.0138,  0.0072,  0.0064]]],


        [[[ 0.0134,  0.0140,  0.0115],
          [ 0.0059,  0.0073,  0.0051],
          [ 0.0053,  0.0067,  0.0062]],

         [[ 0.0164,  0.0179,  0.0151],
          [ 0.0106,  0.0129,  0.0101],
          [ 0.0099,  0.0118,  0.0110]],

         [[ 0.0192,  0.0208,  0.0187],
          [ 0.0154,  0.0175,  0.0153],
          [ 0.0146,  0.0164,  0.0160]]],


        [[[ 0.0007,  0.0003, -0.0007],
          [-0.0005, -0.0005, -0.0016],
          [-0.0009, -0.0009, -0.0014]],

         [[ 0.0014,  0.0013,  0.0006],
          [-0.0000,  0.0001, -0.0007],
          [-0.0007, -0.0007, -0.0010]],

         [[ 0.0036,  0.0037,  0.0031],
          [ 0.0023,  0.0026,  0.0019],
          [ 0.0017,  0.0018,  0.0017]]],


        ...,


        [[[ 0.0015,  0.0004, -0.0002],
          [ 0.0026,  0.0012,  0.0002],
          [ 0.0025,  0.0005, -0.0010]],

         [[ 0.0018,  0.0002, -0.0006],
          [ 0.0025,  0.0005, -0.0008],
          [ 0.0024, -0.0001, -0.0020]],

         [[ 0.0042,  0.0029,  0.0022],
          [ 0.0047,  0.0029,  0.0020],
          [ 0.0043,  0.0025,  0.0010]]],


        [[[-0.0020, -0.0013,  0.0013],
          [ 0.0024,  0.0023,  0.0039],
          [ 0.0006,  0.0008,  0.0008]],

         [[-0.0014, -0.0011,  0.0018],
          [ 0.0014,  0.0010,  0.0022],
          [-0.0015, -0.0020, -0.0021]],

         [[ 0.0018,  0.0018,  0.0040],
          [ 0.0032,  0.0025,  0.0032],
          [ 0.0004, -0.0003, -0.0007]]],


        [[[ 0.0050,  0.0059,  0.0033],
          [ 0.0078,  0.0103,  0.0078],
          [ 0.0046,  0.0061,  0.0035]],

         [[-0.0000,  0.0004, -0.0023],
          [ 0.0024,  0.0037,  0.0013],
          [-0.0003, -0.0002, -0.0025]],

         [[-0.0035, -0.0033, -0.0061],
          [ 0.0001,  0.0009, -0.0018],
          [-0.0011, -0.0013, -0.0028]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.]], device='cuda:0')

percentage_weight_grad None

Epoch: 1 | Batch_idx: 0 |  Loss: (1.7741) |  Loss2: (0.0000) | Acc: (40.00%) (52/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8041) |  Loss2: (0.0000) | Acc: (33.00%) (470/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.7970) |  Loss2: (0.0000) | Acc: (34.00%) (921/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.7917) |  Loss2: (0.0000) | Acc: (34.00%) (1378/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.7902) |  Loss2: (0.0000) | Acc: (34.00%) (1822/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7885) |  Loss2: (0.0000) | Acc: (34.00%) (2268/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7825) |  Loss2: (0.0000) | Acc: (34.00%) (2704/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7797) |  Loss2: (0.0000) | Acc: (34.00%) (3128/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7782) |  Loss2: (0.0000) | Acc: (34.00%) (3577/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7740) |  Loss2: (0.0000) | Acc: (34.00%) (4049/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7719) |  Loss2: (0.0000) | Acc: (34.00%) (4465/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7682) |  Loss2: (0.0000) | Acc: (34.00%) (4950/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7644) |  Loss2: (0.0000) | Acc: (34.00%) (5411/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7615) |  Loss2: (0.0000) | Acc: (35.00%) (5883/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7584) |  Loss2: (0.0000) | Acc: (35.00%) (6388/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7536) |  Loss2: (0.0000) | Acc: (35.00%) (6841/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7500) |  Loss2: (0.0000) | Acc: (35.00%) (7328/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7474) |  Loss2: (0.0000) | Acc: (35.00%) (7808/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7452) |  Loss2: (0.0000) | Acc: (35.00%) (8289/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7426) |  Loss2: (0.0000) | Acc: (35.00%) (8766/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7382) |  Loss2: (0.0000) | Acc: (36.00%) (9268/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7356) |  Loss2: (0.0000) | Acc: (36.00%) (9760/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7332) |  Loss2: (0.0000) | Acc: (36.00%) (10218/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7306) |  Loss2: (0.0000) | Acc: (36.00%) (10703/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7281) |  Loss2: (0.0000) | Acc: (36.00%) (11162/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7245) |  Loss2: (0.0000) | Acc: (36.00%) (11658/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7214) |  Loss2: (0.0000) | Acc: (36.00%) (12171/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7188) |  Loss2: (0.0000) | Acc: (36.00%) (12665/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7158) |  Loss2: (0.0000) | Acc: (36.00%) (13174/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7132) |  Loss2: (0.0000) | Acc: (36.00%) (13695/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7109) |  Loss2: (0.0000) | Acc: (36.00%) (14221/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7074) |  Loss2: (0.0000) | Acc: (37.00%) (14738/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7049) |  Loss2: (0.0000) | Acc: (37.00%) (15215/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7019) |  Loss2: (0.0000) | Acc: (37.00%) (15705/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7000) |  Loss2: (0.0000) | Acc: (37.00%) (16229/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.6973) |  Loss2: (0.0000) | Acc: (37.00%) (16736/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.6949) |  Loss2: (0.0000) | Acc: (37.00%) (17262/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.6920) |  Loss2: (0.0000) | Acc: (37.00%) (17801/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.6897) |  Loss2: (0.0000) | Acc: (37.00%) (18328/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.6872) |  Loss2: (0.0000) | Acc: (37.00%) (18854/50000)
# TEST : Loss: (1.7184) | Acc: (34.00%) (3424/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1445,  0.0660, -0.0212],
          [ 0.0821, -0.1721, -0.1473],
          [ 0.1118, -0.1936, -0.0325]],

         [[ 0.0369, -0.1036,  0.1917],
          [ 0.0191,  0.0119,  0.0298],
          [-0.0045, -0.0357, -0.0329]],

         [[ 0.0550, -0.0564,  0.1115],
          [ 0.1281, -0.0759,  0.1393],
          [-0.0993,  0.0652, -0.1388]]],


        [[[ 0.1543, -0.0407,  0.0193],
          [ 0.1564,  0.1884,  0.1610],
          [ 0.0565, -0.0288,  0.1321]],

         [[-0.1587, -0.1714,  0.1613],
          [ 0.0215, -0.0414,  0.0749],
          [ 0.1565, -0.1671,  0.1435]],

         [[ 0.0913, -0.0432, -0.0301],
          [-0.0041,  0.0393, -0.1572],
          [-0.0238, -0.1246, -0.0180]]],


        [[[-0.0546,  0.1035, -0.0396],
          [-0.1029,  0.0791,  0.1259],
          [ 0.1435,  0.1836,  0.0074]],

         [[ 0.1059,  0.1305,  0.1065],
          [ 0.0863, -0.0406, -0.0422],
          [-0.0286, -0.0264,  0.1462]],

         [[-0.0513,  0.1393,  0.0643],
          [-0.0375, -0.0495, -0.0654],
          [-0.0300, -0.0061,  0.0477]]],


        ...,


        [[[ 0.1752, -0.1536, -0.1527],
          [-0.0714, -0.1072, -0.1179],
          [-0.1342,  0.0062,  0.0289]],

         [[ 0.1273,  0.1501, -0.1613],
          [ 0.1847, -0.1558, -0.1357],
          [-0.1048, -0.1012, -0.0567]],

         [[ 0.1607,  0.0770, -0.1199],
          [-0.1063, -0.1693, -0.0740],
          [ 0.0180,  0.1626, -0.1423]]],


        [[[-0.1365,  0.0691, -0.1905],
          [-0.1519, -0.1494,  0.1079],
          [-0.1652,  0.1544, -0.1311]],

         [[ 0.0848, -0.1325,  0.0632],
          [ 0.1619,  0.0187, -0.0865],
          [-0.1477,  0.1288, -0.1824]],

         [[ 0.0994, -0.0490, -0.0393],
          [ 0.1279, -0.1897, -0.0048],
          [-0.0460,  0.0096, -0.1200]]],


        [[[ 0.1083,  0.1631,  0.0779],
          [-0.0675, -0.1080,  0.1688],
          [-0.1256,  0.0530, -0.0280]],

         [[-0.0314, -0.0789,  0.1120],
          [-0.0376, -0.1878, -0.0505],
          [-0.0616,  0.0320, -0.0300]],

         [[-0.1483, -0.1665,  0.1311],
          [ 0.1352, -0.1658, -0.1212],
          [-0.0602, -0.1086,  0.1803]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0330, -0.0433, -0.0352],
          [-0.0078, -0.0127, -0.0074],
          [-0.0194, -0.0158, -0.0051]],

         [[-0.0155, -0.0285, -0.0217],
          [ 0.0062,  0.0012,  0.0059],
          [-0.0049,  0.0010,  0.0127]],

         [[-0.0392, -0.0532, -0.0490],
          [-0.0258, -0.0298, -0.0242],
          [-0.0409, -0.0323, -0.0191]]],


        [[[-0.0081, -0.0068, -0.0069],
          [-0.0028, -0.0033, -0.0047],
          [-0.0018, -0.0068, -0.0084]],

         [[ 0.0060,  0.0057,  0.0046],
          [ 0.0131,  0.0103,  0.0083],
          [ 0.0159,  0.0089,  0.0067]],

         [[-0.0098, -0.0112, -0.0124],
          [-0.0044, -0.0083, -0.0105],
          [-0.0016, -0.0092, -0.0118]]],


        [[[-0.0000,  0.0008,  0.0010],
          [-0.0000,  0.0007,  0.0021],
          [ 0.0016,  0.0018,  0.0026]],

         [[-0.0030, -0.0023, -0.0019],
          [-0.0030, -0.0023, -0.0006],
          [-0.0005, -0.0003,  0.0006]],

         [[-0.0087, -0.0082, -0.0076],
          [-0.0085, -0.0079, -0.0060],
          [-0.0059, -0.0057, -0.0046]]],


        ...,


        [[[ 0.0182,  0.0099,  0.0059],
          [ 0.0157,  0.0046,  0.0003],
          [ 0.0215,  0.0123,  0.0084]],

         [[ 0.0115,  0.0035,  0.0007],
          [ 0.0086, -0.0024, -0.0060],
          [ 0.0160,  0.0072,  0.0037]],

         [[ 0.0147,  0.0080,  0.0053],
          [ 0.0133,  0.0039,  0.0004],
          [ 0.0213,  0.0141,  0.0105]]],


        [[[ 0.0024, -0.0016, -0.0007],
          [ 0.0013,  0.0013,  0.0024],
          [-0.0018, -0.0013, -0.0024]],

         [[-0.0031, -0.0061, -0.0027],
          [-0.0017, -0.0007,  0.0040],
          [-0.0051, -0.0029, -0.0010]],

         [[-0.0023, -0.0048, -0.0016],
          [-0.0001,  0.0011,  0.0052],
          [-0.0024,  0.0003,  0.0015]]],


        [[[ 0.0713,  0.0569,  0.0507],
          [ 0.0554,  0.0372,  0.0332],
          [ 0.0346,  0.0228,  0.0217]],

         [[ 0.0407,  0.0253,  0.0204],
          [ 0.0290,  0.0114,  0.0088],
          [ 0.0128,  0.0022,  0.0012]],

         [[ 0.0425,  0.0304,  0.0262],
          [ 0.0369,  0.0229,  0.0203],
          [ 0.0256,  0.0181,  0.0170]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.]], device='cuda:0')

percentage_weight_grad None

Epoch: 2 | Batch_idx: 0 |  Loss: (1.5872) |  Loss2: (0.0000) | Acc: (35.00%) (45/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.6037) |  Loss2: (0.0000) | Acc: (39.00%) (559/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5898) |  Loss2: (0.0000) | Acc: (40.00%) (1081/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5865) |  Loss2: (0.0000) | Acc: (41.00%) (1630/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5836) |  Loss2: (0.0000) | Acc: (41.00%) (2156/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5827) |  Loss2: (0.0000) | Acc: (41.00%) (2689/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5793) |  Loss2: (0.0000) | Acc: (41.00%) (3239/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5777) |  Loss2: (0.0000) | Acc: (41.00%) (3796/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5734) |  Loss2: (0.0000) | Acc: (41.00%) (4352/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5656) |  Loss2: (0.0000) | Acc: (42.00%) (4932/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5645) |  Loss2: (0.0000) | Acc: (42.00%) (5472/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5579) |  Loss2: (0.0000) | Acc: (42.00%) (6056/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5582) |  Loss2: (0.0000) | Acc: (42.00%) (6586/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5537) |  Loss2: (0.0000) | Acc: (42.00%) (7169/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5515) |  Loss2: (0.0000) | Acc: (42.00%) (7748/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5496) |  Loss2: (0.0000) | Acc: (42.00%) (8287/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5483) |  Loss2: (0.0000) | Acc: (42.00%) (8808/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5468) |  Loss2: (0.0000) | Acc: (42.00%) (9364/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5466) |  Loss2: (0.0000) | Acc: (42.00%) (9902/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5453) |  Loss2: (0.0000) | Acc: (42.00%) (10459/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5438) |  Loss2: (0.0000) | Acc: (42.00%) (11038/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5414) |  Loss2: (0.0000) | Acc: (42.00%) (11604/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5402) |  Loss2: (0.0000) | Acc: (43.00%) (12165/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5373) |  Loss2: (0.0000) | Acc: (43.00%) (12755/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5342) |  Loss2: (0.0000) | Acc: (43.00%) (13337/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5318) |  Loss2: (0.0000) | Acc: (43.00%) (13885/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5296) |  Loss2: (0.0000) | Acc: (43.00%) (14447/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5279) |  Loss2: (0.0000) | Acc: (43.00%) (15044/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5278) |  Loss2: (0.0000) | Acc: (43.00%) (15588/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5261) |  Loss2: (0.0000) | Acc: (43.00%) (16180/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5239) |  Loss2: (0.0000) | Acc: (43.00%) (16770/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5214) |  Loss2: (0.0000) | Acc: (43.00%) (17360/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5180) |  Loss2: (0.0000) | Acc: (43.00%) (17963/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5165) |  Loss2: (0.0000) | Acc: (43.00%) (18563/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5156) |  Loss2: (0.0000) | Acc: (43.00%) (19144/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5140) |  Loss2: (0.0000) | Acc: (43.00%) (19732/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5128) |  Loss2: (0.0000) | Acc: (43.00%) (20320/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5099) |  Loss2: (0.0000) | Acc: (44.00%) (20941/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5086) |  Loss2: (0.0000) | Acc: (44.00%) (21536/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5069) |  Loss2: (0.0000) | Acc: (44.00%) (22128/50000)
# TEST : Loss: (1.4076) | Acc: (47.00%) (4704/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1373,  0.0706, -0.0096],
          [ 0.0905, -0.1708, -0.1395],
          [ 0.1202, -0.1915, -0.0260]],

         [[ 0.0402, -0.1038,  0.1959],
          [ 0.0261,  0.0108,  0.0329],
          [ 0.0019, -0.0365, -0.0316]],

         [[ 0.0542, -0.0610,  0.1100],
          [ 0.1317, -0.0804,  0.1377],
          [-0.0955,  0.0619, -0.1406]]],


        [[[ 0.1535, -0.0403,  0.0185],
          [ 0.1575,  0.1895,  0.1622],
          [ 0.0566, -0.0280,  0.1331]],

         [[-0.1588, -0.1711,  0.1604],
          [ 0.0209, -0.0424,  0.0742],
          [ 0.1543, -0.1690,  0.1417]],

         [[ 0.0924, -0.0417, -0.0289],
          [-0.0040,  0.0392, -0.1561],
          [-0.0246, -0.1250, -0.0176]]],


        [[[-0.0549,  0.1033, -0.0397],
          [-0.1033,  0.0787,  0.1259],
          [ 0.1429,  0.1830,  0.0070]],

         [[ 0.1058,  0.1309,  0.1070],
          [ 0.0858, -0.0408, -0.0420],
          [-0.0294, -0.0271,  0.1456]],

         [[-0.0512,  0.1399,  0.0648],
          [-0.0376, -0.0493, -0.0653],
          [-0.0305, -0.0063,  0.0472]]],


        ...,


        [[[ 0.1792, -0.1514, -0.1519],
          [-0.0679, -0.1056, -0.1177],
          [-0.1318,  0.0066,  0.0289]],

         [[ 0.1325,  0.1538, -0.1590],
          [ 0.1892, -0.1528, -0.1345],
          [-0.1016, -0.0996, -0.0560]],

         [[ 0.1654,  0.0806, -0.1178],
          [-0.1015, -0.1656, -0.0724],
          [ 0.0220,  0.1653, -0.1407]]],


        [[[-0.1352,  0.0698, -0.1905],
          [-0.1514, -0.1490,  0.1086],
          [-0.1658,  0.1555, -0.1306]],

         [[ 0.0857, -0.1316,  0.0637],
          [ 0.1619,  0.0189, -0.0855],
          [-0.1490,  0.1296, -0.1818]],

         [[ 0.1011, -0.0475, -0.0377],
          [ 0.1290, -0.1885, -0.0028],
          [-0.0457,  0.0116, -0.1183]]],


        [[[ 0.1103,  0.1608,  0.0795],
          [-0.0651, -0.1068,  0.1706],
          [-0.1234,  0.0534, -0.0268]],

         [[-0.0296, -0.0813,  0.1158],
          [-0.0346, -0.1855, -0.0443],
          [-0.0556,  0.0368, -0.0213]],

         [[-0.1485, -0.1691,  0.1359],
          [ 0.1359, -0.1638, -0.1131],
          [-0.0554, -0.1029,  0.1920]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0231, -0.0413, -0.0640],
          [-0.0127, -0.0198, -0.0387],
          [-0.0086, -0.0138, -0.0176]],

         [[-0.0214, -0.0383, -0.0553],
          [-0.0105, -0.0153, -0.0290],
          [-0.0043, -0.0075, -0.0073]],

         [[ 0.0127, -0.0006, -0.0092],
          [ 0.0210,  0.0206,  0.0137],
          [ 0.0218,  0.0245,  0.0295]]],


        [[[ 0.0157,  0.0082,  0.0065],
          [ 0.0114,  0.0023,  0.0014],
          [ 0.0170,  0.0043,  0.0023]],

         [[ 0.0147,  0.0053,  0.0027],
          [ 0.0137,  0.0029,  0.0005],
          [ 0.0201,  0.0074,  0.0054]],

         [[ 0.0310,  0.0226,  0.0180],
          [ 0.0331,  0.0239,  0.0201],
          [ 0.0382,  0.0279,  0.0257]]],


        [[[-0.0034, -0.0017, -0.0002],
          [-0.0023, -0.0008,  0.0012],
          [ 0.0001,  0.0021,  0.0044]],

         [[-0.0048, -0.0031, -0.0021],
          [-0.0049, -0.0037, -0.0020],
          [-0.0033, -0.0018, -0.0000]],

         [[-0.0056, -0.0037, -0.0025],
          [-0.0068, -0.0052, -0.0032],
          [-0.0064, -0.0048, -0.0025]]],


        ...,


        [[[-0.0081, -0.0072, -0.0050],
          [-0.0061, -0.0056, -0.0056],
          [-0.0068, -0.0074, -0.0057]],

         [[-0.0011,  0.0012,  0.0024],
          [ 0.0004,  0.0026,  0.0028],
          [-0.0014, -0.0004,  0.0022]],

         [[ 0.0012,  0.0033,  0.0040],
          [ 0.0024,  0.0046,  0.0052],
          [ 0.0014,  0.0023,  0.0055]]],


        [[[-0.0014, -0.0045, -0.0030],
          [-0.0010, -0.0042, -0.0016],
          [ 0.0017, -0.0011,  0.0028]],

         [[ 0.0035, -0.0012, -0.0001],
          [ 0.0039, -0.0007,  0.0013],
          [ 0.0069,  0.0018,  0.0052]],

         [[-0.0027, -0.0058, -0.0036],
          [-0.0020, -0.0049, -0.0012],
          [-0.0003, -0.0032,  0.0016]]],


        [[[-0.0967, -0.0875, -0.0865],
          [-0.0902, -0.0781, -0.0751],
          [-0.1096, -0.0971, -0.0914]],

         [[-0.0678, -0.0626, -0.0668],
          [-0.0647, -0.0563, -0.0562],
          [-0.0847, -0.0740, -0.0692]],

         [[-0.0122, -0.0126, -0.0238],
          [-0.0097, -0.0078, -0.0168],
          [-0.0285, -0.0241, -0.0286]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.]], device='cuda:0')

percentage_weight_grad None

Epoch: 3 | Batch_idx: 0 |  Loss: (1.3017) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4157) |  Loss2: (0.0000) | Acc: (47.00%) (666/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4060) |  Loss2: (0.0000) | Acc: (48.00%) (1311/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4303) |  Loss2: (0.0000) | Acc: (48.00%) (1905/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4282) |  Loss2: (0.0000) | Acc: (47.00%) (2514/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4300) |  Loss2: (0.0000) | Acc: (47.00%) (3111/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4213) |  Loss2: (0.0000) | Acc: (48.00%) (3749/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4198) |  Loss2: (0.0000) | Acc: (47.00%) (4354/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4154) |  Loss2: (0.0000) | Acc: (48.00%) (4997/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4128) |  Loss2: (0.0000) | Acc: (48.00%) (5618/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4120) |  Loss2: (0.0000) | Acc: (48.00%) (6232/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4077) |  Loss2: (0.0000) | Acc: (48.00%) (6883/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4038) |  Loss2: (0.0000) | Acc: (48.00%) (7546/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4041) |  Loss2: (0.0000) | Acc: (48.00%) (8142/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4007) |  Loss2: (0.0000) | Acc: (48.00%) (8797/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.3987) |  Loss2: (0.0000) | Acc: (48.00%) (9437/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (48.00%) (10067/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.3959) |  Loss2: (0.0000) | Acc: (48.00%) (10706/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.3953) |  Loss2: (0.0000) | Acc: (49.00%) (11361/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.3945) |  Loss2: (0.0000) | Acc: (49.00%) (11994/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.3939) |  Loss2: (0.0000) | Acc: (49.00%) (12637/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.3916) |  Loss2: (0.0000) | Acc: (49.00%) (13307/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.3903) |  Loss2: (0.0000) | Acc: (49.00%) (13963/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.3887) |  Loss2: (0.0000) | Acc: (49.00%) (14603/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.3873) |  Loss2: (0.0000) | Acc: (49.00%) (15283/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.3855) |  Loss2: (0.0000) | Acc: (49.00%) (15947/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3835) |  Loss2: (0.0000) | Acc: (49.00%) (16588/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3815) |  Loss2: (0.0000) | Acc: (49.00%) (17242/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3783) |  Loss2: (0.0000) | Acc: (49.00%) (17937/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3767) |  Loss2: (0.0000) | Acc: (49.00%) (18589/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3762) |  Loss2: (0.0000) | Acc: (49.00%) (19236/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3751) |  Loss2: (0.0000) | Acc: (49.00%) (19892/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3744) |  Loss2: (0.0000) | Acc: (50.00%) (20563/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3728) |  Loss2: (0.0000) | Acc: (50.00%) (21230/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3700) |  Loss2: (0.0000) | Acc: (50.00%) (21893/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3685) |  Loss2: (0.0000) | Acc: (50.00%) (22588/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3659) |  Loss2: (0.0000) | Acc: (50.00%) (23297/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3645) |  Loss2: (0.0000) | Acc: (50.00%) (23960/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3638) |  Loss2: (0.0000) | Acc: (50.00%) (24627/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3618) |  Loss2: (0.0000) | Acc: (50.00%) (25300/50000)
# TEST : Loss: (1.3215) | Acc: (52.00%) (5202/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1317,  0.0734, -0.0025],
          [ 0.0956, -0.1719, -0.1370],
          [ 0.1270, -0.1894, -0.0213]],

         [[ 0.0430, -0.1034,  0.1979],
          [ 0.0304,  0.0092,  0.0325],
          [ 0.0067, -0.0362, -0.0313]],

         [[ 0.0560, -0.0625,  0.1082],
          [ 0.1347, -0.0836,  0.1338],
          [-0.0921,  0.0602, -0.1439]]],


        [[[ 0.1502, -0.0417,  0.0168],
          [ 0.1567,  0.1902,  0.1636],
          [ 0.0549, -0.0275,  0.1346]],

         [[-0.1613, -0.1726,  0.1584],
          [ 0.0191, -0.0437,  0.0737],
          [ 0.1514, -0.1705,  0.1409]],

         [[ 0.0891, -0.0439, -0.0305],
          [-0.0069,  0.0373, -0.1562],
          [-0.0279, -0.1264, -0.0174]]],


        [[[-0.0551,  0.1033, -0.0398],
          [-0.1040,  0.0782,  0.1255],
          [ 0.1417,  0.1823,  0.0068]],

         [[ 0.1062,  0.1314,  0.1075],
          [ 0.0852, -0.0411, -0.0420],
          [-0.0305, -0.0276,  0.1454]],

         [[-0.0515,  0.1398,  0.0644],
          [-0.0385, -0.0499, -0.0660],
          [-0.0315, -0.0070,  0.0466]]],


        ...,


        [[[ 0.1832, -0.1486, -0.1503],
          [-0.0645, -0.1043, -0.1167],
          [-0.1294,  0.0078,  0.0306]],

         [[ 0.1366,  0.1574, -0.1566],
          [ 0.1924, -0.1508, -0.1332],
          [-0.0996, -0.0983, -0.0545]],

         [[ 0.1691,  0.0839, -0.1163],
          [-0.0978, -0.1632, -0.0714],
          [ 0.0247,  0.1671, -0.1394]]],


        [[[-0.1337,  0.0696, -0.1905],
          [-0.1515, -0.1496,  0.1095],
          [-0.1657,  0.1556, -0.1294]],

         [[ 0.0869, -0.1317,  0.0636],
          [ 0.1612,  0.0180, -0.0848],
          [-0.1493,  0.1294, -0.1810]],

         [[ 0.1020, -0.0477, -0.0383],
          [ 0.1279, -0.1898, -0.0033],
          [-0.0459,  0.0114, -0.1182]]],


        [[[ 0.1074,  0.1546,  0.0795],
          [-0.0663, -0.1091,  0.1735],
          [-0.1227,  0.0523, -0.0235]],

         [[-0.0297, -0.0863,  0.1178],
          [-0.0328, -0.1858, -0.0379],
          [-0.0495,  0.0402, -0.0126]],

         [[-0.1474, -0.1725,  0.1376],
          [ 0.1384, -0.1625, -0.1058],
          [-0.0478, -0.0962,  0.2036]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-6.4687e-03,  1.6291e-02, -2.5765e-02],
          [ 1.2419e-02,  3.7017e-02, -5.5773e-03],
          [-4.5141e-02, -8.9867e-03, -3.6707e-02]],

         [[ 1.3913e-02,  3.6317e-02, -1.2960e-02],
          [ 2.5902e-02,  5.1724e-02,  3.0147e-03],
          [-2.8229e-02,  6.8048e-03, -2.6536e-02]],

         [[ 5.0743e-02,  7.8817e-02,  4.4020e-02],
          [ 4.7970e-02,  7.9195e-02,  4.3151e-02],
          [-7.8049e-03,  3.1320e-02,  9.7887e-03]]],


        [[[ 2.0312e-02,  2.8984e-02,  2.9780e-02],
          [ 1.8550e-03,  1.0693e-02,  1.1931e-02],
          [-7.7237e-03, -3.5560e-04,  1.3936e-03]],

         [[ 2.5838e-02,  3.2786e-02,  3.2298e-02],
          [ 7.3678e-03,  1.6495e-02,  1.6999e-02],
          [-1.8926e-03,  7.1179e-03,  9.8356e-03]],

         [[ 2.3468e-02,  2.8548e-02,  2.8177e-02],
          [ 4.6402e-03,  1.3851e-02,  1.3791e-02],
          [-5.0757e-03,  4.3757e-03,  7.0437e-03]]],


        [[[ 5.0336e-03, -5.1505e-04, -5.0835e-06],
          [ 7.2178e-03,  2.3897e-03,  2.1709e-03],
          [ 8.8094e-03,  4.4408e-03,  5.3467e-03]],

         [[-7.5808e-04, -5.0948e-03, -3.7898e-03],
          [ 1.4749e-03, -2.4485e-03, -2.0083e-03],
          [ 3.9070e-03,  9.8400e-06,  1.4541e-03]],

         [[-1.9523e-03, -5.3456e-03, -3.7141e-03],
          [ 3.5289e-04, -2.2807e-03, -1.4113e-03],
          [ 2.8810e-03,  1.2664e-04,  1.8927e-03]]],


        ...,


        [[[ 9.1404e-03,  5.6798e-03, -3.2432e-03],
          [ 3.2180e-03,  1.6529e-03, -3.5395e-03],
          [ 1.0609e-02,  1.0104e-02,  5.4884e-03]],

         [[ 1.5130e-02,  1.1593e-02,  9.8540e-04],
          [ 1.1419e-02,  9.7854e-03,  2.3770e-03],
          [ 1.9108e-02,  1.7374e-02,  9.5952e-03]],

         [[ 2.2521e-02,  2.2093e-02,  1.4566e-02],
          [ 1.7738e-02,  1.9781e-02,  1.5314e-02],
          [ 2.1677e-02,  2.2563e-02,  1.7355e-02]]],


        [[[-7.1679e-03, -1.0843e-02, -9.8804e-03],
          [ 5.5403e-03, -1.0684e-03, -1.9885e-03],
          [ 8.5263e-03, -8.8359e-04, -4.0334e-03]],

         [[-4.4554e-03, -1.0108e-02, -1.0142e-02],
          [ 5.8649e-03, -1.6768e-03, -2.4836e-03],
          [ 8.1724e-03, -1.4045e-03, -4.2528e-03]],

         [[ 6.4697e-03,  2.8502e-03,  1.9602e-03],
          [ 1.4932e-02,  9.4685e-03,  8.0981e-03],
          [ 1.6453e-02,  9.2854e-03,  6.7866e-03]]],


        [[[ 3.7214e-02,  2.5203e-02,  2.0536e-02],
          [ 3.2973e-02,  1.6235e-02,  1.4850e-02],
          [ 3.2355e-02,  1.4609e-02,  1.2580e-02]],

         [[ 2.2847e-02,  9.1872e-03,  4.8376e-03],
          [ 1.8200e-02,  3.0399e-03, -2.2228e-04],
          [ 1.3885e-02, -1.4675e-03, -5.3380e-03]],

         [[ 9.0086e-03, -6.2581e-03, -1.7166e-02],
          [ 3.4797e-03, -9.7253e-03, -1.9787e-02],
          [-4.7230e-03, -1.7612e-02, -2.6466e-02]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.]], device='cuda:0')

percentage_weight_grad None

Epoch: 4 | Batch_idx: 0 |  Loss: (1.2918) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3034) |  Loss2: (0.0000) | Acc: (53.00%) (755/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3155) |  Loss2: (0.0000) | Acc: (52.00%) (1417/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3139) |  Loss2: (0.0000) | Acc: (52.00%) (2094/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3042) |  Loss2: (0.0000) | Acc: (53.00%) (2787/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.2921) |  Loss2: (0.0000) | Acc: (53.00%) (3515/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2859) |  Loss2: (0.0000) | Acc: (54.00%) (4217/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2859) |  Loss2: (0.0000) | Acc: (53.00%) (4904/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (5595/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (53.00%) (6283/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2826) |  Loss2: (0.0000) | Acc: (54.00%) (7006/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2810) |  Loss2: (0.0000) | Acc: (54.00%) (7715/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2806) |  Loss2: (0.0000) | Acc: (54.00%) (8411/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2786) |  Loss2: (0.0000) | Acc: (54.00%) (9113/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2740) |  Loss2: (0.0000) | Acc: (54.00%) (9867/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2720) |  Loss2: (0.0000) | Acc: (54.00%) (10561/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2723) |  Loss2: (0.0000) | Acc: (54.00%) (11237/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2704) |  Loss2: (0.0000) | Acc: (54.00%) (11965/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2694) |  Loss2: (0.0000) | Acc: (54.00%) (12669/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2665) |  Loss2: (0.0000) | Acc: (54.00%) (13386/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2630) |  Loss2: (0.0000) | Acc: (54.00%) (14127/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2613) |  Loss2: (0.0000) | Acc: (55.00%) (14868/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2580) |  Loss2: (0.0000) | Acc: (55.00%) (15618/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2573) |  Loss2: (0.0000) | Acc: (55.00%) (16320/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2561) |  Loss2: (0.0000) | Acc: (55.00%) (17047/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2553) |  Loss2: (0.0000) | Acc: (55.00%) (17758/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2545) |  Loss2: (0.0000) | Acc: (55.00%) (18475/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2544) |  Loss2: (0.0000) | Acc: (55.00%) (19180/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2523) |  Loss2: (0.0000) | Acc: (55.00%) (19916/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2510) |  Loss2: (0.0000) | Acc: (55.00%) (20643/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2477) |  Loss2: (0.0000) | Acc: (55.00%) (21406/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2446) |  Loss2: (0.0000) | Acc: (55.00%) (22178/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2433) |  Loss2: (0.0000) | Acc: (55.00%) (22907/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2434) |  Loss2: (0.0000) | Acc: (55.00%) (23633/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2422) |  Loss2: (0.0000) | Acc: (55.00%) (24367/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2411) |  Loss2: (0.0000) | Acc: (55.00%) (25109/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2397) |  Loss2: (0.0000) | Acc: (55.00%) (25836/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2366) |  Loss2: (0.0000) | Acc: (55.00%) (26579/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2340) |  Loss2: (0.0000) | Acc: (56.00%) (27351/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2335) |  Loss2: (0.0000) | Acc: (56.00%) (28029/50000)
# TEST : Loss: (1.2388) | Acc: (55.00%) (5530/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1287,  0.0741,  0.0050],
          [ 0.0980, -0.1743, -0.1342],
          [ 0.1340, -0.1862, -0.0164]],

         [[ 0.0407, -0.1067,  0.1992],
          [ 0.0294,  0.0047,  0.0312],
          [ 0.0086, -0.0375, -0.0331]],

         [[ 0.0518, -0.0677,  0.1061],
          [ 0.1310, -0.0904,  0.1289],
          [-0.0940,  0.0550, -0.1503]]],


        [[[ 0.1482, -0.0421,  0.0163],
          [ 0.1571,  0.1911,  0.1655],
          [ 0.0538, -0.0273,  0.1354]],

         [[-0.1626, -0.1732,  0.1584],
          [ 0.0183, -0.0444,  0.0751],
          [ 0.1497, -0.1714,  0.1415]],

         [[ 0.0885, -0.0434, -0.0289],
          [-0.0068,  0.0377, -0.1529],
          [-0.0279, -0.1255, -0.0141]]],


        [[[-0.0550,  0.1033, -0.0398],
          [-0.1046,  0.0780,  0.1254],
          [ 0.1405,  0.1819,  0.0066]],

         [[ 0.1064,  0.1315,  0.1077],
          [ 0.0843, -0.0416, -0.0422],
          [-0.0316, -0.0281,  0.1452]],

         [[-0.0509,  0.1401,  0.0647],
          [-0.0390, -0.0502, -0.0662],
          [-0.0322, -0.0072,  0.0463]]],


        ...,


        [[[ 0.1858, -0.1464, -0.1490],
          [-0.0618, -0.1038, -0.1164],
          [-0.1273,  0.0089,  0.0324]],

         [[ 0.1399,  0.1610, -0.1542],
          [ 0.1957, -0.1488, -0.1320],
          [-0.0970, -0.0962, -0.0524]],

         [[ 0.1700,  0.0850, -0.1165],
          [-0.0956, -0.1623, -0.0717],
          [ 0.0268,  0.1685, -0.1384]]],


        [[[-0.1327,  0.0701, -0.1893],
          [-0.1516, -0.1501,  0.1103],
          [-0.1667,  0.1542, -0.1295]],

         [[ 0.0870, -0.1315,  0.0643],
          [ 0.1601,  0.0170, -0.0847],
          [-0.1512,  0.1274, -0.1818]],

         [[ 0.1027, -0.0467, -0.0367],
          [ 0.1269, -0.1905, -0.0031],
          [-0.0474,  0.0098, -0.1187]]],


        [[[ 0.1057,  0.1532,  0.0832],
          [-0.0668, -0.1094,  0.1773],
          [-0.1206,  0.0546, -0.0172]],

         [[-0.0290, -0.0860,  0.1249],
          [-0.0312, -0.1834, -0.0290],
          [-0.0440,  0.0466, -0.0004]],

         [[-0.1419, -0.1680,  0.1470],
          [ 0.1450, -0.1556, -0.0936],
          [-0.0379, -0.0849,  0.2200]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0469, -0.0481, -0.0190],
          [-0.0398, -0.0400,  0.0009],
          [-0.0419, -0.0481, -0.0090]],

         [[-0.0717, -0.0752, -0.0539],
          [-0.0571, -0.0612, -0.0271],
          [-0.0558, -0.0640, -0.0288]],

         [[-0.0555, -0.0591, -0.0460],
          [-0.0417, -0.0442, -0.0195],
          [-0.0409, -0.0455, -0.0197]]],


        [[[ 0.0218,  0.0134, -0.0044],
          [ 0.0079, -0.0030, -0.0149],
          [ 0.0037, -0.0029, -0.0109]],

         [[ 0.0198,  0.0137, -0.0046],
          [ 0.0068, -0.0032, -0.0173],
          [ 0.0038, -0.0037, -0.0152]],

         [[ 0.0012, -0.0018, -0.0176],
          [-0.0136, -0.0203, -0.0322],
          [-0.0175, -0.0226, -0.0320]]],


        [[[-0.0047, -0.0022,  0.0015],
          [-0.0068, -0.0024,  0.0006],
          [-0.0061, -0.0008,  0.0002]],

         [[-0.0034, -0.0010,  0.0030],
          [-0.0051, -0.0011,  0.0022],
          [-0.0040,  0.0008,  0.0024]],

         [[-0.0024, -0.0005,  0.0025],
          [-0.0052, -0.0021,  0.0007],
          [-0.0050, -0.0010,  0.0002]]],


        ...,


        [[[ 0.0091,  0.0073,  0.0014],
          [ 0.0081,  0.0076,  0.0035],
          [ 0.0112,  0.0065,  0.0036]],

         [[ 0.0109,  0.0050, -0.0042],
          [ 0.0133,  0.0076, -0.0003],
          [ 0.0191,  0.0092,  0.0018]],

         [[ 0.0144,  0.0090,  0.0023],
          [ 0.0171,  0.0120,  0.0060],
          [ 0.0244,  0.0159,  0.0098]]],


        [[[-0.0042, -0.0050, -0.0024],
          [ 0.0031, -0.0011,  0.0011],
          [ 0.0068,  0.0027,  0.0033]],

         [[-0.0117, -0.0112, -0.0093],
          [-0.0035, -0.0069, -0.0058],
          [ 0.0035, -0.0006, -0.0017]],

         [[-0.0121, -0.0112, -0.0098],
          [-0.0074, -0.0092, -0.0087],
          [-0.0028, -0.0058, -0.0067]]],


        [[[-0.0042, -0.0275, -0.0267],
          [ 0.0013, -0.0173, -0.0175],
          [-0.0034, -0.0085, -0.0142]],

         [[-0.0080, -0.0298, -0.0257],
          [-0.0085, -0.0269, -0.0236],
          [-0.0151, -0.0238, -0.0295]],

         [[-0.0111, -0.0304, -0.0311],
          [-0.0175, -0.0354, -0.0373],
          [-0.0210, -0.0317, -0.0417]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.]], device='cuda:0')

percentage_weight_grad None

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.6043) |  Loss2: (0.4000) | Acc: (60.00%) (77/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.6428) |  Loss2: (0.4000) | Acc: (55.00%) (782/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.6625) |  Loss2: (0.4000) | Acc: (55.00%) (1483/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.6798) |  Loss2: (0.4000) | Acc: (55.00%) (2184/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.6907) |  Loss2: (0.4000) | Acc: (54.00%) (2855/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.7015) |  Loss2: (0.4000) | Acc: (53.00%) (3515/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.7075) |  Loss2: (0.4000) | Acc: (53.00%) (4176/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.7021) |  Loss2: (0.4000) | Acc: (53.00%) (4866/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.7030) |  Loss2: (0.4000) | Acc: (53.00%) (5556/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.7008) |  Loss2: (0.4000) | Acc: (53.00%) (6257/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.7066) |  Loss2: (0.4000) | Acc: (53.00%) (6906/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.7035) |  Loss2: (0.4000) | Acc: (53.00%) (7599/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.7039) |  Loss2: (0.4000) | Acc: (53.00%) (8277/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.7037) |  Loss2: (0.4000) | Acc: (53.00%) (8960/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.7004) |  Loss2: (0.4000) | Acc: (53.00%) (9677/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.6998) |  Loss2: (0.4000) | Acc: (53.00%) (10383/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.6964) |  Loss2: (0.4000) | Acc: (53.00%) (11097/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.6925) |  Loss2: (0.4000) | Acc: (54.00%) (11822/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.6913) |  Loss2: (0.4000) | Acc: (53.00%) (12505/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.6888) |  Loss2: (0.3999) | Acc: (53.00%) (13192/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.6860) |  Loss2: (0.3999) | Acc: (54.00%) (13914/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.6833) |  Loss2: (0.3999) | Acc: (54.00%) (14621/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.6816) |  Loss2: (0.3999) | Acc: (54.00%) (15328/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.6801) |  Loss2: (0.3999) | Acc: (54.00%) (16007/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.6780) |  Loss2: (0.3999) | Acc: (54.00%) (16719/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.6754) |  Loss2: (0.3999) | Acc: (54.00%) (17442/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.6719) |  Loss2: (0.3999) | Acc: (54.00%) (18174/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.6716) |  Loss2: (0.3999) | Acc: (54.00%) (18862/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.6697) |  Loss2: (0.3999) | Acc: (54.00%) (19602/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.6680) |  Loss2: (0.3999) | Acc: (54.00%) (20320/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.6669) |  Loss2: (0.3999) | Acc: (54.00%) (21029/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.6667) |  Loss2: (0.3999) | Acc: (54.00%) (21740/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.6663) |  Loss2: (0.3999) | Acc: (54.00%) (22439/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.6644) |  Loss2: (0.3999) | Acc: (54.00%) (23178/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.6639) |  Loss2: (0.3999) | Acc: (54.00%) (23902/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.6631) |  Loss2: (0.3999) | Acc: (54.00%) (24628/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.6611) |  Loss2: (0.3999) | Acc: (54.00%) (25362/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.6596) |  Loss2: (0.3999) | Acc: (54.00%) (26090/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.6591) |  Loss2: (0.3998) | Acc: (54.00%) (26810/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.6584) |  Loss2: (0.3998) | Acc: (54.00%) (27477/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_005.pth.tar'
# TEST : Loss: (1.1942) | Acc: (56.00%) (5692/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1268,  0.0760,  0.0067],
          [ 0.0989, -0.1732, -0.1333],
          [ 0.1350, -0.1851, -0.0159]],

         [[ 0.0428, -0.1045,  0.2009],
          [ 0.0306,  0.0060,  0.0321],
          [ 0.0099, -0.0364, -0.0325]],

         [[ 0.0534, -0.0662,  0.1073],
          [ 0.1317, -0.0897,  0.1293],
          [-0.0933,  0.0556, -0.1501]]],


        [[[ 0.1478, -0.0423,  0.0164],
          [ 0.1568,  0.1909,  0.1655],
          [ 0.0537, -0.0273,  0.1354]],

         [[-0.1630, -0.1735,  0.1582],
          [ 0.0180, -0.0446,  0.0751],
          [ 0.1493, -0.1715,  0.1415]],

         [[ 0.0884, -0.0436, -0.0288],
          [-0.0068,  0.0377, -0.1526],
          [-0.0278, -0.1253, -0.0139]]],


        [[[-0.0549,  0.1033, -0.0398],
          [-0.1045,  0.0779,  0.1253],
          [ 0.1405,  0.1818,  0.0065]],

         [[ 0.1065,  0.1315,  0.1077],
          [ 0.0844, -0.0416, -0.0422],
          [-0.0316, -0.0282,  0.1451]],

         [[-0.0508,  0.1401,  0.0648],
          [-0.0389, -0.0501, -0.0662],
          [-0.0322, -0.0072,  0.0463]]],


        ...,


        [[[ 0.1857, -0.1463, -0.1488],
          [-0.0620, -0.1039, -0.1164],
          [-0.1274,  0.0087,  0.0323]],

         [[ 0.1400,  0.1611, -0.1539],
          [ 0.1956, -0.1487, -0.1319],
          [-0.0971, -0.0962, -0.0524]],

         [[ 0.1701,  0.0852, -0.1162],
          [-0.0955, -0.1622, -0.0716],
          [ 0.0267,  0.1685, -0.1384]]],


        [[[-0.1324,  0.0702, -0.1891],
          [-0.1515, -0.1501,  0.1103],
          [-0.1666,  0.1541, -0.1294]],

         [[ 0.0872, -0.1313,  0.0644],
          [ 0.1601,  0.0170, -0.0846],
          [-0.1511,  0.1273, -0.1817]],

         [[ 0.1028, -0.0467, -0.0366],
          [ 0.1269, -0.1906, -0.0032],
          [-0.0475,  0.0096, -0.1188]]],


        [[[ 0.1052,  0.1530,  0.0838],
          [-0.0673, -0.1096,  0.1777],
          [-0.1206,  0.0548, -0.0166]],

         [[-0.0282, -0.0847,  0.1269],
          [-0.0305, -0.1823, -0.0271],
          [-0.0430,  0.0479,  0.0015]],

         [[-0.1411, -0.1668,  0.1489],
          [ 0.1457, -0.1545, -0.0917],
          [-0.0370, -0.0836,  0.2218]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2680e-05,  7.6002e-06,  6.6741e-07],
          [ 9.8903e-06, -1.7320e-05, -1.3335e-05],
          [ 1.3496e-05, -1.8514e-05, -1.5870e-06]],

         [[ 4.2844e-06, -1.0449e-05,  2.0094e-05],
          [ 3.0593e-06,  5.9543e-07,  3.2129e-06],
          [ 9.8706e-07, -3.6356e-06, -3.2524e-06]],

         [[ 5.3371e-06, -6.6196e-06,  1.0735e-05],
          [ 1.3167e-05, -8.9674e-06,  1.2934e-05],
          [-9.3252e-06,  5.5588e-06, -1.5008e-05]]],


        [[[ 1.4784e-05, -4.2274e-06,  1.6403e-06],
          [ 1.5681e-05,  1.9093e-05,  1.6548e-05],
          [ 5.3695e-06, -2.7302e-06,  1.3544e-05]],

         [[-1.6299e-05, -1.7353e-05,  1.5818e-05],
          [ 1.7983e-06, -4.4636e-06,  7.5052e-06],
          [ 1.4934e-05, -1.7151e-05,  1.4152e-05]],

         [[ 8.8367e-06, -4.3550e-06, -2.8807e-06],
          [-6.7994e-07,  3.7735e-06, -1.5262e-05],
          [-2.7842e-06, -1.2534e-05, -1.3874e-06]]],


        [[[-5.4875e-06,  1.0329e-05, -3.9779e-06],
          [-1.0452e-05,  7.7873e-06,  1.2531e-05],
          [ 1.4049e-05,  1.8180e-05,  6.5454e-07]],

         [[ 1.0651e-05,  1.3153e-05,  1.0772e-05],
          [ 8.4368e-06, -4.1554e-06, -4.2221e-06],
          [-3.1598e-06, -2.8159e-06,  1.4506e-05]],

         [[-5.0754e-06,  1.4015e-05,  6.4812e-06],
          [-3.8857e-06, -5.0128e-06, -6.6164e-06],
          [-3.2157e-06, -7.2422e-07,  4.6274e-06]]],


        ...,


        [[[ 1.8572e-05, -1.4628e-05, -1.4879e-05],
          [-6.1982e-06, -1.0386e-05, -1.1643e-05],
          [-1.2745e-05,  8.7220e-07,  3.2306e-06]],

         [[ 1.3996e-05,  1.6110e-05, -1.5388e-05],
          [ 1.9561e-05, -1.4869e-05, -1.3185e-05],
          [-9.7058e-06, -9.6225e-06, -5.2410e-06]],

         [[ 1.7008e-05,  8.5233e-06, -1.1616e-05],
          [-9.5491e-06, -1.6216e-05, -7.1574e-06],
          [ 2.6735e-06,  1.6845e-05, -1.3839e-05]]],


        [[[-1.3244e-05,  7.0162e-06, -1.8907e-05],
          [-1.5153e-05, -1.5007e-05,  1.1026e-05],
          [-1.6657e-05,  1.5407e-05, -1.2939e-05]],

         [[ 8.7222e-06, -1.3127e-05,  6.4400e-06],
          [ 1.6009e-05,  1.6950e-06, -8.4638e-06],
          [-1.5112e-05,  1.2729e-05, -1.8173e-05]],

         [[ 1.0278e-05, -4.6654e-06, -3.6645e-06],
          [ 1.2686e-05, -1.9057e-05, -3.2125e-07],
          [-4.7480e-06,  9.6428e-07, -1.1880e-05]]],


        [[[ 1.0523e-05,  1.5304e-05,  8.3775e-06],
          [-6.7310e-06, -1.0961e-05,  1.7773e-05],
          [-1.2063e-05,  5.4840e-06, -1.6641e-06]],

         [[-2.8181e-06, -8.4746e-06,  1.2694e-05],
          [-3.0516e-06, -1.8232e-05, -2.7108e-06],
          [-4.3045e-06,  4.7929e-06,  1.4722e-07]],

         [[-1.4113e-05, -1.6683e-05,  1.4886e-05],
          [ 1.4566e-05, -1.5454e-05, -9.1743e-06],
          [-3.6981e-06, -8.3601e-06,  2.2180e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0230]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0442]], device='cuda:0')

Epoch: 6 | Batch_idx: 0 |  Loss: (1.6408) |  Loss2: (0.3996) | Acc: (57.00%) (73/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.6327) |  Loss2: (0.3996) | Acc: (55.00%) (787/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.6416) |  Loss2: (0.3996) | Acc: (55.00%) (1497/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.6266) |  Loss2: (0.3996) | Acc: (55.00%) (2220/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.6272) |  Loss2: (0.3996) | Acc: (56.00%) (2959/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.6253) |  Loss2: (0.3996) | Acc: (56.00%) (3690/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.6293) |  Loss2: (0.3996) | Acc: (56.00%) (4403/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.6224) |  Loss2: (0.3996) | Acc: (56.00%) (5151/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.6196) |  Loss2: (0.3996) | Acc: (56.00%) (5869/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.6176) |  Loss2: (0.3996) | Acc: (56.00%) (6608/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.6161) |  Loss2: (0.3996) | Acc: (56.00%) (7340/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.6141) |  Loss2: (0.3996) | Acc: (56.00%) (8080/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.6123) |  Loss2: (0.3996) | Acc: (56.00%) (8799/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.6109) |  Loss2: (0.3996) | Acc: (56.00%) (9540/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.6092) |  Loss2: (0.3996) | Acc: (56.00%) (10254/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.6108) |  Loss2: (0.3996) | Acc: (56.00%) (10978/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.6094) |  Loss2: (0.3996) | Acc: (56.00%) (11707/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.6102) |  Loss2: (0.3995) | Acc: (56.00%) (12411/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.6096) |  Loss2: (0.3995) | Acc: (56.00%) (13138/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.6088) |  Loss2: (0.3995) | Acc: (56.00%) (13868/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.6079) |  Loss2: (0.3995) | Acc: (56.00%) (14593/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.6063) |  Loss2: (0.3995) | Acc: (56.00%) (15331/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.6056) |  Loss2: (0.3995) | Acc: (56.00%) (16061/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.6077) |  Loss2: (0.3995) | Acc: (56.00%) (16754/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.6056) |  Loss2: (0.3995) | Acc: (56.00%) (17514/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.6057) |  Loss2: (0.3995) | Acc: (56.00%) (18236/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.6065) |  Loss2: (0.3995) | Acc: (56.00%) (18953/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.6084) |  Loss2: (0.3995) | Acc: (56.00%) (19638/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.6078) |  Loss2: (0.3995) | Acc: (56.00%) (20367/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.6067) |  Loss2: (0.3995) | Acc: (56.00%) (21133/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.6063) |  Loss2: (0.3995) | Acc: (56.00%) (21872/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.6060) |  Loss2: (0.3994) | Acc: (56.00%) (22617/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.6044) |  Loss2: (0.3994) | Acc: (56.00%) (23368/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.6039) |  Loss2: (0.3994) | Acc: (56.00%) (24100/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.6033) |  Loss2: (0.3994) | Acc: (56.00%) (24843/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.6029) |  Loss2: (0.3994) | Acc: (56.00%) (25581/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.6020) |  Loss2: (0.3994) | Acc: (56.00%) (26328/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.6020) |  Loss2: (0.3994) | Acc: (56.00%) (27047/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.6008) |  Loss2: (0.3994) | Acc: (56.00%) (27785/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.6008) |  Loss2: (0.3994) | Acc: (56.00%) (28457/50000)
# TEST : Loss: (1.1713) | Acc: (57.00%) (5778/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1267,  0.0760,  0.0067],
          [ 0.0989, -0.1731, -0.1333],
          [ 0.1349, -0.1851, -0.0159]],

         [[ 0.0428, -0.1045,  0.2009],
          [ 0.0306,  0.0060,  0.0321],
          [ 0.0099, -0.0363, -0.0325]],

         [[ 0.0534, -0.0662,  0.1073],
          [ 0.1316, -0.0896,  0.1293],
          [-0.0932,  0.0556, -0.1500]]],


        [[[ 0.1478, -0.0423,  0.0164],
          [ 0.1567,  0.1909,  0.1654],
          [ 0.0537, -0.0273,  0.1354]],

         [[-0.1629, -0.1735,  0.1581],
          [ 0.0180, -0.0446,  0.0750],
          [ 0.1493, -0.1714,  0.1415]],

         [[ 0.0883, -0.0435, -0.0288],
          [-0.0068,  0.0377, -0.1526],
          [-0.0278, -0.1253, -0.0139]]],


        [[[-0.0549,  0.1033, -0.0398],
          [-0.1045,  0.0778,  0.1253],
          [ 0.1404,  0.1817,  0.0065]],

         [[ 0.1065,  0.1315,  0.1077],
          [ 0.0843, -0.0415, -0.0422],
          [-0.0316, -0.0281,  0.1450]],

         [[-0.0507,  0.1401,  0.0648],
          [-0.0388, -0.0501, -0.0661],
          [-0.0321, -0.0072,  0.0463]]],


        ...,


        [[[ 0.1857, -0.1462, -0.1487],
          [-0.0620, -0.1038, -0.1164],
          [-0.1274,  0.0087,  0.0323]],

         [[ 0.1399,  0.1610, -0.1538],
          [ 0.1955, -0.1486, -0.1318],
          [-0.0970, -0.0962, -0.0524]],

         [[ 0.1700,  0.0852, -0.1161],
          [-0.0955, -0.1621, -0.0715],
          [ 0.0267,  0.1684, -0.1383]]],


        [[[-0.1324,  0.0701, -0.1890],
          [-0.1515, -0.1500,  0.1102],
          [-0.1665,  0.1540, -0.1293]],

         [[ 0.0872, -0.1312,  0.0644],
          [ 0.1600,  0.0169, -0.0846],
          [-0.1511,  0.1272, -0.1817]],

         [[ 0.1027, -0.0466, -0.0366],
          [ 0.1268, -0.1905, -0.0032],
          [-0.0475,  0.0096, -0.1188]]],


        [[[ 0.1052,  0.1530,  0.0837],
          [-0.0673, -0.1096,  0.1777],
          [-0.1206,  0.0548, -0.0166]],

         [[-0.0282, -0.0847,  0.1269],
          [-0.0305, -0.1823, -0.0271],
          [-0.0430,  0.0479,  0.0015]],

         [[-0.1411, -0.1668,  0.1488],
          [ 0.1456, -0.1545, -0.0917],
          [-0.0370, -0.0836,  0.2217]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2675e-05,  7.5972e-06,  6.6715e-07],
          [ 9.8865e-06, -1.7313e-05, -1.3329e-05],
          [ 1.3491e-05, -1.8507e-05, -1.5863e-06]],

         [[ 4.2828e-06, -1.0445e-05,  2.0086e-05],
          [ 3.0581e-06,  5.9519e-07,  3.2116e-06],
          [ 9.8666e-07, -3.6342e-06, -3.2511e-06]],

         [[ 5.3350e-06, -6.6170e-06,  1.0731e-05],
          [ 1.3162e-05, -8.9639e-06,  1.2928e-05],
          [-9.3214e-06,  5.5566e-06, -1.5002e-05]]],


        [[[ 1.4778e-05, -4.2258e-06,  1.6396e-06],
          [ 1.5674e-05,  1.9086e-05,  1.6542e-05],
          [ 5.3674e-06, -2.7291e-06,  1.3539e-05]],

         [[-1.6293e-05, -1.7346e-05,  1.5811e-05],
          [ 1.7976e-06, -4.4619e-06,  7.5023e-06],
          [ 1.4928e-05, -1.7144e-05,  1.4147e-05]],

         [[ 8.8332e-06, -4.3533e-06, -2.8796e-06],
          [-6.7966e-07,  3.7720e-06, -1.5256e-05],
          [-2.7831e-06, -1.2530e-05, -1.3869e-06]]],


        [[[-5.4853e-06,  1.0325e-05, -3.9763e-06],
          [-1.0448e-05,  7.7844e-06,  1.2526e-05],
          [ 1.4044e-05,  1.8173e-05,  6.5428e-07]],

         [[ 1.0647e-05,  1.3148e-05,  1.0768e-05],
          [ 8.4336e-06, -4.1538e-06, -4.2205e-06],
          [-3.1586e-06, -2.8148e-06,  1.4501e-05]],

         [[-5.0734e-06,  1.4010e-05,  6.4785e-06],
          [-3.8842e-06, -5.0109e-06, -6.6138e-06],
          [-3.2143e-06, -7.2393e-07,  4.6256e-06]]],


        ...,


        [[[ 1.8565e-05, -1.4622e-05, -1.4873e-05],
          [-6.1957e-06, -1.0382e-05, -1.1638e-05],
          [-1.2740e-05,  8.7187e-07,  3.2293e-06]],

         [[ 1.3990e-05,  1.6103e-05, -1.5382e-05],
          [ 1.9554e-05, -1.4863e-05, -1.3180e-05],
          [-9.7020e-06, -9.6187e-06, -5.2389e-06]],

         [[ 1.7001e-05,  8.5201e-06, -1.1611e-05],
          [-9.5454e-06, -1.6210e-05, -7.1545e-06],
          [ 2.6724e-06,  1.6839e-05, -1.3834e-05]]],


        [[[-1.3239e-05,  7.0135e-06, -1.8900e-05],
          [-1.5147e-05, -1.5002e-05,  1.1022e-05],
          [-1.6651e-05,  1.5402e-05, -1.2934e-05]],

         [[ 8.7187e-06, -1.3122e-05,  6.4374e-06],
          [ 1.6002e-05,  1.6944e-06, -8.4606e-06],
          [-1.5107e-05,  1.2724e-05, -1.8166e-05]],

         [[ 1.0274e-05, -4.6635e-06, -3.6631e-06],
          [ 1.2681e-05, -1.9050e-05, -3.2113e-07],
          [-4.7461e-06,  9.6392e-07, -1.1875e-05]]],


        [[[ 1.0519e-05,  1.5298e-05,  8.3743e-06],
          [-6.7284e-06, -1.0957e-05,  1.7766e-05],
          [-1.2058e-05,  5.4818e-06, -1.6635e-06]],

         [[-2.8170e-06, -8.4714e-06,  1.2689e-05],
          [-3.0504e-06, -1.8225e-05, -2.7097e-06],
          [-4.3027e-06,  4.7910e-06,  1.4716e-07]],

         [[-1.4108e-05, -1.6676e-05,  1.4880e-05],
          [ 1.4560e-05, -1.5448e-05, -9.1708e-06],
          [-3.6967e-06, -8.3569e-06,  2.2171e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0080]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0374]], device='cuda:0')

Epoch: 7 | Batch_idx: 0 |  Loss: (1.6143) |  Loss2: (0.3990) | Acc: (57.00%) (73/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.5985) |  Loss2: (0.3990) | Acc: (57.00%) (808/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.5866) |  Loss2: (0.3990) | Acc: (58.00%) (1567/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.5928) |  Loss2: (0.3990) | Acc: (57.00%) (2299/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.5937) |  Loss2: (0.3990) | Acc: (57.00%) (3021/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.5904) |  Loss2: (0.3990) | Acc: (57.00%) (3762/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.5899) |  Loss2: (0.3989) | Acc: (57.00%) (4512/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.5905) |  Loss2: (0.3989) | Acc: (57.00%) (5214/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.5899) |  Loss2: (0.3989) | Acc: (57.00%) (5949/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.5898) |  Loss2: (0.3989) | Acc: (57.00%) (6682/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.5855) |  Loss2: (0.3989) | Acc: (57.00%) (7454/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.5895) |  Loss2: (0.3989) | Acc: (57.00%) (8188/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.5892) |  Loss2: (0.3989) | Acc: (57.00%) (8928/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.5903) |  Loss2: (0.3989) | Acc: (57.00%) (9666/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.5906) |  Loss2: (0.3988) | Acc: (57.00%) (10399/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.5885) |  Loss2: (0.3988) | Acc: (57.00%) (11144/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.5875) |  Loss2: (0.3988) | Acc: (57.00%) (11901/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.5882) |  Loss2: (0.3988) | Acc: (57.00%) (12635/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.5859) |  Loss2: (0.3988) | Acc: (57.00%) (13409/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.5864) |  Loss2: (0.3988) | Acc: (57.00%) (14138/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.5894) |  Loss2: (0.3988) | Acc: (57.00%) (14851/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.5889) |  Loss2: (0.3987) | Acc: (57.00%) (15593/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.5861) |  Loss2: (0.3987) | Acc: (57.00%) (16364/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.5866) |  Loss2: (0.3987) | Acc: (57.00%) (17096/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.5850) |  Loss2: (0.3987) | Acc: (57.00%) (17838/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.5848) |  Loss2: (0.3987) | Acc: (57.00%) (18565/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.5844) |  Loss2: (0.3987) | Acc: (57.00%) (19328/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.5830) |  Loss2: (0.3987) | Acc: (57.00%) (20089/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.5835) |  Loss2: (0.3986) | Acc: (57.00%) (20814/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.5834) |  Loss2: (0.3986) | Acc: (57.00%) (21554/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.5825) |  Loss2: (0.3986) | Acc: (57.00%) (22312/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.5825) |  Loss2: (0.3986) | Acc: (57.00%) (23043/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.5837) |  Loss2: (0.3986) | Acc: (57.00%) (23756/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.5823) |  Loss2: (0.3986) | Acc: (57.00%) (24516/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.5832) |  Loss2: (0.3986) | Acc: (57.00%) (25242/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.5824) |  Loss2: (0.3985) | Acc: (57.00%) (25986/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.5831) |  Loss2: (0.3985) | Acc: (57.00%) (26709/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.5829) |  Loss2: (0.3985) | Acc: (57.00%) (27468/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.5830) |  Loss2: (0.3985) | Acc: (57.00%) (28211/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.5834) |  Loss2: (0.3985) | Acc: (57.00%) (28894/50000)
# TEST : Loss: (1.1614) | Acc: (58.00%) (5817/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1267,  0.0759,  0.0067],
          [ 0.0988, -0.1731, -0.1332],
          [ 0.1349, -0.1850, -0.0159]],

         [[ 0.0428, -0.1044,  0.2008],
          [ 0.0306,  0.0059,  0.0321],
          [ 0.0099, -0.0363, -0.0325]],

         [[ 0.0533, -0.0661,  0.1073],
          [ 0.1316, -0.0896,  0.1292],
          [-0.0932,  0.0555, -0.1500]]],


        [[[ 0.1477, -0.0422,  0.0164],
          [ 0.1567,  0.1908,  0.1654],
          [ 0.0537, -0.0273,  0.1353]],

         [[-0.1629, -0.1734,  0.1581],
          [ 0.0180, -0.0446,  0.0750],
          [ 0.1492, -0.1714,  0.1414]],

         [[ 0.0883, -0.0435, -0.0288],
          [-0.0068,  0.0377, -0.1525],
          [-0.0278, -0.1253, -0.0139]]],


        [[[-0.0548,  0.1032, -0.0397],
          [-0.1044,  0.0778,  0.1252],
          [ 0.1404,  0.1817,  0.0065]],

         [[ 0.1064,  0.1314,  0.1076],
          [ 0.0843, -0.0415, -0.0422],
          [-0.0316, -0.0281,  0.1449]],

         [[-0.0507,  0.1400,  0.0648],
          [-0.0388, -0.0501, -0.0661],
          [-0.0321, -0.0072,  0.0462]]],


        ...,


        [[[ 0.1856, -0.1462, -0.1487],
          [-0.0619, -0.1038, -0.1163],
          [-0.1273,  0.0087,  0.0323]],

         [[ 0.1399,  0.1610, -0.1538],
          [ 0.1955, -0.1486, -0.1317],
          [-0.0970, -0.0961, -0.0524]],

         [[ 0.1700,  0.0852, -0.1161],
          [-0.0954, -0.1620, -0.0715],
          [ 0.0267,  0.1683, -0.1383]]],


        [[[-0.1323,  0.0701, -0.1889],
          [-0.1514, -0.1500,  0.1102],
          [-0.1664,  0.1540, -0.1293]],

         [[ 0.0872, -0.1312,  0.0643],
          [ 0.1600,  0.0169, -0.0846],
          [-0.1510,  0.1272, -0.1816]],

         [[ 0.1027, -0.0466, -0.0366],
          [ 0.1268, -0.1904, -0.0032],
          [-0.0474,  0.0096, -0.1187]]],


        [[[ 0.1051,  0.1529,  0.0837],
          [-0.0673, -0.1095,  0.1776],
          [-0.1205,  0.0548, -0.0166]],

         [[-0.0282, -0.0847,  0.1268],
          [-0.0305, -0.1822, -0.0271],
          [-0.0430,  0.0479,  0.0015]],

         [[-0.1410, -0.1667,  0.1487],
          [ 0.1455, -0.1544, -0.0917],
          [-0.0370, -0.0835,  0.2216]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2669e-05,  7.5943e-06,  6.6690e-07],
          [ 9.8828e-06, -1.7306e-05, -1.3324e-05],
          [ 1.3486e-05, -1.8500e-05, -1.5856e-06]],

         [[ 4.2812e-06, -1.0441e-05,  2.0079e-05],
          [ 3.0570e-06,  5.9495e-07,  3.2103e-06],
          [ 9.8626e-07, -3.6327e-06, -3.2498e-06]],

         [[ 5.3330e-06, -6.6144e-06,  1.0726e-05],
          [ 1.3157e-05, -8.9604e-06,  1.2923e-05],
          [-9.3176e-06,  5.5544e-06, -1.4996e-05]]],


        [[[ 1.4772e-05, -4.2242e-06,  1.6389e-06],
          [ 1.5668e-05,  1.9078e-05,  1.6536e-05],
          [ 5.3654e-06, -2.7280e-06,  1.3534e-05]],

         [[-1.6286e-05, -1.7339e-05,  1.5805e-05],
          [ 1.7969e-06, -4.4601e-06,  7.4994e-06],
          [ 1.4922e-05, -1.7137e-05,  1.4141e-05]],

         [[ 8.8297e-06, -4.3515e-06, -2.8785e-06],
          [-6.7939e-07,  3.7706e-06, -1.5250e-05],
          [-2.7820e-06, -1.2525e-05, -1.3863e-06]]],


        [[[-5.4831e-06,  1.0321e-05, -3.9747e-06],
          [-1.0444e-05,  7.7815e-06,  1.2521e-05],
          [ 1.4039e-05,  1.8166e-05,  6.5403e-07]],

         [[ 1.0643e-05,  1.3143e-05,  1.0764e-05],
          [ 8.4304e-06, -4.1522e-06, -4.2189e-06],
          [-3.1574e-06, -2.8138e-06,  1.4495e-05]],

         [[-5.0713e-06,  1.4004e-05,  6.4759e-06],
          [-3.8828e-06, -5.0090e-06, -6.6112e-06],
          [-3.2130e-06, -7.2364e-07,  4.6239e-06]]],


        ...,


        [[[ 1.8558e-05, -1.4616e-05, -1.4867e-05],
          [-6.1933e-06, -1.0378e-05, -1.1633e-05],
          [-1.2734e-05,  8.7154e-07,  3.2280e-06]],

         [[ 1.3985e-05,  1.6097e-05, -1.5376e-05],
          [ 1.9546e-05, -1.4858e-05, -1.3175e-05],
          [-9.6982e-06, -9.6149e-06, -5.2369e-06]],

         [[ 1.6995e-05,  8.5169e-06, -1.1607e-05],
          [-9.5416e-06, -1.6203e-05, -7.1516e-06],
          [ 2.6714e-06,  1.6833e-05, -1.3828e-05]]],


        [[[-1.3233e-05,  7.0109e-06, -1.8892e-05],
          [-1.5141e-05, -1.4996e-05,  1.1018e-05],
          [-1.6645e-05,  1.5396e-05, -1.2929e-05]],

         [[ 8.7152e-06, -1.3116e-05,  6.4347e-06],
          [ 1.5996e-05,  1.6937e-06, -8.4574e-06],
          [-1.5101e-05,  1.2719e-05, -1.8159e-05]],

         [[ 1.0270e-05, -4.6616e-06, -3.6616e-06],
          [ 1.2676e-05, -1.9042e-05, -3.2100e-07],
          [-4.7442e-06,  9.6355e-07, -1.1871e-05]]],


        [[[ 1.0515e-05,  1.5292e-05,  8.3711e-06],
          [-6.7258e-06, -1.0953e-05,  1.7759e-05],
          [-1.2053e-05,  5.4797e-06, -1.6628e-06]],

         [[-2.8160e-06, -8.4682e-06,  1.2683e-05],
          [-3.0492e-06, -1.8218e-05, -2.7086e-06],
          [-4.3010e-06,  4.7891e-06,  1.4710e-07]],

         [[-1.4103e-05, -1.6670e-05,  1.4875e-05],
          [ 1.4554e-05, -1.5442e-05, -9.1673e-06],
          [-3.6952e-06, -8.3537e-06,  2.2162e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0189]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0348]], device='cuda:0')

Epoch: 8 | Batch_idx: 0 |  Loss: (1.8156) |  Loss2: (0.3979) | Acc: (48.00%) (62/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.6009) |  Loss2: (0.3979) | Acc: (57.00%) (804/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.5870) |  Loss2: (0.3979) | Acc: (57.00%) (1550/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.5878) |  Loss2: (0.3978) | Acc: (57.00%) (2288/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.5815) |  Loss2: (0.3978) | Acc: (57.00%) (3034/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.5812) |  Loss2: (0.3978) | Acc: (57.00%) (3784/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.5826) |  Loss2: (0.3978) | Acc: (57.00%) (4523/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.5822) |  Loss2: (0.3978) | Acc: (57.00%) (5257/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.5805) |  Loss2: (0.3978) | Acc: (57.00%) (6004/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.5797) |  Loss2: (0.3977) | Acc: (57.00%) (6744/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.5801) |  Loss2: (0.3977) | Acc: (57.00%) (7459/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.5775) |  Loss2: (0.3977) | Acc: (57.00%) (8215/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.5792) |  Loss2: (0.3977) | Acc: (57.00%) (8953/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.5786) |  Loss2: (0.3977) | Acc: (57.00%) (9683/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.5771) |  Loss2: (0.3976) | Acc: (57.00%) (10425/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.5769) |  Loss2: (0.3976) | Acc: (57.00%) (11157/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.5756) |  Loss2: (0.3976) | Acc: (57.00%) (11920/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.5764) |  Loss2: (0.3976) | Acc: (57.00%) (12669/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.5748) |  Loss2: (0.3976) | Acc: (57.00%) (13409/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.5749) |  Loss2: (0.3976) | Acc: (57.00%) (14140/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.5732) |  Loss2: (0.3975) | Acc: (57.00%) (14912/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.5722) |  Loss2: (0.3975) | Acc: (58.00%) (15672/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.5702) |  Loss2: (0.3975) | Acc: (58.00%) (16428/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.5703) |  Loss2: (0.3975) | Acc: (57.00%) (17149/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.5712) |  Loss2: (0.3975) | Acc: (57.00%) (17890/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.5727) |  Loss2: (0.3974) | Acc: (57.00%) (18633/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.5719) |  Loss2: (0.3974) | Acc: (57.00%) (19368/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.5713) |  Loss2: (0.3974) | Acc: (57.00%) (20107/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.5710) |  Loss2: (0.3974) | Acc: (58.00%) (20863/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.5712) |  Loss2: (0.3973) | Acc: (57.00%) (21597/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.5715) |  Loss2: (0.3973) | Acc: (57.00%) (22334/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.5718) |  Loss2: (0.3973) | Acc: (57.00%) (23088/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.5713) |  Loss2: (0.3973) | Acc: (58.00%) (23837/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.5701) |  Loss2: (0.3973) | Acc: (58.00%) (24575/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.5708) |  Loss2: (0.3972) | Acc: (57.00%) (25302/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.5694) |  Loss2: (0.3972) | Acc: (57.00%) (26047/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.5687) |  Loss2: (0.3972) | Acc: (58.00%) (26804/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.5687) |  Loss2: (0.3972) | Acc: (57.00%) (27539/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.5667) |  Loss2: (0.3972) | Acc: (58.00%) (28318/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.5658) |  Loss2: (0.3971) | Acc: (58.00%) (29036/50000)
# TEST : Loss: (1.1507) | Acc: (58.00%) (5870/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1266,  0.0759,  0.0067],
          [ 0.0988, -0.1730, -0.1332],
          [ 0.1348, -0.1849, -0.0158]],

         [[ 0.0428, -0.1044,  0.2007],
          [ 0.0306,  0.0059,  0.0321],
          [ 0.0099, -0.0363, -0.0325]],

         [[ 0.0533, -0.0661,  0.1072],
          [ 0.1315, -0.0896,  0.1292],
          [-0.0931,  0.0555, -0.1499]]],


        [[[ 0.1477, -0.0422,  0.0164],
          [ 0.1566,  0.1907,  0.1653],
          [ 0.0536, -0.0273,  0.1353]],

         [[-0.1628, -0.1733,  0.1580],
          [ 0.0180, -0.0446,  0.0750],
          [ 0.1492, -0.1713,  0.1414]],

         [[ 0.0883, -0.0435, -0.0288],
          [-0.0068,  0.0377, -0.1524],
          [-0.0278, -0.1252, -0.0139]]],


        [[[-0.0548,  0.1032, -0.0397],
          [-0.1044,  0.0778,  0.1252],
          [ 0.1403,  0.1816,  0.0065]],

         [[ 0.1064,  0.1314,  0.1076],
          [ 0.0843, -0.0415, -0.0422],
          [-0.0316, -0.0281,  0.1449]],

         [[-0.0507,  0.1400,  0.0647],
          [-0.0388, -0.0501, -0.0661],
          [-0.0321, -0.0072,  0.0462]]],


        ...,


        [[[ 0.1855, -0.1461, -0.1486],
          [-0.0619, -0.1037, -0.1163],
          [-0.1273,  0.0087,  0.0323]],

         [[ 0.1398,  0.1609, -0.1537],
          [ 0.1954, -0.1485, -0.1317],
          [-0.0969, -0.0961, -0.0523]],

         [[ 0.1699,  0.0851, -0.1160],
          [-0.0954, -0.1620, -0.0715],
          [ 0.0267,  0.1683, -0.1382]]],


        [[[-0.1323,  0.0701, -0.1888],
          [-0.1514, -0.1499,  0.1101],
          [-0.1664,  0.1539, -0.1292]],

         [[ 0.0871, -0.1311,  0.0643],
          [ 0.1599,  0.0169, -0.0845],
          [-0.1509,  0.1271, -0.1815]],

         [[ 0.1027, -0.0466, -0.0366],
          [ 0.1267, -0.1903, -0.0032],
          [-0.0474,  0.0096, -0.1187]]],


        [[[ 0.1051,  0.1529,  0.0837],
          [-0.0672, -0.1095,  0.1775],
          [-0.1205,  0.0548, -0.0166]],

         [[-0.0281, -0.0847,  0.1268],
          [-0.0305, -0.1821, -0.0271],
          [-0.0430,  0.0479,  0.0015]],

         [[-0.1410, -0.1666,  0.1487],
          [ 0.1455, -0.1544, -0.0916],
          [-0.0369, -0.0835,  0.2215]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2664e-05,  7.5914e-06,  6.6664e-07],
          [ 9.8790e-06, -1.7299e-05, -1.3319e-05],
          [ 1.3481e-05, -1.8493e-05, -1.5850e-06]],

         [[ 4.2796e-06, -1.0437e-05,  2.0071e-05],
          [ 3.0558e-06,  5.9472e-07,  3.2090e-06],
          [ 9.8586e-07, -3.6313e-06, -3.2485e-06]],

         [[ 5.3310e-06, -6.6117e-06,  1.0722e-05],
          [ 1.3152e-05, -8.9569e-06,  1.2918e-05],
          [-9.3138e-06,  5.5523e-06, -1.4990e-05]]],


        [[[ 1.4766e-05, -4.2226e-06,  1.6383e-06],
          [ 1.5661e-05,  1.9071e-05,  1.6529e-05],
          [ 5.3634e-06, -2.7269e-06,  1.3528e-05]],

         [[-1.6280e-05, -1.7332e-05,  1.5799e-05],
          [ 1.7961e-06, -4.4584e-06,  7.4965e-06],
          [ 1.4916e-05, -1.7130e-05,  1.4136e-05]],

         [[ 8.8262e-06, -4.3498e-06, -2.8774e-06],
          [-6.7912e-07,  3.7691e-06, -1.5244e-05],
          [-2.7809e-06, -1.2520e-05, -1.3858e-06]]],


        [[[-5.4809e-06,  1.0317e-05, -3.9731e-06],
          [-1.0440e-05,  7.7785e-06,  1.2517e-05],
          [ 1.4033e-05,  1.8159e-05,  6.5378e-07]],

         [[ 1.0639e-05,  1.3138e-05,  1.0760e-05],
          [ 8.4272e-06, -4.1506e-06, -4.2173e-06],
          [-3.1563e-06, -2.8127e-06,  1.4489e-05]],

         [[-5.0693e-06,  1.3999e-05,  6.4733e-06],
          [-3.8813e-06, -5.0071e-06, -6.6085e-06],
          [-3.2117e-06, -7.2334e-07,  4.6221e-06]]],


        ...,


        [[[ 1.8551e-05, -1.4611e-05, -1.4861e-05],
          [-6.1908e-06, -1.0374e-05, -1.1629e-05],
          [-1.2729e-05,  8.7122e-07,  3.2267e-06]],

         [[ 1.3980e-05,  1.6090e-05, -1.5370e-05],
          [ 1.9538e-05, -1.4852e-05, -1.3170e-05],
          [-9.6945e-06, -9.6111e-06, -5.2349e-06]],

         [[ 1.6989e-05,  8.5137e-06, -1.1602e-05],
          [-9.5378e-06, -1.6197e-05, -7.1487e-06],
          [ 2.6704e-06,  1.6826e-05, -1.3823e-05]]],


        [[[-1.3228e-05,  7.0083e-06, -1.8885e-05],
          [-1.5135e-05, -1.4990e-05,  1.1013e-05],
          [-1.6638e-05,  1.5390e-05, -1.2923e-05]],

         [[ 8.7117e-06, -1.3111e-05,  6.4321e-06],
          [ 1.5990e-05,  1.6931e-06, -8.4542e-06],
          [-1.5095e-05,  1.2713e-05, -1.8152e-05]],

         [[ 1.0265e-05, -4.6597e-06, -3.6602e-06],
          [ 1.2670e-05, -1.9035e-05, -3.2087e-07],
          [-4.7423e-06,  9.6319e-07, -1.1866e-05]]],


        [[[ 1.0511e-05,  1.5286e-05,  8.3679e-06],
          [-6.7232e-06, -1.0948e-05,  1.7752e-05],
          [-1.2049e-05,  5.4775e-06, -1.6622e-06]],

         [[-2.8149e-06, -8.4650e-06,  1.2678e-05],
          [-3.0481e-06, -1.8211e-05, -2.7075e-06],
          [-4.2992e-06,  4.7872e-06,  1.4704e-07]],

         [[-1.4097e-05, -1.6663e-05,  1.4869e-05],
          [ 1.4548e-05, -1.5437e-05, -9.1638e-06],
          [-3.6938e-06, -8.3505e-06,  2.2154e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0212]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0051]], device='cuda:0')

Epoch: 9 | Batch_idx: 0 |  Loss: (1.4698) |  Loss2: (0.3963) | Acc: (57.00%) (74/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.5629) |  Loss2: (0.3963) | Acc: (57.00%) (815/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.5693) |  Loss2: (0.3962) | Acc: (57.00%) (1552/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.5627) |  Loss2: (0.3962) | Acc: (58.00%) (2314/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.5758) |  Loss2: (0.3962) | Acc: (57.00%) (3021/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.5831) |  Loss2: (0.3962) | Acc: (57.00%) (3742/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.5738) |  Loss2: (0.3962) | Acc: (57.00%) (4498/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.5741) |  Loss2: (0.3961) | Acc: (57.00%) (5241/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.5711) |  Loss2: (0.3961) | Acc: (57.00%) (6005/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.5678) |  Loss2: (0.3961) | Acc: (58.00%) (6768/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.5669) |  Loss2: (0.3961) | Acc: (58.00%) (7514/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.5645) |  Loss2: (0.3960) | Acc: (58.00%) (8254/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.5655) |  Loss2: (0.3960) | Acc: (58.00%) (8986/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.5641) |  Loss2: (0.3960) | Acc: (58.00%) (9735/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.5617) |  Loss2: (0.3960) | Acc: (58.00%) (10489/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.5608) |  Loss2: (0.3960) | Acc: (58.00%) (11240/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.5624) |  Loss2: (0.3959) | Acc: (58.00%) (11987/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.5635) |  Loss2: (0.3959) | Acc: (58.00%) (12722/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.5613) |  Loss2: (0.3959) | Acc: (58.00%) (13474/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.5607) |  Loss2: (0.3959) | Acc: (58.00%) (14224/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.5602) |  Loss2: (0.3958) | Acc: (58.00%) (14979/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.5591) |  Loss2: (0.3958) | Acc: (58.00%) (15725/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.5576) |  Loss2: (0.3958) | Acc: (58.00%) (16504/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.5579) |  Loss2: (0.3958) | Acc: (58.00%) (17212/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.5572) |  Loss2: (0.3957) | Acc: (58.00%) (17968/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.5558) |  Loss2: (0.3957) | Acc: (58.00%) (18727/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.5561) |  Loss2: (0.3957) | Acc: (58.00%) (19476/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.5557) |  Loss2: (0.3957) | Acc: (58.00%) (20220/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.5568) |  Loss2: (0.3956) | Acc: (58.00%) (20952/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.5567) |  Loss2: (0.3956) | Acc: (58.00%) (21726/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.5574) |  Loss2: (0.3956) | Acc: (58.00%) (22457/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.5561) |  Loss2: (0.3956) | Acc: (58.00%) (23224/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.5554) |  Loss2: (0.3955) | Acc: (58.00%) (23973/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.5558) |  Loss2: (0.3955) | Acc: (58.00%) (24705/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.5570) |  Loss2: (0.3955) | Acc: (58.00%) (25426/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.5571) |  Loss2: (0.3955) | Acc: (58.00%) (26178/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.5556) |  Loss2: (0.3955) | Acc: (58.00%) (26984/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.5562) |  Loss2: (0.3954) | Acc: (58.00%) (27713/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.5563) |  Loss2: (0.3954) | Acc: (58.00%) (28449/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.5558) |  Loss2: (0.3954) | Acc: (58.00%) (29172/50000)
# TEST : Loss: (1.1435) | Acc: (58.00%) (5875/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1266,  0.0759,  0.0067],
          [ 0.0988, -0.1729, -0.1331],
          [ 0.1348, -0.1849, -0.0158]],

         [[ 0.0428, -0.1043,  0.2006],
          [ 0.0305,  0.0059,  0.0321],
          [ 0.0099, -0.0363, -0.0325]],

         [[ 0.0533, -0.0661,  0.1072],
          [ 0.1315, -0.0895,  0.1291],
          [-0.0931,  0.0555, -0.1498]]],


        [[[ 0.1476, -0.0422,  0.0164],
          [ 0.1565,  0.1906,  0.1652],
          [ 0.0536, -0.0273,  0.1352]],

         [[-0.1627, -0.1733,  0.1579],
          [ 0.0180, -0.0446,  0.0749],
          [ 0.1491, -0.1712,  0.1413]],

         [[ 0.0882, -0.0435, -0.0288],
          [-0.0068,  0.0377, -0.1524],
          [-0.0278, -0.1252, -0.0139]]],


        [[[-0.0548,  0.1031, -0.0397],
          [-0.1044,  0.0778,  0.1251],
          [ 0.1403,  0.1815,  0.0065]],

         [[ 0.1063,  0.1313,  0.1076],
          [ 0.0842, -0.0415, -0.0422],
          [-0.0316, -0.0281,  0.1448]],

         [[-0.0507,  0.1399,  0.0647],
          [-0.0388, -0.0501, -0.0661],
          [-0.0321, -0.0072,  0.0462]]],


        ...,


        [[[ 0.1854, -0.1460, -0.1486],
          [-0.0619, -0.1037, -0.1162],
          [-0.1272,  0.0087,  0.0323]],

         [[ 0.1397,  0.1608, -0.1536],
          [ 0.1953, -0.1485, -0.1316],
          [-0.0969, -0.0961, -0.0523]],

         [[ 0.1698,  0.0851, -0.1160],
          [-0.0953, -0.1619, -0.0715],
          [ 0.0267,  0.1682, -0.1382]]],


        [[[-0.1322,  0.0701, -0.1888],
          [-0.1513, -0.1498,  0.1101],
          [-0.1663,  0.1538, -0.1292]],

         [[ 0.0871, -0.1311,  0.0643],
          [ 0.1598,  0.0169, -0.0845],
          [-0.1509,  0.1271, -0.1814]],

         [[ 0.1026, -0.0466, -0.0366],
          [ 0.1267, -0.1903, -0.0032],
          [-0.0474,  0.0096, -0.1186]]],


        [[[ 0.1051,  0.1528,  0.0836],
          [-0.0672, -0.1094,  0.1775],
          [-0.1204,  0.0548, -0.0166]],

         [[-0.0281, -0.0846,  0.1267],
          [-0.0305, -0.1820, -0.0271],
          [-0.0430,  0.0479,  0.0015]],

         [[-0.1409, -0.1666,  0.1486],
          [ 0.1454, -0.1543, -0.0916],
          [-0.0369, -0.0835,  0.2214]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2660e-05,  7.5885e-06,  6.6639e-07],
          [ 9.8752e-06, -1.7292e-05, -1.3314e-05],
          [ 1.3475e-05, -1.8486e-05, -1.5843e-06]],

         [[ 4.2780e-06, -1.0433e-05,  2.0064e-05],
          [ 3.0546e-06,  5.9448e-07,  3.2077e-06],
          [ 9.8546e-07, -3.6298e-06, -3.2472e-06]],

         [[ 5.3289e-06, -6.6091e-06,  1.0718e-05],
          [ 1.3146e-05, -8.9534e-06,  1.2913e-05],
          [-9.3103e-06,  5.5501e-06, -1.4985e-05]]],


        [[[ 1.4760e-05, -4.2210e-06,  1.6376e-06],
          [ 1.5655e-05,  1.9063e-05,  1.6523e-05],
          [ 5.3613e-06, -2.7258e-06,  1.3523e-05]],

         [[-1.6273e-05, -1.7325e-05,  1.5792e-05],
          [ 1.7954e-06, -4.4566e-06,  7.4936e-06],
          [ 1.4911e-05, -1.7124e-05,  1.4131e-05]],

         [[ 8.8227e-06, -4.3480e-06, -2.8763e-06],
          [-6.7885e-07,  3.7677e-06, -1.5239e-05],
          [-2.7798e-06, -1.2516e-05, -1.3852e-06]]],


        [[[-5.4787e-06,  1.0313e-05, -3.9715e-06],
          [-1.0435e-05,  7.7756e-06,  1.2512e-05],
          [ 1.4028e-05,  1.8152e-05,  6.5352e-07]],

         [[ 1.0635e-05,  1.3132e-05,  1.0756e-05],
          [ 8.4239e-06, -4.1490e-06, -4.2157e-06],
          [-3.1551e-06, -2.8116e-06,  1.4483e-05]],

         [[-5.0673e-06,  1.3994e-05,  6.4707e-06],
          [-3.8798e-06, -5.0052e-06, -6.6059e-06],
          [-3.2104e-06, -7.2305e-07,  4.6204e-06]]],


        ...,


        [[[ 1.8544e-05, -1.4605e-05, -1.4855e-05],
          [-6.1883e-06, -1.0370e-05, -1.1624e-05],
          [-1.2724e-05,  8.7089e-07,  3.2254e-06]],

         [[ 1.3975e-05,  1.6084e-05, -1.5365e-05],
          [ 1.9531e-05, -1.4846e-05, -1.3164e-05],
          [-9.6907e-06, -9.6073e-06, -5.2328e-06]],

         [[ 1.6982e-05,  8.5105e-06, -1.1597e-05],
          [-9.5340e-06, -1.6191e-05, -7.1458e-06],
          [ 2.6694e-06,  1.6820e-05, -1.3818e-05]]],


        [[[-1.3223e-05,  7.0057e-06, -1.8877e-05],
          [-1.5129e-05, -1.4984e-05,  1.1009e-05],
          [-1.6632e-05,  1.5384e-05, -1.2918e-05]],

         [[ 8.7082e-06, -1.3106e-05,  6.4295e-06],
          [ 1.5983e-05,  1.6924e-06, -8.4510e-06],
          [-1.5089e-05,  1.2708e-05, -1.8145e-05]],

         [[ 1.0261e-05, -4.6578e-06, -3.6587e-06],
          [ 1.2665e-05, -1.9027e-05, -3.2074e-07],
          [-4.7404e-06,  9.6282e-07, -1.1861e-05]]],


        [[[ 1.0507e-05,  1.5281e-05,  8.3647e-06],
          [-6.7206e-06, -1.0944e-05,  1.7745e-05],
          [-1.2044e-05,  5.4753e-06, -1.6615e-06]],

         [[-2.8138e-06, -8.4618e-06,  1.2673e-05],
          [-3.0469e-06, -1.8204e-05, -2.7064e-06],
          [-4.2975e-06,  4.7853e-06,  1.4698e-07]],

         [[-1.4092e-05, -1.6657e-05,  1.4863e-05],
          [ 1.4542e-05, -1.5431e-05, -9.1603e-06],
          [-3.6923e-06, -8.3473e-06,  2.2145e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0138]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0130]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.0979) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1723) |  Loss2: (0.0000) | Acc: (58.00%) (827/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1758) |  Loss2: (0.0000) | Acc: (58.00%) (1562/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1713) |  Loss2: (0.0000) | Acc: (58.00%) (2311/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1513) |  Loss2: (0.0000) | Acc: (58.00%) (3090/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1499) |  Loss2: (0.0000) | Acc: (59.00%) (3859/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1480) |  Loss2: (0.0000) | Acc: (59.00%) (4624/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1483) |  Loss2: (0.0000) | Acc: (59.00%) (5378/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1432) |  Loss2: (0.0000) | Acc: (59.00%) (6142/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (59.00%) (6893/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1409) |  Loss2: (0.0000) | Acc: (59.00%) (7669/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1387) |  Loss2: (0.0000) | Acc: (59.00%) (8441/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1412) |  Loss2: (0.0000) | Acc: (59.00%) (9209/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1387) |  Loss2: (0.0000) | Acc: (59.00%) (9986/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1379) |  Loss2: (0.0000) | Acc: (59.00%) (10744/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1357) |  Loss2: (0.0000) | Acc: (59.00%) (11512/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1384) |  Loss2: (0.0000) | Acc: (59.00%) (12239/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1371) |  Loss2: (0.0000) | Acc: (59.00%) (12992/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1389) |  Loss2: (0.0000) | Acc: (59.00%) (13724/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1389) |  Loss2: (0.0000) | Acc: (59.00%) (14483/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1395) |  Loss2: (0.0000) | Acc: (59.00%) (15249/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (59.00%) (16050/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1367) |  Loss2: (0.0000) | Acc: (59.00%) (16779/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1351) |  Loss2: (0.0000) | Acc: (59.00%) (17543/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1330) |  Loss2: (0.0000) | Acc: (59.00%) (18325/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1294) |  Loss2: (0.0000) | Acc: (59.00%) (19140/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1276) |  Loss2: (0.0000) | Acc: (59.00%) (19932/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1273) |  Loss2: (0.0000) | Acc: (59.00%) (20709/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1257) |  Loss2: (0.0000) | Acc: (59.00%) (21507/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1241) |  Loss2: (0.0000) | Acc: (59.00%) (22286/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1226) |  Loss2: (0.0000) | Acc: (59.00%) (23076/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1210) |  Loss2: (0.0000) | Acc: (59.00%) (23876/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1204) |  Loss2: (0.0000) | Acc: (59.00%) (24645/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1208) |  Loss2: (0.0000) | Acc: (59.00%) (25405/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1195) |  Loss2: (0.0000) | Acc: (60.00%) (26201/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1177) |  Loss2: (0.0000) | Acc: (60.00%) (26983/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1173) |  Loss2: (0.0000) | Acc: (60.00%) (27758/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1165) |  Loss2: (0.0000) | Acc: (60.00%) (28536/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1145) |  Loss2: (0.0000) | Acc: (60.00%) (29341/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1132) |  Loss2: (0.0000) | Acc: (60.00%) (30118/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_010.pth.tar'
# TEST : Loss: (1.0671) | Acc: (61.00%) (6116/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1245,  0.0791,  0.0127],
          [ 0.1033, -0.1690, -0.1259],
          [ 0.1395, -0.1827, -0.0139]],

         [[ 0.0451, -0.1012,  0.2037],
          [ 0.0366,  0.0105,  0.0377],
          [ 0.0142, -0.0359, -0.0343]],

         [[ 0.0519, -0.0680,  0.1052],
          [ 0.1329, -0.0906,  0.1289],
          [-0.0945,  0.0492, -0.1581]]],


        [[[ 0.1445, -0.0454,  0.0142],
          [ 0.1549,  0.1890,  0.1655],
          [ 0.0517, -0.0283,  0.1350]],

         [[-0.1658, -0.1765,  0.1572],
          [ 0.0148, -0.0482,  0.0750],
          [ 0.1461, -0.1739,  0.1408]],

         [[ 0.0849, -0.0464, -0.0287],
          [-0.0099,  0.0346, -0.1514],
          [-0.0308, -0.1275, -0.0137]]],


        [[[-0.0544,  0.1032, -0.0395],
          [-0.1045,  0.0775,  0.1251],
          [ 0.1397,  0.1811,  0.0066]],

         [[ 0.1068,  0.1314,  0.1076],
          [ 0.0839, -0.0420, -0.0425],
          [-0.0324, -0.0289,  0.1444]],

         [[-0.0498,  0.1403,  0.0650],
          [-0.0388, -0.0504, -0.0663],
          [-0.0325, -0.0078,  0.0460]]],


        ...,


        [[[ 0.1885, -0.1437, -0.1460],
          [-0.0606, -0.1042, -0.1158],
          [-0.1249,  0.0099,  0.0346]],

         [[ 0.1439,  0.1649, -0.1499],
          [ 0.1976, -0.1470, -0.1300],
          [-0.0944, -0.0941, -0.0497]],

         [[ 0.1728,  0.0880, -0.1136],
          [-0.0932, -0.1610, -0.0709],
          [ 0.0292,  0.1696, -0.1366]]],


        [[[-0.1308,  0.0720, -0.1866],
          [-0.1501, -0.1483,  0.1120],
          [-0.1672,  0.1538, -0.1289]],

         [[ 0.0886, -0.1291,  0.0661],
          [ 0.1607,  0.0180, -0.0833],
          [-0.1525,  0.1262, -0.1823]],

         [[ 0.1031, -0.0451, -0.0351],
          [ 0.1265, -0.1897, -0.0026],
          [-0.0499,  0.0083, -0.1199]]],


        [[[ 0.0998,  0.1471,  0.0791],
          [-0.0745, -0.1183,  0.1723],
          [-0.1288,  0.0459, -0.0240]],

         [[-0.0314, -0.0880,  0.1269],
          [-0.0341, -0.1858, -0.0249],
          [-0.0438,  0.0485,  0.0040]],

         [[-0.1383, -0.1636,  0.1545],
          [ 0.1466, -0.1520, -0.0835],
          [-0.0325, -0.0758,  0.2313]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0096, -0.0046, -0.0469],
          [-0.0360, -0.0210, -0.0487],
          [-0.0664, -0.0381, -0.0567]],

         [[-0.0071,  0.0155, -0.0209],
          [-0.0306,  0.0039, -0.0138],
          [-0.0563, -0.0081, -0.0191]],

         [[ 0.0232,  0.0494,  0.0273],
          [-0.0066,  0.0296,  0.0224],
          [-0.0367,  0.0126,  0.0075]]],


        [[[ 0.0376,  0.0294,  0.0293],
          [ 0.0352,  0.0276,  0.0286],
          [ 0.0364,  0.0271,  0.0270]],

         [[ 0.0482,  0.0435,  0.0445],
          [ 0.0470,  0.0438,  0.0473],
          [ 0.0476,  0.0425,  0.0459]],

         [[ 0.0897,  0.0864,  0.0809],
          [ 0.0916,  0.0887,  0.0843],
          [ 0.0899,  0.0862,  0.0823]]],


        [[[-0.0026,  0.0015, -0.0017],
          [-0.0051,  0.0017, -0.0008],
          [-0.0054,  0.0003, -0.0020]],

         [[-0.0009,  0.0024, -0.0014],
          [-0.0039,  0.0020, -0.0013],
          [-0.0050, -0.0000, -0.0030]],

         [[-0.0020,  0.0009, -0.0036],
          [-0.0045,  0.0000, -0.0041],
          [-0.0052, -0.0015, -0.0054]]],


        ...,


        [[[-0.0193, -0.0128,  0.0047],
          [-0.0339, -0.0207, -0.0007],
          [-0.0396, -0.0249, -0.0094]],

         [[-0.0263, -0.0221, -0.0054],
          [-0.0361, -0.0244, -0.0060],
          [-0.0399, -0.0268, -0.0135]],

         [[-0.0047, -0.0008,  0.0148],
          [-0.0143, -0.0044,  0.0124],
          [-0.0168, -0.0062,  0.0057]]],


        [[[ 0.0033,  0.0060,  0.0091],
          [ 0.0042,  0.0022,  0.0034],
          [ 0.0062,  0.0033,  0.0030]],

         [[ 0.0015,  0.0013,  0.0015],
          [ 0.0030, -0.0013, -0.0031],
          [ 0.0036, -0.0016, -0.0046]],

         [[ 0.0010, -0.0007, -0.0010],
          [ 0.0002, -0.0058, -0.0075],
          [ 0.0005, -0.0067, -0.0094]]],


        [[[-0.0979, -0.0794, -0.1122],
          [-0.0982, -0.0787, -0.1083],
          [-0.0746, -0.0609, -0.0811]],

         [[-0.0860, -0.0645, -0.0948],
          [-0.0873, -0.0638, -0.0902],
          [-0.0677, -0.0471, -0.0636]],

         [[-0.1099, -0.0948, -0.1128],
          [-0.1173, -0.0992, -0.1120],
          [-0.0989, -0.0824, -0.0887]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0117]], device='cuda:0')

percentage_weight_grad tensor([[1.1678e-06]], device='cuda:0')

Epoch: 11 | Batch_idx: 0 |  Loss: (1.1925) |  Loss2: (0.0000) | Acc: (56.00%) (72/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0516) |  Loss2: (0.0000) | Acc: (61.00%) (869/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0466) |  Loss2: (0.0000) | Acc: (62.00%) (1682/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0428) |  Loss2: (0.0000) | Acc: (62.00%) (2479/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0399) |  Loss2: (0.0000) | Acc: (62.00%) (3298/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0325) |  Loss2: (0.0000) | Acc: (62.00%) (4106/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0297) |  Loss2: (0.0000) | Acc: (63.00%) (4928/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0284) |  Loss2: (0.0000) | Acc: (63.00%) (5743/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0280) |  Loss2: (0.0000) | Acc: (63.00%) (6558/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0291) |  Loss2: (0.0000) | Acc: (63.00%) (7360/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0320) |  Loss2: (0.0000) | Acc: (63.00%) (8145/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0348) |  Loss2: (0.0000) | Acc: (62.00%) (8923/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0329) |  Loss2: (0.0000) | Acc: (62.00%) (9744/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0356) |  Loss2: (0.0000) | Acc: (62.00%) (10550/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0348) |  Loss2: (0.0000) | Acc: (62.00%) (11353/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0357) |  Loss2: (0.0000) | Acc: (62.00%) (12163/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0366) |  Loss2: (0.0000) | Acc: (62.00%) (12972/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0379) |  Loss2: (0.0000) | Acc: (62.00%) (13764/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0370) |  Loss2: (0.0000) | Acc: (62.00%) (14589/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0359) |  Loss2: (0.0000) | Acc: (62.00%) (15387/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0346) |  Loss2: (0.0000) | Acc: (63.00%) (16214/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0345) |  Loss2: (0.0000) | Acc: (63.00%) (17020/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0364) |  Loss2: (0.0000) | Acc: (62.00%) (17804/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0333) |  Loss2: (0.0000) | Acc: (63.00%) (18647/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0326) |  Loss2: (0.0000) | Acc: (63.00%) (19479/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0308) |  Loss2: (0.0000) | Acc: (63.00%) (20323/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0316) |  Loss2: (0.0000) | Acc: (63.00%) (21104/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0306) |  Loss2: (0.0000) | Acc: (63.00%) (21938/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0302) |  Loss2: (0.0000) | Acc: (63.00%) (22759/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0297) |  Loss2: (0.0000) | Acc: (63.00%) (23584/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0289) |  Loss2: (0.0000) | Acc: (63.00%) (24390/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0280) |  Loss2: (0.0000) | Acc: (63.00%) (25231/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0268) |  Loss2: (0.0000) | Acc: (63.00%) (26056/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0254) |  Loss2: (0.0000) | Acc: (63.00%) (26887/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0240) |  Loss2: (0.0000) | Acc: (63.00%) (27737/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0237) |  Loss2: (0.0000) | Acc: (63.00%) (28550/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0231) |  Loss2: (0.0000) | Acc: (63.00%) (29365/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0227) |  Loss2: (0.0000) | Acc: (63.00%) (30190/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0220) |  Loss2: (0.0000) | Acc: (63.00%) (31021/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0207) |  Loss2: (0.0000) | Acc: (63.00%) (31823/50000)
# TEST : Loss: (0.9956) | Acc: (64.00%) (6419/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1212,  0.0784,  0.0204],
          [ 0.1062, -0.1706, -0.1198],
          [ 0.1461, -0.1803, -0.0068]],

         [[ 0.0452, -0.1049,  0.2060],
          [ 0.0378,  0.0065,  0.0393],
          [ 0.0176, -0.0374, -0.0329]],

         [[ 0.0509, -0.0747,  0.1022],
          [ 0.1318, -0.0982,  0.1250],
          [-0.0956,  0.0419, -0.1635]]],


        [[[ 0.1393, -0.0506,  0.0122],
          [ 0.1532,  0.1865,  0.1668],
          [ 0.0489, -0.0306,  0.1349]],

         [[-0.1686, -0.1797,  0.1573],
          [ 0.0141, -0.0499,  0.0774],
          [ 0.1446, -0.1753,  0.1419]],

         [[ 0.0835, -0.0483, -0.0276],
          [-0.0096,  0.0336, -0.1485],
          [-0.0318, -0.1290, -0.0130]]],


        [[[-0.0545,  0.1032, -0.0390],
          [-0.1054,  0.0768,  0.1250],
          [ 0.1384,  0.1803,  0.0064]],

         [[ 0.1068,  0.1316,  0.1081],
          [ 0.0828, -0.0427, -0.0427],
          [-0.0338, -0.0298,  0.1439]],

         [[-0.0493,  0.1409,  0.0659],
          [-0.0394, -0.0508, -0.0662],
          [-0.0337, -0.0086,  0.0455]]],


        ...,


        [[[ 0.1908, -0.1433, -0.1437],
          [-0.0601, -0.1060, -0.1148],
          [-0.1250,  0.0080,  0.0360]],

         [[ 0.1476,  0.1676, -0.1456],
          [ 0.1998, -0.1463, -0.1272],
          [-0.0932, -0.0946, -0.0477]],

         [[ 0.1763,  0.0908, -0.1094],
          [-0.0905, -0.1599, -0.0680],
          [ 0.0311,  0.1697, -0.1345]]],


        [[[-0.1295,  0.0716, -0.1853],
          [-0.1491, -0.1489,  0.1127],
          [-0.1670,  0.1530, -0.1290]],

         [[ 0.0896, -0.1292,  0.0670],
          [ 0.1615,  0.0171, -0.0831],
          [-0.1526,  0.1250, -0.1829]],

         [[ 0.1042, -0.0445, -0.0333],
          [ 0.1272, -0.1900, -0.0017],
          [-0.0500,  0.0075, -0.1200]]],


        [[[ 0.0955,  0.1455,  0.0824],
          [-0.0766, -0.1186,  0.1775],
          [-0.1310,  0.0429, -0.0253]],

         [[-0.0325, -0.0878,  0.1326],
          [-0.0315, -0.1817, -0.0143],
          [-0.0384,  0.0530,  0.0094]],

         [[-0.1397, -0.1662,  0.1566],
          [ 0.1469, -0.1522, -0.0780],
          [-0.0290, -0.0744,  0.2329]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0551, -0.0500, -0.0566],
          [-0.0454, -0.0522, -0.0501],
          [-0.0578, -0.0597, -0.0652]],

         [[-0.0703, -0.0707, -0.0789],
          [-0.0684, -0.0798, -0.0762],
          [-0.0847, -0.0846, -0.0907]],

         [[-0.0721, -0.0651, -0.0710],
          [-0.0673, -0.0638, -0.0617],
          [-0.0799, -0.0598, -0.0616]]],


        [[[ 0.0015,  0.0102,  0.0173],
          [-0.0052,  0.0026,  0.0072],
          [-0.0024,  0.0051,  0.0112]],

         [[ 0.0089,  0.0151,  0.0224],
          [ 0.0043,  0.0107,  0.0171],
          [ 0.0034,  0.0120,  0.0233]],

         [[ 0.0145,  0.0170,  0.0219],
          [ 0.0063,  0.0102,  0.0154],
          [ 0.0025,  0.0102,  0.0194]]],


        [[[ 0.0018, -0.0007, -0.0042],
          [ 0.0068,  0.0040, -0.0006],
          [ 0.0099,  0.0073,  0.0022]],

         [[-0.0007, -0.0033, -0.0065],
          [ 0.0044,  0.0012, -0.0028],
          [ 0.0082,  0.0051,  0.0008]],

         [[-0.0009, -0.0031, -0.0053],
          [ 0.0042,  0.0012, -0.0022],
          [ 0.0086,  0.0055,  0.0016]]],


        ...,


        [[[-0.0083, -0.0120, -0.0162],
          [ 0.0084,  0.0031, -0.0009],
          [ 0.0149,  0.0081,  0.0032]],

         [[-0.0091, -0.0119, -0.0182],
          [ 0.0081,  0.0033, -0.0035],
          [ 0.0136,  0.0055,  0.0005]],

         [[-0.0049, -0.0103, -0.0174],
          [ 0.0093,  0.0022, -0.0056],
          [ 0.0143,  0.0049, -0.0025]]],


        [[[-0.0025, -0.0019, -0.0078],
          [ 0.0033,  0.0015, -0.0031],
          [ 0.0074,  0.0068,  0.0024]],

         [[-0.0053, -0.0025, -0.0084],
          [ 0.0001, -0.0004, -0.0039],
          [ 0.0037,  0.0042,  0.0005]],

         [[-0.0083, -0.0070, -0.0102],
          [-0.0034, -0.0049, -0.0054],
          [ 0.0002, -0.0003, -0.0018]]],


        [[[ 0.0466,  0.0584,  0.0638],
          [ 0.0477,  0.0653,  0.0792],
          [ 0.0314,  0.0577,  0.0798]],

         [[ 0.0419,  0.0481,  0.0438],
          [ 0.0496,  0.0699,  0.0739],
          [ 0.0406,  0.0756,  0.0904]],

         [[ 0.0584,  0.0623,  0.0517],
          [ 0.0642,  0.0791,  0.0744],
          [ 0.0560,  0.0853,  0.0914]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0117]], device='cuda:0')

percentage_weight_grad tensor([[1.1673e-06]], device='cuda:0')

Epoch: 12 | Batch_idx: 0 |  Loss: (0.9253) |  Loss2: (0.0000) | Acc: (62.00%) (80/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (1.0040) |  Loss2: (0.0000) | Acc: (63.00%) (894/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9956) |  Loss2: (0.0000) | Acc: (63.00%) (1719/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9825) |  Loss2: (0.0000) | Acc: (65.00%) (2582/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9910) |  Loss2: (0.0000) | Acc: (64.00%) (3387/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9850) |  Loss2: (0.0000) | Acc: (64.00%) (4233/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9791) |  Loss2: (0.0000) | Acc: (64.00%) (5072/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9795) |  Loss2: (0.0000) | Acc: (64.00%) (5907/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9808) |  Loss2: (0.0000) | Acc: (64.00%) (6720/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9803) |  Loss2: (0.0000) | Acc: (64.00%) (7556/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9793) |  Loss2: (0.0000) | Acc: (65.00%) (8409/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9788) |  Loss2: (0.0000) | Acc: (65.00%) (9261/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9769) |  Loss2: (0.0000) | Acc: (65.00%) (10110/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9757) |  Loss2: (0.0000) | Acc: (65.00%) (10953/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9780) |  Loss2: (0.0000) | Acc: (65.00%) (11773/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9769) |  Loss2: (0.0000) | Acc: (65.00%) (12629/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9777) |  Loss2: (0.0000) | Acc: (65.00%) (13454/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9746) |  Loss2: (0.0000) | Acc: (65.00%) (14329/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9752) |  Loss2: (0.0000) | Acc: (65.00%) (15165/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9731) |  Loss2: (0.0000) | Acc: (65.00%) (16025/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9723) |  Loss2: (0.0000) | Acc: (65.00%) (16870/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9715) |  Loss2: (0.0000) | Acc: (65.00%) (17735/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9713) |  Loss2: (0.0000) | Acc: (65.00%) (18590/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9704) |  Loss2: (0.0000) | Acc: (65.00%) (19437/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9714) |  Loss2: (0.0000) | Acc: (65.00%) (20266/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9675) |  Loss2: (0.0000) | Acc: (65.00%) (21158/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9664) |  Loss2: (0.0000) | Acc: (65.00%) (21989/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9650) |  Loss2: (0.0000) | Acc: (65.00%) (22831/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9637) |  Loss2: (0.0000) | Acc: (65.00%) (23690/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9627) |  Loss2: (0.0000) | Acc: (65.00%) (24561/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9627) |  Loss2: (0.0000) | Acc: (65.00%) (25408/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9595) |  Loss2: (0.0000) | Acc: (66.00%) (26307/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9595) |  Loss2: (0.0000) | Acc: (66.00%) (27158/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9576) |  Loss2: (0.0000) | Acc: (66.00%) (28035/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9573) |  Loss2: (0.0000) | Acc: (66.00%) (28882/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9576) |  Loss2: (0.0000) | Acc: (66.00%) (29735/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9560) |  Loss2: (0.0000) | Acc: (66.00%) (30620/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9551) |  Loss2: (0.0000) | Acc: (66.00%) (31493/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9542) |  Loss2: (0.0000) | Acc: (66.00%) (32367/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (33186/50000)
# TEST : Loss: (0.9476) | Acc: (65.00%) (6587/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1295,  0.0707,  0.0239],
          [ 0.1020, -0.1735, -0.1148],
          [ 0.1499, -0.1780, -0.0030]],

         [[ 0.0361, -0.1133,  0.2056],
          [ 0.0338,  0.0027,  0.0400],
          [ 0.0200, -0.0390, -0.0359]],

         [[ 0.0469, -0.0796,  0.1035],
          [ 0.1319, -0.1005,  0.1253],
          [-0.0919,  0.0394, -0.1679]]],


        [[[ 0.1362, -0.0545,  0.0093],
          [ 0.1512,  0.1837,  0.1668],
          [ 0.0450, -0.0343,  0.1326]],

         [[-0.1701, -0.1832,  0.1555],
          [ 0.0120, -0.0540,  0.0769],
          [ 0.1406, -0.1805,  0.1387]],

         [[ 0.0825, -0.0509, -0.0283],
          [-0.0119,  0.0294, -0.1485],
          [-0.0361, -0.1347, -0.0163]]],


        [[[-0.0550,  0.1029, -0.0386],
          [-0.1064,  0.0762,  0.1250],
          [ 0.1374,  0.1794,  0.0059]],

         [[ 0.1071,  0.1320,  0.1090],
          [ 0.0824, -0.0427, -0.0422],
          [-0.0341, -0.0301,  0.1437]],

         [[-0.0487,  0.1415,  0.0670],
          [-0.0394, -0.0505, -0.0656],
          [-0.0338, -0.0086,  0.0455]]],


        ...,


        [[[ 0.1951, -0.1407, -0.1410],
          [-0.0565, -0.1043, -0.1126],
          [-0.1227,  0.0096,  0.0387]],

         [[ 0.1526,  0.1710, -0.1428],
          [ 0.2042, -0.1439, -0.1256],
          [-0.0907, -0.0930, -0.0460]],

         [[ 0.1809,  0.0945, -0.1061],
          [-0.0854, -0.1565, -0.0653],
          [ 0.0351,  0.1729, -0.1311]]],


        [[[-0.1294,  0.0711, -0.1848],
          [-0.1483, -0.1491,  0.1130],
          [-0.1672,  0.1522, -0.1288]],

         [[ 0.0892, -0.1303,  0.0665],
          [ 0.1618,  0.0165, -0.0834],
          [-0.1529,  0.1242, -0.1832]],

         [[ 0.1043, -0.0449, -0.0333],
          [ 0.1280, -0.1898, -0.0016],
          [-0.0497,  0.0074, -0.1199]]],


        [[[ 0.0906,  0.1443,  0.0823],
          [-0.0806, -0.1197,  0.1798],
          [-0.1326,  0.0420, -0.0267]],

         [[-0.0363, -0.0902,  0.1329],
          [-0.0338, -0.1824, -0.0100],
          [-0.0357,  0.0556,  0.0116]],

         [[-0.1416, -0.1684,  0.1565],
          [ 0.1457, -0.1528, -0.0737],
          [-0.0256, -0.0711,  0.2369]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1196, -0.1419, -0.1262],
          [-0.0963, -0.1087, -0.0926],
          [-0.0532, -0.0939, -0.0707]],

         [[-0.1084, -0.1421, -0.1408],
          [-0.0852, -0.1059, -0.1051],
          [-0.0460, -0.0901, -0.0743]],

         [[-0.0722, -0.0908, -0.0800],
          [-0.0408, -0.0428, -0.0304],
          [ 0.0098, -0.0162,  0.0085]]],


        [[[-0.0118, -0.0045, -0.0076],
          [ 0.0073,  0.0044, -0.0011],
          [ 0.0247,  0.0185,  0.0096]],

         [[ 0.0003,  0.0054, -0.0049],
          [ 0.0246,  0.0213,  0.0064],
          [ 0.0443,  0.0388,  0.0226]],

         [[-0.0152, -0.0115, -0.0205],
          [ 0.0093,  0.0036, -0.0115],
          [ 0.0284,  0.0203,  0.0040]]],


        [[[-0.0055, -0.0019,  0.0013],
          [-0.0075, -0.0024,  0.0002],
          [-0.0058, -0.0008,  0.0025]],

         [[-0.0043, -0.0010,  0.0017],
          [-0.0057, -0.0009,  0.0013],
          [-0.0033,  0.0010,  0.0041]],

         [[-0.0023,  0.0004,  0.0030],
          [-0.0027,  0.0011,  0.0028],
          [ 0.0001,  0.0034,  0.0057]]],


        ...,


        [[[ 0.0136,  0.0084, -0.0031],
          [ 0.0136,  0.0159,  0.0031],
          [ 0.0140,  0.0210,  0.0159]],

         [[ 0.0148,  0.0051, -0.0089],
          [ 0.0186,  0.0165, -0.0002],
          [ 0.0181,  0.0212,  0.0139]],

         [[ 0.0511,  0.0394,  0.0264],
          [ 0.0520,  0.0493,  0.0356],
          [ 0.0507,  0.0522,  0.0482]]],


        [[[-0.0131, -0.0104, -0.0039],
          [-0.0049, -0.0045,  0.0005],
          [ 0.0030,  0.0012,  0.0058]],

         [[-0.0053, -0.0040,  0.0009],
          [ 0.0021,  0.0015,  0.0062],
          [ 0.0068,  0.0040,  0.0095]],

         [[-0.0044, -0.0061, -0.0040],
          [ 0.0002, -0.0027, -0.0005],
          [ 0.0017, -0.0024,  0.0007]]],


        [[[-0.0686, -0.0823, -0.0828],
          [-0.0765, -0.1012, -0.1069],
          [-0.0784, -0.1150, -0.1262]],

         [[-0.0777, -0.0878, -0.0974],
          [-0.0824, -0.1082, -0.1252],
          [-0.0877, -0.1220, -0.1424]],

         [[-0.0959, -0.0930, -0.1072],
          [-0.0946, -0.1122, -0.1395],
          [-0.0964, -0.1241, -0.1561]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0117]], device='cuda:0')

percentage_weight_grad tensor([[1.1668e-06]], device='cuda:0')

Epoch: 13 | Batch_idx: 0 |  Loss: (0.8922) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9211) |  Loss2: (0.0000) | Acc: (65.00%) (926/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9020) |  Loss2: (0.0000) | Acc: (66.00%) (1796/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9068) |  Loss2: (0.0000) | Acc: (67.00%) (2661/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9101) |  Loss2: (0.0000) | Acc: (67.00%) (3521/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9091) |  Loss2: (0.0000) | Acc: (67.00%) (4380/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9115) |  Loss2: (0.0000) | Acc: (67.00%) (5248/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9126) |  Loss2: (0.0000) | Acc: (67.00%) (6128/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9140) |  Loss2: (0.0000) | Acc: (67.00%) (6986/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9092) |  Loss2: (0.0000) | Acc: (67.00%) (7881/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.9057) |  Loss2: (0.0000) | Acc: (67.00%) (8759/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.9101) |  Loss2: (0.0000) | Acc: (67.00%) (9603/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.9080) |  Loss2: (0.0000) | Acc: (67.00%) (10486/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.9102) |  Loss2: (0.0000) | Acc: (67.00%) (11346/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.9101) |  Loss2: (0.0000) | Acc: (67.00%) (12212/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.9098) |  Loss2: (0.0000) | Acc: (67.00%) (13071/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.9081) |  Loss2: (0.0000) | Acc: (67.00%) (13951/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9074) |  Loss2: (0.0000) | Acc: (67.00%) (14818/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9073) |  Loss2: (0.0000) | Acc: (67.00%) (15700/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9063) |  Loss2: (0.0000) | Acc: (67.00%) (16581/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9046) |  Loss2: (0.0000) | Acc: (67.00%) (17474/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.9041) |  Loss2: (0.0000) | Acc: (67.00%) (18343/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.9039) |  Loss2: (0.0000) | Acc: (67.00%) (19205/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.9033) |  Loss2: (0.0000) | Acc: (67.00%) (20074/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.9010) |  Loss2: (0.0000) | Acc: (67.00%) (20969/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.9002) |  Loss2: (0.0000) | Acc: (68.00%) (21848/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8999) |  Loss2: (0.0000) | Acc: (68.00%) (22725/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8996) |  Loss2: (0.0000) | Acc: (68.00%) (23611/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8993) |  Loss2: (0.0000) | Acc: (68.00%) (24515/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8980) |  Loss2: (0.0000) | Acc: (68.00%) (25407/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8986) |  Loss2: (0.0000) | Acc: (68.00%) (26257/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8977) |  Loss2: (0.0000) | Acc: (68.00%) (27132/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8985) |  Loss2: (0.0000) | Acc: (68.00%) (28005/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8981) |  Loss2: (0.0000) | Acc: (68.00%) (28894/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8974) |  Loss2: (0.0000) | Acc: (68.00%) (29786/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8961) |  Loss2: (0.0000) | Acc: (68.00%) (30670/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (31564/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8937) |  Loss2: (0.0000) | Acc: (68.00%) (32462/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8934) |  Loss2: (0.0000) | Acc: (68.00%) (33347/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (34189/50000)
# TEST : Loss: (0.9430) | Acc: (66.00%) (6614/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1316,  0.0713,  0.0311],
          [ 0.1040, -0.1734, -0.1073],
          [ 0.1588, -0.1748,  0.0033]],

         [[ 0.0319, -0.1148,  0.2075],
          [ 0.0340,  0.0012,  0.0439],
          [ 0.0242, -0.0400, -0.0349]],

         [[ 0.0431, -0.0817,  0.1026],
          [ 0.1316, -0.1038,  0.1253],
          [-0.0892,  0.0353, -0.1715]]],


        [[[ 0.1340, -0.0571,  0.0073],
          [ 0.1509,  0.1826,  0.1668],
          [ 0.0427, -0.0369,  0.1311]],

         [[-0.1708, -0.1853,  0.1547],
          [ 0.0117, -0.0561,  0.0776],
          [ 0.1380, -0.1840,  0.1380]],

         [[ 0.0840, -0.0511, -0.0270],
          [-0.0103,  0.0292, -0.1459],
          [-0.0369, -0.1367, -0.0156]]],


        [[[-0.0554,  0.1028, -0.0384],
          [-0.1075,  0.0755,  0.1246],
          [ 0.1358,  0.1784,  0.0052]],

         [[ 0.1070,  0.1325,  0.1098],
          [ 0.0816, -0.0429, -0.0420],
          [-0.0352, -0.0307,  0.1434]],

         [[-0.0482,  0.1425,  0.0683],
          [-0.0397, -0.0504, -0.0652],
          [-0.0345, -0.0090,  0.0455]]],


        ...,


        [[[ 0.2005, -0.1365, -0.1371],
          [-0.0540, -0.1026, -0.1103],
          [-0.1191,  0.0128,  0.0431]],

         [[ 0.1578,  0.1754, -0.1397],
          [ 0.2070, -0.1417, -0.1241],
          [-0.0868, -0.0899, -0.0425]],

         [[ 0.1830,  0.0960, -0.1051],
          [-0.0846, -0.1564, -0.0656],
          [ 0.0368,  0.1736, -0.1296]]],


        [[[-0.1295,  0.0709, -0.1844],
          [-0.1476, -0.1482,  0.1144],
          [-0.1676,  0.1522, -0.1279]],

         [[ 0.0895, -0.1302,  0.0662],
          [ 0.1627,  0.0172, -0.0826],
          [-0.1532,  0.1238, -0.1832]],

         [[ 0.1052, -0.0440, -0.0327],
          [ 0.1293, -0.1886, -0.0004],
          [-0.0493,  0.0076, -0.1198]]],


        [[[ 0.0864,  0.1425,  0.0805],
          [-0.0853, -0.1227,  0.1789],
          [-0.1352,  0.0398, -0.0279]],

         [[-0.0391, -0.0919,  0.1322],
          [-0.0352, -0.1829, -0.0080],
          [-0.0319,  0.0598,  0.0154]],

         [[-0.1418, -0.1709,  0.1537],
          [ 0.1467, -0.1534, -0.0722],
          [-0.0196, -0.0657,  0.2418]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1588, -0.1393, -0.1597],
          [-0.1576, -0.1226, -0.1374],
          [-0.1861, -0.1483, -0.1672]],

         [[-0.1461, -0.1203, -0.1299],
          [-0.1493, -0.1105, -0.1115],
          [-0.1684, -0.1306, -0.1359]],

         [[-0.1420, -0.1270, -0.1304],
          [-0.1502, -0.1179, -0.1119],
          [-0.1585, -0.1259, -0.1216]]],


        [[[ 0.0205,  0.0254,  0.0153],
          [ 0.0251,  0.0332,  0.0266],
          [ 0.0258,  0.0301,  0.0233]],

         [[ 0.0190,  0.0266,  0.0216],
          [ 0.0246,  0.0352,  0.0324],
          [ 0.0282,  0.0342,  0.0294]],

         [[ 0.0411,  0.0454,  0.0359],
          [ 0.0481,  0.0560,  0.0483],
          [ 0.0542,  0.0581,  0.0487]]],


        [[[ 0.0022,  0.0031,  0.0012],
          [ 0.0033,  0.0034,  0.0013],
          [ 0.0012,  0.0004, -0.0014]],

         [[ 0.0006,  0.0011, -0.0009],
          [ 0.0029,  0.0025,  0.0004],
          [ 0.0021,  0.0010, -0.0007]],

         [[-0.0004,  0.0001, -0.0022],
          [ 0.0025,  0.0021, -0.0005],
          [ 0.0024,  0.0015, -0.0009]]],


        ...,


        [[[-0.0073,  0.0003,  0.0008],
          [-0.0108, -0.0042, -0.0013],
          [-0.0222, -0.0133, -0.0129]],

         [[ 0.0045,  0.0113,  0.0095],
          [ 0.0015,  0.0081,  0.0090],
          [-0.0067,  0.0012, -0.0008]],

         [[ 0.0155,  0.0216,  0.0209],
          [ 0.0097,  0.0171,  0.0188],
          [ 0.0027,  0.0087,  0.0074]]],


        [[[ 0.0014,  0.0042,  0.0022],
          [-0.0034,  0.0033,  0.0028],
          [ 0.0047,  0.0113,  0.0077]],

         [[-0.0073, -0.0047, -0.0068],
          [-0.0096, -0.0041, -0.0047],
          [-0.0003,  0.0046,  0.0002]],

         [[-0.0170, -0.0171, -0.0184],
          [-0.0171, -0.0145, -0.0156],
          [-0.0046, -0.0020, -0.0074]]],


        [[[-0.0136,  0.0026, -0.0038],
          [-0.0240, -0.0095, -0.0134],
          [-0.0213, -0.0203, -0.0163]],

         [[-0.0125,  0.0091,  0.0100],
          [-0.0180,  0.0033,  0.0076],
          [-0.0121, -0.0012,  0.0077]],

         [[ 0.0274,  0.0501,  0.0594],
          [ 0.0124,  0.0332,  0.0425],
          [ 0.0056,  0.0187,  0.0306]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0117]], device='cuda:0')

percentage_weight_grad tensor([[1.1664e-06]], device='cuda:0')

Epoch: 14 | Batch_idx: 0 |  Loss: (0.7934) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8646) |  Loss2: (0.0000) | Acc: (67.00%) (954/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8597) |  Loss2: (0.0000) | Acc: (69.00%) (1855/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8606) |  Loss2: (0.0000) | Acc: (69.00%) (2752/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (3639/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8719) |  Loss2: (0.0000) | Acc: (69.00%) (4517/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8631) |  Loss2: (0.0000) | Acc: (69.00%) (5446/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8618) |  Loss2: (0.0000) | Acc: (69.00%) (6338/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8614) |  Loss2: (0.0000) | Acc: (69.00%) (7221/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8595) |  Loss2: (0.0000) | Acc: (69.00%) (8121/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (8985/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8668) |  Loss2: (0.0000) | Acc: (69.00%) (9847/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8639) |  Loss2: (0.0000) | Acc: (69.00%) (10766/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8626) |  Loss2: (0.0000) | Acc: (69.00%) (11664/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8606) |  Loss2: (0.0000) | Acc: (69.00%) (12598/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8625) |  Loss2: (0.0000) | Acc: (69.00%) (13474/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8618) |  Loss2: (0.0000) | Acc: (69.00%) (14356/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8615) |  Loss2: (0.0000) | Acc: (69.00%) (15254/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8617) |  Loss2: (0.0000) | Acc: (69.00%) (16149/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8602) |  Loss2: (0.0000) | Acc: (69.00%) (17057/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8584) |  Loss2: (0.0000) | Acc: (69.00%) (17979/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8576) |  Loss2: (0.0000) | Acc: (69.00%) (18885/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8588) |  Loss2: (0.0000) | Acc: (69.00%) (19757/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8589) |  Loss2: (0.0000) | Acc: (69.00%) (20629/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8570) |  Loss2: (0.0000) | Acc: (69.00%) (21541/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8563) |  Loss2: (0.0000) | Acc: (69.00%) (22423/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8576) |  Loss2: (0.0000) | Acc: (69.00%) (23316/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8577) |  Loss2: (0.0000) | Acc: (69.00%) (24198/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8580) |  Loss2: (0.0000) | Acc: (69.00%) (25050/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8574) |  Loss2: (0.0000) | Acc: (69.00%) (25945/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (69.00%) (26872/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8541) |  Loss2: (0.0000) | Acc: (69.00%) (27799/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8533) |  Loss2: (0.0000) | Acc: (69.00%) (28706/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8524) |  Loss2: (0.0000) | Acc: (69.00%) (29609/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (30492/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8519) |  Loss2: (0.0000) | Acc: (69.00%) (31400/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8509) |  Loss2: (0.0000) | Acc: (69.00%) (32311/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8504) |  Loss2: (0.0000) | Acc: (69.00%) (33207/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (69.00%) (34112/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (69.00%) (34988/50000)
# TEST : Loss: (0.9062) | Acc: (68.00%) (6843/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1323,  0.0696,  0.0360],
          [ 0.1034, -0.1757, -0.1021],
          [ 0.1631, -0.1703,  0.0106]],

         [[ 0.0295, -0.1183,  0.2096],
          [ 0.0338, -0.0017,  0.0469],
          [ 0.0275, -0.0378, -0.0324]],

         [[ 0.0400, -0.0865,  0.1031],
          [ 0.1317, -0.1078,  0.1264],
          [-0.0869,  0.0343, -0.1731]]],


        [[[ 0.1336, -0.0578,  0.0077],
          [ 0.1517,  0.1828,  0.1692],
          [ 0.0426, -0.0375,  0.1312]],

         [[-0.1672, -0.1834,  0.1581],
          [ 0.0159, -0.0538,  0.0829],
          [ 0.1408, -0.1828,  0.1407]],

         [[ 0.0885, -0.0487, -0.0231],
          [-0.0058,  0.0312, -0.1406],
          [-0.0336, -0.1356, -0.0125]]],


        [[[-0.0558,  0.1025, -0.0383],
          [-0.1079,  0.0755,  0.1250],
          [ 0.1353,  0.1783,  0.0055]],

         [[ 0.1068,  0.1323,  0.1099],
          [ 0.0814, -0.0427, -0.0415],
          [-0.0354, -0.0306,  0.1435]],

         [[-0.0483,  0.1425,  0.0687],
          [-0.0397, -0.0501, -0.0647],
          [-0.0346, -0.0089,  0.0455]]],


        ...,


        [[[ 0.2000, -0.1352, -0.1337],
          [-0.0568, -0.1043, -0.1100],
          [-0.1205,  0.0118,  0.0432]],

         [[ 0.1583,  0.1777, -0.1361],
          [ 0.2049, -0.1428, -0.1241],
          [-0.0882, -0.0912, -0.0434]],

         [[ 0.1850,  0.0994, -0.1012],
          [-0.0841, -0.1556, -0.0646],
          [ 0.0383,  0.1745, -0.1291]]],


        [[[-0.1277,  0.0722, -0.1827],
          [-0.1466, -0.1477,  0.1148],
          [-0.1673,  0.1516, -0.1288]],

         [[ 0.0907, -0.1294,  0.0672],
          [ 0.1631,  0.0170, -0.0829],
          [-0.1536,  0.1226, -0.1848]],

         [[ 0.1073, -0.0424, -0.0308],
          [ 0.1310, -0.1875,  0.0004],
          [-0.0487,  0.0073, -0.1205]]],


        [[[ 0.0812,  0.1402,  0.0794],
          [-0.0922, -0.1288,  0.1779],
          [-0.1402,  0.0344, -0.0292]],

         [[-0.0365, -0.0884,  0.1364],
          [-0.0338, -0.1813, -0.0018],
          [-0.0275,  0.0635,  0.0215]],

         [[-0.1366, -0.1672,  0.1586],
          [ 0.1481, -0.1519, -0.0655],
          [-0.0146, -0.0602,  0.2496]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0731,  0.0560,  0.0525],
          [ 0.0788,  0.0721,  0.0637],
          [ 0.1099,  0.0822,  0.0833]],

         [[ 0.1433,  0.1129,  0.0985],
          [ 0.1561,  0.1348,  0.1101],
          [ 0.1890,  0.1490,  0.1380]],

         [[ 0.0590,  0.0353,  0.0281],
          [ 0.0662,  0.0528,  0.0368],
          [ 0.0961,  0.0680,  0.0653]]],


        [[[-0.0050, -0.0068,  0.0074],
          [-0.0105, -0.0163,  0.0051],
          [-0.0106, -0.0112,  0.0075]],

         [[-0.0132, -0.0144,  0.0018],
          [-0.0191, -0.0227,  0.0014],
          [-0.0182, -0.0167,  0.0051]],

         [[ 0.0080,  0.0056,  0.0191],
          [ 0.0071,  0.0003,  0.0218],
          [ 0.0075,  0.0054,  0.0236]]],


        [[[ 0.0052,  0.0007,  0.0001],
          [ 0.0042,  0.0003, -0.0001],
          [ 0.0030, -0.0003, -0.0021]],

         [[ 0.0050,  0.0007,  0.0005],
          [ 0.0039,  0.0005,  0.0005],
          [ 0.0029,  0.0002, -0.0009]],

         [[ 0.0021, -0.0010, -0.0005],
          [ 0.0016, -0.0011, -0.0004],
          [ 0.0015, -0.0007, -0.0015]]],


        ...,


        [[[ 0.0365,  0.0316,  0.0263],
          [ 0.0281,  0.0214,  0.0181],
          [ 0.0281,  0.0274,  0.0236]],

         [[ 0.0398,  0.0340,  0.0333],
          [ 0.0226,  0.0178,  0.0184],
          [ 0.0218,  0.0228,  0.0201]],

         [[ 0.0396,  0.0316,  0.0288],
          [ 0.0146,  0.0087,  0.0093],
          [ 0.0113,  0.0116,  0.0094]]],


        [[[ 0.0028,  0.0000, -0.0004],
          [-0.0013,  0.0019, -0.0030],
          [ 0.0034,  0.0081,  0.0034]],

         [[ 0.0005, -0.0020, -0.0005],
          [-0.0035, -0.0004, -0.0049],
          [ 0.0013,  0.0059,  0.0007]],

         [[ 0.0060,  0.0021,  0.0021],
          [-0.0017, -0.0011, -0.0043],
          [ 0.0028,  0.0052,  0.0027]]],


        [[[-0.0695, -0.0358, -0.0414],
          [-0.0212,  0.0090, -0.0080],
          [ 0.0002, -0.0002, -0.0151]],

         [[-0.1175, -0.0956, -0.0925],
          [-0.0717, -0.0492, -0.0611],
          [-0.0527, -0.0451, -0.0560]],

         [[-0.1054, -0.0883, -0.0842],
          [-0.0752, -0.0565, -0.0673],
          [-0.0607, -0.0555, -0.0678]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0117]], device='cuda:0')

percentage_weight_grad tensor([[1.1659e-06]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (1.1172) |  Loss2: (0.3944) | Acc: (73.00%) (94/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (1.2700) |  Loss2: (0.3944) | Acc: (69.00%) (972/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (1.3216) |  Loss2: (0.3944) | Acc: (66.00%) (1797/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (1.3608) |  Loss2: (0.3944) | Acc: (65.00%) (2596/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (1.3686) |  Loss2: (0.3944) | Acc: (65.00%) (3432/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (1.3779) |  Loss2: (0.3943) | Acc: (64.00%) (4239/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (1.3836) |  Loss2: (0.3943) | Acc: (64.00%) (5052/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (1.3969) |  Loss2: (0.3943) | Acc: (64.00%) (5842/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (1.3997) |  Loss2: (0.3943) | Acc: (64.00%) (6649/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (1.4023) |  Loss2: (0.3943) | Acc: (63.00%) (7450/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (1.3980) |  Loss2: (0.3943) | Acc: (64.00%) (8299/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (1.3947) |  Loss2: (0.3943) | Acc: (64.00%) (9127/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (1.3954) |  Loss2: (0.3943) | Acc: (64.00%) (9953/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (1.3942) |  Loss2: (0.3943) | Acc: (64.00%) (10799/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (1.3891) |  Loss2: (0.3943) | Acc: (64.00%) (11668/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (1.3831) |  Loss2: (0.3942) | Acc: (64.00%) (12533/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (1.3820) |  Loss2: (0.3942) | Acc: (64.00%) (13357/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (1.3811) |  Loss2: (0.3942) | Acc: (64.00%) (14184/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (1.3820) |  Loss2: (0.3942) | Acc: (64.00%) (15004/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (1.3789) |  Loss2: (0.3942) | Acc: (64.00%) (15850/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (1.3758) |  Loss2: (0.3942) | Acc: (64.00%) (16712/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (1.3756) |  Loss2: (0.3942) | Acc: (65.00%) (17567/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (1.3724) |  Loss2: (0.3942) | Acc: (65.00%) (18431/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (1.3710) |  Loss2: (0.3941) | Acc: (65.00%) (19284/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (1.3699) |  Loss2: (0.3941) | Acc: (65.00%) (20153/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (1.3690) |  Loss2: (0.3941) | Acc: (65.00%) (20994/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (1.3652) |  Loss2: (0.3941) | Acc: (65.00%) (21860/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (1.3652) |  Loss2: (0.3941) | Acc: (65.00%) (22700/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (1.3628) |  Loss2: (0.3940) | Acc: (65.00%) (23563/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (1.3603) |  Loss2: (0.3940) | Acc: (65.00%) (24425/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (1.3590) |  Loss2: (0.3940) | Acc: (65.00%) (25284/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (1.3579) |  Loss2: (0.3940) | Acc: (65.00%) (26131/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (1.3539) |  Loss2: (0.3940) | Acc: (65.00%) (27027/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (1.3512) |  Loss2: (0.3939) | Acc: (65.00%) (27909/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (1.3492) |  Loss2: (0.3939) | Acc: (65.00%) (28788/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (1.3475) |  Loss2: (0.3939) | Acc: (66.00%) (29666/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (1.3457) |  Loss2: (0.3939) | Acc: (66.00%) (30539/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (1.3453) |  Loss2: (0.3939) | Acc: (66.00%) (31414/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (1.3436) |  Loss2: (0.3938) | Acc: (66.00%) (32277/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (1.3434) |  Loss2: (0.3938) | Acc: (66.00%) (33103/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_015.pth.tar'
# TEST : Loss: (0.8958) | Acc: (68.00%) (6810/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1337,  0.0682,  0.0347],
          [ 0.1020, -0.1769, -0.1034],
          [ 0.1616, -0.1712,  0.0091]],

         [[ 0.0272, -0.1203,  0.2078],
          [ 0.0315, -0.0037,  0.0452],
          [ 0.0252, -0.0396, -0.0343]],

         [[ 0.0383, -0.0879,  0.1018],
          [ 0.1299, -0.1093,  0.1250],
          [-0.0887,  0.0327, -0.1747]]],


        [[[ 0.1338, -0.0575,  0.0078],
          [ 0.1521,  0.1833,  0.1695],
          [ 0.0429, -0.0372,  0.1313]],

         [[-0.1665, -0.1827,  0.1586],
          [ 0.0167, -0.0529,  0.0836],
          [ 0.1416, -0.1820,  0.1412]],

         [[ 0.0892, -0.0480, -0.0226],
          [-0.0049,  0.0321, -0.1398],
          [-0.0328, -0.1348, -0.0119]]],


        [[[-0.0559,  0.1024, -0.0382],
          [-0.1080,  0.0754,  0.1250],
          [ 0.1351,  0.1782,  0.0055]],

         [[ 0.1067,  0.1323,  0.1099],
          [ 0.0812, -0.0427, -0.0415],
          [-0.0356, -0.0306,  0.1434]],

         [[-0.0482,  0.1425,  0.0687],
          [-0.0398, -0.0501, -0.0646],
          [-0.0347, -0.0089,  0.0455]]],


        ...,


        [[[ 0.1999, -0.1352, -0.1338],
          [-0.0568, -0.1043, -0.1101],
          [-0.1204,  0.0119,  0.0433]],

         [[ 0.1580,  0.1776, -0.1362],
          [ 0.2047, -0.1428, -0.1243],
          [-0.0882, -0.0911, -0.0434]],

         [[ 0.1845,  0.0992, -0.1015],
          [-0.0843, -0.1556, -0.0648],
          [ 0.0381,  0.1744, -0.1291]]],


        [[[-0.1277,  0.0722, -0.1826],
          [-0.1466, -0.1478,  0.1146],
          [-0.1671,  0.1515, -0.1288]],

         [[ 0.0906, -0.1294,  0.0672],
          [ 0.1629,  0.0168, -0.0830],
          [-0.1535,  0.1224, -0.1849]],

         [[ 0.1071, -0.0424, -0.0308],
          [ 0.1308, -0.1876,  0.0003],
          [-0.0487,  0.0072, -0.1206]]],


        [[[ 0.0822,  0.1410,  0.0803],
          [-0.0917, -0.1283,  0.1784],
          [-0.1401,  0.0346, -0.0290]],

         [[-0.0353, -0.0874,  0.1374],
          [-0.0331, -0.1806, -0.0010],
          [-0.0271,  0.0638,  0.0218]],

         [[-0.1353, -0.1660,  0.1598],
          [ 0.1490, -0.1509, -0.0644],
          [-0.0139, -0.0595,  0.2503]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3374e-05,  6.8186e-06,  3.4659e-06],
          [ 1.0198e-05, -1.7694e-05, -1.0342e-05],
          [ 1.6155e-05, -1.7124e-05,  9.1221e-07]],

         [[ 2.7183e-06, -1.2030e-05,  2.0780e-05],
          [ 3.1497e-06, -3.6624e-07,  4.5192e-06],
          [ 2.5202e-06, -3.9562e-06, -3.4347e-06]],

         [[ 3.8305e-06, -8.7929e-06,  1.0182e-05],
          [ 1.2987e-05, -1.0934e-05,  1.2500e-05],
          [-8.8729e-06,  3.2736e-06, -1.7475e-05]]],


        [[[ 1.3383e-05, -5.7520e-06,  7.7972e-07],
          [ 1.5212e-05,  1.8328e-05,  1.6946e-05],
          [ 4.2892e-06, -3.7166e-06,  1.3132e-05]],

         [[-1.6649e-05, -1.8267e-05,  1.5857e-05],
          [ 1.6743e-06, -5.2892e-06,  8.3619e-06],
          [ 1.4156e-05, -1.8203e-05,  1.4122e-05]],

         [[ 8.9161e-06, -4.8038e-06, -2.2576e-06],
          [-4.8916e-07,  3.2123e-06, -1.3980e-05],
          [-3.2781e-06, -1.3482e-05, -1.1875e-06]]],


        [[[-5.5875e-06,  1.0243e-05, -3.8237e-06],
          [-1.0797e-05,  7.5432e-06,  1.2497e-05],
          [ 1.3514e-05,  1.7816e-05,  5.4621e-07]],

         [[ 1.0669e-05,  1.3228e-05,  1.0992e-05],
          [ 8.1226e-06, -4.2707e-06, -4.1523e-06],
          [-3.5558e-06, -3.0644e-06,  1.4343e-05]],

         [[-4.8250e-06,  1.4248e-05,  6.8700e-06],
          [-3.9775e-06, -5.0097e-06, -6.4631e-06],
          [-3.4715e-06, -8.9469e-07,  4.5547e-06]]],


        ...,


        [[[ 1.9987e-05, -1.3519e-05, -1.3379e-05],
          [-5.6816e-06, -1.0432e-05, -1.1007e-05],
          [-1.2036e-05,  1.1915e-06,  4.3269e-06]],

         [[ 1.5804e-05,  1.7755e-05, -1.3620e-05],
          [ 2.0469e-05, -1.4282e-05, -1.2427e-05],
          [-8.8209e-06, -9.1103e-06, -4.3382e-06]],

         [[ 1.8454e-05,  9.9190e-06, -1.0146e-05],
          [-8.4313e-06, -1.5564e-05, -6.4763e-06],
          [ 3.8129e-06,  1.7442e-05, -1.2912e-05]]],


        [[[-1.2770e-05,  7.2174e-06, -1.8262e-05],
          [-1.4665e-05, -1.4778e-05,  1.1463e-05],
          [-1.6714e-05,  1.5149e-05, -1.2876e-05]],

         [[ 9.0580e-06, -1.2944e-05,  6.7156e-06],
          [ 1.6294e-05,  1.6825e-06, -8.3030e-06],
          [-1.5350e-05,  1.2243e-05, -1.8487e-05]],

         [[ 1.0709e-05, -4.2450e-06, -3.0836e-06],
          [ 1.3078e-05, -1.8760e-05,  2.6675e-08],
          [-4.8651e-06,  7.1884e-07, -1.2063e-05]]],


        [[[ 8.2179e-06,  1.4099e-05,  8.0251e-06],
          [-9.1654e-06, -1.2833e-05,  1.7842e-05],
          [-1.4009e-05,  3.4605e-06, -2.8960e-06]],

         [[-3.5303e-06, -8.7412e-06,  1.3743e-05],
          [-3.3105e-06, -1.8057e-05, -1.0035e-07],
          [-2.7128e-06,  6.3813e-06,  2.1828e-06]],

         [[-1.3525e-05, -1.6604e-05,  1.5979e-05],
          [ 1.4898e-05, -1.5093e-05, -6.4383e-06],
          [-1.3923e-06, -5.9506e-06,  2.5027e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0202]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0740]], device='cuda:0')

Epoch: 16 | Batch_idx: 0 |  Loss: (1.2966) |  Loss2: (0.3929) | Acc: (67.00%) (87/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (1.2700) |  Loss2: (0.3929) | Acc: (67.00%) (955/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (1.2745) |  Loss2: (0.3928) | Acc: (68.00%) (1836/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (1.2904) |  Loss2: (0.3928) | Acc: (67.00%) (2697/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (1.2836) |  Loss2: (0.3928) | Acc: (68.00%) (3578/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (1.2771) |  Loss2: (0.3927) | Acc: (68.00%) (4481/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (1.2728) |  Loss2: (0.3927) | Acc: (68.00%) (5367/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (1.2804) |  Loss2: (0.3927) | Acc: (68.00%) (6229/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (1.2782) |  Loss2: (0.3926) | Acc: (68.00%) (7114/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (1.2801) |  Loss2: (0.3926) | Acc: (68.00%) (7985/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (1.2778) |  Loss2: (0.3926) | Acc: (68.00%) (8878/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (1.2733) |  Loss2: (0.3925) | Acc: (68.00%) (9789/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (1.2692) |  Loss2: (0.3925) | Acc: (69.00%) (10690/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (1.2687) |  Loss2: (0.3925) | Acc: (69.00%) (11570/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (1.2679) |  Loss2: (0.3924) | Acc: (69.00%) (12454/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (1.2677) |  Loss2: (0.3924) | Acc: (69.00%) (13360/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (1.2660) |  Loss2: (0.3924) | Acc: (69.00%) (14263/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (1.2681) |  Loss2: (0.3923) | Acc: (69.00%) (15141/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (1.2662) |  Loss2: (0.3923) | Acc: (69.00%) (16053/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (1.2683) |  Loss2: (0.3923) | Acc: (69.00%) (16911/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (1.2691) |  Loss2: (0.3922) | Acc: (69.00%) (17783/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (1.2690) |  Loss2: (0.3922) | Acc: (69.00%) (18678/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (1.2689) |  Loss2: (0.3922) | Acc: (69.00%) (19545/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (1.2693) |  Loss2: (0.3922) | Acc: (69.00%) (20409/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (1.2672) |  Loss2: (0.3921) | Acc: (69.00%) (21299/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (1.2665) |  Loss2: (0.3921) | Acc: (69.00%) (22184/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (1.2653) |  Loss2: (0.3921) | Acc: (69.00%) (23085/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (1.2645) |  Loss2: (0.3920) | Acc: (69.00%) (23988/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (1.2652) |  Loss2: (0.3920) | Acc: (69.00%) (24869/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (1.2654) |  Loss2: (0.3920) | Acc: (69.00%) (25721/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (1.2648) |  Loss2: (0.3919) | Acc: (69.00%) (26608/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (1.2628) |  Loss2: (0.3919) | Acc: (69.00%) (27518/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (1.2619) |  Loss2: (0.3919) | Acc: (69.00%) (28438/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (1.2612) |  Loss2: (0.3918) | Acc: (69.00%) (29344/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (1.2605) |  Loss2: (0.3918) | Acc: (69.00%) (30241/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (1.2606) |  Loss2: (0.3918) | Acc: (69.00%) (31123/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (1.2596) |  Loss2: (0.3917) | Acc: (69.00%) (32019/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (1.2599) |  Loss2: (0.3917) | Acc: (69.00%) (32902/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (1.2607) |  Loss2: (0.3917) | Acc: (69.00%) (33763/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (1.2594) |  Loss2: (0.3916) | Acc: (69.00%) (34609/50000)
# TEST : Loss: (0.8615) | Acc: (69.00%) (6952/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1337,  0.0682,  0.0346],
          [ 0.1019, -0.1769, -0.1034],
          [ 0.1615, -0.1712,  0.0091]],

         [[ 0.0272, -0.1202,  0.2077],
          [ 0.0315, -0.0037,  0.0452],
          [ 0.0252, -0.0395, -0.0343]],

         [[ 0.0383, -0.0879,  0.1018],
          [ 0.1298, -0.1093,  0.1250],
          [-0.0887,  0.0327, -0.1747]]],


        [[[ 0.1338, -0.0575,  0.0078],
          [ 0.1521,  0.1832,  0.1694],
          [ 0.0429, -0.0372,  0.1313]],

         [[-0.1664, -0.1826,  0.1585],
          [ 0.0167, -0.0529,  0.0836],
          [ 0.1415, -0.1820,  0.1412]],

         [[ 0.0891, -0.0480, -0.0226],
          [-0.0049,  0.0321, -0.1397],
          [-0.0328, -0.1348, -0.0119]]],


        [[[-0.0559,  0.1024, -0.0382],
          [-0.1079,  0.0754,  0.1249],
          [ 0.1351,  0.1781,  0.0055]],

         [[ 0.1066,  0.1322,  0.1099],
          [ 0.0812, -0.0427, -0.0415],
          [-0.0355, -0.0306,  0.1434]],

         [[-0.0482,  0.1424,  0.0687],
          [-0.0398, -0.0501, -0.0646],
          [-0.0347, -0.0089,  0.0455]]],


        ...,


        [[[ 0.1998, -0.1351, -0.1337],
          [-0.0568, -0.1043, -0.1100],
          [-0.1203,  0.0119,  0.0433]],

         [[ 0.1580,  0.1775, -0.1362],
          [ 0.2046, -0.1428, -0.1242],
          [-0.0882, -0.0911, -0.0434]],

         [[ 0.1845,  0.0992, -0.1014],
          [-0.0843, -0.1556, -0.0647],
          [ 0.0381,  0.1744, -0.1291]]],


        [[[-0.1276,  0.0721, -0.1825],
          [-0.1466, -0.1477,  0.1146],
          [-0.1671,  0.1514, -0.1287]],

         [[ 0.0905, -0.1294,  0.0671],
          [ 0.1629,  0.0168, -0.0830],
          [-0.1534,  0.1224, -0.1848]],

         [[ 0.1071, -0.0424, -0.0308],
          [ 0.1307, -0.1875,  0.0003],
          [-0.0486,  0.0072, -0.1206]]],


        [[[ 0.0821,  0.1409,  0.0802],
          [-0.0916, -0.1283,  0.1783],
          [-0.1400,  0.0346, -0.0289]],

         [[-0.0353, -0.0874,  0.1374],
          [-0.0331, -0.1805, -0.0010],
          [-0.0271,  0.0638,  0.0218]],

         [[-0.1352, -0.1660,  0.1597],
          [ 0.1489, -0.1509, -0.0644],
          [-0.0139, -0.0595,  0.2502]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3368e-05,  6.8160e-06,  3.4646e-06],
          [ 1.0194e-05, -1.7687e-05, -1.0338e-05],
          [ 1.6149e-05, -1.7118e-05,  9.1185e-07]],

         [[ 2.7172e-06, -1.2025e-05,  2.0772e-05],
          [ 3.1486e-06, -3.6610e-07,  4.5174e-06],
          [ 2.5192e-06, -3.9546e-06, -3.4334e-06]],

         [[ 3.8290e-06, -8.7894e-06,  1.0178e-05],
          [ 1.2982e-05, -1.0930e-05,  1.2495e-05],
          [-8.8694e-06,  3.2722e-06, -1.7468e-05]]],


        [[[ 1.3377e-05, -5.7498e-06,  7.7941e-07],
          [ 1.5206e-05,  1.8321e-05,  1.6940e-05],
          [ 4.2874e-06, -3.7152e-06,  1.3127e-05]],

         [[-1.6642e-05, -1.8260e-05,  1.5850e-05],
          [ 1.6737e-06, -5.2872e-06,  8.3587e-06],
          [ 1.4151e-05, -1.8196e-05,  1.4117e-05]],

         [[ 8.9126e-06, -4.8019e-06, -2.2568e-06],
          [-4.8896e-07,  3.2110e-06, -1.3975e-05],
          [-3.2768e-06, -1.3477e-05, -1.1871e-06]]],


        [[[-5.5853e-06,  1.0239e-05, -3.8223e-06],
          [-1.0792e-05,  7.5403e-06,  1.2492e-05],
          [ 1.3509e-05,  1.7809e-05,  5.4599e-07]],

         [[ 1.0665e-05,  1.3223e-05,  1.0988e-05],
          [ 8.1194e-06, -4.2691e-06, -4.1507e-06],
          [-3.5544e-06, -3.0632e-06,  1.4337e-05]],

         [[-4.8231e-06,  1.4242e-05,  6.8674e-06],
          [-3.9759e-06, -5.0078e-06, -6.4604e-06],
          [-3.4702e-06, -8.9432e-07,  4.5530e-06]]],


        ...,


        [[[ 1.9979e-05, -1.3514e-05, -1.3374e-05],
          [-5.6795e-06, -1.0428e-05, -1.1003e-05],
          [-1.2031e-05,  1.1910e-06,  4.3252e-06]],

         [[ 1.5797e-05,  1.7748e-05, -1.3615e-05],
          [ 2.0461e-05, -1.4276e-05, -1.2422e-05],
          [-8.8175e-06, -9.1068e-06, -4.3364e-06]],

         [[ 1.8447e-05,  9.9152e-06, -1.0141e-05],
          [-8.4281e-06, -1.5558e-05, -6.4736e-06],
          [ 3.8114e-06,  1.7435e-05, -1.2907e-05]]],


        [[[-1.2764e-05,  7.2145e-06, -1.8255e-05],
          [-1.4659e-05, -1.4772e-05,  1.1459e-05],
          [-1.6708e-05,  1.5143e-05, -1.2871e-05]],

         [[ 9.0545e-06, -1.2939e-05,  6.7130e-06],
          [ 1.6287e-05,  1.6818e-06, -8.2998e-06],
          [-1.5344e-05,  1.2239e-05, -1.8480e-05]],

         [[ 1.0705e-05, -4.2434e-06, -3.0824e-06],
          [ 1.3073e-05, -1.8752e-05,  2.6665e-08],
          [-4.8632e-06,  7.1857e-07, -1.2058e-05]]],


        [[[ 8.2147e-06,  1.4094e-05,  8.0219e-06],
          [-9.1619e-06, -1.2827e-05,  1.7835e-05],
          [-1.4004e-05,  3.4592e-06, -2.8948e-06]],

         [[-3.5289e-06, -8.7377e-06,  1.3737e-05],
          [-3.3092e-06, -1.8050e-05, -1.0031e-07],
          [-2.7117e-06,  6.3787e-06,  2.1819e-06]],

         [[-1.3520e-05, -1.6598e-05,  1.5972e-05],
          [ 1.4892e-05, -1.5087e-05, -6.4357e-06],
          [-1.3918e-06, -5.9482e-06,  2.5018e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0538]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0250]], device='cuda:0')

Epoch: 17 | Batch_idx: 0 |  Loss: (1.2409) |  Loss2: (0.3904) | Acc: (75.00%) (96/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (1.2978) |  Loss2: (0.3904) | Acc: (67.00%) (944/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (1.2683) |  Loss2: (0.3904) | Acc: (68.00%) (1854/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (1.2692) |  Loss2: (0.3903) | Acc: (68.00%) (2722/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (1.2497) |  Loss2: (0.3903) | Acc: (69.00%) (3627/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (1.2505) |  Loss2: (0.3903) | Acc: (69.00%) (4508/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (1.2460) |  Loss2: (0.3902) | Acc: (69.00%) (5402/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (1.2425) |  Loss2: (0.3902) | Acc: (69.00%) (6300/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (1.2395) |  Loss2: (0.3902) | Acc: (69.00%) (7207/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (1.2392) |  Loss2: (0.3901) | Acc: (69.00%) (8098/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (1.2314) |  Loss2: (0.3901) | Acc: (69.00%) (9036/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (1.2280) |  Loss2: (0.3901) | Acc: (69.00%) (9939/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (1.2308) |  Loss2: (0.3900) | Acc: (69.00%) (10814/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (1.2294) |  Loss2: (0.3900) | Acc: (69.00%) (11714/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (1.2264) |  Loss2: (0.3900) | Acc: (69.00%) (12632/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (1.2268) |  Loss2: (0.3899) | Acc: (70.00%) (13531/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (1.2267) |  Loss2: (0.3899) | Acc: (70.00%) (14440/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (1.2248) |  Loss2: (0.3899) | Acc: (70.00%) (15360/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (1.2273) |  Loss2: (0.3898) | Acc: (70.00%) (16237/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (1.2279) |  Loss2: (0.3898) | Acc: (70.00%) (17154/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (1.2282) |  Loss2: (0.3898) | Acc: (70.00%) (18058/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (1.2274) |  Loss2: (0.3897) | Acc: (70.00%) (18957/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (1.2290) |  Loss2: (0.3897) | Acc: (70.00%) (19858/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (1.2289) |  Loss2: (0.3897) | Acc: (70.00%) (20760/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (1.2292) |  Loss2: (0.3896) | Acc: (70.00%) (21653/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (1.2286) |  Loss2: (0.3896) | Acc: (70.00%) (22557/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (1.2274) |  Loss2: (0.3896) | Acc: (70.00%) (23470/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (1.2265) |  Loss2: (0.3895) | Acc: (70.00%) (24382/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (1.2266) |  Loss2: (0.3895) | Acc: (70.00%) (25294/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (1.2258) |  Loss2: (0.3895) | Acc: (70.00%) (26201/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (1.2251) |  Loss2: (0.3894) | Acc: (70.00%) (27113/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (1.2265) |  Loss2: (0.3894) | Acc: (70.00%) (27999/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (1.2283) |  Loss2: (0.3893) | Acc: (70.00%) (28854/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (1.2278) |  Loss2: (0.3893) | Acc: (70.00%) (29752/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (1.2267) |  Loss2: (0.3893) | Acc: (70.00%) (30674/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (1.2264) |  Loss2: (0.3892) | Acc: (70.00%) (31570/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (1.2272) |  Loss2: (0.3892) | Acc: (70.00%) (32464/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (1.2282) |  Loss2: (0.3892) | Acc: (70.00%) (33356/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (1.2284) |  Loss2: (0.3891) | Acc: (70.00%) (34255/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (1.2288) |  Loss2: (0.3891) | Acc: (70.00%) (35110/50000)
# TEST : Loss: (0.8434) | Acc: (70.00%) (7009/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1336,  0.0681,  0.0346],
          [ 0.1019, -0.1768, -0.1033],
          [ 0.1614, -0.1711,  0.0091]],

         [[ 0.0272, -0.1202,  0.2076],
          [ 0.0315, -0.0037,  0.0452],
          [ 0.0252, -0.0395, -0.0343]],

         [[ 0.0383, -0.0879,  0.1017],
          [ 0.1298, -0.1093,  0.1249],
          [-0.0887,  0.0327, -0.1746]]],


        [[[ 0.1337, -0.0575,  0.0078],
          [ 0.1520,  0.1831,  0.1693],
          [ 0.0429, -0.0371,  0.1312]],

         [[-0.1664, -0.1825,  0.1584],
          [ 0.0167, -0.0529,  0.0836],
          [ 0.1415, -0.1819,  0.1411]],

         [[ 0.0891, -0.0480, -0.0226],
          [-0.0049,  0.0321, -0.1397],
          [-0.0328, -0.1347, -0.0119]]],


        [[[-0.0558,  0.1023, -0.0382],
          [-0.1079,  0.0754,  0.1249],
          [ 0.1350,  0.1780,  0.0055]],

         [[ 0.1066,  0.1322,  0.1098],
          [ 0.0812, -0.0427, -0.0415],
          [-0.0355, -0.0306,  0.1433]],

         [[-0.0482,  0.1424,  0.0686],
          [-0.0397, -0.0501, -0.0646],
          [-0.0347, -0.0089,  0.0455]]],


        ...,


        [[[ 0.1997, -0.1351, -0.1337],
          [-0.0568, -0.1042, -0.1100],
          [-0.1203,  0.0119,  0.0432]],

         [[ 0.1579,  0.1774, -0.1361],
          [ 0.2045, -0.1427, -0.1242],
          [-0.0881, -0.0910, -0.0433]],

         [[ 0.1844,  0.0991, -0.1014],
          [-0.0842, -0.1555, -0.0647],
          [ 0.0381,  0.1743, -0.1290]]],


        [[[-0.1276,  0.0721, -0.1825],
          [-0.1465, -0.1477,  0.1145],
          [-0.1670,  0.1514, -0.1287]],

         [[ 0.0905, -0.1293,  0.0671],
          [ 0.1628,  0.0168, -0.0830],
          [-0.1534,  0.1223, -0.1847]],

         [[ 0.1070, -0.0424, -0.0308],
          [ 0.1307, -0.1874,  0.0003],
          [-0.0486,  0.0072, -0.1205]]],


        [[[ 0.0821,  0.1409,  0.0802],
          [-0.0916, -0.1282,  0.1783],
          [-0.1400,  0.0346, -0.0289]],

         [[-0.0353, -0.0873,  0.1373],
          [-0.0331, -0.1804, -0.0010],
          [-0.0271,  0.0638,  0.0218]],

         [[-0.1351, -0.1659,  0.1597],
          [ 0.1489, -0.1508, -0.0643],
          [-0.0139, -0.0595,  0.2501]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3363e-05,  6.8133e-06,  3.4633e-06],
          [ 1.0190e-05, -1.7680e-05, -1.0334e-05],
          [ 1.6143e-05, -1.7112e-05,  9.1148e-07]],

         [[ 2.7161e-06, -1.2020e-05,  2.0763e-05],
          [ 3.1474e-06, -3.6595e-07,  4.5157e-06],
          [ 2.5181e-06, -3.9530e-06, -3.4320e-06]],

         [[ 3.8275e-06, -8.7859e-06,  1.0174e-05],
          [ 1.2976e-05, -1.0925e-05,  1.2490e-05],
          [-8.8660e-06,  3.2709e-06, -1.7461e-05]]],


        [[[ 1.3372e-05, -5.7476e-06,  7.7910e-07],
          [ 1.5201e-05,  1.8314e-05,  1.6933e-05],
          [ 4.2857e-06, -3.7137e-06,  1.3122e-05]],

         [[-1.6636e-05, -1.8253e-05,  1.5844e-05],
          [ 1.6730e-06, -5.2852e-06,  8.3555e-06],
          [ 1.4146e-05, -1.8189e-05,  1.4112e-05]],

         [[ 8.9091e-06, -4.8000e-06, -2.2559e-06],
          [-4.8877e-07,  3.2097e-06, -1.3969e-05],
          [-3.2755e-06, -1.3472e-05, -1.1866e-06]]],


        [[[-5.5831e-06,  1.0235e-05, -3.8208e-06],
          [-1.0788e-05,  7.5373e-06,  1.2488e-05],
          [ 1.3504e-05,  1.7802e-05,  5.4577e-07]],

         [[ 1.0661e-05,  1.3217e-05,  1.0983e-05],
          [ 8.1162e-06, -4.2675e-06, -4.1491e-06],
          [-3.5529e-06, -3.0620e-06,  1.4331e-05]],

         [[-4.8212e-06,  1.4236e-05,  6.8647e-06],
          [-3.9743e-06, -5.0059e-06, -6.4578e-06],
          [-3.4689e-06, -8.9396e-07,  4.5512e-06]]],


        ...,


        [[[ 1.9971e-05, -1.3509e-05, -1.3369e-05],
          [-5.6773e-06, -1.0424e-05, -1.0999e-05],
          [-1.2026e-05,  1.1905e-06,  4.3234e-06]],

         [[ 1.5791e-05,  1.7741e-05, -1.3610e-05],
          [ 2.0452e-05, -1.4271e-05, -1.2417e-05],
          [-8.8140e-06, -9.1033e-06, -4.3347e-06]],

         [[ 1.8440e-05,  9.9115e-06, -1.0137e-05],
          [-8.4249e-06, -1.5552e-05, -6.4710e-06],
          [ 3.8100e-06,  1.7428e-05, -1.2901e-05]]],


        [[[-1.2759e-05,  7.2116e-06, -1.8248e-05],
          [-1.4653e-05, -1.4766e-05,  1.1455e-05],
          [-1.6701e-05,  1.5137e-05, -1.2866e-05]],

         [[ 9.0510e-06, -1.2933e-05,  6.7104e-06],
          [ 1.6281e-05,  1.6812e-06, -8.2966e-06],
          [-1.5338e-05,  1.2234e-05, -1.8473e-05]],

         [[ 1.0701e-05, -4.2418e-06, -3.0811e-06],
          [ 1.3068e-05, -1.8744e-05,  2.6654e-08],
          [-4.8614e-06,  7.1830e-07, -1.2054e-05]]],


        [[[ 8.2115e-06,  1.4088e-05,  8.0187e-06],
          [-9.1584e-06, -1.2822e-05,  1.7828e-05],
          [-1.3999e-05,  3.4579e-06, -2.8937e-06]],

         [[-3.5276e-06, -8.7342e-06,  1.3732e-05],
          [-3.3079e-06, -1.8043e-05, -1.0027e-07],
          [-2.7106e-06,  6.3761e-06,  2.1810e-06]],

         [[-1.3515e-05, -1.6591e-05,  1.5966e-05],
          [ 1.4886e-05, -1.5081e-05, -6.4331e-06],
          [-1.3912e-06, -5.9459e-06,  2.5009e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0751]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0339]], device='cuda:0')

Epoch: 18 | Batch_idx: 0 |  Loss: (1.0509) |  Loss2: (0.3878) | Acc: (75.00%) (97/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (1.2195) |  Loss2: (0.3877) | Acc: (70.00%) (992/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (1.2299) |  Loss2: (0.3877) | Acc: (70.00%) (1885/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (1.2322) |  Loss2: (0.3877) | Acc: (70.00%) (2779/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (1.2305) |  Loss2: (0.3877) | Acc: (70.00%) (3674/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (1.2267) |  Loss2: (0.3876) | Acc: (70.00%) (4601/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (1.2239) |  Loss2: (0.3876) | Acc: (70.00%) (5514/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (1.2260) |  Loss2: (0.3876) | Acc: (70.00%) (6420/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (1.2223) |  Loss2: (0.3875) | Acc: (70.00%) (7343/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (1.2219) |  Loss2: (0.3875) | Acc: (70.00%) (8249/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (1.2200) |  Loss2: (0.3875) | Acc: (70.00%) (9169/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (1.2176) |  Loss2: (0.3874) | Acc: (71.00%) (10093/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (1.2177) |  Loss2: (0.3874) | Acc: (71.00%) (11006/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (1.2160) |  Loss2: (0.3874) | Acc: (71.00%) (11923/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (1.2200) |  Loss2: (0.3873) | Acc: (70.00%) (12797/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (1.2221) |  Loss2: (0.3873) | Acc: (70.00%) (13694/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (1.2246) |  Loss2: (0.3873) | Acc: (70.00%) (14574/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (1.2249) |  Loss2: (0.3872) | Acc: (70.00%) (15470/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (1.2258) |  Loss2: (0.3872) | Acc: (70.00%) (16358/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (1.2224) |  Loss2: (0.3872) | Acc: (70.00%) (17268/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (1.2196) |  Loss2: (0.3871) | Acc: (70.00%) (18185/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (1.2177) |  Loss2: (0.3871) | Acc: (70.00%) (19108/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (1.2161) |  Loss2: (0.3871) | Acc: (70.00%) (20035/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (1.2143) |  Loss2: (0.3870) | Acc: (70.00%) (20953/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (1.2156) |  Loss2: (0.3870) | Acc: (70.00%) (21836/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (1.2157) |  Loss2: (0.3870) | Acc: (70.00%) (22751/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (1.2144) |  Loss2: (0.3869) | Acc: (70.00%) (23666/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (1.2143) |  Loss2: (0.3869) | Acc: (70.00%) (24562/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (1.2152) |  Loss2: (0.3869) | Acc: (70.00%) (25454/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (1.2141) |  Loss2: (0.3868) | Acc: (70.00%) (26374/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (1.2144) |  Loss2: (0.3868) | Acc: (70.00%) (27273/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (1.2146) |  Loss2: (0.3868) | Acc: (70.00%) (28185/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (1.2145) |  Loss2: (0.3867) | Acc: (70.00%) (29102/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (1.2142) |  Loss2: (0.3867) | Acc: (70.00%) (30014/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (1.2138) |  Loss2: (0.3867) | Acc: (70.00%) (30929/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (1.2135) |  Loss2: (0.3866) | Acc: (70.00%) (31844/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (1.2135) |  Loss2: (0.3866) | Acc: (70.00%) (32741/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (1.2132) |  Loss2: (0.3866) | Acc: (70.00%) (33643/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (1.2145) |  Loss2: (0.3865) | Acc: (70.00%) (34526/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (1.2143) |  Loss2: (0.3865) | Acc: (70.00%) (35391/50000)
# TEST : Loss: (0.8384) | Acc: (70.00%) (7029/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1336,  0.0681,  0.0346],
          [ 0.1019, -0.1767, -0.1033],
          [ 0.1614, -0.1711,  0.0091]],

         [[ 0.0272, -0.1202,  0.2076],
          [ 0.0315, -0.0037,  0.0451],
          [ 0.0252, -0.0395, -0.0343]],

         [[ 0.0383, -0.0878,  0.1017],
          [ 0.1297, -0.1092,  0.1249],
          [-0.0886,  0.0327, -0.1745]]],


        [[[ 0.1337, -0.0575,  0.0078],
          [ 0.1519,  0.1831,  0.1693],
          [ 0.0428, -0.0371,  0.1312]],

         [[-0.1663, -0.1825,  0.1584],
          [ 0.0167, -0.0528,  0.0835],
          [ 0.1414, -0.1818,  0.1411]],

         [[ 0.0891, -0.0480, -0.0226],
          [-0.0049,  0.0321, -0.1396],
          [-0.0327, -0.1347, -0.0119]]],


        [[[-0.0558,  0.1023, -0.0382],
          [-0.1078,  0.0753,  0.1248],
          [ 0.1350,  0.1779,  0.0055]],

         [[ 0.1066,  0.1321,  0.1098],
          [ 0.0811, -0.0427, -0.0415],
          [-0.0355, -0.0306,  0.1433]],

         [[-0.0482,  0.1423,  0.0686],
          [-0.0397, -0.0500, -0.0646],
          [-0.0347, -0.0089,  0.0455]]],


        ...,


        [[[ 0.1996, -0.1350, -0.1336],
          [-0.0568, -0.1042, -0.1099],
          [-0.1202,  0.0119,  0.0432]],

         [[ 0.1578,  0.1773, -0.1360],
          [ 0.2044, -0.1426, -0.1241],
          [-0.0881, -0.0910, -0.0433]],

         [[ 0.1843,  0.0991, -0.1013],
          [-0.0842, -0.1555, -0.0647],
          [ 0.0381,  0.1742, -0.1290]]],


        [[[-0.1275,  0.0721, -0.1824],
          [-0.1465, -0.1476,  0.1145],
          [-0.1669,  0.1513, -0.1286]],

         [[ 0.0905, -0.1293,  0.0671],
          [ 0.1627,  0.0168, -0.0829],
          [-0.1533,  0.1223, -0.1847]],

         [[ 0.1070, -0.0424, -0.0308],
          [ 0.1306, -0.1874,  0.0003],
          [-0.0486,  0.0072, -0.1205]]],


        [[[ 0.0821,  0.1408,  0.0802],
          [-0.0915, -0.1282,  0.1782],
          [-0.1399,  0.0346, -0.0289]],

         [[-0.0353, -0.0873,  0.1373],
          [-0.0331, -0.1804, -0.0010],
          [-0.0271,  0.0637,  0.0218]],

         [[-0.1351, -0.1659,  0.1596],
          [ 0.1488, -0.1508, -0.0643],
          [-0.0139, -0.0594,  0.2500]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3358e-05,  6.8107e-06,  3.4620e-06],
          [ 1.0186e-05, -1.7673e-05, -1.0330e-05],
          [ 1.6136e-05, -1.7105e-05,  9.1112e-07]],

         [[ 2.7150e-06, -1.2016e-05,  2.0755e-05],
          [ 3.1462e-06, -3.6581e-07,  4.5139e-06],
          [ 2.5171e-06, -3.9514e-06, -3.4307e-06]],

         [[ 3.8261e-06, -8.7824e-06,  1.0170e-05],
          [ 1.2971e-05, -1.0921e-05,  1.2485e-05],
          [-8.8625e-06,  3.2696e-06, -1.7454e-05]]],


        [[[ 1.3367e-05, -5.7454e-06,  7.7879e-07],
          [ 1.5195e-05,  1.8307e-05,  1.6927e-05],
          [ 4.2839e-06, -3.7123e-06,  1.3116e-05]],

         [[-1.6630e-05, -1.8246e-05,  1.5837e-05],
          [ 1.6724e-06, -5.2831e-06,  8.3523e-06],
          [ 1.4141e-05, -1.8182e-05,  1.4106e-05]],

         [[ 8.9056e-06, -4.7981e-06, -2.2550e-06],
          [-4.8859e-07,  3.2084e-06, -1.3964e-05],
          [-3.2742e-06, -1.3467e-05, -1.1861e-06]]],


        [[[-5.5809e-06,  1.0231e-05, -3.8193e-06],
          [-1.0784e-05,  7.5344e-06,  1.2483e-05],
          [ 1.3499e-05,  1.7795e-05,  5.4555e-07]],

         [[ 1.0656e-05,  1.3212e-05,  1.0979e-05],
          [ 8.1130e-06, -4.2659e-06, -4.1475e-06],
          [-3.5515e-06, -3.0609e-06,  1.4326e-05]],

         [[-4.8193e-06,  1.4230e-05,  6.8621e-06],
          [-3.9727e-06, -5.0040e-06, -6.4552e-06],
          [-3.4676e-06, -8.9360e-07,  4.5495e-06]]],


        ...,


        [[[ 1.9964e-05, -1.3504e-05, -1.3364e-05],
          [-5.6751e-06, -1.0420e-05, -1.0994e-05],
          [-1.2022e-05,  1.1900e-06,  4.3217e-06]],

         [[ 1.5784e-05,  1.7734e-05, -1.3605e-05],
          [ 2.0444e-05, -1.4265e-05, -1.2412e-05],
          [-8.8105e-06, -9.0998e-06, -4.3329e-06]],

         [[ 1.8434e-05,  9.9077e-06, -1.0133e-05],
          [-8.4217e-06, -1.5547e-05, -6.4684e-06],
          [ 3.8085e-06,  1.7421e-05, -1.2896e-05]]],


        [[[-1.2754e-05,  7.2087e-06, -1.8241e-05],
          [-1.4647e-05, -1.4760e-05,  1.1450e-05],
          [-1.6695e-05,  1.5132e-05, -1.2860e-05]],

         [[ 9.0475e-06, -1.2928e-05,  6.7077e-06],
          [ 1.6275e-05,  1.6805e-06, -8.2934e-06],
          [-1.5332e-05,  1.2229e-05, -1.8466e-05]],

         [[ 1.0697e-05, -4.2402e-06, -3.0799e-06],
          [ 1.3062e-05, -1.8737e-05,  2.6644e-08],
          [-4.8595e-06,  7.1802e-07, -1.2049e-05]]],


        [[[ 8.2083e-06,  1.4083e-05,  8.0155e-06],
          [-9.1549e-06, -1.2817e-05,  1.7821e-05],
          [-1.3993e-05,  3.4566e-06, -2.8925e-06]],

         [[-3.5263e-06, -8.7307e-06,  1.3727e-05],
          [-3.3065e-06, -1.8036e-05, -1.0023e-07],
          [-2.7095e-06,  6.3735e-06,  2.1802e-06]],

         [[-1.3510e-05, -1.6585e-05,  1.5959e-05],
          [ 1.4881e-05, -1.5076e-05, -6.4305e-06],
          [-1.3907e-06, -5.9436e-06,  2.4999e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0831]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0410]], device='cuda:0')

Epoch: 19 | Batch_idx: 0 |  Loss: (1.1833) |  Loss2: (0.3852) | Acc: (70.00%) (90/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (1.2641) |  Loss2: (0.3852) | Acc: (69.00%) (983/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (1.2204) |  Loss2: (0.3851) | Acc: (70.00%) (1892/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (1.2021) |  Loss2: (0.3851) | Acc: (70.00%) (2811/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (1.2141) |  Loss2: (0.3851) | Acc: (70.00%) (3696/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (1.2083) |  Loss2: (0.3850) | Acc: (70.00%) (4614/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (1.2142) |  Loss2: (0.3850) | Acc: (70.00%) (5492/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (1.2194) |  Loss2: (0.3850) | Acc: (70.00%) (6381/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (1.2172) |  Loss2: (0.3849) | Acc: (70.00%) (7287/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (1.2138) |  Loss2: (0.3849) | Acc: (70.00%) (8192/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (1.2123) |  Loss2: (0.3849) | Acc: (70.00%) (9118/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (1.2108) |  Loss2: (0.3849) | Acc: (70.00%) (10007/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (1.2108) |  Loss2: (0.3848) | Acc: (70.00%) (10900/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (1.2125) |  Loss2: (0.3848) | Acc: (70.00%) (11812/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (1.2096) |  Loss2: (0.3848) | Acc: (70.00%) (12744/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (1.2085) |  Loss2: (0.3847) | Acc: (70.00%) (13640/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (1.2061) |  Loss2: (0.3847) | Acc: (70.00%) (14542/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (1.2068) |  Loss2: (0.3847) | Acc: (70.00%) (15435/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (1.2071) |  Loss2: (0.3846) | Acc: (70.00%) (16332/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (1.2091) |  Loss2: (0.3846) | Acc: (70.00%) (17222/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (1.2089) |  Loss2: (0.3846) | Acc: (70.00%) (18142/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (1.2104) |  Loss2: (0.3845) | Acc: (70.00%) (19026/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (1.2089) |  Loss2: (0.3845) | Acc: (70.00%) (19933/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (1.2087) |  Loss2: (0.3845) | Acc: (70.00%) (20835/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (1.2090) |  Loss2: (0.3844) | Acc: (70.00%) (21725/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (1.2090) |  Loss2: (0.3844) | Acc: (70.00%) (22614/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (1.2072) |  Loss2: (0.3844) | Acc: (70.00%) (23560/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (1.2054) |  Loss2: (0.3843) | Acc: (70.00%) (24493/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (1.2040) |  Loss2: (0.3843) | Acc: (70.00%) (25402/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (1.2044) |  Loss2: (0.3843) | Acc: (70.00%) (26309/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (1.2055) |  Loss2: (0.3842) | Acc: (70.00%) (27195/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (1.2044) |  Loss2: (0.3842) | Acc: (70.00%) (28111/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (1.2037) |  Loss2: (0.3842) | Acc: (70.00%) (29033/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (1.2034) |  Loss2: (0.3841) | Acc: (70.00%) (29951/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (1.2033) |  Loss2: (0.3841) | Acc: (70.00%) (30860/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (1.2041) |  Loss2: (0.3841) | Acc: (70.00%) (31746/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (1.2050) |  Loss2: (0.3840) | Acc: (70.00%) (32635/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (1.2060) |  Loss2: (0.3840) | Acc: (70.00%) (33505/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (1.2045) |  Loss2: (0.3840) | Acc: (70.00%) (34440/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (1.2033) |  Loss2: (0.3839) | Acc: (70.00%) (35335/50000)
# TEST : Loss: (0.8315) | Acc: (70.00%) (7057/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1335,  0.0681,  0.0346],
          [ 0.1018, -0.1767, -0.1033],
          [ 0.1613, -0.1710,  0.0091]],

         [[ 0.0271, -0.1201,  0.2075],
          [ 0.0315, -0.0037,  0.0451],
          [ 0.0252, -0.0395, -0.0343]],

         [[ 0.0382, -0.0878,  0.1017],
          [ 0.1297, -0.1092,  0.1248],
          [-0.0886,  0.0327, -0.1745]]],


        [[[ 0.1336, -0.0574,  0.0078],
          [ 0.1519,  0.1830,  0.1692],
          [ 0.0428, -0.0371,  0.1311]],

         [[-0.1662, -0.1824,  0.1583],
          [ 0.0167, -0.0528,  0.0835],
          [ 0.1414, -0.1818,  0.1410]],

         [[ 0.0890, -0.0480, -0.0225],
          [-0.0049,  0.0321, -0.1396],
          [-0.0327, -0.1346, -0.0119]]],


        [[[-0.0558,  0.1023, -0.0382],
          [-0.1078,  0.0753,  0.1248],
          [ 0.1349,  0.1779,  0.0055]],

         [[ 0.1065,  0.1321,  0.1097],
          [ 0.0811, -0.0426, -0.0415],
          [-0.0355, -0.0306,  0.1432]],

         [[-0.0482,  0.1422,  0.0686],
          [-0.0397, -0.0500, -0.0645],
          [-0.0347, -0.0089,  0.0455]]],


        ...,


        [[[ 0.1996, -0.1350, -0.1336],
          [-0.0567, -0.1042, -0.1099],
          [-0.1202,  0.0119,  0.0432]],

         [[ 0.1578,  0.1773, -0.1360],
          [ 0.2044, -0.1426, -0.1241],
          [-0.0881, -0.0910, -0.0433]],

         [[ 0.1843,  0.0990, -0.1013],
          [-0.0842, -0.1554, -0.0647],
          [ 0.0381,  0.1741, -0.1289]]],


        [[[-0.1275,  0.0721, -0.1823],
          [-0.1464, -0.1475,  0.1145],
          [-0.1669,  0.1513, -0.1286]],

         [[ 0.0904, -0.1292,  0.0671],
          [ 0.1627,  0.0168, -0.0829],
          [-0.1533,  0.1222, -0.1846]],

         [[ 0.1069, -0.0424, -0.0308],
          [ 0.1306, -0.1873,  0.0003],
          [-0.0486,  0.0072, -0.1204]]],


        [[[ 0.0821,  0.1408,  0.0801],
          [-0.0915, -0.1281,  0.1781],
          [-0.1399,  0.0346, -0.0289]],

         [[-0.0353, -0.0873,  0.1372],
          [-0.0331, -0.1803, -0.0010],
          [-0.0271,  0.0637,  0.0218]],

         [[-0.1350, -0.1658,  0.1595],
          [ 0.1487, -0.1507, -0.0643],
          [-0.0139, -0.0594,  0.2499]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3353e-05,  6.8081e-06,  3.4607e-06],
          [ 1.0182e-05, -1.7666e-05, -1.0326e-05],
          [ 1.6130e-05, -1.7099e-05,  9.1075e-07]],

         [[ 2.7139e-06, -1.2011e-05,  2.0747e-05],
          [ 3.1451e-06, -3.6566e-07,  4.5122e-06],
          [ 2.5161e-06, -3.9498e-06, -3.4294e-06]],

         [[ 3.8246e-06, -8.7789e-06,  1.0166e-05],
          [ 1.2966e-05, -1.0917e-05,  1.2480e-05],
          [-8.8590e-06,  3.2683e-06, -1.7447e-05]]],


        [[[ 1.3362e-05, -5.7433e-06,  7.7848e-07],
          [ 1.5189e-05,  1.8300e-05,  1.6921e-05],
          [ 4.2823e-06, -3.7108e-06,  1.3111e-05]],

         [[-1.6623e-05, -1.8239e-05,  1.5831e-05],
          [ 1.6717e-06, -5.2811e-06,  8.3491e-06],
          [ 1.4135e-05, -1.8175e-05,  1.4101e-05]],

         [[ 8.9021e-06, -4.7963e-06, -2.2541e-06],
          [-4.8841e-07,  3.2071e-06, -1.3959e-05],
          [-3.2729e-06, -1.3461e-05, -1.1856e-06]]],


        [[[-5.5788e-06,  1.0227e-05, -3.8179e-06],
          [-1.0780e-05,  7.5315e-06,  1.2478e-05],
          [ 1.3493e-05,  1.7788e-05,  5.4533e-07]],

         [[ 1.0652e-05,  1.3207e-05,  1.0975e-05],
          [ 8.1098e-06, -4.2643e-06, -4.1459e-06],
          [-3.5500e-06, -3.0597e-06,  1.4320e-05]],

         [[-4.8174e-06,  1.4224e-05,  6.8595e-06],
          [-3.9711e-06, -5.0021e-06, -6.4526e-06],
          [-3.4663e-06, -8.9323e-07,  4.5477e-06]]],


        ...,


        [[[ 1.9956e-05, -1.3498e-05, -1.3358e-05],
          [-5.6729e-06, -1.0416e-05, -1.0990e-05],
          [-1.2017e-05,  1.1896e-06,  4.3199e-06]],

         [[ 1.5778e-05,  1.7727e-05, -1.3599e-05],
          [ 2.0436e-05, -1.4259e-05, -1.2407e-05],
          [-8.8070e-06, -9.0963e-06, -4.3312e-06]],

         [[ 1.8427e-05,  9.9039e-06, -1.0129e-05],
          [-8.4185e-06, -1.5541e-05, -6.4658e-06],
          [ 3.8070e-06,  1.7414e-05, -1.2891e-05]]],


        [[[-1.2749e-05,  7.2057e-06, -1.8234e-05],
          [-1.4642e-05, -1.4754e-05,  1.1446e-05],
          [-1.6688e-05,  1.5126e-05, -1.2855e-05]],

         [[ 9.0440e-06, -1.2923e-05,  6.7051e-06],
          [ 1.6268e-05,  1.6798e-06, -8.2902e-06],
          [-1.5327e-05,  1.2225e-05, -1.8459e-05]],

         [[ 1.0693e-05, -4.2386e-06, -3.0786e-06],
          [ 1.3057e-05, -1.8729e-05,  2.6634e-08],
          [-4.8576e-06,  7.1775e-07, -1.2044e-05]]],


        [[[ 8.2051e-06,  1.4078e-05,  8.0123e-06],
          [-9.1514e-06, -1.2812e-05,  1.7814e-05],
          [-1.3988e-05,  3.4553e-06, -2.8913e-06]],

         [[-3.5250e-06, -8.7272e-06,  1.3722e-05],
          [-3.3052e-06, -1.8029e-05, -1.0019e-07],
          [-2.7084e-06,  6.3708e-06,  2.1793e-06]],

         [[-1.3504e-05, -1.6579e-05,  1.5953e-05],
          [ 1.4875e-05, -1.5070e-05, -6.4278e-06],
          [-1.3901e-06, -5.9412e-06,  2.4989e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0851]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0256]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.9425) |  Loss2: (0.0000) | Acc: (65.00%) (84/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8273) |  Loss2: (0.0000) | Acc: (70.00%) (987/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (70.00%) (1895/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (2819/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8231) |  Loss2: (0.0000) | Acc: (70.00%) (3700/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8263) |  Loss2: (0.0000) | Acc: (69.00%) (4566/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8195) |  Loss2: (0.0000) | Acc: (70.00%) (5515/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8160) |  Loss2: (0.0000) | Acc: (70.00%) (6429/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (70.00%) (7354/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8214) |  Loss2: (0.0000) | Acc: (70.00%) (8241/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8209) |  Loss2: (0.0000) | Acc: (70.00%) (9167/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8193) |  Loss2: (0.0000) | Acc: (70.00%) (10087/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8161) |  Loss2: (0.0000) | Acc: (71.00%) (11023/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8134) |  Loss2: (0.0000) | Acc: (71.00%) (11943/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8141) |  Loss2: (0.0000) | Acc: (71.00%) (12843/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8149) |  Loss2: (0.0000) | Acc: (71.00%) (13746/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8125) |  Loss2: (0.0000) | Acc: (71.00%) (14674/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8154) |  Loss2: (0.0000) | Acc: (71.00%) (15553/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8144) |  Loss2: (0.0000) | Acc: (71.00%) (16494/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8132) |  Loss2: (0.0000) | Acc: (71.00%) (17422/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8123) |  Loss2: (0.0000) | Acc: (71.00%) (18346/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8119) |  Loss2: (0.0000) | Acc: (71.00%) (19252/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8111) |  Loss2: (0.0000) | Acc: (71.00%) (20172/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (21087/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.8127) |  Loss2: (0.0000) | Acc: (71.00%) (21985/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.8108) |  Loss2: (0.0000) | Acc: (71.00%) (22933/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (23850/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.8102) |  Loss2: (0.0000) | Acc: (71.00%) (24772/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.8110) |  Loss2: (0.0000) | Acc: (71.00%) (25674/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.8106) |  Loss2: (0.0000) | Acc: (71.00%) (26606/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (71.00%) (27511/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (28429/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.8092) |  Loss2: (0.0000) | Acc: (71.00%) (29347/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.8096) |  Loss2: (0.0000) | Acc: (71.00%) (30248/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.8088) |  Loss2: (0.0000) | Acc: (71.00%) (31183/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.8086) |  Loss2: (0.0000) | Acc: (71.00%) (32111/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.8085) |  Loss2: (0.0000) | Acc: (71.00%) (33013/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.8077) |  Loss2: (0.0000) | Acc: (71.00%) (33941/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.8073) |  Loss2: (0.0000) | Acc: (71.00%) (34847/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.8068) |  Loss2: (0.0000) | Acc: (71.00%) (35728/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_020.pth.tar'
# TEST : Loss: (0.8752) | Acc: (68.00%) (6865/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1295,  0.0713,  0.0387],
          [ 0.1069, -0.1758, -0.1005],
          [ 0.1695, -0.1633,  0.0120]],

         [[ 0.0312, -0.1163,  0.2098],
          [ 0.0376, -0.0022,  0.0463],
          [ 0.0324, -0.0338, -0.0358]],

         [[ 0.0437, -0.0825,  0.1055],
          [ 0.1373, -0.1056,  0.1281],
          [-0.0818,  0.0377, -0.1761]]],


        [[[ 0.1280, -0.0644,  0.0037],
          [ 0.1503,  0.1797,  0.1700],
          [ 0.0381, -0.0428,  0.1292]],

         [[-0.1670, -0.1867,  0.1567],
          [ 0.0185, -0.0548,  0.0858],
          [ 0.1395, -0.1862,  0.1407]],

         [[ 0.0905, -0.0499, -0.0222],
          [-0.0009,  0.0319, -0.1360],
          [-0.0314, -0.1366, -0.0104]]],


        [[[-0.0559,  0.1023, -0.0377],
          [-0.1080,  0.0752,  0.1250],
          [ 0.1344,  0.1775,  0.0054]],

         [[ 0.1063,  0.1320,  0.1102],
          [ 0.0806, -0.0428, -0.0412],
          [-0.0362, -0.0310,  0.1430]],

         [[-0.0487,  0.1418,  0.0686],
          [-0.0405, -0.0507, -0.0649],
          [-0.0357, -0.0100,  0.0446]]],


        ...,


        [[[ 0.2045, -0.1316, -0.1297],
          [-0.0539, -0.1034, -0.1076],
          [-0.1188,  0.0113,  0.0446]],

         [[ 0.1611,  0.1797, -0.1339],
          [ 0.2068, -0.1414, -0.1224],
          [-0.0863, -0.0908, -0.0421]],

         [[ 0.1853,  0.0996, -0.1014],
          [-0.0824, -0.1548, -0.0638],
          [ 0.0399,  0.1746, -0.1276]]],


        [[[-0.1276,  0.0723, -0.1816],
          [-0.1469, -0.1481,  0.1139],
          [-0.1662,  0.1520, -0.1276]],

         [[ 0.0900, -0.1294,  0.0665],
          [ 0.1617,  0.0153, -0.0850],
          [-0.1533,  0.1219, -0.1853]],

         [[ 0.1070, -0.0421, -0.0311],
          [ 0.1308, -0.1874, -0.0009],
          [-0.0472,  0.0083, -0.1202]]],


        [[[ 0.0797,  0.1406,  0.0811],
          [-0.0929, -0.1253,  0.1869],
          [-0.1427,  0.0363, -0.0232]],

         [[-0.0369, -0.0893,  0.1357],
          [-0.0304, -0.1754,  0.0085],
          [-0.0237,  0.0700,  0.0295]],

         [[-0.1385, -0.1701,  0.1549],
          [ 0.1466, -0.1506, -0.0596],
          [-0.0153, -0.0580,  0.2537]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 5.8134e-02,  6.5829e-02,  8.7155e-02],
          [ 5.0548e-02,  6.6136e-02,  8.7490e-02],
          [ 4.6570e-02,  5.2951e-02,  7.4321e-02]],

         [[ 2.9376e-02,  3.5468e-02,  6.4201e-02],
          [ 3.3467e-02,  4.4253e-02,  6.8796e-02],
          [ 3.8673e-02,  3.3285e-02,  5.1090e-02]],

         [[ 1.1643e-02,  1.7438e-02,  3.6145e-02],
          [ 3.1503e-02,  3.4702e-02,  4.5860e-02],
          [ 4.1038e-02,  2.6178e-02,  3.2664e-02]]],


        [[[-4.0180e-02, -2.4123e-02, -2.2019e-02],
          [-2.9870e-02, -2.1646e-02, -2.3870e-02],
          [-1.8742e-02, -1.6960e-02, -2.0845e-02]],

         [[-6.8070e-02, -5.1127e-02, -4.7385e-02],
          [-6.1673e-02, -5.2399e-02, -5.2338e-02],
          [-5.6260e-02, -5.3569e-02, -5.5139e-02]],

         [[-7.6217e-02, -5.4847e-02, -4.8874e-02],
          [-6.9330e-02, -5.5974e-02, -5.4701e-02],
          [-6.7045e-02, -6.0577e-02, -6.0924e-02]]],


        [[[ 5.5133e-04,  1.7053e-03,  2.1943e-03],
          [ 9.6582e-04,  2.7083e-03,  3.2590e-03],
          [ 4.5468e-04,  2.3788e-03,  2.4931e-03]],

         [[-3.1984e-03, -1.7574e-03, -1.1077e-03],
          [-2.0605e-03,  7.7379e-06,  6.8889e-04],
          [-2.3493e-03, -2.3848e-04, -1.1826e-04]],

         [[ 1.5937e-03,  2.8532e-03,  3.6697e-03],
          [ 3.7830e-03,  5.6135e-03,  6.5443e-03],
          [ 2.9164e-03,  4.7210e-03,  4.5946e-03]]],


        ...,


        [[[ 1.1389e-02,  1.1764e-02,  1.2009e-02],
          [-3.7774e-03, -3.1940e-03, -1.4781e-03],
          [-1.8900e-02, -1.4527e-02, -1.5249e-02]],

         [[ 1.5634e-02,  1.9911e-02,  2.3514e-02],
          [ 9.5714e-03,  1.3006e-02,  1.6526e-02],
          [-3.2237e-03,  9.6981e-04,  1.8659e-03]],

         [[ 1.5496e-02,  2.1940e-02,  2.5910e-02],
          [ 1.3877e-02,  1.7828e-02,  2.1884e-02],
          [ 3.9407e-03,  8.7443e-03,  1.0033e-02]]],


        [[[ 7.4669e-03, -6.4649e-03, -1.7732e-02],
          [ 2.0675e-03, -5.5772e-03, -1.1887e-02],
          [ 3.8150e-03,  6.9748e-04, -3.9254e-03]],

         [[ 8.6164e-03, -1.0906e-03, -9.2816e-03],
          [ 6.8345e-03,  2.8207e-03, -4.2425e-04],
          [ 1.0227e-02,  1.0195e-02,  8.1247e-03]],

         [[ 8.4553e-03,  1.0877e-03, -5.8047e-03],
          [ 7.3872e-03,  4.7338e-03,  2.5722e-03],
          [ 1.0497e-02,  1.1607e-02,  1.0603e-02]]],


        [[[-2.2747e-01, -2.0663e-01, -1.9608e-01],
          [-2.6056e-01, -2.3122e-01, -2.1925e-01],
          [-2.5178e-01, -2.1975e-01, -1.9134e-01]],

         [[-1.0501e-01, -9.5546e-02, -8.3012e-02],
          [-1.4199e-01, -1.2200e-01, -1.1354e-01],
          [-1.3556e-01, -1.0998e-01, -8.9779e-02]],

         [[ 1.3665e-03,  1.0873e-02,  1.0707e-02],
          [-3.3687e-02, -1.4813e-02, -2.0397e-02],
          [-2.7813e-02, -5.1855e-03, -1.6794e-03]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0847]], device='cuda:0')

percentage_weight_grad tensor([[8.4690e-06]], device='cuda:0')

Epoch: 21 | Batch_idx: 0 |  Loss: (0.6761) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7930) |  Loss2: (0.0000) | Acc: (72.00%) (1021/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7787) |  Loss2: (0.0000) | Acc: (73.00%) (1975/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7919) |  Loss2: (0.0000) | Acc: (72.00%) (2881/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7840) |  Loss2: (0.0000) | Acc: (72.00%) (3827/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7924) |  Loss2: (0.0000) | Acc: (72.00%) (4750/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7816) |  Loss2: (0.0000) | Acc: (72.00%) (5698/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7871) |  Loss2: (0.0000) | Acc: (72.00%) (6586/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7889) |  Loss2: (0.0000) | Acc: (72.00%) (7494/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7876) |  Loss2: (0.0000) | Acc: (72.00%) (8425/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7863) |  Loss2: (0.0000) | Acc: (72.00%) (9352/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7859) |  Loss2: (0.0000) | Acc: (72.00%) (10296/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7821) |  Loss2: (0.0000) | Acc: (72.00%) (11252/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7813) |  Loss2: (0.0000) | Acc: (72.00%) (12192/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7796) |  Loss2: (0.0000) | Acc: (72.00%) (13139/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7768) |  Loss2: (0.0000) | Acc: (72.00%) (14084/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7753) |  Loss2: (0.0000) | Acc: (72.00%) (15022/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7753) |  Loss2: (0.0000) | Acc: (72.00%) (15935/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7718) |  Loss2: (0.0000) | Acc: (72.00%) (16911/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7706) |  Loss2: (0.0000) | Acc: (73.00%) (17861/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7704) |  Loss2: (0.0000) | Acc: (73.00%) (18784/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7698) |  Loss2: (0.0000) | Acc: (73.00%) (19731/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7686) |  Loss2: (0.0000) | Acc: (73.00%) (20677/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7685) |  Loss2: (0.0000) | Acc: (73.00%) (21601/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7679) |  Loss2: (0.0000) | Acc: (73.00%) (22559/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7689) |  Loss2: (0.0000) | Acc: (73.00%) (23492/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7674) |  Loss2: (0.0000) | Acc: (73.00%) (24449/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7678) |  Loss2: (0.0000) | Acc: (73.00%) (25380/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7670) |  Loss2: (0.0000) | Acc: (73.00%) (26320/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7669) |  Loss2: (0.0000) | Acc: (73.00%) (27268/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7664) |  Loss2: (0.0000) | Acc: (73.00%) (28225/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7655) |  Loss2: (0.0000) | Acc: (73.00%) (29173/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7663) |  Loss2: (0.0000) | Acc: (73.00%) (30095/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7667) |  Loss2: (0.0000) | Acc: (73.00%) (31032/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7657) |  Loss2: (0.0000) | Acc: (73.00%) (31979/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7650) |  Loss2: (0.0000) | Acc: (73.00%) (32926/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (73.00%) (33891/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7632) |  Loss2: (0.0000) | Acc: (73.00%) (34858/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7637) |  Loss2: (0.0000) | Acc: (73.00%) (35791/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7623) |  Loss2: (0.0000) | Acc: (73.00%) (36719/50000)
# TEST : Loss: (0.8584) | Acc: (69.00%) (6980/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1419,  0.0661,  0.0374],
          [ 0.1004, -0.1812, -0.1003],
          [ 0.1679, -0.1614,  0.0119]],

         [[ 0.0211, -0.1189,  0.2089],
          [ 0.0327, -0.0057,  0.0457],
          [ 0.0307, -0.0325, -0.0402]],

         [[ 0.0395, -0.0817,  0.1068],
          [ 0.1359, -0.1079,  0.1278],
          [-0.0819,  0.0383, -0.1812]]],


        [[[ 0.1256, -0.0664,  0.0038],
          [ 0.1497,  0.1794,  0.1720],
          [ 0.0364, -0.0446,  0.1282]],

         [[-0.1661, -0.1863,  0.1593],
          [ 0.0202, -0.0537,  0.0898],
          [ 0.1398, -0.1866,  0.1419]],

         [[ 0.0942, -0.0476, -0.0180],
          [ 0.0032,  0.0339, -0.1311],
          [-0.0289, -0.1361, -0.0081]]],


        [[[-0.0562,  0.1021, -0.0376],
          [-0.1087,  0.0748,  0.1248],
          [ 0.1335,  0.1770,  0.0052]],

         [[ 0.1063,  0.1324,  0.1105],
          [ 0.0802, -0.0428, -0.0412],
          [-0.0366, -0.0312,  0.1428]],

         [[-0.0483,  0.1423,  0.0690],
          [-0.0404, -0.0505, -0.0649],
          [-0.0358, -0.0100,  0.0444]]],


        ...,


        [[[ 0.2101, -0.1291, -0.1271],
          [-0.0472, -0.1003, -0.1042],
          [-0.1135,  0.0143,  0.0482]],

         [[ 0.1664,  0.1829, -0.1309],
          [ 0.2138, -0.1372, -0.1187],
          [-0.0809, -0.0873, -0.0385]],

         [[ 0.1910,  0.1030, -0.0988],
          [-0.0752, -0.1509, -0.0612],
          [ 0.0454,  0.1777, -0.1252]]],


        [[[-0.1283,  0.0730, -0.1791],
          [-0.1470, -0.1475,  0.1154],
          [-0.1677,  0.1507, -0.1279]],

         [[ 0.0891, -0.1289,  0.0686],
          [ 0.1615,  0.0152, -0.0844],
          [-0.1549,  0.1200, -0.1866]],

         [[ 0.1068, -0.0408, -0.0281],
          [ 0.1316, -0.1865,  0.0007],
          [-0.0474,  0.0075, -0.1209]]],


        [[[ 0.0757,  0.1381,  0.0812],
          [-0.0956, -0.1265,  0.1907],
          [-0.1470,  0.0313, -0.0253]],

         [[-0.0378, -0.0902,  0.1371],
          [-0.0292, -0.1735,  0.0155],
          [-0.0213,  0.0717,  0.0335]],

         [[-0.1413, -0.1732,  0.1527],
          [ 0.1451, -0.1510, -0.0558],
          [-0.0153, -0.0583,  0.2550]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0867, -0.0256,  0.0121],
          [-0.0974, -0.0416, -0.0003],
          [-0.1221, -0.0658, -0.0303]],

         [[-0.1290, -0.0593, -0.0063],
          [-0.1368, -0.0702, -0.0133],
          [-0.1596, -0.0917, -0.0443]],

         [[-0.1244, -0.0727, -0.0315],
          [-0.1245, -0.0767, -0.0335],
          [-0.1404, -0.0896, -0.0564]]],


        [[[-0.0064, -0.0173, -0.0109],
          [-0.0084, -0.0144, -0.0090],
          [-0.0167, -0.0208, -0.0138]],

         [[-0.0307, -0.0448, -0.0391],
          [-0.0359, -0.0455, -0.0382],
          [-0.0447, -0.0521, -0.0432]],

         [[ 0.0030, -0.0101, -0.0045],
          [-0.0013, -0.0108, -0.0029],
          [-0.0098, -0.0179, -0.0077]]],


        [[[ 0.0005,  0.0026,  0.0023],
          [ 0.0004,  0.0023,  0.0012],
          [-0.0003,  0.0008, -0.0014]],

         [[-0.0024, -0.0001, -0.0003],
          [-0.0024, -0.0005, -0.0017],
          [-0.0031, -0.0021, -0.0045]],

         [[-0.0018,  0.0006,  0.0008],
          [-0.0024, -0.0005, -0.0016],
          [-0.0030, -0.0021, -0.0047]]],


        ...,


        [[[ 0.0011,  0.0056, -0.0036],
          [ 0.0017,  0.0103,  0.0003],
          [-0.0149,  0.0015, -0.0057]],

         [[-0.0084, -0.0050, -0.0103],
          [-0.0054,  0.0016, -0.0040],
          [-0.0234, -0.0078, -0.0105]],

         [[-0.0219, -0.0165, -0.0235],
          [-0.0163, -0.0088, -0.0165],
          [-0.0281, -0.0146, -0.0200]]],


        [[[-0.0018, -0.0129, -0.0098],
          [-0.0063, -0.0179, -0.0152],
          [-0.0112, -0.0170, -0.0090]],

         [[ 0.0224,  0.0121,  0.0144],
          [ 0.0181,  0.0079,  0.0101],
          [ 0.0122,  0.0077,  0.0159]],

         [[ 0.0346,  0.0271,  0.0282],
          [ 0.0311,  0.0235,  0.0249],
          [ 0.0242,  0.0217,  0.0292]]],


        [[[-0.0908, -0.0720, -0.0836],
          [-0.0828, -0.0671, -0.0843],
          [-0.1061, -0.0882, -0.0975]],

         [[-0.1339, -0.1124, -0.1190],
          [-0.1279, -0.1075, -0.1210],
          [-0.1493, -0.1301, -0.1366]],

         [[-0.1375, -0.1169, -0.1200],
          [-0.1346, -0.1188, -0.1305],
          [-0.1589, -0.1467, -0.1524]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0847]], device='cuda:0')

percentage_weight_grad tensor([[8.4658e-06]], device='cuda:0')

Epoch: 22 | Batch_idx: 0 |  Loss: (0.6925) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7203) |  Loss2: (0.0000) | Acc: (74.00%) (1049/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7142) |  Loss2: (0.0000) | Acc: (74.00%) (1996/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7304) |  Loss2: (0.0000) | Acc: (73.00%) (2923/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7381) |  Loss2: (0.0000) | Acc: (73.00%) (3859/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7361) |  Loss2: (0.0000) | Acc: (73.00%) (4814/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7360) |  Loss2: (0.0000) | Acc: (73.00%) (5768/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7368) |  Loss2: (0.0000) | Acc: (73.00%) (6708/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7354) |  Loss2: (0.0000) | Acc: (73.00%) (7670/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7309) |  Loss2: (0.0000) | Acc: (74.00%) (8640/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7296) |  Loss2: (0.0000) | Acc: (74.00%) (9592/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7326) |  Loss2: (0.0000) | Acc: (74.00%) (10533/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7306) |  Loss2: (0.0000) | Acc: (74.00%) (11500/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7271) |  Loss2: (0.0000) | Acc: (74.00%) (12470/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7265) |  Loss2: (0.0000) | Acc: (74.00%) (13426/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7245) |  Loss2: (0.0000) | Acc: (74.00%) (14393/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7272) |  Loss2: (0.0000) | Acc: (74.00%) (15331/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7267) |  Loss2: (0.0000) | Acc: (74.00%) (16297/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7228) |  Loss2: (0.0000) | Acc: (74.00%) (17289/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7207) |  Loss2: (0.0000) | Acc: (74.00%) (18250/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7211) |  Loss2: (0.0000) | Acc: (74.00%) (19210/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7215) |  Loss2: (0.0000) | Acc: (74.00%) (20165/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7235) |  Loss2: (0.0000) | Acc: (74.00%) (21124/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7234) |  Loss2: (0.0000) | Acc: (74.00%) (22091/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7238) |  Loss2: (0.0000) | Acc: (74.00%) (23061/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7264) |  Loss2: (0.0000) | Acc: (74.00%) (23981/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7265) |  Loss2: (0.0000) | Acc: (74.00%) (24936/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7267) |  Loss2: (0.0000) | Acc: (74.00%) (25864/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7257) |  Loss2: (0.0000) | Acc: (74.00%) (26839/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7248) |  Loss2: (0.0000) | Acc: (74.00%) (27805/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7259) |  Loss2: (0.0000) | Acc: (74.00%) (28739/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7250) |  Loss2: (0.0000) | Acc: (74.00%) (29708/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7241) |  Loss2: (0.0000) | Acc: (74.00%) (30670/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7235) |  Loss2: (0.0000) | Acc: (74.00%) (31641/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7220) |  Loss2: (0.0000) | Acc: (74.00%) (32612/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7218) |  Loss2: (0.0000) | Acc: (74.00%) (33553/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7209) |  Loss2: (0.0000) | Acc: (74.00%) (34535/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7198) |  Loss2: (0.0000) | Acc: (74.00%) (35503/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7198) |  Loss2: (0.0000) | Acc: (74.00%) (36457/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7186) |  Loss2: (0.0000) | Acc: (74.00%) (37396/50000)
# TEST : Loss: (0.7675) | Acc: (72.00%) (7287/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1454,  0.0657,  0.0416],
          [ 0.0982, -0.1853, -0.0995],
          [ 0.1764, -0.1497,  0.0153]],

         [[ 0.0181, -0.1176,  0.2128],
          [ 0.0312, -0.0090,  0.0458],
          [ 0.0371, -0.0229, -0.0391]],

         [[ 0.0332, -0.0850,  0.1061],
          [ 0.1300, -0.1160,  0.1233],
          [-0.0838,  0.0394, -0.1866]]],


        [[[ 0.1210, -0.0705,  0.0015],
          [ 0.1471,  0.1779,  0.1732],
          [ 0.0335, -0.0461,  0.1288]],

         [[-0.1678, -0.1893,  0.1574],
          [ 0.0194, -0.0556,  0.0905],
          [ 0.1382, -0.1887,  0.1420]],

         [[ 0.0941, -0.0492, -0.0184],
          [ 0.0034,  0.0326, -0.1296],
          [-0.0290, -0.1372, -0.0068]]],


        [[[-0.0568,  0.1018, -0.0376],
          [-0.1094,  0.0745,  0.1248],
          [ 0.1330,  0.1769,  0.0053]],

         [[ 0.1058,  0.1322,  0.1105],
          [ 0.0796, -0.0430, -0.0412],
          [-0.0370, -0.0312,  0.1427]],

         [[-0.0492,  0.1418,  0.0687],
          [-0.0415, -0.0511, -0.0653],
          [-0.0367, -0.0105,  0.0439]]],


        ...,


        [[[ 0.2135, -0.1259, -0.1238],
          [-0.0446, -0.0989, -0.1023],
          [-0.1111,  0.0154,  0.0486]],

         [[ 0.1684,  0.1858, -0.1274],
          [ 0.2151, -0.1366, -0.1175],
          [-0.0802, -0.0878, -0.0396]],

         [[ 0.1917,  0.1039, -0.0977],
          [-0.0748, -0.1516, -0.0620],
          [ 0.0451,  0.1760, -0.1282]]],


        [[[-0.1261,  0.0740, -0.1781],
          [-0.1459, -0.1475,  0.1150],
          [-0.1656,  0.1515, -0.1281]],

         [[ 0.0902, -0.1290,  0.0682],
          [ 0.1611,  0.0135, -0.0863],
          [-0.1541,  0.1191, -0.1884]],

         [[ 0.1088, -0.0401, -0.0275],
          [ 0.1327, -0.1868, -0.0001],
          [-0.0456,  0.0075, -0.1218]]],


        [[[ 0.0753,  0.1348,  0.0796],
          [-0.0975, -0.1298,  0.1918],
          [-0.1462,  0.0329, -0.0227]],

         [[-0.0387, -0.0951,  0.1332],
          [-0.0321, -0.1776,  0.0151],
          [-0.0209,  0.0732,  0.0356]],

         [[-0.1429, -0.1782,  0.1492],
          [ 0.1416, -0.1548, -0.0557],
          [-0.0169, -0.0584,  0.2550]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1785,  0.2111,  0.2406],
          [ 0.1338,  0.1586,  0.1910],
          [ 0.1115,  0.1356,  0.1680]],

         [[ 0.1615,  0.1952,  0.2255],
          [ 0.1124,  0.1425,  0.1739],
          [ 0.0915,  0.1211,  0.1525]],

         [[ 0.1030,  0.1412,  0.1696],
          [ 0.0564,  0.0945,  0.1220],
          [ 0.0389,  0.0768,  0.1010]]],


        [[[-0.0319, -0.0276, -0.0124],
          [-0.0186, -0.0155, -0.0055],
          [-0.0255, -0.0229, -0.0164]],

         [[-0.0310, -0.0249, -0.0104],
          [-0.0190, -0.0151, -0.0063],
          [-0.0253, -0.0228, -0.0189]],

         [[-0.0309, -0.0243, -0.0160],
          [-0.0209, -0.0168, -0.0148],
          [-0.0256, -0.0230, -0.0253]]],


        [[[-0.0014, -0.0011, -0.0008],
          [ 0.0015,  0.0006,  0.0001],
          [ 0.0046,  0.0027,  0.0019]],

         [[-0.0039, -0.0025, -0.0015],
          [-0.0007, -0.0006, -0.0006],
          [ 0.0033,  0.0023,  0.0017]],

         [[-0.0064, -0.0051, -0.0040],
          [-0.0040, -0.0038, -0.0034],
          [-0.0001, -0.0004, -0.0005]]],


        ...,


        [[[-0.0177, -0.0117, -0.0096],
          [-0.0162, -0.0123, -0.0116],
          [-0.0080, -0.0092, -0.0079]],

         [[-0.0175, -0.0121, -0.0109],
          [-0.0172, -0.0138, -0.0134],
          [-0.0073, -0.0105, -0.0105]],

         [[-0.0092, -0.0051, -0.0030],
          [-0.0080, -0.0061, -0.0043],
          [ 0.0015, -0.0031, -0.0037]]],


        [[[-0.0034,  0.0026,  0.0047],
          [-0.0090, -0.0028,  0.0011],
          [-0.0101, -0.0059, -0.0000]],

         [[ 0.0030,  0.0090,  0.0115],
          [-0.0043,  0.0024,  0.0062],
          [-0.0052, -0.0014,  0.0040]],

         [[-0.0017,  0.0049,  0.0082],
          [-0.0082, -0.0021,  0.0032],
          [-0.0078, -0.0051, -0.0001]]],


        [[[ 0.0247, -0.0252, -0.0248],
          [-0.0113, -0.0688, -0.0721],
          [-0.0080, -0.0531, -0.0593]],

         [[-0.0126, -0.0635, -0.0692],
          [-0.0450, -0.1002, -0.1073],
          [-0.0358, -0.0791, -0.0911]],

         [[-0.0460, -0.0884, -0.0975],
          [-0.0706, -0.1192, -0.1359],
          [-0.0544, -0.0946, -0.1198]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0846]], device='cuda:0')

percentage_weight_grad tensor([[8.4626e-06]], device='cuda:0')

Epoch: 23 | Batch_idx: 0 |  Loss: (0.6241) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (75.00%) (1066/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6711) |  Loss2: (0.0000) | Acc: (76.00%) (2043/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6745) |  Loss2: (0.0000) | Acc: (75.00%) (3005/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6685) |  Loss2: (0.0000) | Acc: (75.00%) (3985/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6662) |  Loss2: (0.0000) | Acc: (75.00%) (4958/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6761) |  Loss2: (0.0000) | Acc: (75.00%) (5912/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6807) |  Loss2: (0.0000) | Acc: (75.00%) (6859/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6814) |  Loss2: (0.0000) | Acc: (75.00%) (7825/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6788) |  Loss2: (0.0000) | Acc: (75.00%) (8813/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6791) |  Loss2: (0.0000) | Acc: (75.00%) (9794/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6833) |  Loss2: (0.0000) | Acc: (75.00%) (10756/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6808) |  Loss2: (0.0000) | Acc: (75.00%) (11746/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6806) |  Loss2: (0.0000) | Acc: (75.00%) (12729/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6836) |  Loss2: (0.0000) | Acc: (75.00%) (13696/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6858) |  Loss2: (0.0000) | Acc: (75.00%) (14651/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6882) |  Loss2: (0.0000) | Acc: (75.00%) (15608/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6898) |  Loss2: (0.0000) | Acc: (75.00%) (16578/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6899) |  Loss2: (0.0000) | Acc: (75.00%) (17553/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6893) |  Loss2: (0.0000) | Acc: (75.00%) (18513/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6875) |  Loss2: (0.0000) | Acc: (75.00%) (19505/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6868) |  Loss2: (0.0000) | Acc: (75.00%) (20480/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (21447/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6871) |  Loss2: (0.0000) | Acc: (75.00%) (22404/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6880) |  Loss2: (0.0000) | Acc: (75.00%) (23355/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6879) |  Loss2: (0.0000) | Acc: (75.00%) (24323/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6875) |  Loss2: (0.0000) | Acc: (75.00%) (25309/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6880) |  Loss2: (0.0000) | Acc: (75.00%) (26270/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6872) |  Loss2: (0.0000) | Acc: (75.00%) (27258/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6869) |  Loss2: (0.0000) | Acc: (75.00%) (28240/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6876) |  Loss2: (0.0000) | Acc: (75.00%) (29199/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6884) |  Loss2: (0.0000) | Acc: (75.00%) (30156/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6880) |  Loss2: (0.0000) | Acc: (75.00%) (31144/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6890) |  Loss2: (0.0000) | Acc: (75.00%) (32106/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6883) |  Loss2: (0.0000) | Acc: (75.00%) (33096/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6878) |  Loss2: (0.0000) | Acc: (75.00%) (34081/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6880) |  Loss2: (0.0000) | Acc: (75.00%) (35060/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6875) |  Loss2: (0.0000) | Acc: (75.00%) (36049/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (37030/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6856) |  Loss2: (0.0000) | Acc: (75.00%) (37990/50000)
# TEST : Loss: (0.7027) | Acc: (75.00%) (7572/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1456,  0.0672,  0.0506],
          [ 0.0948, -0.1887, -0.0944],
          [ 0.1764, -0.1443,  0.0188]],

         [[ 0.0200, -0.1128,  0.2209],
          [ 0.0298, -0.0110,  0.0485],
          [ 0.0360, -0.0199, -0.0414]],

         [[ 0.0332, -0.0835,  0.1104],
          [ 0.1283, -0.1190,  0.1247],
          [-0.0864,  0.0394, -0.1901]]],


        [[[ 0.1176, -0.0740, -0.0014],
          [ 0.1457,  0.1767,  0.1740],
          [ 0.0317, -0.0480,  0.1281]],

         [[-0.1669, -0.1899,  0.1572],
          [ 0.0208, -0.0554,  0.0930],
          [ 0.1382, -0.1894,  0.1429]],

         [[ 0.0956, -0.0496, -0.0183],
          [ 0.0046,  0.0319, -0.1280],
          [-0.0294, -0.1392, -0.0069]]],


        [[[-0.0575,  0.1014, -0.0378],
          [-0.1100,  0.0742,  0.1247],
          [ 0.1325,  0.1769,  0.0055]],

         [[ 0.1055,  0.1322,  0.1105],
          [ 0.0791, -0.0430, -0.0411],
          [-0.0374, -0.0312,  0.1427]],

         [[-0.0494,  0.1416,  0.0686],
          [-0.0418, -0.0512, -0.0655],
          [-0.0370, -0.0106,  0.0435]]],


        ...,


        [[[ 0.2134, -0.1263, -0.1231],
          [-0.0444, -0.1001, -0.1031],
          [-0.1096,  0.0142,  0.0471]],

         [[ 0.1676,  0.1852, -0.1265],
          [ 0.2148, -0.1377, -0.1185],
          [-0.0787, -0.0887, -0.0415]],

         [[ 0.1917,  0.1038, -0.0968],
          [-0.0745, -0.1526, -0.0631],
          [ 0.0469,  0.1751, -0.1302]]],


        [[[-0.1268,  0.0738, -0.1769],
          [-0.1457, -0.1470,  0.1164],
          [-0.1651,  0.1516, -0.1272]],

         [[ 0.0884, -0.1307,  0.0674],
          [ 0.1603,  0.0123, -0.0868],
          [-0.1544,  0.1178, -0.1894]],

         [[ 0.1071, -0.0413, -0.0280],
          [ 0.1321, -0.1877, -0.0007],
          [-0.0456,  0.0064, -0.1232]]],


        [[[ 0.0711,  0.1331,  0.0779],
          [-0.1017, -0.1345,  0.1906],
          [-0.1517,  0.0262, -0.0258]],

         [[-0.0377, -0.0917,  0.1366],
          [-0.0302, -0.1762,  0.0202],
          [-0.0185,  0.0743,  0.0398]],

         [[-0.1424, -0.1755,  0.1514],
          [ 0.1416, -0.1539, -0.0507],
          [-0.0160, -0.0572,  0.2594]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0774, -0.1097, -0.1137],
          [-0.0321, -0.0593, -0.0735],
          [-0.0040, -0.0287, -0.0418]],

         [[-0.0631, -0.0839, -0.0770],
          [-0.0358, -0.0507, -0.0509],
          [-0.0151, -0.0310, -0.0316]],

         [[ 0.0721,  0.0562,  0.0416],
          [ 0.0845,  0.0822,  0.0653],
          [ 0.0856,  0.0823,  0.0687]]],


        [[[-0.0448, -0.0349,  0.0024],
          [-0.0385, -0.0240,  0.0128],
          [-0.0255, -0.0194,  0.0094]],

         [[-0.0064, -0.0005,  0.0338],
          [-0.0064,  0.0021,  0.0372],
          [ 0.0038,  0.0022,  0.0315]],

         [[ 0.0009,  0.0039,  0.0311],
          [-0.0015,  0.0030,  0.0316],
          [ 0.0055,  0.0022,  0.0279]]],


        [[[ 0.0034,  0.0018,  0.0019],
          [ 0.0048,  0.0026,  0.0014],
          [ 0.0070,  0.0058,  0.0050]],

         [[-0.0030, -0.0049, -0.0051],
          [-0.0030, -0.0056, -0.0070],
          [-0.0017, -0.0034, -0.0042]],

         [[-0.0151, -0.0165, -0.0156],
          [-0.0147, -0.0167, -0.0170],
          [-0.0132, -0.0144, -0.0144]]],


        ...,


        [[[ 0.0148,  0.0196,  0.0114],
          [ 0.0075,  0.0122,  0.0050],
          [ 0.0043,  0.0072, -0.0004]],

         [[-0.0049,  0.0059, -0.0014],
          [-0.0161, -0.0052, -0.0086],
          [-0.0206, -0.0092, -0.0130]],

         [[ 0.0053,  0.0144,  0.0083],
          [-0.0035,  0.0054,  0.0008],
          [-0.0052,  0.0033, -0.0007]]],


        [[[-0.0030, -0.0044,  0.0019],
          [ 0.0027, -0.0002,  0.0019],
          [ 0.0094,  0.0035, -0.0049]],

         [[-0.0106, -0.0120, -0.0036],
          [-0.0026, -0.0055, -0.0021],
          [ 0.0027, -0.0017, -0.0080]],

         [[-0.0059, -0.0080, -0.0016],
          [ 0.0013, -0.0006,  0.0015],
          [ 0.0058,  0.0043, -0.0007]]],


        [[[ 0.1022,  0.1007,  0.1027],
          [ 0.0911,  0.0896,  0.0878],
          [ 0.0732,  0.0891,  0.0822]],

         [[ 0.0595,  0.0441,  0.0466],
          [ 0.0510,  0.0389,  0.0389],
          [ 0.0425,  0.0407,  0.0343]],

         [[ 0.0928,  0.0794,  0.0705],
          [ 0.0869,  0.0766,  0.0670],
          [ 0.0738,  0.0720,  0.0586]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0846]], device='cuda:0')

percentage_weight_grad tensor([[8.4594e-06]], device='cuda:0')

Epoch: 24 | Batch_idx: 0 |  Loss: (0.6435) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6561) |  Loss2: (0.0000) | Acc: (76.00%) (1082/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6669) |  Loss2: (0.0000) | Acc: (76.00%) (2059/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6777) |  Loss2: (0.0000) | Acc: (76.00%) (3023/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6782) |  Loss2: (0.0000) | Acc: (76.00%) (3997/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6683) |  Loss2: (0.0000) | Acc: (76.00%) (4992/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6720) |  Loss2: (0.0000) | Acc: (76.00%) (5965/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6711) |  Loss2: (0.0000) | Acc: (76.00%) (6956/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6666) |  Loss2: (0.0000) | Acc: (76.00%) (7955/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6646) |  Loss2: (0.0000) | Acc: (76.00%) (8955/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6686) |  Loss2: (0.0000) | Acc: (76.00%) (9927/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6670) |  Loss2: (0.0000) | Acc: (76.00%) (10918/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6669) |  Loss2: (0.0000) | Acc: (76.00%) (11888/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6668) |  Loss2: (0.0000) | Acc: (76.00%) (12866/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6640) |  Loss2: (0.0000) | Acc: (76.00%) (13870/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6612) |  Loss2: (0.0000) | Acc: (76.00%) (14861/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6608) |  Loss2: (0.0000) | Acc: (76.00%) (15841/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6623) |  Loss2: (0.0000) | Acc: (76.00%) (16817/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6615) |  Loss2: (0.0000) | Acc: (76.00%) (17806/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6624) |  Loss2: (0.0000) | Acc: (76.00%) (18782/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6616) |  Loss2: (0.0000) | Acc: (76.00%) (19777/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (76.00%) (20767/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6593) |  Loss2: (0.0000) | Acc: (76.00%) (21753/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6584) |  Loss2: (0.0000) | Acc: (76.00%) (22755/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6598) |  Loss2: (0.0000) | Acc: (76.00%) (23728/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6600) |  Loss2: (0.0000) | Acc: (76.00%) (24709/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6600) |  Loss2: (0.0000) | Acc: (76.00%) (25707/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6572) |  Loss2: (0.0000) | Acc: (77.00%) (26726/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6585) |  Loss2: (0.0000) | Acc: (77.00%) (27701/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6570) |  Loss2: (0.0000) | Acc: (77.00%) (28723/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6574) |  Loss2: (0.0000) | Acc: (77.00%) (29706/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6584) |  Loss2: (0.0000) | Acc: (77.00%) (30664/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6587) |  Loss2: (0.0000) | Acc: (77.00%) (31647/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (77.00%) (32659/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6572) |  Loss2: (0.0000) | Acc: (77.00%) (33637/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6572) |  Loss2: (0.0000) | Acc: (77.00%) (34610/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6570) |  Loss2: (0.0000) | Acc: (77.00%) (35599/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6564) |  Loss2: (0.0000) | Acc: (77.00%) (36617/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (37640/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (38590/50000)
# TEST : Loss: (0.7940) | Acc: (72.00%) (7226/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1507,  0.0647,  0.0523],
          [ 0.0899, -0.1957, -0.0948],
          [ 0.1777, -0.1421,  0.0200]],

         [[ 0.0159, -0.1123,  0.2228],
          [ 0.0267, -0.0160,  0.0474],
          [ 0.0362, -0.0182, -0.0426]],

         [[ 0.0314, -0.0802,  0.1145],
          [ 0.1246, -0.1234,  0.1236],
          [-0.0888,  0.0391, -0.1932]]],


        [[[ 0.1169, -0.0733,  0.0000],
          [ 0.1461,  0.1774,  0.1750],
          [ 0.0324, -0.0476,  0.1278]],

         [[-0.1662, -0.1894,  0.1585],
          [ 0.0217, -0.0555,  0.0931],
          [ 0.1386, -0.1902,  0.1413]],

         [[ 0.0968, -0.0491, -0.0169],
          [ 0.0061,  0.0321, -0.1269],
          [-0.0281, -0.1396, -0.0074]]],


        [[[-0.0578,  0.1014, -0.0377],
          [-0.1107,  0.0741,  0.1247],
          [ 0.1317,  0.1768,  0.0054]],

         [[ 0.1051,  0.1322,  0.1105],
          [ 0.0784, -0.0431, -0.0412],
          [-0.0380, -0.0313,  0.1425]],

         [[-0.0494,  0.1419,  0.0689],
          [-0.0420, -0.0512, -0.0654],
          [-0.0372, -0.0107,  0.0433]]],


        ...,


        [[[ 0.2157, -0.1228, -0.1198],
          [-0.0433, -0.0993, -0.1010],
          [-0.1098,  0.0141,  0.0490]],

         [[ 0.1692,  0.1889, -0.1233],
          [ 0.2157, -0.1365, -0.1170],
          [-0.0790, -0.0889, -0.0405]],

         [[ 0.1934,  0.1059, -0.0951],
          [-0.0732, -0.1527, -0.0633],
          [ 0.0472,  0.1740, -0.1304]]],


        [[[-0.1268,  0.0741, -0.1758],
          [-0.1466, -0.1473,  0.1160],
          [-0.1646,  0.1518, -0.1280]],

         [[ 0.0882, -0.1304,  0.0683],
          [ 0.1595,  0.0119, -0.0874],
          [-0.1536,  0.1180, -0.1903]],

         [[ 0.1074, -0.0404, -0.0266],
          [ 0.1319, -0.1875, -0.0008],
          [-0.0442,  0.0070, -0.1238]]],


        [[[ 0.0706,  0.1336,  0.0792],
          [-0.1027, -0.1370,  0.1907],
          [-0.1520,  0.0237, -0.0287]],

         [[-0.0395, -0.0935,  0.1360],
          [-0.0312, -0.1792,  0.0202],
          [-0.0171,  0.0733,  0.0381]],

         [[-0.1455, -0.1774,  0.1507],
          [ 0.1378, -0.1575, -0.0509],
          [-0.0166, -0.0587,  0.2573]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1585, -0.1511, -0.1091],
          [-0.1117, -0.0936, -0.0496],
          [-0.1041, -0.0829, -0.0417]],

         [[-0.1622, -0.1683, -0.1354],
          [-0.1181, -0.1138, -0.0860],
          [-0.1151, -0.1033, -0.0845]],

         [[-0.1101, -0.1132, -0.0912],
          [-0.0699, -0.0603, -0.0438],
          [-0.0789, -0.0648, -0.0541]]],


        [[[-0.0007,  0.0190,  0.0185],
          [-0.0053,  0.0113,  0.0057],
          [-0.0325, -0.0144, -0.0219]],

         [[-0.0031,  0.0208,  0.0345],
          [-0.0104,  0.0067,  0.0166],
          [-0.0312, -0.0162, -0.0140]],

         [[-0.0096,  0.0082,  0.0148],
          [-0.0150, -0.0040, -0.0022],
          [-0.0315, -0.0219, -0.0260]]],


        [[[-0.0015,  0.0003,  0.0006],
          [ 0.0024,  0.0035,  0.0023],
          [ 0.0027,  0.0041,  0.0021]],

         [[-0.0044, -0.0018, -0.0013],
          [ 0.0012,  0.0030,  0.0027],
          [ 0.0027,  0.0046,  0.0032]],

         [[-0.0023, -0.0007, -0.0017],
          [ 0.0030,  0.0040,  0.0022],
          [ 0.0045,  0.0058,  0.0035]]],


        ...,


        [[[-0.0439, -0.0287, -0.0345],
          [-0.0350, -0.0233, -0.0258],
          [-0.0400, -0.0349, -0.0310]],

         [[-0.0234, -0.0098, -0.0172],
          [-0.0191, -0.0066, -0.0093],
          [-0.0240, -0.0149, -0.0100]],

         [[-0.0240, -0.0104, -0.0117],
          [-0.0186, -0.0064, -0.0050],
          [-0.0228, -0.0130, -0.0042]]],


        [[[ 0.0050,  0.0035, -0.0057],
          [ 0.0015, -0.0091, -0.0107],
          [-0.0039, -0.0088, -0.0081]],

         [[ 0.0139,  0.0158,  0.0054],
          [ 0.0147,  0.0072,  0.0034],
          [ 0.0110,  0.0096,  0.0083]],

         [[ 0.0138,  0.0157,  0.0034],
          [ 0.0143,  0.0089,  0.0029],
          [ 0.0105,  0.0102,  0.0076]]],


        [[[ 0.0629,  0.1122,  0.1480],
          [ 0.0917,  0.1256,  0.1580],
          [ 0.1017,  0.1247,  0.1217]],

         [[ 0.1045,  0.1585,  0.1863],
          [ 0.1064,  0.1506,  0.1725],
          [ 0.0966,  0.1379,  0.1248]],

         [[ 0.1034,  0.1589,  0.1854],
          [ 0.0997,  0.1425,  0.1587],
          [ 0.0858,  0.1239,  0.1111]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0846]], device='cuda:0')

percentage_weight_grad tensor([[8.4562e-06]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.9708) |  Loss2: (0.3826) | Acc: (77.00%) (99/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (1.0279) |  Loss2: (0.3826) | Acc: (76.00%) (1081/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (1.0830) |  Loss2: (0.3826) | Acc: (74.00%) (2012/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (1.1099) |  Loss2: (0.3826) | Acc: (73.00%) (2926/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (1.1503) |  Loss2: (0.3825) | Acc: (72.00%) (3816/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (1.1585) |  Loss2: (0.3825) | Acc: (72.00%) (4729/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (1.1669) |  Loss2: (0.3825) | Acc: (72.00%) (5643/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (1.1790) |  Loss2: (0.3825) | Acc: (71.00%) (6525/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (1.1831) |  Loss2: (0.3825) | Acc: (71.00%) (7427/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (1.1780) |  Loss2: (0.3824) | Acc: (71.00%) (8374/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (1.1794) |  Loss2: (0.3824) | Acc: (71.00%) (9299/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (1.1766) |  Loss2: (0.3824) | Acc: (71.00%) (10228/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (1.1752) |  Loss2: (0.3824) | Acc: (72.00%) (11156/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (1.1724) |  Loss2: (0.3824) | Acc: (72.00%) (12103/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (1.1680) |  Loss2: (0.3823) | Acc: (72.00%) (13045/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (1.1636) |  Loss2: (0.3823) | Acc: (72.00%) (14001/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (1.1619) |  Loss2: (0.3823) | Acc: (72.00%) (14939/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (1.1606) |  Loss2: (0.3823) | Acc: (72.00%) (15871/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (1.1598) |  Loss2: (0.3822) | Acc: (72.00%) (16796/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (1.1550) |  Loss2: (0.3822) | Acc: (72.00%) (17756/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (1.1548) |  Loss2: (0.3822) | Acc: (72.00%) (18694/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (1.1541) |  Loss2: (0.3821) | Acc: (72.00%) (19640/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (1.1527) |  Loss2: (0.3821) | Acc: (72.00%) (20577/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (1.1502) |  Loss2: (0.3821) | Acc: (72.00%) (21522/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (1.1482) |  Loss2: (0.3821) | Acc: (72.00%) (22487/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (1.1488) |  Loss2: (0.3820) | Acc: (72.00%) (23395/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (1.1466) |  Loss2: (0.3820) | Acc: (72.00%) (24348/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (1.1450) |  Loss2: (0.3820) | Acc: (72.00%) (25310/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (1.1443) |  Loss2: (0.3819) | Acc: (72.00%) (26254/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (1.1403) |  Loss2: (0.3819) | Acc: (73.00%) (27234/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (1.1380) |  Loss2: (0.3819) | Acc: (73.00%) (28214/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (1.1368) |  Loss2: (0.3818) | Acc: (73.00%) (29169/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (1.1356) |  Loss2: (0.3818) | Acc: (73.00%) (30127/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (1.1337) |  Loss2: (0.3818) | Acc: (73.00%) (31108/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (1.1311) |  Loss2: (0.3817) | Acc: (73.00%) (32083/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (1.1298) |  Loss2: (0.3817) | Acc: (73.00%) (33044/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (1.1276) |  Loss2: (0.3817) | Acc: (73.00%) (34028/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (1.1261) |  Loss2: (0.3816) | Acc: (73.00%) (34993/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (1.1249) |  Loss2: (0.3816) | Acc: (73.00%) (35951/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (1.1231) |  Loss2: (0.3815) | Acc: (73.00%) (36891/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_025.pth.tar'
# TEST : Loss: (0.7274) | Acc: (74.00%) (7471/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1490,  0.0668,  0.0544],
          [ 0.0910, -0.1943, -0.0933],
          [ 0.1781, -0.1411,  0.0211]],

         [[ 0.0175, -0.1099,  0.2251],
          [ 0.0275, -0.0146,  0.0489],
          [ 0.0364, -0.0174, -0.0415]],

         [[ 0.0330, -0.0780,  0.1167],
          [ 0.1253, -0.1223,  0.1250],
          [-0.0885,  0.0396, -0.1921]]],


        [[[ 0.1169, -0.0736, -0.0005],
          [ 0.1462,  0.1771,  0.1746],
          [ 0.0328, -0.0475,  0.1277]],

         [[-0.1666, -0.1902,  0.1573],
          [ 0.0214, -0.0561,  0.0922],
          [ 0.1385, -0.1905,  0.1408]],

         [[ 0.0965, -0.0495, -0.0174],
          [ 0.0061,  0.0318, -0.1271],
          [-0.0278, -0.1394, -0.0074]]],


        [[[-0.0578,  0.1014, -0.0377],
          [-0.1107,  0.0740,  0.1246],
          [ 0.1316,  0.1766,  0.0054]],

         [[ 0.1051,  0.1322,  0.1105],
          [ 0.0783, -0.0432, -0.0413],
          [-0.0381, -0.0314,  0.1423]],

         [[-0.0494,  0.1418,  0.0688],
          [-0.0422, -0.0513, -0.0655],
          [-0.0375, -0.0109,  0.0431]]],


        ...,


        [[[ 0.2156, -0.1228, -0.1197],
          [-0.0434, -0.0993, -0.1010],
          [-0.1097,  0.0142,  0.0491]],

         [[ 0.1690,  0.1886, -0.1234],
          [ 0.2154, -0.1366, -0.1172],
          [-0.0791, -0.0889, -0.0406]],

         [[ 0.1933,  0.1058, -0.0951],
          [-0.0731, -0.1526, -0.0633],
          [ 0.0473,  0.1741, -0.1304]]],


        [[[-0.1270,  0.0739, -0.1757],
          [-0.1466, -0.1471,  0.1161],
          [-0.1644,  0.1519, -0.1278]],

         [[ 0.0877, -0.1307,  0.0680],
          [ 0.1591,  0.0117, -0.0874],
          [-0.1537,  0.1178, -0.1904]],

         [[ 0.1072, -0.0405, -0.0265],
          [ 0.1318, -0.1874, -0.0008],
          [-0.0441,  0.0070, -0.1237]]],


        [[[ 0.0705,  0.1330,  0.0784],
          [-0.1031, -0.1376,  0.1897],
          [-0.1525,  0.0230, -0.0294]],

         [[-0.0396, -0.0941,  0.1352],
          [-0.0314, -0.1797,  0.0195],
          [-0.0171,  0.0729,  0.0377]],

         [[-0.1463, -0.1787,  0.1494],
          [ 0.1369, -0.1585, -0.0520],
          [-0.0173, -0.0596,  0.2564]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4898e-05,  6.6763e-06,  5.4363e-06],
          [ 9.0953e-06, -1.9425e-05, -9.3330e-06],
          [ 1.7811e-05, -1.4109e-05,  2.1134e-06]],

         [[ 1.7463e-06, -1.0993e-05,  2.2505e-05],
          [ 2.7546e-06, -1.4627e-06,  4.8934e-06],
          [ 3.6393e-06, -1.7374e-06, -4.1456e-06]],

         [[ 3.2960e-06, -7.8001e-06,  1.1673e-05],
          [ 1.2532e-05, -1.2226e-05,  1.2502e-05],
          [-8.8528e-06,  3.9622e-06, -1.9207e-05]]],


        [[[ 1.1685e-05, -7.3582e-06, -4.5771e-08],
          [ 1.4619e-05,  1.7715e-05,  1.7459e-05],
          [ 3.2829e-06, -4.7507e-06,  1.2771e-05]],

         [[-1.6665e-05, -1.9023e-05,  1.5731e-05],
          [ 2.1384e-06, -5.6132e-06,  9.2182e-06],
          [ 1.3849e-05, -1.9045e-05,  1.4075e-05]],

         [[ 9.6529e-06, -4.9516e-06, -1.7445e-06],
          [ 6.1052e-07,  3.1835e-06, -1.2714e-05],
          [-2.7767e-06, -1.3941e-05, -7.3597e-07]]],


        [[[-5.7774e-06,  1.0140e-05, -3.7659e-06],
          [-1.1068e-05,  7.4041e-06,  1.2463e-05],
          [ 1.3160e-05,  1.7664e-05,  5.3985e-07]],

         [[ 1.0506e-05,  1.3222e-05,  1.1050e-05],
          [ 7.8324e-06, -4.3181e-06, -4.1251e-06],
          [-3.8076e-06, -3.1367e-06,  1.4229e-05]],

         [[-4.9446e-06,  1.4181e-05,  6.8772e-06],
          [-4.2177e-06, -5.1317e-06, -6.5521e-06],
          [-3.7454e-06, -1.0909e-06,  4.3099e-06]]],


        ...,


        [[[ 2.1556e-05, -1.2282e-05, -1.1972e-05],
          [-4.3403e-06, -9.9250e-06, -1.0096e-05],
          [-1.0974e-05,  1.4203e-06,  4.9142e-06]],

         [[ 1.6895e-05,  1.8860e-05, -1.2342e-05],
          [ 2.1544e-05, -1.3660e-05, -1.1716e-05],
          [-7.9057e-06, -8.8907e-06, -4.0632e-06]],

         [[ 1.9331e-05,  1.0580e-05, -9.5150e-06],
          [-7.3086e-06, -1.5264e-05, -6.3327e-06],
          [ 4.7256e-06,  1.7407e-05, -1.3038e-05]]],


        [[[-1.2701e-05,  7.3949e-06, -1.7573e-05],
          [-1.4658e-05, -1.4714e-05,  1.1609e-05],
          [-1.6441e-05,  1.5189e-05, -1.2784e-05]],

         [[ 8.7744e-06, -1.3067e-05,  6.8050e-06],
          [ 1.5914e-05,  1.1714e-06, -8.7423e-06],
          [-1.5370e-05,  1.1781e-05, -1.9037e-05]],

         [[ 1.0716e-05, -4.0515e-06, -2.6529e-06],
          [ 1.3180e-05, -1.8737e-05, -7.6378e-08],
          [-4.4137e-06,  7.0490e-07, -1.2374e-05]]],


        [[[ 7.0524e-06,  1.3301e-05,  7.8393e-06],
          [-1.0308e-05, -1.3759e-05,  1.8970e-05],
          [-1.5249e-05,  2.2957e-06, -2.9397e-06]],

         [[-3.9577e-06, -9.4062e-06,  1.3523e-05],
          [-3.1385e-06, -1.7971e-05,  1.9456e-06],
          [-1.7139e-06,  7.2891e-06,  3.7726e-06]],

         [[-1.4628e-05, -1.7867e-05,  1.4939e-05],
          [ 1.3686e-05, -1.5855e-05, -5.2016e-06],
          [-1.7300e-06, -5.9567e-06,  2.5640e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0003]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0439]], device='cuda:0')

Epoch: 26 | Batch_idx: 0 |  Loss: (0.9679) |  Loss2: (0.3801) | Acc: (82.00%) (105/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (1.0724) |  Loss2: (0.3801) | Acc: (75.00%) (1070/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (1.0751) |  Loss2: (0.3800) | Acc: (76.00%) (2047/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (1.0530) |  Loss2: (0.3800) | Acc: (76.00%) (3048/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (1.0555) |  Loss2: (0.3799) | Acc: (76.00%) (4020/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (1.0536) |  Loss2: (0.3799) | Acc: (76.00%) (5005/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (1.0525) |  Loss2: (0.3798) | Acc: (76.00%) (5978/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (1.0442) |  Loss2: (0.3798) | Acc: (76.00%) (6978/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (1.0458) |  Loss2: (0.3798) | Acc: (76.00%) (7952/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (1.0435) |  Loss2: (0.3797) | Acc: (76.00%) (8935/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (1.0460) |  Loss2: (0.3797) | Acc: (76.00%) (9917/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (1.0479) |  Loss2: (0.3796) | Acc: (76.00%) (10887/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (1.0504) |  Loss2: (0.3796) | Acc: (76.00%) (11835/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (1.0518) |  Loss2: (0.3795) | Acc: (76.00%) (12808/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (1.0515) |  Loss2: (0.3795) | Acc: (76.00%) (13795/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (1.0554) |  Loss2: (0.3794) | Acc: (76.00%) (14743/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (1.0549) |  Loss2: (0.3794) | Acc: (76.00%) (15726/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (1.0516) |  Loss2: (0.3794) | Acc: (76.00%) (16729/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (1.0504) |  Loss2: (0.3793) | Acc: (76.00%) (17725/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (1.0506) |  Loss2: (0.3793) | Acc: (76.00%) (18710/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (1.0507) |  Loss2: (0.3792) | Acc: (76.00%) (19702/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (1.0504) |  Loss2: (0.3792) | Acc: (76.00%) (20672/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (1.0521) |  Loss2: (0.3791) | Acc: (76.00%) (21621/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (1.0527) |  Loss2: (0.3791) | Acc: (76.00%) (22596/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (1.0511) |  Loss2: (0.3790) | Acc: (76.00%) (23579/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (1.0523) |  Loss2: (0.3790) | Acc: (76.00%) (24556/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (1.0529) |  Loss2: (0.3790) | Acc: (76.00%) (25526/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (1.0514) |  Loss2: (0.3789) | Acc: (76.00%) (26525/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (1.0501) |  Loss2: (0.3789) | Acc: (76.00%) (27536/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (1.0523) |  Loss2: (0.3788) | Acc: (76.00%) (28480/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (1.0534) |  Loss2: (0.3788) | Acc: (76.00%) (29438/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (1.0522) |  Loss2: (0.3788) | Acc: (76.00%) (30437/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (1.0504) |  Loss2: (0.3787) | Acc: (76.00%) (31428/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (1.0511) |  Loss2: (0.3787) | Acc: (76.00%) (32392/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (1.0512) |  Loss2: (0.3786) | Acc: (76.00%) (33379/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (1.0509) |  Loss2: (0.3786) | Acc: (76.00%) (34349/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (1.0509) |  Loss2: (0.3785) | Acc: (76.00%) (35333/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (1.0519) |  Loss2: (0.3785) | Acc: (76.00%) (36302/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (1.0515) |  Loss2: (0.3785) | Acc: (76.00%) (37299/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (1.0528) |  Loss2: (0.3784) | Acc: (76.00%) (38222/50000)
# TEST : Loss: (0.6922) | Acc: (76.00%) (7622/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1489,  0.0667,  0.0543],
          [ 0.0909, -0.1942, -0.0933],
          [ 0.1780, -0.1410,  0.0211]],

         [[ 0.0175, -0.1099,  0.2250],
          [ 0.0275, -0.0146,  0.0489],
          [ 0.0364, -0.0174, -0.0414]],

         [[ 0.0329, -0.0780,  0.1167],
          [ 0.1253, -0.1222,  0.1250],
          [-0.0885,  0.0396, -0.1920]]],


        [[[ 0.1168, -0.0736, -0.0005],
          [ 0.1461,  0.1771,  0.1745],
          [ 0.0328, -0.0475,  0.1277]],

         [[-0.1666, -0.1902,  0.1572],
          [ 0.0214, -0.0561,  0.0921],
          [ 0.1384, -0.1904,  0.1407]],

         [[ 0.0965, -0.0495, -0.0174],
          [ 0.0061,  0.0318, -0.1271],
          [-0.0278, -0.1394, -0.0074]]],


        [[[-0.0578,  0.1014, -0.0376],
          [-0.1106,  0.0740,  0.1246],
          [ 0.1315,  0.1766,  0.0054]],

         [[ 0.1050,  0.1322,  0.1105],
          [ 0.0783, -0.0432, -0.0412],
          [-0.0381, -0.0314,  0.1422]],

         [[-0.0494,  0.1417,  0.0687],
          [-0.0422, -0.0513, -0.0655],
          [-0.0374, -0.0109,  0.0431]]],


        ...,


        [[[ 0.2155, -0.1228, -0.1197],
          [-0.0434, -0.0992, -0.1009],
          [-0.1097,  0.0142,  0.0491]],

         [[ 0.1689,  0.1885, -0.1234],
          [ 0.2154, -0.1365, -0.1171],
          [-0.0790, -0.0889, -0.0406]],

         [[ 0.1932,  0.1058, -0.0951],
          [-0.0731, -0.1526, -0.0633],
          [ 0.0472,  0.1740, -0.1303]]],


        [[[-0.1270,  0.0739, -0.1757],
          [-0.1465, -0.1471,  0.1160],
          [-0.1643,  0.1518, -0.1278]],

         [[ 0.0877, -0.1306,  0.0680],
          [ 0.1591,  0.0117, -0.0874],
          [-0.1536,  0.1178, -0.1903]],

         [[ 0.1071, -0.0405, -0.0265],
          [ 0.1317, -0.1873, -0.0008],
          [-0.0441,  0.0070, -0.1237]]],


        [[[ 0.0705,  0.1330,  0.0784],
          [-0.1030, -0.1375,  0.1896],
          [-0.1524,  0.0229, -0.0294]],

         [[-0.0396, -0.0940,  0.1352],
          [-0.0314, -0.1796,  0.0194],
          [-0.0171,  0.0729,  0.0377]],

         [[-0.1462, -0.1786,  0.1493],
          [ 0.1368, -0.1585, -0.0520],
          [-0.0173, -0.0595,  0.2563]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4892e-05,  6.6737e-06,  5.4341e-06],
          [ 9.0918e-06, -1.9418e-05, -9.3292e-06],
          [ 1.7804e-05, -1.4104e-05,  2.1126e-06]],

         [[ 1.7456e-06, -1.0988e-05,  2.2497e-05],
          [ 2.7535e-06, -1.4621e-06,  4.8915e-06],
          [ 3.6379e-06, -1.7367e-06, -4.1440e-06]],

         [[ 3.2946e-06, -7.7971e-06,  1.1668e-05],
          [ 1.2527e-05, -1.2221e-05,  1.2498e-05],
          [-8.8493e-06,  3.9606e-06, -1.9200e-05]]],


        [[[ 1.1680e-05, -7.3553e-06, -4.5752e-08],
          [ 1.4613e-05,  1.7708e-05,  1.7452e-05],
          [ 3.2816e-06, -4.7488e-06,  1.2765e-05]],

         [[-1.6658e-05, -1.9016e-05,  1.5724e-05],
          [ 2.1376e-06, -5.6110e-06,  9.2147e-06],
          [ 1.3844e-05, -1.9038e-05,  1.4070e-05]],

         [[ 9.6491e-06, -4.9497e-06, -1.7438e-06],
          [ 6.1029e-07,  3.1822e-06, -1.2709e-05],
          [-2.7756e-06, -1.3936e-05, -7.3568e-07]]],


        [[[-5.7751e-06,  1.0136e-05, -3.7644e-06],
          [-1.1064e-05,  7.4012e-06,  1.2458e-05],
          [ 1.3155e-05,  1.7657e-05,  5.3963e-07]],

         [[ 1.0502e-05,  1.3216e-05,  1.1046e-05],
          [ 7.8291e-06, -4.3164e-06, -4.1235e-06],
          [-3.8062e-06, -3.1355e-06,  1.4224e-05]],

         [[-4.9427e-06,  1.4175e-05,  6.8746e-06],
          [-4.2161e-06, -5.1296e-06, -6.5495e-06],
          [-3.7439e-06, -1.0904e-06,  4.3082e-06]]],


        ...,


        [[[ 2.1548e-05, -1.2277e-05, -1.1967e-05],
          [-4.3385e-06, -9.9212e-06, -1.0092e-05],
          [-1.0970e-05,  1.4198e-06,  4.9123e-06]],

         [[ 1.6889e-05,  1.8853e-05, -1.2337e-05],
          [ 2.1536e-05, -1.3654e-05, -1.1711e-05],
          [-7.9024e-06, -8.8872e-06, -4.0616e-06]],

         [[ 1.9323e-05,  1.0576e-05, -9.5112e-06],
          [-7.3057e-06, -1.5259e-05, -6.3303e-06],
          [ 4.7237e-06,  1.7400e-05, -1.3033e-05]]],


        [[[-1.2696e-05,  7.3920e-06, -1.7566e-05],
          [-1.4652e-05, -1.4709e-05,  1.1605e-05],
          [-1.6434e-05,  1.5183e-05, -1.2778e-05]],

         [[ 8.7709e-06, -1.3062e-05,  6.8023e-06],
          [ 1.5908e-05,  1.1710e-06, -8.7388e-06],
          [-1.5364e-05,  1.1776e-05, -1.9030e-05]],

         [[ 1.0712e-05, -4.0499e-06, -2.6519e-06],
          [ 1.3175e-05, -1.8729e-05, -7.6348e-08],
          [-4.4120e-06,  7.0462e-07, -1.2369e-05]]],


        [[[ 7.0498e-06,  1.3296e-05,  7.8360e-06],
          [-1.0304e-05, -1.3754e-05,  1.8962e-05],
          [-1.5243e-05,  2.2949e-06, -2.9386e-06]],

         [[-3.9561e-06, -9.4024e-06,  1.3517e-05],
          [-3.1373e-06, -1.7964e-05,  1.9449e-06],
          [-1.7132e-06,  7.2862e-06,  3.7711e-06]],

         [[-1.4623e-05, -1.7860e-05,  1.4933e-05],
          [ 1.3681e-05, -1.5848e-05, -5.1995e-06],
          [-1.7294e-06, -5.9544e-06,  2.5630e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0017]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0417]], device='cuda:0')

Epoch: 27 | Batch_idx: 0 |  Loss: (1.0404) |  Loss2: (0.3768) | Acc: (82.00%) (105/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (1.0175) |  Loss2: (0.3768) | Acc: (78.00%) (1100/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (1.0400) |  Loss2: (0.3767) | Acc: (77.00%) (2086/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (1.0447) |  Loss2: (0.3767) | Acc: (77.00%) (3057/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (1.0383) |  Loss2: (0.3767) | Acc: (77.00%) (4056/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (1.0447) |  Loss2: (0.3766) | Acc: (77.00%) (5034/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (1.0374) |  Loss2: (0.3766) | Acc: (77.00%) (6046/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (1.0367) |  Loss2: (0.3765) | Acc: (77.00%) (7041/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (1.0319) |  Loss2: (0.3765) | Acc: (77.00%) (8040/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (1.0377) |  Loss2: (0.3764) | Acc: (77.00%) (8994/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (1.0406) |  Loss2: (0.3764) | Acc: (77.00%) (9971/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (1.0393) |  Loss2: (0.3764) | Acc: (77.00%) (10965/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (1.0373) |  Loss2: (0.3763) | Acc: (77.00%) (11965/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (1.0361) |  Loss2: (0.3763) | Acc: (77.00%) (12955/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (1.0355) |  Loss2: (0.3762) | Acc: (77.00%) (13944/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (1.0341) |  Loss2: (0.3762) | Acc: (77.00%) (14938/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (1.0357) |  Loss2: (0.3761) | Acc: (77.00%) (15913/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (1.0341) |  Loss2: (0.3761) | Acc: (77.00%) (16916/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (1.0326) |  Loss2: (0.3760) | Acc: (77.00%) (17910/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (1.0313) |  Loss2: (0.3760) | Acc: (77.00%) (18902/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (1.0313) |  Loss2: (0.3760) | Acc: (77.00%) (19898/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (1.0318) |  Loss2: (0.3759) | Acc: (77.00%) (20883/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (1.0302) |  Loss2: (0.3759) | Acc: (77.00%) (21874/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (1.0300) |  Loss2: (0.3758) | Acc: (77.00%) (22867/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (1.0312) |  Loss2: (0.3758) | Acc: (77.00%) (23841/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (1.0330) |  Loss2: (0.3757) | Acc: (77.00%) (24810/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (1.0316) |  Loss2: (0.3757) | Acc: (77.00%) (25822/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (1.0315) |  Loss2: (0.3757) | Acc: (77.00%) (26817/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (1.0307) |  Loss2: (0.3756) | Acc: (77.00%) (27825/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (1.0309) |  Loss2: (0.3756) | Acc: (77.00%) (28802/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (1.0290) |  Loss2: (0.3755) | Acc: (77.00%) (29821/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (1.0293) |  Loss2: (0.3755) | Acc: (77.00%) (30793/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (1.0286) |  Loss2: (0.3755) | Acc: (77.00%) (31802/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (1.0294) |  Loss2: (0.3754) | Acc: (77.00%) (32783/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (1.0298) |  Loss2: (0.3754) | Acc: (77.00%) (33761/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (1.0280) |  Loss2: (0.3753) | Acc: (77.00%) (34769/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (1.0263) |  Loss2: (0.3753) | Acc: (77.00%) (35780/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (1.0257) |  Loss2: (0.3752) | Acc: (77.00%) (36798/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (1.0254) |  Loss2: (0.3752) | Acc: (77.00%) (37790/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (1.0255) |  Loss2: (0.3752) | Acc: (77.00%) (38743/50000)
# TEST : Loss: (0.6798) | Acc: (76.00%) (7680/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1489,  0.0667,  0.0543],
          [ 0.0909, -0.1941, -0.0933],
          [ 0.1780, -0.1410,  0.0211]],

         [[ 0.0174, -0.1098,  0.2249],
          [ 0.0275, -0.0146,  0.0489],
          [ 0.0364, -0.0174, -0.0414]],

         [[ 0.0329, -0.0779,  0.1166],
          [ 0.1252, -0.1222,  0.1249],
          [-0.0885,  0.0396, -0.1919]]],


        [[[ 0.1168, -0.0735, -0.0005],
          [ 0.1461,  0.1770,  0.1745],
          [ 0.0328, -0.0475,  0.1276]],

         [[-0.1665, -0.1901,  0.1572],
          [ 0.0214, -0.0561,  0.0921],
          [ 0.1384, -0.1903,  0.1406]],

         [[ 0.0965, -0.0495, -0.0174],
          [ 0.0061,  0.0318, -0.1270],
          [-0.0277, -0.1393, -0.0074]]],


        [[[-0.0577,  0.1013, -0.0376],
          [-0.1106,  0.0740,  0.1245],
          [ 0.1315,  0.1765,  0.0054]],

         [[ 0.1050,  0.1321,  0.1104],
          [ 0.0783, -0.0431, -0.0412],
          [-0.0380, -0.0313,  0.1422]],

         [[-0.0494,  0.1417,  0.0687],
          [-0.0421, -0.0513, -0.0655],
          [-0.0374, -0.0109,  0.0431]]],


        ...,


        [[[ 0.2154, -0.1227, -0.1196],
          [-0.0434, -0.0992, -0.1009],
          [-0.1097,  0.0142,  0.0491]],

         [[ 0.1688,  0.1884, -0.1233],
          [ 0.2153, -0.1365, -0.1171],
          [-0.0790, -0.0888, -0.0406]],

         [[ 0.1932,  0.1057, -0.0951],
          [-0.0730, -0.1525, -0.0633],
          [ 0.0472,  0.1739, -0.1303]]],


        [[[-0.1269,  0.0739, -0.1756],
          [-0.1465, -0.1470,  0.1160],
          [-0.1643,  0.1518, -0.1277]],

         [[ 0.0877, -0.1306,  0.0680],
          [ 0.1590,  0.0117, -0.0874],
          [-0.1536,  0.1177, -0.1902]],

         [[ 0.1071, -0.0405, -0.0265],
          [ 0.1317, -0.1872, -0.0008],
          [-0.0441,  0.0070, -0.1236]]],


        [[[ 0.0705,  0.1329,  0.0783],
          [-0.1030, -0.1375,  0.1895],
          [-0.1524,  0.0229, -0.0294]],

         [[-0.0395, -0.0940,  0.1351],
          [-0.0314, -0.1796,  0.0194],
          [-0.0171,  0.0728,  0.0377]],

         [[-0.1462, -0.1785,  0.1493],
          [ 0.1368, -0.1584, -0.0520],
          [-0.0173, -0.0595,  0.2562]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4886e-05,  6.6711e-06,  5.4319e-06],
          [ 9.0883e-06, -1.9410e-05, -9.3255e-06],
          [ 1.7797e-05, -1.4098e-05,  2.1118e-06]],

         [[ 1.7449e-06, -1.0984e-05,  2.2488e-05],
          [ 2.7524e-06, -1.4615e-06,  4.8896e-06],
          [ 3.6364e-06, -1.7361e-06, -4.1424e-06]],

         [[ 3.2933e-06, -7.7942e-06,  1.1663e-05],
          [ 1.2523e-05, -1.2216e-05,  1.2493e-05],
          [-8.8458e-06,  3.9590e-06, -1.9192e-05]]],


        [[[ 1.1676e-05, -7.3524e-06, -4.5734e-08],
          [ 1.4607e-05,  1.7701e-05,  1.7445e-05],
          [ 3.2803e-06, -4.7469e-06,  1.2760e-05]],

         [[-1.6652e-05, -1.9008e-05,  1.5718e-05],
          [ 2.1368e-06, -5.6088e-06,  9.2112e-06],
          [ 1.3838e-05, -1.9030e-05,  1.4065e-05]],

         [[ 9.6453e-06, -4.9478e-06, -1.7432e-06],
          [ 6.1005e-07,  3.1809e-06, -1.2704e-05],
          [-2.7745e-06, -1.3931e-05, -7.3538e-07]]],


        [[[-5.7728e-06,  1.0132e-05, -3.7630e-06],
          [-1.1059e-05,  7.3983e-06,  1.2453e-05],
          [ 1.3149e-05,  1.7650e-05,  5.3941e-07]],

         [[ 1.0498e-05,  1.3211e-05,  1.1042e-05],
          [ 7.8259e-06, -4.3146e-06, -4.1219e-06],
          [-3.8047e-06, -3.1343e-06,  1.4218e-05]],

         [[-4.9408e-06,  1.4169e-05,  6.8720e-06],
          [-4.2145e-06, -5.1276e-06, -6.5469e-06],
          [-3.7425e-06, -1.0900e-06,  4.3064e-06]]],


        ...,


        [[[ 2.1540e-05, -1.2272e-05, -1.1962e-05],
          [-4.3368e-06, -9.9175e-06, -1.0088e-05],
          [-1.0966e-05,  1.4192e-06,  4.9104e-06]],

         [[ 1.6883e-05,  1.8845e-05, -1.2332e-05],
          [ 2.1528e-05, -1.3649e-05, -1.1706e-05],
          [-7.8992e-06, -8.8837e-06, -4.0600e-06]],

         [[ 1.9316e-05,  1.0572e-05, -9.5074e-06],
          [-7.3028e-06, -1.5253e-05, -6.3280e-06],
          [ 4.7218e-06,  1.7393e-05, -1.3027e-05]]],


        [[[-1.2691e-05,  7.3891e-06, -1.7559e-05],
          [-1.4646e-05, -1.4703e-05,  1.1600e-05],
          [-1.6428e-05,  1.5177e-05, -1.2773e-05]],

         [[ 8.7674e-06, -1.3057e-05,  6.7997e-06],
          [ 1.5901e-05,  1.1705e-06, -8.7353e-06],
          [-1.5358e-05,  1.1772e-05, -1.9022e-05]],

         [[ 1.0708e-05, -4.0483e-06, -2.6508e-06],
          [ 1.3169e-05, -1.8722e-05, -7.6319e-08],
          [-4.4102e-06,  7.0435e-07, -1.2364e-05]]],


        [[[ 7.0472e-06,  1.3290e-05,  7.8328e-06],
          [-1.0300e-05, -1.3749e-05,  1.8955e-05],
          [-1.5237e-05,  2.2940e-06, -2.9374e-06]],

         [[-3.9545e-06, -9.3986e-06,  1.3512e-05],
          [-3.1361e-06, -1.7957e-05,  1.9441e-06],
          [-1.7126e-06,  7.2833e-06,  3.7696e-06]],

         [[-1.4617e-05, -1.7853e-05,  1.4927e-05],
          [ 1.3676e-05, -1.5842e-05, -5.1975e-06],
          [-1.7287e-06, -5.9521e-06,  2.5619e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0058]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0119]], device='cuda:0')

Epoch: 28 | Batch_idx: 0 |  Loss: (0.9299) |  Loss2: (0.3735) | Acc: (80.00%) (103/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.9751) |  Loss2: (0.3735) | Acc: (78.00%) (1110/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.9766) |  Loss2: (0.3734) | Acc: (78.00%) (2110/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.9917) |  Loss2: (0.3734) | Acc: (77.00%) (3083/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (1.0034) |  Loss2: (0.3734) | Acc: (77.00%) (4064/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (1.0127) |  Loss2: (0.3733) | Acc: (77.00%) (5048/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (1.0112) |  Loss2: (0.3733) | Acc: (77.00%) (6059/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (1.0140) |  Loss2: (0.3733) | Acc: (77.00%) (7039/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (1.0208) |  Loss2: (0.3732) | Acc: (77.00%) (8007/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (1.0213) |  Loss2: (0.3732) | Acc: (77.00%) (8982/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (1.0172) |  Loss2: (0.3731) | Acc: (77.00%) (9983/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (1.0149) |  Loss2: (0.3731) | Acc: (77.00%) (10995/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (1.0137) |  Loss2: (0.3731) | Acc: (77.00%) (11985/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (1.0135) |  Loss2: (0.3730) | Acc: (77.00%) (12971/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (1.0102) |  Loss2: (0.3730) | Acc: (77.00%) (13999/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (1.0086) |  Loss2: (0.3729) | Acc: (77.00%) (15008/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (1.0093) |  Loss2: (0.3729) | Acc: (77.00%) (15995/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (1.0085) |  Loss2: (0.3729) | Acc: (77.00%) (17007/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (1.0064) |  Loss2: (0.3728) | Acc: (77.00%) (18014/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (1.0068) |  Loss2: (0.3728) | Acc: (77.00%) (18995/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (1.0067) |  Loss2: (0.3727) | Acc: (77.00%) (19994/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (1.0069) |  Loss2: (0.3727) | Acc: (77.00%) (20997/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (1.0058) |  Loss2: (0.3727) | Acc: (77.00%) (22010/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (1.0059) |  Loss2: (0.3726) | Acc: (77.00%) (23014/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (1.0079) |  Loss2: (0.3726) | Acc: (77.00%) (23984/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (1.0102) |  Loss2: (0.3725) | Acc: (77.00%) (24965/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (1.0104) |  Loss2: (0.3725) | Acc: (77.00%) (25951/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (1.0098) |  Loss2: (0.3725) | Acc: (77.00%) (26962/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (1.0103) |  Loss2: (0.3724) | Acc: (77.00%) (27970/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (1.0115) |  Loss2: (0.3724) | Acc: (77.00%) (28956/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (1.0127) |  Loss2: (0.3723) | Acc: (77.00%) (29932/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (1.0128) |  Loss2: (0.3723) | Acc: (77.00%) (30915/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (1.0118) |  Loss2: (0.3723) | Acc: (77.00%) (31925/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (1.0117) |  Loss2: (0.3722) | Acc: (77.00%) (32917/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (1.0130) |  Loss2: (0.3722) | Acc: (77.00%) (33882/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (1.0132) |  Loss2: (0.3722) | Acc: (77.00%) (34881/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (1.0132) |  Loss2: (0.3721) | Acc: (77.00%) (35880/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (1.0124) |  Loss2: (0.3721) | Acc: (77.00%) (36893/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (1.0112) |  Loss2: (0.3720) | Acc: (77.00%) (37891/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (1.0119) |  Loss2: (0.3720) | Acc: (77.00%) (38833/50000)
# TEST : Loss: (0.6713) | Acc: (77.00%) (7706/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1488,  0.0667,  0.0543],
          [ 0.0908, -0.1940, -0.0932],
          [ 0.1779, -0.1409,  0.0211]],

         [[ 0.0174, -0.1098,  0.2248],
          [ 0.0275, -0.0146,  0.0489],
          [ 0.0363, -0.0174, -0.0414]],

         [[ 0.0329, -0.0779,  0.1166],
          [ 0.1252, -0.1221,  0.1249],
          [-0.0884,  0.0396, -0.1918]]],


        [[[ 0.1167, -0.0735, -0.0005],
          [ 0.1460,  0.1769,  0.1744],
          [ 0.0328, -0.0474,  0.1275]],

         [[-0.1665, -0.1900,  0.1571],
          [ 0.0214, -0.0561,  0.0921],
          [ 0.1383, -0.1902,  0.1406]],

         [[ 0.0964, -0.0495, -0.0174],
          [ 0.0061,  0.0318, -0.1270],
          [-0.0277, -0.1393, -0.0074]]],


        [[[-0.0577,  0.1013, -0.0376],
          [-0.1106,  0.0740,  0.1245],
          [ 0.1314,  0.1764,  0.0054]],

         [[ 0.1049,  0.1321,  0.1104],
          [ 0.0782, -0.0431, -0.0412],
          [-0.0380, -0.0313,  0.1421]],

         [[-0.0494,  0.1416,  0.0687],
          [-0.0421, -0.0513, -0.0654],
          [-0.0374, -0.0109,  0.0430]]],


        ...,


        [[[ 0.2153, -0.1227, -0.1196],
          [-0.0434, -0.0991, -0.1008],
          [-0.1096,  0.0142,  0.0491]],

         [[ 0.1688,  0.1884, -0.1233],
          [ 0.2152, -0.1364, -0.1170],
          [-0.0790, -0.0888, -0.0406]],

         [[ 0.1931,  0.1057, -0.0950],
          [-0.0730, -0.1525, -0.0633],
          [ 0.0472,  0.1739, -0.1302]]],


        [[[-0.1269,  0.0739, -0.1755],
          [-0.1464, -0.1470,  0.1160],
          [-0.1642,  0.1517, -0.1277]],

         [[ 0.0876, -0.1305,  0.0680],
          [ 0.1589,  0.0117, -0.0873],
          [-0.1535,  0.1177, -0.1901]],

         [[ 0.1070, -0.0405, -0.0265],
          [ 0.1316, -0.1871, -0.0008],
          [-0.0441,  0.0070, -0.1236]]],


        [[[ 0.0704,  0.1329,  0.0783],
          [-0.1030, -0.1374,  0.1895],
          [-0.1523,  0.0229, -0.0294]],

         [[-0.0395, -0.0939,  0.1351],
          [-0.0313, -0.1795,  0.0194],
          [-0.0171,  0.0728,  0.0377]],

         [[-0.1461, -0.1785,  0.1492],
          [ 0.1367, -0.1584, -0.0520],
          [-0.0173, -0.0595,  0.2561]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4881e-05,  6.6684e-06,  5.4298e-06],
          [ 9.0848e-06, -1.9403e-05, -9.3217e-06],
          [ 1.7790e-05, -1.4093e-05,  2.1110e-06]],

         [[ 1.7443e-06, -1.0979e-05,  2.2479e-05],
          [ 2.7513e-06, -1.4610e-06,  4.8877e-06],
          [ 3.6350e-06, -1.7354e-06, -4.1408e-06]],

         [[ 3.2920e-06, -7.7913e-06,  1.1659e-05],
          [ 1.2518e-05, -1.2212e-05,  1.2488e-05],
          [-8.8423e-06,  3.9574e-06, -1.9184e-05]]],


        [[[ 1.1671e-05, -7.3494e-06, -4.5716e-08],
          [ 1.4601e-05,  1.7694e-05,  1.7438e-05],
          [ 3.2790e-06, -4.7450e-06,  1.2755e-05]],

         [[-1.6646e-05, -1.9000e-05,  1.5711e-05],
          [ 2.1360e-06, -5.6067e-06,  9.2077e-06],
          [ 1.3833e-05, -1.9022e-05,  1.4059e-05]],

         [[ 9.6415e-06, -4.9459e-06, -1.7425e-06],
          [ 6.0981e-07,  3.1795e-06, -1.2699e-05],
          [-2.7734e-06, -1.3926e-05, -7.3509e-07]]],


        [[[-5.7706e-06,  1.0128e-05, -3.7615e-06],
          [-1.1055e-05,  7.3954e-06,  1.2448e-05],
          [ 1.3144e-05,  1.7643e-05,  5.3919e-07]],

         [[ 1.0494e-05,  1.3206e-05,  1.1037e-05],
          [ 7.8228e-06, -4.3129e-06, -4.1203e-06],
          [-3.8032e-06, -3.1332e-06,  1.4212e-05]],

         [[-4.9389e-06,  1.4163e-05,  6.8694e-06],
          [-4.2129e-06, -5.1255e-06, -6.5443e-06],
          [-3.7410e-06, -1.0895e-06,  4.3047e-06]]],


        ...,


        [[[ 2.1532e-05, -1.2268e-05, -1.1958e-05],
          [-4.3350e-06, -9.9137e-06, -1.0084e-05],
          [-1.0961e-05,  1.4187e-06,  4.9085e-06]],

         [[ 1.6876e-05,  1.8837e-05, -1.2327e-05],
          [ 2.1520e-05, -1.3644e-05, -1.1702e-05],
          [-7.8960e-06, -8.8802e-06, -4.0584e-06]],

         [[ 1.9308e-05,  1.0568e-05, -9.5036e-06],
          [-7.2999e-06, -1.5247e-05, -6.3257e-06],
          [ 4.7199e-06,  1.7386e-05, -1.3022e-05]]],


        [[[-1.2686e-05,  7.3862e-06, -1.7552e-05],
          [-1.4640e-05, -1.4697e-05,  1.1595e-05],
          [-1.6422e-05,  1.5171e-05, -1.2768e-05]],

         [[ 8.7639e-06, -1.3052e-05,  6.7971e-06],
          [ 1.5895e-05,  1.1700e-06, -8.7318e-06],
          [-1.5352e-05,  1.1767e-05, -1.9014e-05]],

         [[ 1.0704e-05, -4.0467e-06, -2.6498e-06],
          [ 1.3164e-05, -1.8714e-05, -7.6289e-08],
          [-4.4085e-06,  7.0408e-07, -1.2360e-05]]],


        [[[ 7.0445e-06,  1.3285e-05,  7.8296e-06],
          [-1.0296e-05, -1.3744e-05,  1.8947e-05],
          [-1.5232e-05,  2.2931e-06, -2.9362e-06]],

         [[-3.9528e-06, -9.3948e-06,  1.3507e-05],
          [-3.1350e-06, -1.7950e-05,  1.9434e-06],
          [-1.7119e-06,  7.2804e-06,  3.7682e-06]],

         [[-1.4611e-05, -1.7846e-05,  1.4921e-05],
          [ 1.3671e-05, -1.5835e-05, -5.1954e-06],
          [-1.7281e-06, -5.9497e-06,  2.5609e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0112]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0873]], device='cuda:0')

Epoch: 29 | Batch_idx: 0 |  Loss: (0.9898) |  Loss2: (0.3705) | Acc: (81.00%) (104/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.9918) |  Loss2: (0.3704) | Acc: (77.00%) (1097/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (1.0121) |  Loss2: (0.3704) | Acc: (77.00%) (2093/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.9961) |  Loss2: (0.3703) | Acc: (78.00%) (3108/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (1.0080) |  Loss2: (0.3703) | Acc: (78.00%) (4097/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (1.0029) |  Loss2: (0.3703) | Acc: (78.00%) (5104/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (1.0102) |  Loss2: (0.3702) | Acc: (78.00%) (6103/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (1.0110) |  Loss2: (0.3702) | Acc: (78.00%) (7095/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (1.0113) |  Loss2: (0.3702) | Acc: (77.00%) (8083/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (1.0123) |  Loss2: (0.3701) | Acc: (77.00%) (9066/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (1.0109) |  Loss2: (0.3701) | Acc: (77.00%) (10079/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (1.0082) |  Loss2: (0.3700) | Acc: (78.00%) (11085/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (1.0099) |  Loss2: (0.3700) | Acc: (77.00%) (12068/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (1.0087) |  Loss2: (0.3700) | Acc: (77.00%) (13078/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (1.0081) |  Loss2: (0.3699) | Acc: (77.00%) (14060/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (1.0077) |  Loss2: (0.3699) | Acc: (77.00%) (15060/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (1.0073) |  Loss2: (0.3699) | Acc: (77.00%) (16056/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (1.0056) |  Loss2: (0.3698) | Acc: (77.00%) (17065/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (1.0051) |  Loss2: (0.3698) | Acc: (77.00%) (18059/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (1.0046) |  Loss2: (0.3697) | Acc: (77.00%) (19066/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (1.0049) |  Loss2: (0.3697) | Acc: (78.00%) (20070/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (1.0035) |  Loss2: (0.3697) | Acc: (78.00%) (21078/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (1.0040) |  Loss2: (0.3696) | Acc: (78.00%) (22076/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (1.0043) |  Loss2: (0.3696) | Acc: (78.00%) (23065/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (1.0041) |  Loss2: (0.3695) | Acc: (78.00%) (24070/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (1.0039) |  Loss2: (0.3695) | Acc: (78.00%) (25062/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (1.0035) |  Loss2: (0.3695) | Acc: (78.00%) (26061/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (1.0034) |  Loss2: (0.3694) | Acc: (77.00%) (27056/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (1.0032) |  Loss2: (0.3694) | Acc: (77.00%) (28049/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (1.0036) |  Loss2: (0.3694) | Acc: (77.00%) (29034/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (1.0033) |  Loss2: (0.3693) | Acc: (77.00%) (30039/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (1.0034) |  Loss2: (0.3693) | Acc: (77.00%) (31048/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (1.0028) |  Loss2: (0.3693) | Acc: (77.00%) (32046/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (1.0021) |  Loss2: (0.3692) | Acc: (78.00%) (33050/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (1.0010) |  Loss2: (0.3692) | Acc: (78.00%) (34054/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (1.0002) |  Loss2: (0.3691) | Acc: (78.00%) (35065/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (1.0000) |  Loss2: (0.3691) | Acc: (78.00%) (36063/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (1.0001) |  Loss2: (0.3691) | Acc: (78.00%) (37046/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (1.0005) |  Loss2: (0.3690) | Acc: (78.00%) (38045/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (1.0002) |  Loss2: (0.3690) | Acc: (78.00%) (39017/50000)
# TEST : Loss: (0.6660) | Acc: (77.00%) (7720/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1487,  0.0667,  0.0543],
          [ 0.0908, -0.1940, -0.0932],
          [ 0.1778, -0.1409,  0.0211]],

         [[ 0.0174, -0.1098,  0.2247],
          [ 0.0275, -0.0146,  0.0489],
          [ 0.0363, -0.0173, -0.0414]],

         [[ 0.0329, -0.0779,  0.1165],
          [ 0.1251, -0.1221,  0.1248],
          [-0.0884,  0.0396, -0.1918]]],


        [[[ 0.1167, -0.0735, -0.0005],
          [ 0.1460,  0.1769,  0.1743],
          [ 0.0328, -0.0474,  0.1275]],

         [[-0.1664, -0.1899,  0.1570],
          [ 0.0214, -0.0560,  0.0920],
          [ 0.1383, -0.1901,  0.1405]],

         [[ 0.0964, -0.0494, -0.0174],
          [ 0.0061,  0.0318, -0.1269],
          [-0.0277, -0.1392, -0.0073]]],


        [[[-0.0577,  0.1012, -0.0376],
          [-0.1105,  0.0739,  0.1244],
          [ 0.1314,  0.1764,  0.0054]],

         [[ 0.1049,  0.1320,  0.1103],
          [ 0.0782, -0.0431, -0.0412],
          [-0.0380, -0.0313,  0.1421]],

         [[-0.0494,  0.1416,  0.0687],
          [-0.0421, -0.0512, -0.0654],
          [-0.0374, -0.0109,  0.0430]]],


        ...,


        [[[ 0.2152, -0.1226, -0.1195],
          [-0.0433, -0.0991, -0.1008],
          [-0.1096,  0.0142,  0.0491]],

         [[ 0.1687,  0.1883, -0.1232],
          [ 0.2151, -0.1364, -0.1170],
          [-0.0789, -0.0888, -0.0406]],

         [[ 0.1930,  0.1056, -0.0950],
          [-0.0730, -0.1524, -0.0632],
          [ 0.0472,  0.1738, -0.1302]]],


        [[[-0.1268,  0.0738, -0.1755],
          [-0.1463, -0.1469,  0.1159],
          [-0.1642,  0.1517, -0.1276]],

         [[ 0.0876, -0.1305,  0.0679],
          [ 0.1589,  0.0117, -0.0873],
          [-0.1535,  0.1176, -0.1901]],

         [[ 0.1070, -0.0405, -0.0265],
          [ 0.1316, -0.1871, -0.0008],
          [-0.0441,  0.0070, -0.1235]]],


        [[[ 0.0704,  0.1328,  0.0783],
          [-0.1029, -0.1374,  0.1894],
          [-0.1523,  0.0229, -0.0294]],

         [[-0.0395, -0.0939,  0.1350],
          [-0.0313, -0.1794,  0.0194],
          [-0.0171,  0.0728,  0.0377]],

         [[-0.1461, -0.1784,  0.1492],
          [ 0.1367, -0.1583, -0.0519],
          [-0.0173, -0.0595,  0.2560]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4875e-05,  6.6658e-06,  5.4276e-06],
          [ 9.0813e-06, -1.9395e-05, -9.3179e-06],
          [ 1.7783e-05, -1.4088e-05,  2.1102e-06]],

         [[ 1.7436e-06, -1.0975e-05,  2.2471e-05],
          [ 2.7502e-06, -1.4604e-06,  4.8858e-06],
          [ 3.6335e-06, -1.7347e-06, -4.1392e-06]],

         [[ 3.2907e-06, -7.7884e-06,  1.1654e-05],
          [ 1.2513e-05, -1.2207e-05,  1.2483e-05],
          [-8.8388e-06,  3.9558e-06, -1.9177e-05]]],


        [[[ 1.1666e-05, -7.3465e-06, -4.5698e-08],
          [ 1.4596e-05,  1.7687e-05,  1.7431e-05],
          [ 3.2777e-06, -4.7431e-06,  1.2750e-05]],

         [[-1.6639e-05, -1.8993e-05,  1.5705e-05],
          [ 2.1352e-06, -5.6045e-06,  9.2042e-06],
          [ 1.3828e-05, -1.9015e-05,  1.4054e-05]],

         [[ 9.6377e-06, -4.9440e-06, -1.7418e-06],
          [ 6.0958e-07,  3.1782e-06, -1.2693e-05],
          [-2.7723e-06, -1.3920e-05, -7.3480e-07]]],


        [[[-5.7684e-06,  1.0124e-05, -3.7600e-06],
          [-1.1051e-05,  7.3925e-06,  1.2443e-05],
          [ 1.3139e-05,  1.7636e-05,  5.3897e-07]],

         [[ 1.0490e-05,  1.3201e-05,  1.1033e-05],
          [ 7.8199e-06, -4.3111e-06, -4.1187e-06],
          [-3.8018e-06, -3.1320e-06,  1.4206e-05]],

         [[-4.9370e-06,  1.4157e-05,  6.8667e-06],
          [-4.2113e-06, -5.1235e-06, -6.5416e-06],
          [-3.7396e-06, -1.0891e-06,  4.3029e-06]]],


        ...,


        [[[ 2.1524e-05, -1.2263e-05, -1.1953e-05],
          [-4.3333e-06, -9.9099e-06, -1.0080e-05],
          [-1.0957e-05,  1.4181e-06,  4.9066e-06]],

         [[ 1.6870e-05,  1.8830e-05, -1.2322e-05],
          [ 2.1511e-05, -1.3639e-05, -1.1697e-05],
          [-7.8928e-06, -8.8767e-06, -4.0568e-06]],

         [[ 1.9301e-05,  1.0564e-05, -9.4998e-06],
          [-7.2969e-06, -1.5241e-05, -6.3233e-06],
          [ 4.7180e-06,  1.7379e-05, -1.3017e-05]]],


        [[[-1.2680e-05,  7.3833e-06, -1.7545e-05],
          [-1.4635e-05, -1.4691e-05,  1.1591e-05],
          [-1.6415e-05,  1.5165e-05, -1.2763e-05]],

         [[ 8.7604e-06, -1.3046e-05,  6.7945e-06],
          [ 1.5888e-05,  1.1695e-06, -8.7283e-06],
          [-1.5346e-05,  1.1762e-05, -1.9007e-05]],

         [[ 1.0700e-05, -4.0451e-06, -2.6488e-06],
          [ 1.3159e-05, -1.8707e-05, -7.6260e-08],
          [-4.4068e-06,  7.0380e-07, -1.2355e-05]]],


        [[[ 7.0419e-06,  1.3280e-05,  7.8264e-06],
          [-1.0292e-05, -1.3738e-05,  1.8939e-05],
          [-1.5226e-05,  2.2922e-06, -2.9351e-06]],

         [[-3.9512e-06, -9.3910e-06,  1.3502e-05],
          [-3.1338e-06, -1.7943e-05,  1.9427e-06],
          [-1.7113e-06,  7.2774e-06,  3.7667e-06]],

         [[-1.4605e-05, -1.7839e-05,  1.4916e-05],
          [ 1.3666e-05, -1.5829e-05, -5.1934e-06],
          [-1.7274e-06, -5.9474e-06,  2.5598e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0186]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0867]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (78.00%) (1111/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6073) |  Loss2: (0.0000) | Acc: (78.00%) (2102/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (3100/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (77.00%) (4091/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (77.00%) (5071/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (77.00%) (6069/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6282) |  Loss2: (0.0000) | Acc: (78.00%) (7089/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6368) |  Loss2: (0.0000) | Acc: (77.00%) (8046/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (9026/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (10015/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (10987/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (77.00%) (12020/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6333) |  Loss2: (0.0000) | Acc: (77.00%) (13028/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6332) |  Loss2: (0.0000) | Acc: (77.00%) (14026/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6334) |  Loss2: (0.0000) | Acc: (77.00%) (15019/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6334) |  Loss2: (0.0000) | Acc: (77.00%) (16015/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (17018/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6372) |  Loss2: (0.0000) | Acc: (77.00%) (18000/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6370) |  Loss2: (0.0000) | Acc: (77.00%) (19003/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6374) |  Loss2: (0.0000) | Acc: (77.00%) (20008/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (77.00%) (21025/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6346) |  Loss2: (0.0000) | Acc: (77.00%) (22027/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6340) |  Loss2: (0.0000) | Acc: (77.00%) (23039/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (77.00%) (24029/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (77.00%) (25017/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (26013/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6341) |  Loss2: (0.0000) | Acc: (77.00%) (27017/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6328) |  Loss2: (0.0000) | Acc: (77.00%) (28022/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6325) |  Loss2: (0.0000) | Acc: (77.00%) (29039/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (77.00%) (30035/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6332) |  Loss2: (0.0000) | Acc: (77.00%) (31026/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6332) |  Loss2: (0.0000) | Acc: (77.00%) (32029/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6333) |  Loss2: (0.0000) | Acc: (77.00%) (33022/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (34021/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6331) |  Loss2: (0.0000) | Acc: (77.00%) (35015/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6323) |  Loss2: (0.0000) | Acc: (77.00%) (36021/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6317) |  Loss2: (0.0000) | Acc: (77.00%) (37038/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6306) |  Loss2: (0.0000) | Acc: (78.00%) (38048/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6302) |  Loss2: (0.0000) | Acc: (78.00%) (39006/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_030.pth.tar'
# TEST : Loss: (0.6911) | Acc: (76.00%) (7647/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1479,  0.0624,  0.0578],
          [ 0.0925, -0.2023, -0.0905],
          [ 0.1889, -0.1327,  0.0262]],

         [[ 0.0149, -0.1159,  0.2233],
          [ 0.0255, -0.0258,  0.0461],
          [ 0.0414, -0.0151, -0.0439]],

         [[ 0.0299, -0.0837,  0.1164],
          [ 0.1215, -0.1323,  0.1253],
          [-0.0865,  0.0409, -0.1905]]],


        [[[ 0.1150, -0.0745, -0.0025],
          [ 0.1456,  0.1767,  0.1741],
          [ 0.0322, -0.0479,  0.1259]],

         [[-0.1688, -0.1927,  0.1537],
          [ 0.0203, -0.0578,  0.0910],
          [ 0.1373, -0.1916,  0.1385]],

         [[ 0.0941, -0.0518, -0.0195],
          [ 0.0045,  0.0295, -0.1278],
          [-0.0291, -0.1412, -0.0091]]],


        [[[-0.0590,  0.1005, -0.0380],
          [-0.1120,  0.0730,  0.1236],
          [ 0.1299,  0.1757,  0.0049]],

         [[ 0.1050,  0.1327,  0.1110],
          [ 0.0780, -0.0427, -0.0409],
          [-0.0383, -0.0309,  0.1423]],

         [[-0.0481,  0.1432,  0.0703],
          [-0.0409, -0.0497, -0.0642],
          [-0.0363, -0.0094,  0.0441]]],


        ...,


        [[[ 0.2180, -0.1208, -0.1178],
          [-0.0409, -0.0986, -0.0999],
          [-0.1059,  0.0167,  0.0511]],

         [[ 0.1686,  0.1881, -0.1234],
          [ 0.2158, -0.1372, -0.1179],
          [-0.0758, -0.0871, -0.0402]],

         [[ 0.1918,  0.1041, -0.0963],
          [-0.0725, -0.1537, -0.0646],
          [ 0.0502,  0.1753, -0.1302]]],


        [[[-0.1263,  0.0746, -0.1731],
          [-0.1473, -0.1476,  0.1166],
          [-0.1652,  0.1508, -0.1275]],

         [[ 0.0889, -0.1289,  0.0706],
          [ 0.1588,  0.0115, -0.0862],
          [-0.1535,  0.1173, -0.1897]],

         [[ 0.1089, -0.0384, -0.0235],
          [ 0.1323, -0.1866,  0.0004],
          [-0.0437,  0.0070, -0.1235]]],


        [[[ 0.0684,  0.1308,  0.0810],
          [-0.1015, -0.1400,  0.1914],
          [-0.1525,  0.0187, -0.0318]],

         [[-0.0375, -0.0920,  0.1413],
          [-0.0259, -0.1771,  0.0265],
          [-0.0112,  0.0757,  0.0411]],

         [[-0.1451, -0.1781,  0.1525],
          [ 0.1392, -0.1581, -0.0476],
          [-0.0143, -0.0578,  0.2579]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0242,  0.0443,  0.0895],
          [ 0.0176,  0.0556,  0.0906],
          [ 0.0490,  0.0848,  0.1145]],

         [[ 0.0385,  0.0465,  0.0915],
          [ 0.0352,  0.0607,  0.0955],
          [ 0.0773,  0.0970,  0.1187]],

         [[-0.0239, -0.0079,  0.0396],
          [-0.0266, -0.0022,  0.0330],
          [ 0.0123,  0.0293,  0.0505]]],


        [[[-0.0347, -0.0172, -0.0106],
          [-0.0320, -0.0158, -0.0120],
          [-0.0193, -0.0084, -0.0024]],

         [[-0.0279, -0.0150, -0.0109],
          [-0.0211, -0.0102, -0.0090],
          [-0.0097, -0.0022, -0.0002]],

         [[-0.0495, -0.0406, -0.0407],
          [-0.0472, -0.0386, -0.0397],
          [-0.0409, -0.0351, -0.0356]]],


        [[[ 0.0043,  0.0022, -0.0002],
          [ 0.0014, -0.0011, -0.0027],
          [-0.0014, -0.0036, -0.0055]],

         [[ 0.0059,  0.0044,  0.0017],
          [ 0.0027,  0.0001, -0.0017],
          [ 0.0003, -0.0026, -0.0046]],

         [[ 0.0077,  0.0068,  0.0043],
          [ 0.0049,  0.0018, -0.0002],
          [ 0.0029, -0.0011, -0.0033]]],


        ...,


        [[[-0.0354, -0.0312, -0.0306],
          [-0.0233, -0.0280, -0.0298],
          [-0.0149, -0.0203, -0.0208]],

         [[-0.0495, -0.0444, -0.0402],
          [-0.0382, -0.0401, -0.0374],
          [-0.0248, -0.0284, -0.0262]],

         [[-0.0465, -0.0408, -0.0343],
          [-0.0370, -0.0379, -0.0342],
          [-0.0228, -0.0263, -0.0232]]],


        [[[-0.0168, -0.0061, -0.0075],
          [-0.0122, -0.0017, -0.0001],
          [-0.0063,  0.0023,  0.0051]],

         [[-0.0185, -0.0075, -0.0066],
          [-0.0134, -0.0048, -0.0026],
          [-0.0044,  0.0009,  0.0023]],

         [[-0.0049,  0.0028,  0.0041],
          [-0.0008,  0.0034,  0.0057],
          [ 0.0070,  0.0084,  0.0099]]],


        [[[-0.0244, -0.0001,  0.0026],
          [ 0.0099,  0.0054, -0.0080],
          [ 0.0037, -0.0213, -0.0307]],

         [[-0.0134,  0.0143,  0.0246],
          [ 0.0128,  0.0154,  0.0141],
          [ 0.0065, -0.0101, -0.0093]],

         [[-0.0485, -0.0037,  0.0159],
          [-0.0201, -0.0033,  0.0102],
          [-0.0235, -0.0235, -0.0118]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0192]], device='cuda:0')

percentage_weight_grad tensor([[1.9241e-06]], device='cuda:0')

Epoch: 31 | Batch_idx: 0 |  Loss: (0.5704) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.6072) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (2141/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5992) |  Loss2: (0.0000) | Acc: (79.00%) (3137/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5997) |  Loss2: (0.0000) | Acc: (79.00%) (4152/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (78.00%) (5150/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5991) |  Loss2: (0.0000) | Acc: (78.00%) (6166/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5978) |  Loss2: (0.0000) | Acc: (79.00%) (7183/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5946) |  Loss2: (0.0000) | Acc: (79.00%) (8202/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5944) |  Loss2: (0.0000) | Acc: (79.00%) (9212/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5929) |  Loss2: (0.0000) | Acc: (79.00%) (10237/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5901) |  Loss2: (0.0000) | Acc: (79.00%) (11265/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5887) |  Loss2: (0.0000) | Acc: (79.00%) (12292/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5911) |  Loss2: (0.0000) | Acc: (79.00%) (13309/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5904) |  Loss2: (0.0000) | Acc: (79.00%) (14345/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5897) |  Loss2: (0.0000) | Acc: (79.00%) (15368/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5875) |  Loss2: (0.0000) | Acc: (79.00%) (16410/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5876) |  Loss2: (0.0000) | Acc: (79.00%) (17415/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5870) |  Loss2: (0.0000) | Acc: (79.00%) (18454/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5890) |  Loss2: (0.0000) | Acc: (79.00%) (19469/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5878) |  Loss2: (0.0000) | Acc: (79.00%) (20484/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5877) |  Loss2: (0.0000) | Acc: (79.00%) (21503/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5882) |  Loss2: (0.0000) | Acc: (79.00%) (22517/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5896) |  Loss2: (0.0000) | Acc: (79.00%) (23530/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5889) |  Loss2: (0.0000) | Acc: (79.00%) (24560/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5875) |  Loss2: (0.0000) | Acc: (79.00%) (25570/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5880) |  Loss2: (0.0000) | Acc: (79.00%) (26581/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5894) |  Loss2: (0.0000) | Acc: (79.00%) (27579/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5898) |  Loss2: (0.0000) | Acc: (79.00%) (28595/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5898) |  Loss2: (0.0000) | Acc: (79.00%) (29617/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5893) |  Loss2: (0.0000) | Acc: (79.00%) (30632/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5892) |  Loss2: (0.0000) | Acc: (79.00%) (31655/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5886) |  Loss2: (0.0000) | Acc: (79.00%) (32673/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5880) |  Loss2: (0.0000) | Acc: (79.00%) (33705/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5892) |  Loss2: (0.0000) | Acc: (79.00%) (34698/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5906) |  Loss2: (0.0000) | Acc: (79.00%) (35688/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5904) |  Loss2: (0.0000) | Acc: (79.00%) (36704/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5910) |  Loss2: (0.0000) | Acc: (79.00%) (37705/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5913) |  Loss2: (0.0000) | Acc: (79.00%) (38725/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5918) |  Loss2: (0.0000) | Acc: (79.00%) (39691/50000)
# TEST : Loss: (0.6183) | Acc: (78.00%) (7860/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1489,  0.0650,  0.0647],
          [ 0.0934, -0.2049, -0.0884],
          [ 0.1934, -0.1271,  0.0239]],

         [[ 0.0139, -0.1139,  0.2260],
          [ 0.0265, -0.0293,  0.0446],
          [ 0.0443, -0.0127, -0.0509]],

         [[ 0.0289, -0.0808,  0.1193],
          [ 0.1230, -0.1342,  0.1251],
          [-0.0836,  0.0431, -0.1956]]],


        [[[ 0.1136, -0.0749, -0.0026],
          [ 0.1451,  0.1759,  0.1745],
          [ 0.0294, -0.0507,  0.1242]],

         [[-0.1690, -0.1927,  0.1545],
          [ 0.0207, -0.0584,  0.0919],
          [ 0.1351, -0.1943,  0.1375]],

         [[ 0.0950, -0.0505, -0.0168],
          [ 0.0053,  0.0291, -0.1261],
          [-0.0302, -0.1439, -0.0099]]],


        [[[-0.0604,  0.0998, -0.0385],
          [-0.1133,  0.0724,  0.1230],
          [ 0.1285,  0.1753,  0.0047]],

         [[ 0.1045,  0.1330,  0.1114],
          [ 0.0776, -0.0422, -0.0405],
          [-0.0388, -0.0303,  0.1428]],

         [[-0.0484,  0.1435,  0.0706],
          [-0.0411, -0.0493, -0.0641],
          [-0.0367, -0.0090,  0.0443]]],


        ...,


        [[[ 0.2195, -0.1190, -0.1160],
          [-0.0422, -0.0985, -0.0993],
          [-0.1083,  0.0157,  0.0506]],

         [[ 0.1693,  0.1895, -0.1216],
          [ 0.2145, -0.1368, -0.1172],
          [-0.0779, -0.0881, -0.0409]],

         [[ 0.1915,  0.1043, -0.0957],
          [-0.0744, -0.1540, -0.0650],
          [ 0.0475,  0.1737, -0.1315]]],


        [[[-0.1257,  0.0749, -0.1732],
          [-0.1473, -0.1473,  0.1172],
          [-0.1654,  0.1507, -0.1275]],

         [[ 0.0875, -0.1306,  0.0687],
          [ 0.1574,  0.0105, -0.0865],
          [-0.1547,  0.1164, -0.1904]],

         [[ 0.1080, -0.0394, -0.0245],
          [ 0.1318, -0.1867,  0.0010],
          [-0.0440,  0.0069, -0.1234]]],


        [[[ 0.0704,  0.1335,  0.0844],
          [-0.1017, -0.1405,  0.1944],
          [-0.1563,  0.0132, -0.0355]],

         [[-0.0345, -0.0885,  0.1452],
          [-0.0255, -0.1760,  0.0311],
          [-0.0130,  0.0734,  0.0399]],

         [[-0.1446, -0.1775,  0.1532],
          [ 0.1383, -0.1583, -0.0445],
          [-0.0155, -0.0585,  0.2581]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0885,  0.1115,  0.1741],
          [ 0.1350,  0.1458,  0.1937],
          [ 0.1679,  0.1804,  0.1986]],

         [[ 0.0404,  0.0614,  0.1268],
          [ 0.0914,  0.1006,  0.1503],
          [ 0.1281,  0.1349,  0.1533]],

         [[ 0.0385,  0.0443,  0.0975],
          [ 0.0843,  0.0872,  0.1377],
          [ 0.1258,  0.1318,  0.1567]]],


        [[[ 0.0130,  0.0254,  0.0003],
          [-0.0250, -0.0047, -0.0266],
          [-0.0181,  0.0011, -0.0174]],

         [[-0.0296, -0.0151, -0.0424],
          [-0.0660, -0.0433, -0.0681],
          [-0.0587, -0.0379, -0.0608]],

         [[-0.0756, -0.0591, -0.0854],
          [-0.1086, -0.0881, -0.1103],
          [-0.1047, -0.0870, -0.1079]]],


        [[[ 0.0005, -0.0005, -0.0016],
          [ 0.0016,  0.0018, -0.0001],
          [ 0.0027,  0.0037,  0.0008]],

         [[ 0.0017,  0.0010, -0.0004],
          [ 0.0040,  0.0044,  0.0019],
          [ 0.0053,  0.0057,  0.0022]],

         [[ 0.0028,  0.0011, -0.0011],
          [ 0.0069,  0.0066,  0.0037],
          [ 0.0098,  0.0090,  0.0047]]],


        ...,


        [[[ 0.0314,  0.0203,  0.0217],
          [ 0.0351,  0.0166,  0.0219],
          [ 0.0252,  0.0121,  0.0099]],

         [[ 0.0381,  0.0261,  0.0270],
          [ 0.0412,  0.0243,  0.0300],
          [ 0.0365,  0.0246,  0.0219]],

         [[ 0.0289,  0.0178,  0.0181],
          [ 0.0349,  0.0199,  0.0257],
          [ 0.0363,  0.0247,  0.0243]]],


        [[[-0.0180, -0.0143, -0.0191],
          [-0.0011,  0.0004, -0.0049],
          [ 0.0057,  0.0071,  0.0053]],

         [[-0.0257, -0.0227, -0.0264],
          [-0.0080, -0.0063, -0.0117],
          [-0.0026,  0.0000, -0.0019]],

         [[-0.0137, -0.0134, -0.0166],
          [ 0.0038,  0.0046,  0.0006],
          [ 0.0092,  0.0128,  0.0130]]],


        [[[ 0.1305,  0.1314,  0.1222],
          [ 0.1195,  0.1180,  0.1192],
          [ 0.0851,  0.0949,  0.1100]],

         [[ 0.0785,  0.0821,  0.0662],
          [ 0.0629,  0.0653,  0.0570],
          [ 0.0315,  0.0395,  0.0450]],

         [[-0.0219, -0.0100, -0.0199],
          [-0.0454, -0.0360, -0.0418],
          [-0.0779, -0.0713, -0.0664]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0192]], device='cuda:0')

percentage_weight_grad tensor([[1.9234e-06]], device='cuda:0')

Epoch: 32 | Batch_idx: 0 |  Loss: (0.5978) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.6026) |  Loss2: (0.0000) | Acc: (80.00%) (1134/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5968) |  Loss2: (0.0000) | Acc: (79.00%) (2143/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (79.00%) (3152/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5812) |  Loss2: (0.0000) | Acc: (80.00%) (4204/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5771) |  Loss2: (0.0000) | Acc: (80.00%) (5229/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5752) |  Loss2: (0.0000) | Acc: (80.00%) (6264/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5754) |  Loss2: (0.0000) | Acc: (80.00%) (7296/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5761) |  Loss2: (0.0000) | Acc: (80.00%) (8315/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5720) |  Loss2: (0.0000) | Acc: (80.00%) (9350/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (80.00%) (10386/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5738) |  Loss2: (0.0000) | Acc: (80.00%) (11391/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5750) |  Loss2: (0.0000) | Acc: (80.00%) (12398/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5727) |  Loss2: (0.0000) | Acc: (80.00%) (13447/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5703) |  Loss2: (0.0000) | Acc: (80.00%) (14496/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5722) |  Loss2: (0.0000) | Acc: (80.00%) (15500/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5708) |  Loss2: (0.0000) | Acc: (80.00%) (16532/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5703) |  Loss2: (0.0000) | Acc: (80.00%) (17568/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5704) |  Loss2: (0.0000) | Acc: (80.00%) (18585/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (80.00%) (19617/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5704) |  Loss2: (0.0000) | Acc: (80.00%) (20641/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (21687/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5694) |  Loss2: (0.0000) | Acc: (80.00%) (22703/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (23720/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5703) |  Loss2: (0.0000) | Acc: (80.00%) (24733/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (80.00%) (25748/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5707) |  Loss2: (0.0000) | Acc: (80.00%) (26779/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5709) |  Loss2: (0.0000) | Acc: (80.00%) (27793/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5698) |  Loss2: (0.0000) | Acc: (80.00%) (28840/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5683) |  Loss2: (0.0000) | Acc: (80.00%) (29894/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5693) |  Loss2: (0.0000) | Acc: (80.00%) (30904/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (31916/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (32959/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5697) |  Loss2: (0.0000) | Acc: (80.00%) (33991/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (35012/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5687) |  Loss2: (0.0000) | Acc: (80.00%) (36068/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5674) |  Loss2: (0.0000) | Acc: (80.00%) (37128/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5675) |  Loss2: (0.0000) | Acc: (80.00%) (38160/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5663) |  Loss2: (0.0000) | Acc: (80.00%) (39194/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5673) |  Loss2: (0.0000) | Acc: (80.00%) (40161/50000)
# TEST : Loss: (0.6490) | Acc: (77.00%) (7792/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1501,  0.0682,  0.0744],
          [ 0.0903, -0.2094, -0.0838],
          [ 0.1984, -0.1211,  0.0299]],

         [[ 0.0116, -0.1108,  0.2340],
          [ 0.0229, -0.0341,  0.0474],
          [ 0.0466, -0.0096, -0.0475]],

         [[ 0.0253, -0.0779,  0.1297],
          [ 0.1170, -0.1394,  0.1304],
          [-0.0860,  0.0436, -0.1902]]],


        [[[ 0.1109, -0.0754, -0.0025],
          [ 0.1452,  0.1770,  0.1760],
          [ 0.0301, -0.0493,  0.1249]],

         [[-0.1692, -0.1922,  0.1553],
          [ 0.0221, -0.0571,  0.0940],
          [ 0.1362, -0.1933,  0.1390]],

         [[ 0.0954, -0.0496, -0.0155],
          [ 0.0073,  0.0304, -0.1236],
          [-0.0280, -0.1423, -0.0080]]],


        [[[-0.0608,  0.0995, -0.0386],
          [-0.1140,  0.0722,  0.1232],
          [ 0.1277,  0.1751,  0.0051]],

         [[ 0.1042,  0.1331,  0.1115],
          [ 0.0772, -0.0419, -0.0401],
          [-0.0393, -0.0302,  0.1432]],

         [[-0.0488,  0.1433,  0.0706],
          [-0.0414, -0.0492, -0.0637],
          [-0.0369, -0.0088,  0.0447]]],


        ...,


        [[[ 0.2204, -0.1191, -0.1148],
          [-0.0425, -0.0991, -0.0987],
          [-0.1068,  0.0160,  0.0504]],

         [[ 0.1711,  0.1908, -0.1189],
          [ 0.2155, -0.1361, -0.1156],
          [-0.0748, -0.0865, -0.0401]],

         [[ 0.1934,  0.1057, -0.0936],
          [-0.0732, -0.1530, -0.0638],
          [ 0.0503,  0.1750, -0.1313]]],


        [[[-0.1253,  0.0753, -0.1717],
          [-0.1473, -0.1475,  0.1175],
          [-0.1652,  0.1508, -0.1283]],

         [[ 0.0876, -0.1302,  0.0703],
          [ 0.1573,  0.0105, -0.0857],
          [-0.1545,  0.1170, -0.1905]],

         [[ 0.1087, -0.0385, -0.0225],
          [ 0.1322, -0.1862,  0.0019],
          [-0.0435,  0.0077, -0.1236]]],


        [[[ 0.0635,  0.1283,  0.0814],
          [-0.1030, -0.1419,  0.1958],
          [-0.1590,  0.0120, -0.0360]],

         [[-0.0414, -0.0957,  0.1391],
          [-0.0270, -0.1786,  0.0315],
          [-0.0145,  0.0736,  0.0405]],

         [[-0.1508, -0.1852,  0.1469],
          [ 0.1358, -0.1616, -0.0439],
          [-0.0172, -0.0579,  0.2602]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0847, -0.0518, -0.0207],
          [-0.1310, -0.0850, -0.0609],
          [-0.1636, -0.1309, -0.1075]],

         [[-0.0206, -0.0068,  0.0013],
          [-0.0785, -0.0456, -0.0395],
          [-0.1199, -0.0946, -0.0762]],

         [[ 0.1547,  0.1659,  0.1821],
          [ 0.1088,  0.1390,  0.1480],
          [ 0.0783,  0.1008,  0.1162]]],


        [[[ 0.0127,  0.0333,  0.0122],
          [ 0.0271,  0.0449,  0.0245],
          [ 0.0487,  0.0600,  0.0330]],

         [[ 0.0007,  0.0244,  0.0080],
          [ 0.0148,  0.0361,  0.0194],
          [ 0.0348,  0.0502,  0.0270]],

         [[ 0.0170,  0.0386,  0.0246],
          [ 0.0306,  0.0485,  0.0346],
          [ 0.0457,  0.0576,  0.0381]]],


        [[[ 0.0006,  0.0016,  0.0034],
          [-0.0054, -0.0039, -0.0019],
          [-0.0067, -0.0039, -0.0019]],

         [[ 0.0012,  0.0009,  0.0014],
          [-0.0050, -0.0046, -0.0038],
          [-0.0072, -0.0045, -0.0034]],

         [[ 0.0001,  0.0006,  0.0027],
          [-0.0064, -0.0047, -0.0023],
          [-0.0092, -0.0053, -0.0024]]],


        ...,


        [[[-0.0349, -0.0389, -0.0144],
          [-0.0293, -0.0376, -0.0153],
          [-0.0405, -0.0434, -0.0299]],

         [[-0.0253, -0.0337, -0.0112],
          [-0.0206, -0.0313, -0.0117],
          [-0.0316, -0.0347, -0.0208]],

         [[-0.0355, -0.0449, -0.0230],
          [-0.0296, -0.0405, -0.0220],
          [-0.0383, -0.0394, -0.0256]]],


        [[[-0.0181, -0.0139, -0.0123],
          [-0.0114, -0.0097, -0.0055],
          [ 0.0027,  0.0023,  0.0079]],

         [[-0.0358, -0.0278, -0.0253],
          [-0.0305, -0.0249, -0.0196],
          [-0.0170, -0.0148, -0.0074]],

         [[-0.0214, -0.0120, -0.0092],
          [-0.0170, -0.0100, -0.0048],
          [-0.0070, -0.0027,  0.0042]]],


        [[[ 0.0773,  0.0966,  0.1145],
          [ 0.1099,  0.1045,  0.1217],
          [ 0.1042,  0.0882,  0.1033]],

         [[ 0.0759,  0.1027,  0.1222],
          [ 0.1042,  0.1054,  0.1236],
          [ 0.0977,  0.0873,  0.1028]],

         [[ 0.0560,  0.0792,  0.0938],
          [ 0.0779,  0.0772,  0.0925],
          [ 0.0708,  0.0594,  0.0737]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0192]], device='cuda:0')

percentage_weight_grad tensor([[1.9227e-06]], device='cuda:0')

Epoch: 33 | Batch_idx: 0 |  Loss: (0.7321) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (1095/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5896) |  Loss2: (0.0000) | Acc: (79.00%) (2144/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (3207/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5565) |  Loss2: (0.0000) | Acc: (80.00%) (4248/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (81.00%) (5304/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5423) |  Loss2: (0.0000) | Acc: (81.00%) (6343/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5364) |  Loss2: (0.0000) | Acc: (81.00%) (7408/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5352) |  Loss2: (0.0000) | Acc: (81.00%) (8453/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (9497/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (10529/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (11581/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5374) |  Loss2: (0.0000) | Acc: (81.00%) (12627/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (13678/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (14721/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5361) |  Loss2: (0.0000) | Acc: (81.00%) (15779/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (16823/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (17866/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (18885/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5391) |  Loss2: (0.0000) | Acc: (81.00%) (19949/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5401) |  Loss2: (0.0000) | Acc: (81.00%) (20993/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5415) |  Loss2: (0.0000) | Acc: (81.00%) (22021/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5407) |  Loss2: (0.0000) | Acc: (81.00%) (23069/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5423) |  Loss2: (0.0000) | Acc: (81.00%) (24104/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5412) |  Loss2: (0.0000) | Acc: (81.00%) (25161/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5417) |  Loss2: (0.0000) | Acc: (81.00%) (26186/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5423) |  Loss2: (0.0000) | Acc: (81.00%) (27218/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5421) |  Loss2: (0.0000) | Acc: (81.00%) (28268/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5410) |  Loss2: (0.0000) | Acc: (81.00%) (29332/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5407) |  Loss2: (0.0000) | Acc: (81.00%) (30386/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5415) |  Loss2: (0.0000) | Acc: (81.00%) (31405/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5419) |  Loss2: (0.0000) | Acc: (81.00%) (32445/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5412) |  Loss2: (0.0000) | Acc: (81.00%) (33476/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5406) |  Loss2: (0.0000) | Acc: (81.00%) (34533/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5395) |  Loss2: (0.0000) | Acc: (81.00%) (35594/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5397) |  Loss2: (0.0000) | Acc: (81.00%) (36637/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5385) |  Loss2: (0.0000) | Acc: (81.00%) (37682/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5389) |  Loss2: (0.0000) | Acc: (81.00%) (38713/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5409) |  Loss2: (0.0000) | Acc: (81.00%) (39731/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5421) |  Loss2: (0.0000) | Acc: (81.00%) (40716/50000)
# TEST : Loss: (0.6466) | Acc: (77.00%) (7766/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1515,  0.0628,  0.0711],
          [ 0.0888, -0.2189, -0.0892],
          [ 0.2010, -0.1216,  0.0259]],

         [[ 0.0082, -0.1160,  0.2287],
          [ 0.0201, -0.0442,  0.0398],
          [ 0.0447, -0.0143, -0.0564]],

         [[ 0.0217, -0.0806,  0.1286],
          [ 0.1139, -0.1450,  0.1297],
          [-0.0892,  0.0417, -0.1925]]],


        [[[ 0.1094, -0.0762, -0.0024],
          [ 0.1451,  0.1772,  0.1778],
          [ 0.0293, -0.0503,  0.1257]],

         [[-0.1678, -0.1911,  0.1563],
          [ 0.0247, -0.0555,  0.0965],
          [ 0.1370, -0.1935,  0.1402]],

         [[ 0.0980, -0.0476, -0.0128],
          [ 0.0113,  0.0330, -0.1196],
          [-0.0251, -0.1410, -0.0050]]],


        [[[-0.0611,  0.0993, -0.0386],
          [-0.1145,  0.0720,  0.1231],
          [ 0.1270,  0.1747,  0.0051]],

         [[ 0.1040,  0.1332,  0.1118],
          [ 0.0769, -0.0417, -0.0398],
          [-0.0396, -0.0303,  0.1432]],

         [[-0.0487,  0.1437,  0.0711],
          [-0.0413, -0.0487, -0.0632],
          [-0.0368, -0.0086,  0.0450]]],


        ...,


        [[[ 0.2225, -0.1174, -0.1127],
          [-0.0417, -0.0989, -0.0976],
          [-0.1051,  0.0172,  0.0510]],

         [[ 0.1706,  0.1908, -0.1180],
          [ 0.2142, -0.1377, -0.1164],
          [-0.0756, -0.0878, -0.0420]],

         [[ 0.1925,  0.1052, -0.0935],
          [-0.0746, -0.1547, -0.0651],
          [ 0.0494,  0.1736, -0.1333]]],


        [[[-0.1256,  0.0758, -0.1712],
          [-0.1465, -0.1461,  0.1184],
          [-0.1646,  0.1523, -0.1278]],

         [[ 0.0867, -0.1303,  0.0702],
          [ 0.1572,  0.0108, -0.0860],
          [-0.1549,  0.1174, -0.1914]],

         [[ 0.1093, -0.0372, -0.0209],
          [ 0.1335, -0.1846,  0.0030],
          [-0.0428,  0.0090, -0.1232]]],


        [[[ 0.0718,  0.1328,  0.0860],
          [-0.0993, -0.1407,  0.1993],
          [-0.1608,  0.0104, -0.0354]],

         [[-0.0352, -0.0926,  0.1420],
          [-0.0246, -0.1774,  0.0351],
          [-0.0162,  0.0738,  0.0423]],

         [[-0.1495, -0.1854,  0.1469],
          [ 0.1341, -0.1628, -0.0432],
          [-0.0213, -0.0591,  0.2601]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0416, -0.0332,  0.0135],
          [-0.0170,  0.0006,  0.0605],
          [ 0.0131,  0.0387,  0.0888]],

         [[-0.0727, -0.0664, -0.0281],
          [-0.0735, -0.0479,  0.0080],
          [-0.0566, -0.0187,  0.0292]],

         [[-0.0815, -0.0788, -0.0404],
          [-0.0859, -0.0661, -0.0208],
          [-0.0708, -0.0441, -0.0149]]],


        [[[-0.0112, -0.0188, -0.0204],
          [-0.0029, -0.0034, -0.0064],
          [-0.0074, -0.0016, -0.0028]],

         [[-0.0162, -0.0234, -0.0248],
          [-0.0046, -0.0033, -0.0069],
          [-0.0060,  0.0012, -0.0019]],

         [[-0.0246, -0.0325, -0.0327],
          [-0.0098, -0.0103, -0.0124],
          [-0.0067, -0.0016, -0.0026]]],


        [[[-0.0005,  0.0005,  0.0008],
          [ 0.0007,  0.0014,  0.0011],
          [ 0.0027,  0.0012, -0.0003]],

         [[-0.0029, -0.0014, -0.0004],
          [-0.0032, -0.0016, -0.0012],
          [-0.0012, -0.0021, -0.0030]],

         [[-0.0045, -0.0031, -0.0021],
          [-0.0059, -0.0044, -0.0035],
          [-0.0035, -0.0041, -0.0047]]],


        ...,


        [[[-0.0466, -0.0383, -0.0276],
          [-0.0492, -0.0318, -0.0198],
          [-0.0557, -0.0347, -0.0179]],

         [[-0.0409, -0.0400, -0.0327],
          [-0.0426, -0.0323, -0.0232],
          [-0.0470, -0.0313, -0.0165]],

         [[-0.0428, -0.0450, -0.0377],
          [-0.0469, -0.0407, -0.0336],
          [-0.0514, -0.0413, -0.0308]]],


        [[[-0.0134, -0.0108, -0.0054],
          [-0.0058, -0.0031,  0.0011],
          [-0.0009, -0.0012,  0.0052]],

         [[-0.0223, -0.0210, -0.0153],
          [-0.0122, -0.0105, -0.0073],
          [-0.0050, -0.0070, -0.0024]],

         [[-0.0184, -0.0158, -0.0097],
          [-0.0111, -0.0074, -0.0038],
          [-0.0047, -0.0047,  0.0002]]],


        [[[ 0.0492,  0.0518,  0.0408],
          [ 0.0257,  0.0347,  0.0245],
          [ 0.0363,  0.0438,  0.0325]],

         [[ 0.0375,  0.0464,  0.0438],
          [ 0.0185,  0.0329,  0.0289],
          [ 0.0288,  0.0403,  0.0331]],

         [[-0.0072, -0.0038, -0.0116],
          [-0.0197, -0.0111, -0.0193],
          [-0.0066, -0.0023, -0.0172]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0192]], device='cuda:0')

percentage_weight_grad tensor([[1.9219e-06]], device='cuda:0')

Epoch: 34 | Batch_idx: 0 |  Loss: (0.4710) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.4874) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (83.00%) (2234/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (3288/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (4355/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (5418/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5147) |  Loss2: (0.0000) | Acc: (82.00%) (6450/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (7493/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (8547/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (82.00%) (9580/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5197) |  Loss2: (0.0000) | Acc: (82.00%) (10633/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (11678/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (12730/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (82.00%) (13751/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (14818/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (15866/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5254) |  Loss2: (0.0000) | Acc: (82.00%) (16900/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (17941/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (18992/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5256) |  Loss2: (0.0000) | Acc: (81.00%) (20042/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (82.00%) (21103/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5262) |  Loss2: (0.0000) | Acc: (81.00%) (22141/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5246) |  Loss2: (0.0000) | Acc: (82.00%) (23218/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (82.00%) (24256/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5247) |  Loss2: (0.0000) | Acc: (82.00%) (25312/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5256) |  Loss2: (0.0000) | Acc: (82.00%) (26371/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5291) |  Loss2: (0.0000) | Acc: (81.00%) (27384/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (28426/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5280) |  Loss2: (0.0000) | Acc: (81.00%) (29484/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5268) |  Loss2: (0.0000) | Acc: (81.00%) (30542/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5257) |  Loss2: (0.0000) | Acc: (81.00%) (31591/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5257) |  Loss2: (0.0000) | Acc: (81.00%) (32636/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5256) |  Loss2: (0.0000) | Acc: (82.00%) (33695/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5252) |  Loss2: (0.0000) | Acc: (82.00%) (34757/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5267) |  Loss2: (0.0000) | Acc: (81.00%) (35770/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5267) |  Loss2: (0.0000) | Acc: (81.00%) (36812/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (37875/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (82.00%) (38949/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5249) |  Loss2: (0.0000) | Acc: (82.00%) (40000/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5245) |  Loss2: (0.0000) | Acc: (82.00%) (41022/50000)
# TEST : Loss: (0.5863) | Acc: (80.00%) (8002/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1548,  0.0591,  0.0679],
          [ 0.0884, -0.2269, -0.0936],
          [ 0.2057, -0.1183,  0.0265]],

         [[ 0.0067, -0.1173,  0.2268],
          [ 0.0220, -0.0496,  0.0371],
          [ 0.0513, -0.0093, -0.0540]],

         [[ 0.0247, -0.0765,  0.1363],
          [ 0.1186, -0.1457,  0.1347],
          [-0.0817,  0.0489, -0.1850]]],


        [[[ 0.1107, -0.0756, -0.0032],
          [ 0.1458,  0.1778,  0.1780],
          [ 0.0288, -0.0503,  0.1252]],

         [[-0.1659, -0.1904,  0.1559],
          [ 0.0254, -0.0555,  0.0965],
          [ 0.1359, -0.1949,  0.1387]],

         [[ 0.0998, -0.0468, -0.0122],
          [ 0.0116,  0.0324, -0.1193],
          [-0.0260, -0.1429, -0.0063]]],


        [[[-0.0620,  0.0988, -0.0388],
          [-0.1156,  0.0716,  0.1229],
          [ 0.1256,  0.1742,  0.0049]],

         [[ 0.1038,  0.1334,  0.1121],
          [ 0.0763, -0.0416, -0.0396],
          [-0.0405, -0.0304,  0.1431]],

         [[-0.0490,  0.1436,  0.0710],
          [-0.0422, -0.0491, -0.0637],
          [-0.0379, -0.0092,  0.0443]]],


        ...,


        [[[ 0.2205, -0.1194, -0.1143],
          [-0.0432, -0.1006, -0.0988],
          [-0.1052,  0.0159,  0.0492]],

         [[ 0.1684,  0.1891, -0.1192],
          [ 0.2132, -0.1386, -0.1176],
          [-0.0747, -0.0881, -0.0436]],

         [[ 0.1929,  0.1055, -0.0936],
          [-0.0727, -0.1534, -0.0651],
          [ 0.0527,  0.1753, -0.1337]]],


        [[[-0.1241,  0.0765, -0.1699],
          [-0.1458, -0.1468,  0.1183],
          [-0.1643,  0.1514, -0.1285]],

         [[ 0.0879, -0.1296,  0.0713],
          [ 0.1580,  0.0104, -0.0858],
          [-0.1543,  0.1169, -0.1919]],

         [[ 0.1118, -0.0353, -0.0189],
          [ 0.1357, -0.1837,  0.0039],
          [-0.0408,  0.0095, -0.1234]]],


        [[[ 0.0728,  0.1317,  0.0874],
          [-0.0980, -0.1421,  0.1997],
          [-0.1592,  0.0093, -0.0381]],

         [[-0.0339, -0.0944,  0.1431],
          [-0.0228, -0.1783,  0.0369],
          [-0.0124,  0.0754,  0.0419]],

         [[-0.1477, -0.1866,  0.1483],
          [ 0.1378, -0.1611, -0.0392],
          [-0.0145, -0.0537,  0.2633]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0172, -0.0405, -0.0674],
          [ 0.0292, -0.0228, -0.0368],
          [-0.0069, -0.0348, -0.0341]],

         [[ 0.0277, -0.0264, -0.0531],
          [ 0.0266, -0.0300, -0.0402],
          [-0.0298, -0.0647, -0.0624]],

         [[ 0.1434,  0.1017,  0.0830],
          [ 0.1574,  0.1095,  0.1015],
          [ 0.1209,  0.0888,  0.0857]]],


        [[[-0.0096, -0.0135, -0.0096],
          [-0.0027, -0.0047,  0.0023],
          [-0.0134, -0.0142, -0.0036]],

         [[-0.0071, -0.0092, -0.0045],
          [ 0.0006, -0.0010,  0.0072],
          [-0.0063, -0.0079,  0.0030]],

         [[ 0.0192,  0.0175,  0.0200],
          [ 0.0212,  0.0203,  0.0278],
          [ 0.0080,  0.0087,  0.0191]]],


        [[[ 0.0050,  0.0044,  0.0007],
          [ 0.0050,  0.0030, -0.0000],
          [ 0.0045,  0.0020, -0.0012]],

         [[ 0.0021,  0.0012, -0.0030],
          [ 0.0014, -0.0006, -0.0036],
          [ 0.0005, -0.0018, -0.0048]],

         [[ 0.0090,  0.0089,  0.0051],
          [ 0.0101,  0.0095,  0.0071],
          [ 0.0099,  0.0093,  0.0071]]],


        ...,


        [[[-0.0459, -0.0469, -0.0463],
          [-0.0421, -0.0378, -0.0350],
          [-0.0142, -0.0122, -0.0114]],

         [[-0.0581, -0.0575, -0.0524],
          [-0.0574, -0.0491, -0.0407],
          [-0.0278, -0.0235, -0.0182]],

         [[-0.0405, -0.0382, -0.0326],
          [-0.0416, -0.0350, -0.0283],
          [-0.0186, -0.0164, -0.0139]]],


        [[[ 0.0222,  0.0089,  0.0033],
          [ 0.0175,  0.0085,  0.0074],
          [ 0.0096,  0.0042,  0.0010]],

         [[ 0.0187,  0.0022, -0.0058],
          [ 0.0122,  0.0009, -0.0025],
          [ 0.0027, -0.0035, -0.0084]],

         [[ 0.0243,  0.0100,  0.0036],
          [ 0.0167,  0.0075,  0.0047],
          [ 0.0094,  0.0046,  0.0009]]],


        [[[ 0.0198,  0.0242,  0.0269],
          [ 0.0246,  0.0258,  0.0400],
          [ 0.0459,  0.0413,  0.0372]],

         [[ 0.0510,  0.0449,  0.0417],
          [ 0.0602,  0.0493,  0.0556],
          [ 0.0680,  0.0555,  0.0508]],

         [[-0.0051, -0.0089, -0.0123],
          [ 0.0038, -0.0008, -0.0002],
          [ 0.0205,  0.0108,  0.0060]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.0192]], device='cuda:0')

percentage_weight_grad tensor([[1.9212e-06]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.8043) |  Loss2: (0.3675) | Acc: (85.00%) (110/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.8802) |  Loss2: (0.3675) | Acc: (82.00%) (1168/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.9249) |  Loss2: (0.3675) | Acc: (80.00%) (2166/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.9252) |  Loss2: (0.3674) | Acc: (80.00%) (3199/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.9456) |  Loss2: (0.3674) | Acc: (79.00%) (4180/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.9527) |  Loss2: (0.3674) | Acc: (79.00%) (5184/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.9543) |  Loss2: (0.3674) | Acc: (79.00%) (6202/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.9554) |  Loss2: (0.3673) | Acc: (79.00%) (7207/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.9554) |  Loss2: (0.3673) | Acc: (79.00%) (8224/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.9555) |  Loss2: (0.3673) | Acc: (79.00%) (9240/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.9564) |  Loss2: (0.3672) | Acc: (79.00%) (10247/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.9526) |  Loss2: (0.3672) | Acc: (79.00%) (11283/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.9533) |  Loss2: (0.3672) | Acc: (79.00%) (12296/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.9555) |  Loss2: (0.3671) | Acc: (79.00%) (13292/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.9530) |  Loss2: (0.3671) | Acc: (79.00%) (14321/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.9516) |  Loss2: (0.3671) | Acc: (79.00%) (15343/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.9526) |  Loss2: (0.3670) | Acc: (79.00%) (16354/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.9510) |  Loss2: (0.3670) | Acc: (79.00%) (17381/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.9528) |  Loss2: (0.3670) | Acc: (79.00%) (18391/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.9525) |  Loss2: (0.3669) | Acc: (79.00%) (19412/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.9501) |  Loss2: (0.3669) | Acc: (79.00%) (20449/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.9503) |  Loss2: (0.3669) | Acc: (79.00%) (21457/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.9485) |  Loss2: (0.3668) | Acc: (79.00%) (22502/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.9468) |  Loss2: (0.3668) | Acc: (79.00%) (23545/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.9462) |  Loss2: (0.3668) | Acc: (79.00%) (24573/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.9447) |  Loss2: (0.3667) | Acc: (79.00%) (25616/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.9423) |  Loss2: (0.3667) | Acc: (79.00%) (26673/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.9419) |  Loss2: (0.3666) | Acc: (79.00%) (27700/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.9383) |  Loss2: (0.3666) | Acc: (79.00%) (28772/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.9375) |  Loss2: (0.3666) | Acc: (80.00%) (29824/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.9364) |  Loss2: (0.3665) | Acc: (80.00%) (30855/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.9373) |  Loss2: (0.3665) | Acc: (80.00%) (31858/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.9367) |  Loss2: (0.3665) | Acc: (80.00%) (32883/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.9359) |  Loss2: (0.3664) | Acc: (80.00%) (33929/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.9356) |  Loss2: (0.3664) | Acc: (80.00%) (34955/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.9358) |  Loss2: (0.3663) | Acc: (80.00%) (35999/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.9364) |  Loss2: (0.3663) | Acc: (80.00%) (37014/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.9362) |  Loss2: (0.3663) | Acc: (80.00%) (38045/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.9340) |  Loss2: (0.3662) | Acc: (80.00%) (39108/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.9329) |  Loss2: (0.3662) | Acc: (80.00%) (40116/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_035.pth.tar'
# TEST : Loss: (0.5984) | Acc: (79.00%) (7958/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1557,  0.0586,  0.0679],
          [ 0.0878, -0.2269, -0.0933],
          [ 0.2054, -0.1182,  0.0268]],

         [[ 0.0056, -0.1177,  0.2269],
          [ 0.0211, -0.0497,  0.0375],
          [ 0.0509, -0.0090, -0.0534]],

         [[ 0.0229, -0.0778,  0.1353],
          [ 0.1167, -0.1470,  0.1337],
          [-0.0833,  0.0476, -0.1858]]],


        [[[ 0.1105, -0.0757, -0.0034],
          [ 0.1456,  0.1775,  0.1777],
          [ 0.0288, -0.0503,  0.1251]],

         [[-0.1660, -0.1906,  0.1555],
          [ 0.0252, -0.0558,  0.0961],
          [ 0.1357, -0.1950,  0.1383]],

         [[ 0.0994, -0.0472, -0.0127],
          [ 0.0112,  0.0318, -0.1198],
          [-0.0262, -0.1433, -0.0067]]],


        [[[-0.0619,  0.0987, -0.0388],
          [-0.1156,  0.0715,  0.1228],
          [ 0.1255,  0.1741,  0.0049]],

         [[ 0.1037,  0.1333,  0.1121],
          [ 0.0762, -0.0416, -0.0396],
          [-0.0406, -0.0304,  0.1431]],

         [[-0.0491,  0.1435,  0.0709],
          [-0.0423, -0.0492, -0.0637],
          [-0.0380, -0.0093,  0.0443]]],


        ...,


        [[[ 0.2210, -0.1189, -0.1139],
          [-0.0425, -0.1001, -0.0985],
          [-0.1044,  0.0165,  0.0496]],

         [[ 0.1693,  0.1899, -0.1184],
          [ 0.2141, -0.1377, -0.1169],
          [-0.0736, -0.0872, -0.0429]],

         [[ 0.1936,  0.1061, -0.0930],
          [-0.0717, -0.1525, -0.0645],
          [ 0.0538,  0.1762, -0.1329]]],


        [[[-0.1242,  0.0764, -0.1700],
          [-0.1459, -0.1468,  0.1181],
          [-0.1642,  0.1513, -0.1285]],

         [[ 0.0876, -0.1297,  0.0712],
          [ 0.1578,  0.0104, -0.0858],
          [-0.1542,  0.1168, -0.1918]],

         [[ 0.1115, -0.0355, -0.0191],
          [ 0.1354, -0.1837,  0.0038],
          [-0.0409,  0.0095, -0.1234]]],


        [[[ 0.0705,  0.1294,  0.0854],
          [-0.1000, -0.1441,  0.1979],
          [-0.1605,  0.0078, -0.0393]],

         [[-0.0356, -0.0962,  0.1416],
          [-0.0243, -0.1797,  0.0357],
          [-0.0133,  0.0743,  0.0410]],

         [[-0.1480, -0.1870,  0.1480],
          [ 0.1375, -0.1613, -0.0392],
          [-0.0145, -0.0538,  0.2633]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5575e-05,  5.8612e-06,  6.7923e-06],
          [ 8.7762e-06, -2.2686e-05, -9.3313e-06],
          [ 2.0539e-05, -1.1821e-05,  2.6764e-06]],

         [[ 5.6212e-07, -1.1774e-05,  2.2689e-05],
          [ 2.1139e-06, -4.9704e-06,  3.7497e-06],
          [ 5.0878e-06, -9.0431e-07, -5.3404e-06]],

         [[ 2.2883e-06, -7.7842e-06,  1.3526e-05],
          [ 1.1669e-05, -1.4697e-05,  1.3368e-05],
          [-8.3275e-06,  4.7649e-06, -1.8585e-05]]],


        [[[ 1.1050e-05, -7.5675e-06, -3.4397e-07],
          [ 1.4559e-05,  1.7752e-05,  1.7772e-05],
          [ 2.8784e-06, -5.0318e-06,  1.2509e-05]],

         [[-1.6601e-05, -1.9057e-05,  1.5552e-05],
          [ 2.5150e-06, -5.5767e-06,  9.6098e-06],
          [ 1.3570e-05, -1.9502e-05,  1.3834e-05]],

         [[ 9.9448e-06, -4.7191e-06, -1.2703e-06],
          [ 1.1177e-06,  3.1841e-06, -1.1978e-05],
          [-2.6233e-06, -1.4325e-05, -6.6910e-07]]],


        [[[-6.1949e-06,  9.8745e-06, -3.8792e-06],
          [-1.1561e-05,  7.1510e-06,  1.2280e-05],
          [ 1.2552e-05,  1.7411e-05,  4.8935e-07]],

         [[ 1.0374e-05,  1.3334e-05,  1.1214e-05],
          [ 7.6225e-06, -4.1640e-06, -3.9583e-06],
          [-4.0574e-06, -3.0444e-06,  1.4307e-05]],

         [[-4.9099e-06,  1.4346e-05,  7.0917e-06],
          [-4.2258e-06, -4.9180e-06, -6.3725e-06],
          [-3.8024e-06, -9.3298e-07,  4.4268e-06]]],


        ...,


        [[[ 2.2104e-05, -1.1891e-05, -1.1389e-05],
          [-4.2469e-06, -1.0012e-05, -9.8512e-06],
          [-1.0440e-05,  1.6482e-06,  4.9594e-06]],

         [[ 1.6933e-05,  1.8988e-05, -1.1841e-05],
          [ 2.1413e-05, -1.3770e-05, -1.1685e-05],
          [-7.3586e-06, -8.7153e-06, -4.2859e-06]],

         [[ 1.9361e-05,  1.0613e-05, -9.2958e-06],
          [-7.1722e-06, -1.5255e-05, -6.4488e-06],
          [ 5.3807e-06,  1.7618e-05, -1.3289e-05]]],


        [[[-1.2419e-05,  7.6426e-06, -1.6999e-05],
          [-1.4585e-05, -1.4684e-05,  1.1814e-05],
          [-1.6421e-05,  1.5126e-05, -1.2850e-05]],

         [[ 8.7593e-06, -1.2965e-05,  7.1201e-06],
          [ 1.5776e-05,  1.0366e-06, -8.5798e-06],
          [-1.5424e-05,  1.1684e-05, -1.9181e-05]],

         [[ 1.1148e-05, -3.5470e-06, -1.9081e-06],
          [ 1.3543e-05, -1.8369e-05,  3.8083e-07],
          [-4.0863e-06,  9.5072e-07, -1.2335e-05]]],


        [[[ 7.0469e-06,  1.2937e-05,  8.5412e-06],
          [-9.9967e-06, -1.4407e-05,  1.9790e-05],
          [-1.6048e-05,  7.8460e-07, -3.9280e-06]],

         [[-3.5552e-06, -9.6156e-06,  1.4159e-05],
          [-2.4330e-06, -1.7970e-05,  3.5669e-06],
          [-1.3323e-06,  7.4287e-06,  4.1042e-06]],

         [[-1.4798e-05, -1.8698e-05,  1.4801e-05],
          [ 1.3750e-05, -1.6130e-05, -3.9192e-06],
          [-1.4493e-06, -5.3798e-06,  2.6328e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0392]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0278]], device='cuda:0')

Epoch: 36 | Batch_idx: 0 |  Loss: (0.8742) |  Loss2: (0.3647) | Acc: (85.00%) (110/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.9046) |  Loss2: (0.3646) | Acc: (79.00%) (1123/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.9234) |  Loss2: (0.3646) | Acc: (79.00%) (2136/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.9218) |  Loss2: (0.3645) | Acc: (79.00%) (3166/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.9230) |  Loss2: (0.3645) | Acc: (79.00%) (4185/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.9189) |  Loss2: (0.3645) | Acc: (80.00%) (5225/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.9219) |  Loss2: (0.3644) | Acc: (79.00%) (6242/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.9191) |  Loss2: (0.3644) | Acc: (80.00%) (7281/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.9158) |  Loss2: (0.3643) | Acc: (80.00%) (8329/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.9099) |  Loss2: (0.3643) | Acc: (80.00%) (9394/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.9045) |  Loss2: (0.3643) | Acc: (80.00%) (10441/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.9024) |  Loss2: (0.3642) | Acc: (80.00%) (11485/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.8996) |  Loss2: (0.3642) | Acc: (80.00%) (12531/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.8999) |  Loss2: (0.3641) | Acc: (80.00%) (13571/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.8968) |  Loss2: (0.3641) | Acc: (81.00%) (14635/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.8987) |  Loss2: (0.3641) | Acc: (80.00%) (15652/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.8992) |  Loss2: (0.3640) | Acc: (81.00%) (16695/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.8996) |  Loss2: (0.3640) | Acc: (80.00%) (17729/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.8990) |  Loss2: (0.3639) | Acc: (81.00%) (18776/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.9003) |  Loss2: (0.3639) | Acc: (80.00%) (19791/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.9010) |  Loss2: (0.3639) | Acc: (81.00%) (20841/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.9006) |  Loss2: (0.3638) | Acc: (81.00%) (21879/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.9001) |  Loss2: (0.3638) | Acc: (81.00%) (22931/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.8969) |  Loss2: (0.3637) | Acc: (81.00%) (24009/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.8974) |  Loss2: (0.3637) | Acc: (81.00%) (25041/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.8974) |  Loss2: (0.3637) | Acc: (81.00%) (26069/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.8970) |  Loss2: (0.3636) | Acc: (81.00%) (27122/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.8952) |  Loss2: (0.3636) | Acc: (81.00%) (28205/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.8965) |  Loss2: (0.3636) | Acc: (81.00%) (29239/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.8952) |  Loss2: (0.3635) | Acc: (81.00%) (30290/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.8952) |  Loss2: (0.3635) | Acc: (81.00%) (31326/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.8954) |  Loss2: (0.3634) | Acc: (81.00%) (32379/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.8959) |  Loss2: (0.3634) | Acc: (81.00%) (33425/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.8961) |  Loss2: (0.3634) | Acc: (81.00%) (34458/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.8966) |  Loss2: (0.3633) | Acc: (81.00%) (35496/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.8951) |  Loss2: (0.3633) | Acc: (81.00%) (36573/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.8951) |  Loss2: (0.3632) | Acc: (81.00%) (37622/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.8935) |  Loss2: (0.3632) | Acc: (81.00%) (38695/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.8922) |  Loss2: (0.3632) | Acc: (81.00%) (39774/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.8930) |  Loss2: (0.3631) | Acc: (81.00%) (40763/50000)
# TEST : Loss: (0.5729) | Acc: (80.00%) (8046/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1557,  0.0586,  0.0679],
          [ 0.0877, -0.2268, -0.0933],
          [ 0.2053, -0.1182,  0.0268]],

         [[ 0.0056, -0.1177,  0.2268],
          [ 0.0211, -0.0497,  0.0375],
          [ 0.0509, -0.0090, -0.0534]],

         [[ 0.0229, -0.0778,  0.1352],
          [ 0.1166, -0.1469,  0.1336],
          [-0.0832,  0.0476, -0.1858]]],


        [[[ 0.1105, -0.0756, -0.0034],
          [ 0.1455,  0.1775,  0.1777],
          [ 0.0288, -0.0503,  0.1250]],

         [[-0.1659, -0.1905,  0.1555],
          [ 0.0251, -0.0557,  0.0961],
          [ 0.1356, -0.1949,  0.1383]],

         [[ 0.0994, -0.0472, -0.0127],
          [ 0.0112,  0.0318, -0.1197],
          [-0.0262, -0.1432, -0.0067]]],


        [[[-0.0619,  0.0987, -0.0388],
          [-0.1156,  0.0715,  0.1228],
          [ 0.1255,  0.1740,  0.0049]],

         [[ 0.1037,  0.1333,  0.1121],
          [ 0.0762, -0.0416, -0.0396],
          [-0.0406, -0.0304,  0.1430]],

         [[-0.0491,  0.1434,  0.0709],
          [-0.0422, -0.0492, -0.0637],
          [-0.0380, -0.0093,  0.0443]]],


        ...,


        [[[ 0.2209, -0.1189, -0.1138],
          [-0.0425, -0.1001, -0.0985],
          [-0.1044,  0.0165,  0.0496]],

         [[ 0.1693,  0.1898, -0.1184],
          [ 0.2140, -0.1376, -0.1168],
          [-0.0736, -0.0871, -0.0428]],

         [[ 0.1935,  0.1061, -0.0929],
          [-0.0717, -0.1525, -0.0645],
          [ 0.0538,  0.1761, -0.1328]]],


        [[[-0.1241,  0.0764, -0.1699],
          [-0.1458, -0.1468,  0.1181],
          [-0.1641,  0.1512, -0.1284]],

         [[ 0.0876, -0.1296,  0.0712],
          [ 0.1577,  0.0104, -0.0858],
          [-0.1542,  0.1168, -0.1917]],

         [[ 0.1114, -0.0355, -0.0191],
          [ 0.1354, -0.1836,  0.0038],
          [-0.0408,  0.0095, -0.1233]]],


        [[[ 0.0704,  0.1293,  0.0854],
          [-0.0999, -0.1440,  0.1978],
          [-0.1604,  0.0078, -0.0393]],

         [[-0.0355, -0.0961,  0.1415],
          [-0.0243, -0.1796,  0.0357],
          [-0.0133,  0.0743,  0.0410]],

         [[-0.1479, -0.1869,  0.1480],
          [ 0.1374, -0.1612, -0.0392],
          [-0.0145, -0.0538,  0.2632]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5569e-05,  5.8588e-06,  6.7896e-06],
          [ 8.7727e-06, -2.2677e-05, -9.3275e-06],
          [ 2.0531e-05, -1.1816e-05,  2.6754e-06]],

         [[ 5.6190e-07, -1.1769e-05,  2.2680e-05],
          [ 2.1131e-06, -4.9685e-06,  3.7482e-06],
          [ 5.0858e-06, -9.0394e-07, -5.3384e-06]],

         [[ 2.2874e-06, -7.7813e-06,  1.3521e-05],
          [ 1.1665e-05, -1.4691e-05,  1.3363e-05],
          [-8.3243e-06,  4.7630e-06, -1.8578e-05]]],


        [[[ 1.1046e-05, -7.5646e-06, -3.4383e-07],
          [ 1.4553e-05,  1.7745e-05,  1.7765e-05],
          [ 2.8773e-06, -5.0298e-06,  1.2504e-05]],

         [[-1.6594e-05, -1.9050e-05,  1.5546e-05],
          [ 2.5140e-06, -5.5745e-06,  9.6061e-06],
          [ 1.3565e-05, -1.9494e-05,  1.3829e-05]],

         [[ 9.9411e-06, -4.7172e-06, -1.2698e-06],
          [ 1.1172e-06,  3.1828e-06, -1.1974e-05],
          [-2.6223e-06, -1.4319e-05, -6.6885e-07]]],


        [[[-6.1925e-06,  9.8707e-06, -3.8777e-06],
          [-1.1556e-05,  7.1481e-06,  1.2276e-05],
          [ 1.2548e-05,  1.7404e-05,  4.8915e-07]],

         [[ 1.0370e-05,  1.3329e-05,  1.1210e-05],
          [ 7.6196e-06, -4.1624e-06, -3.9567e-06],
          [-4.0558e-06, -3.0433e-06,  1.4301e-05]],

         [[-4.9080e-06,  1.4340e-05,  7.0888e-06],
          [-4.2242e-06, -4.9161e-06, -6.3699e-06],
          [-3.8009e-06, -9.3262e-07,  4.4250e-06]]],


        ...,


        [[[ 2.2095e-05, -1.1886e-05, -1.1384e-05],
          [-4.2453e-06, -1.0008e-05, -9.8474e-06],
          [-1.0436e-05,  1.6475e-06,  4.9575e-06]],

         [[ 1.6927e-05,  1.8981e-05, -1.1837e-05],
          [ 2.1405e-05, -1.3765e-05, -1.1680e-05],
          [-7.3557e-06, -8.7118e-06, -4.2841e-06]],

         [[ 1.9354e-05,  1.0609e-05, -9.2923e-06],
          [-7.1693e-06, -1.5249e-05, -6.4462e-06],
          [ 5.3787e-06,  1.7611e-05, -1.3283e-05]]],


        [[[-1.2414e-05,  7.6397e-06, -1.6992e-05],
          [-1.4579e-05, -1.4678e-05,  1.1809e-05],
          [-1.6415e-05,  1.5121e-05, -1.2845e-05]],

         [[ 8.7558e-06, -1.2960e-05,  7.1172e-06],
          [ 1.5769e-05,  1.0362e-06, -8.5763e-06],
          [-1.5418e-05,  1.1679e-05, -1.9174e-05]],

         [[ 1.1143e-05, -3.5456e-06, -1.9074e-06],
          [ 1.3538e-05, -1.8362e-05,  3.8068e-07],
          [-4.0846e-06,  9.5036e-07, -1.2330e-05]]],


        [[[ 7.0443e-06,  1.2932e-05,  8.5380e-06],
          [-9.9929e-06, -1.4401e-05,  1.9782e-05],
          [-1.6042e-05,  7.8431e-07, -3.9264e-06]],

         [[-3.5537e-06, -9.6118e-06,  1.4153e-05],
          [-2.4321e-06, -1.7963e-05,  3.5654e-06],
          [-1.3318e-06,  7.4257e-06,  4.1026e-06]],

         [[-1.4792e-05, -1.8691e-05,  1.4795e-05],
          [ 1.3745e-05, -1.6124e-05, -3.9176e-06],
          [-1.4487e-06, -5.3777e-06,  2.6317e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0247]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0341]], device='cuda:0')

Epoch: 37 | Batch_idx: 0 |  Loss: (0.8940) |  Loss2: (0.3615) | Acc: (82.00%) (105/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.8573) |  Loss2: (0.3615) | Acc: (83.00%) (1174/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.8830) |  Loss2: (0.3614) | Acc: (82.00%) (2219/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.8970) |  Loss2: (0.3614) | Acc: (82.00%) (3259/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.8901) |  Loss2: (0.3614) | Acc: (82.00%) (4306/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.8917) |  Loss2: (0.3613) | Acc: (82.00%) (5355/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.8903) |  Loss2: (0.3613) | Acc: (82.00%) (6406/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.8874) |  Loss2: (0.3613) | Acc: (81.00%) (7442/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.8830) |  Loss2: (0.3612) | Acc: (82.00%) (8511/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.8788) |  Loss2: (0.3612) | Acc: (82.00%) (9570/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.8785) |  Loss2: (0.3611) | Acc: (82.00%) (10610/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.8802) |  Loss2: (0.3611) | Acc: (82.00%) (11656/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.8824) |  Loss2: (0.3611) | Acc: (82.00%) (12701/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.8805) |  Loss2: (0.3610) | Acc: (82.00%) (13752/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.8786) |  Loss2: (0.3610) | Acc: (82.00%) (14807/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.8771) |  Loss2: (0.3609) | Acc: (82.00%) (15879/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.8765) |  Loss2: (0.3609) | Acc: (82.00%) (16941/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.8771) |  Loss2: (0.3609) | Acc: (82.00%) (17992/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.8810) |  Loss2: (0.3608) | Acc: (82.00%) (19009/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.8789) |  Loss2: (0.3608) | Acc: (82.00%) (20073/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.8787) |  Loss2: (0.3607) | Acc: (82.00%) (21120/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.8777) |  Loss2: (0.3607) | Acc: (82.00%) (22183/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.8768) |  Loss2: (0.3607) | Acc: (82.00%) (23245/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.8753) |  Loss2: (0.3606) | Acc: (82.00%) (24311/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.8736) |  Loss2: (0.3606) | Acc: (82.00%) (25394/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.8721) |  Loss2: (0.3606) | Acc: (82.00%) (26465/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.8719) |  Loss2: (0.3605) | Acc: (82.00%) (27526/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.8717) |  Loss2: (0.3605) | Acc: (82.00%) (28585/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.8730) |  Loss2: (0.3604) | Acc: (82.00%) (29621/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.8724) |  Loss2: (0.3604) | Acc: (82.00%) (30693/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.8721) |  Loss2: (0.3604) | Acc: (82.00%) (31756/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.8727) |  Loss2: (0.3603) | Acc: (82.00%) (32795/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.8727) |  Loss2: (0.3603) | Acc: (82.00%) (33843/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.8719) |  Loss2: (0.3602) | Acc: (82.00%) (34892/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.8730) |  Loss2: (0.3602) | Acc: (82.00%) (35925/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.8731) |  Loss2: (0.3602) | Acc: (82.00%) (36967/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.8726) |  Loss2: (0.3601) | Acc: (82.00%) (38014/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.8724) |  Loss2: (0.3601) | Acc: (82.00%) (39051/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.8723) |  Loss2: (0.3600) | Acc: (82.00%) (40112/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.8727) |  Loss2: (0.3600) | Acc: (82.00%) (41122/50000)
# TEST : Loss: (0.5591) | Acc: (80.00%) (8087/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1556,  0.0586,  0.0679],
          [ 0.0877, -0.2267, -0.0932],
          [ 0.2052, -0.1181,  0.0267]],

         [[ 0.0056, -0.1176,  0.2267],
          [ 0.0211, -0.0497,  0.0375],
          [ 0.0508, -0.0090, -0.0534]],

         [[ 0.0229, -0.0778,  0.1352],
          [ 0.1166, -0.1468,  0.1336],
          [-0.0832,  0.0476, -0.1857]]],


        [[[ 0.1104, -0.0756, -0.0034],
          [ 0.1455,  0.1774,  0.1776],
          [ 0.0288, -0.0503,  0.1250]],

         [[-0.1659, -0.1904,  0.1554],
          [ 0.0251, -0.0557,  0.0960],
          [ 0.1356, -0.1949,  0.1382]],

         [[ 0.0994, -0.0472, -0.0127],
          [ 0.0112,  0.0318, -0.1197],
          [-0.0262, -0.1431, -0.0067]]],


        [[[-0.0619,  0.0987, -0.0388],
          [-0.1155,  0.0715,  0.1227],
          [ 0.1254,  0.1740,  0.0049]],

         [[ 0.1037,  0.1332,  0.1121],
          [ 0.0762, -0.0416, -0.0396],
          [-0.0405, -0.0304,  0.1430]],

         [[-0.0491,  0.1433,  0.0709],
          [-0.0422, -0.0491, -0.0637],
          [-0.0380, -0.0093,  0.0442]]],


        ...,


        [[[ 0.2209, -0.1188, -0.1138],
          [-0.0424, -0.1000, -0.0984],
          [-0.1043,  0.0165,  0.0496]],

         [[ 0.1692,  0.1897, -0.1183],
          [ 0.2140, -0.1376, -0.1168],
          [-0.0735, -0.0871, -0.0428]],

         [[ 0.1935,  0.1061, -0.0929],
          [-0.0717, -0.1524, -0.0644],
          [ 0.0538,  0.1760, -0.1328]]],


        [[[-0.1241,  0.0764, -0.1699],
          [-0.1457, -0.1467,  0.1180],
          [-0.1641,  0.1511, -0.1284]],

         [[ 0.0875, -0.1295,  0.0711],
          [ 0.1576,  0.0104, -0.0857],
          [-0.1541,  0.1167, -0.1917]],

         [[ 0.1114, -0.0354, -0.0191],
          [ 0.1353, -0.1835,  0.0038],
          [-0.0408,  0.0095, -0.1233]]],


        [[[ 0.0704,  0.1293,  0.0853],
          [-0.0999, -0.1440,  0.1977],
          [-0.1604,  0.0078, -0.0392]],

         [[-0.0355, -0.0961,  0.1415],
          [-0.0243, -0.1796,  0.0356],
          [-0.0133,  0.0742,  0.0410]],

         [[-0.1479, -0.1868,  0.1479],
          [ 0.1374, -0.1612, -0.0392],
          [-0.0145, -0.0538,  0.2631]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5563e-05,  5.8565e-06,  6.7870e-06],
          [ 8.7692e-06, -2.2668e-05, -9.3237e-06],
          [ 2.0522e-05, -1.1812e-05,  2.6744e-06]],

         [[ 5.6169e-07, -1.1765e-05,  2.2671e-05],
          [ 2.1123e-06, -4.9666e-06,  3.7467e-06],
          [ 5.0838e-06, -9.0358e-07, -5.3363e-06]],

         [[ 2.2865e-06, -7.7784e-06,  1.3515e-05],
          [ 1.1660e-05, -1.4685e-05,  1.3358e-05],
          [-8.3211e-06,  4.7611e-06, -1.8571e-05]]],


        [[[ 1.1041e-05, -7.5617e-06, -3.4370e-07],
          [ 1.4547e-05,  1.7738e-05,  1.7758e-05],
          [ 2.8762e-06, -5.0278e-06,  1.2500e-05]],

         [[-1.6588e-05, -1.9042e-05,  1.5540e-05],
          [ 2.5131e-06, -5.5723e-06,  9.6023e-06],
          [ 1.3560e-05, -1.9487e-05,  1.3823e-05]],

         [[ 9.9373e-06, -4.7153e-06, -1.2693e-06],
          [ 1.1168e-06,  3.1815e-06, -1.1969e-05],
          [-2.6213e-06, -1.4313e-05, -6.6859e-07]]],


        [[[-6.1900e-06,  9.8669e-06, -3.8762e-06],
          [-1.1551e-05,  7.1452e-06,  1.2271e-05],
          [ 1.2543e-05,  1.7397e-05,  4.8895e-07]],

         [[ 1.0366e-05,  1.3324e-05,  1.1206e-05],
          [ 7.6167e-06, -4.1608e-06, -3.9551e-06],
          [-4.0542e-06, -3.0421e-06,  1.4295e-05]],

         [[-4.9061e-06,  1.4334e-05,  7.0859e-06],
          [-4.2226e-06, -4.9142e-06, -6.3673e-06],
          [-3.7994e-06, -9.3226e-07,  4.4233e-06]]],


        ...,


        [[[ 2.2086e-05, -1.1881e-05, -1.1380e-05],
          [-4.2437e-06, -1.0004e-05, -9.8436e-06],
          [-1.0432e-05,  1.6469e-06,  4.9556e-06]],

         [[ 1.6920e-05,  1.8973e-05, -1.1832e-05],
          [ 2.1397e-05, -1.3759e-05, -1.1676e-05],
          [-7.3528e-06, -8.7083e-06, -4.2825e-06]],

         [[ 1.9346e-05,  1.0605e-05, -9.2888e-06],
          [-7.1664e-06, -1.5243e-05, -6.4436e-06],
          [ 5.3766e-06,  1.7604e-05, -1.3278e-05]]],


        [[[-1.2409e-05,  7.6368e-06, -1.6986e-05],
          [-1.4573e-05, -1.4672e-05,  1.1804e-05],
          [-1.6408e-05,  1.5115e-05, -1.2840e-05]],

         [[ 8.7523e-06, -1.2955e-05,  7.1142e-06],
          [ 1.5763e-05,  1.0358e-06, -8.5728e-06],
          [-1.5412e-05,  1.1675e-05, -1.9166e-05]],

         [[ 1.1139e-05, -3.5441e-06, -1.9066e-06],
          [ 1.3532e-05, -1.8355e-05,  3.8054e-07],
          [-4.0830e-06,  9.4999e-07, -1.2325e-05]]],


        [[[ 7.0417e-06,  1.2927e-05,  8.5348e-06],
          [-9.9891e-06, -1.4396e-05,  1.9775e-05],
          [-1.6035e-05,  7.8402e-07, -3.9248e-06]],

         [[-3.5523e-06, -9.6080e-06,  1.4148e-05],
          [-2.4311e-06, -1.7956e-05,  3.5640e-06],
          [-1.3313e-06,  7.4228e-06,  4.1010e-06]],

         [[-1.4786e-05, -1.8683e-05,  1.4790e-05],
          [ 1.3739e-05, -1.6117e-05, -3.9160e-06],
          [-1.4482e-06, -5.3757e-06,  2.6307e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0214]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0091]], device='cuda:0')

Epoch: 38 | Batch_idx: 0 |  Loss: (0.8243) |  Loss2: (0.3584) | Acc: (83.00%) (107/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.8911) |  Loss2: (0.3584) | Acc: (82.00%) (1162/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.8859) |  Loss2: (0.3584) | Acc: (81.00%) (2203/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.8792) |  Loss2: (0.3583) | Acc: (82.00%) (3259/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.8804) |  Loss2: (0.3583) | Acc: (82.00%) (4314/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.8805) |  Loss2: (0.3583) | Acc: (82.00%) (5361/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.8703) |  Loss2: (0.3582) | Acc: (82.00%) (6459/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.8661) |  Loss2: (0.3582) | Acc: (82.00%) (7528/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.8648) |  Loss2: (0.3581) | Acc: (82.00%) (8584/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.8643) |  Loss2: (0.3581) | Acc: (82.00%) (9648/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.8644) |  Loss2: (0.3580) | Acc: (82.00%) (10705/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.8652) |  Loss2: (0.3580) | Acc: (82.00%) (11754/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.8662) |  Loss2: (0.3580) | Acc: (82.00%) (12816/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.8652) |  Loss2: (0.3579) | Acc: (82.00%) (13873/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.8657) |  Loss2: (0.3579) | Acc: (82.00%) (14926/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.8641) |  Loss2: (0.3578) | Acc: (82.00%) (15988/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.8641) |  Loss2: (0.3578) | Acc: (82.00%) (17048/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.8624) |  Loss2: (0.3578) | Acc: (82.00%) (18102/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.8604) |  Loss2: (0.3577) | Acc: (82.00%) (19183/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.8605) |  Loss2: (0.3577) | Acc: (82.00%) (20240/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.8604) |  Loss2: (0.3576) | Acc: (82.00%) (21293/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.8595) |  Loss2: (0.3576) | Acc: (82.00%) (22362/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.8603) |  Loss2: (0.3575) | Acc: (82.00%) (23408/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.8620) |  Loss2: (0.3575) | Acc: (82.00%) (24454/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.8624) |  Loss2: (0.3575) | Acc: (82.00%) (25507/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.8618) |  Loss2: (0.3574) | Acc: (82.00%) (26580/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.8621) |  Loss2: (0.3574) | Acc: (82.00%) (27645/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.8623) |  Loss2: (0.3573) | Acc: (82.00%) (28696/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.8627) |  Loss2: (0.3573) | Acc: (82.00%) (29730/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.8630) |  Loss2: (0.3572) | Acc: (82.00%) (30778/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.8623) |  Loss2: (0.3572) | Acc: (82.00%) (31839/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.8621) |  Loss2: (0.3572) | Acc: (82.00%) (32901/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.8622) |  Loss2: (0.3571) | Acc: (82.00%) (33969/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.8616) |  Loss2: (0.3571) | Acc: (82.00%) (35045/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.8622) |  Loss2: (0.3570) | Acc: (82.00%) (36099/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.8614) |  Loss2: (0.3570) | Acc: (82.00%) (37167/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.8622) |  Loss2: (0.3570) | Acc: (82.00%) (38213/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.8624) |  Loss2: (0.3569) | Acc: (82.00%) (39273/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.8627) |  Loss2: (0.3569) | Acc: (82.00%) (40321/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.8629) |  Loss2: (0.3568) | Acc: (82.00%) (41328/50000)
# TEST : Loss: (0.5512) | Acc: (81.00%) (8111/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1556,  0.0585,  0.0678],
          [ 0.0877, -0.2266, -0.0932],
          [ 0.2051, -0.1181,  0.0267]],

         [[ 0.0056, -0.1176,  0.2266],
          [ 0.0211, -0.0496,  0.0375],
          [ 0.0508, -0.0090, -0.0533]],

         [[ 0.0229, -0.0778,  0.1351],
          [ 0.1166, -0.1468,  0.1335],
          [-0.0832,  0.0476, -0.1856]]],


        [[[ 0.1104, -0.0756, -0.0034],
          [ 0.1454,  0.1773,  0.1775],
          [ 0.0288, -0.0503,  0.1249]],

         [[-0.1658, -0.1903,  0.1553],
          [ 0.0251, -0.0557,  0.0960],
          [ 0.1355, -0.1948,  0.1382]],

         [[ 0.0993, -0.0471, -0.0127],
          [ 0.0112,  0.0318, -0.1196],
          [-0.0262, -0.1431, -0.0067]]],


        [[[-0.0619,  0.0986, -0.0387],
          [-0.1155,  0.0714,  0.1227],
          [ 0.1254,  0.1739,  0.0049]],

         [[ 0.1036,  0.1332,  0.1120],
          [ 0.0761, -0.0416, -0.0395],
          [-0.0405, -0.0304,  0.1429]],

         [[-0.0490,  0.1433,  0.0708],
          [-0.0422, -0.0491, -0.0636],
          [-0.0380, -0.0093,  0.0442]]],


        ...,


        [[[ 0.2208, -0.1188, -0.1138],
          [-0.0424, -0.1000, -0.0984],
          [-0.1043,  0.0165,  0.0495]],

         [[ 0.1691,  0.1897, -0.1183],
          [ 0.2139, -0.1375, -0.1167],
          [-0.0735, -0.0870, -0.0428]],

         [[ 0.1934,  0.1060, -0.0929],
          [-0.0716, -0.1524, -0.0644],
          [ 0.0537,  0.1760, -0.1327]]],


        [[[-0.1240,  0.0763, -0.1698],
          [-0.1457, -0.1467,  0.1180],
          [-0.1640,  0.1511, -0.1283]],

         [[ 0.0875, -0.1295,  0.0711],
          [ 0.1576,  0.0104, -0.0857],
          [-0.1541,  0.1167, -0.1916]],

         [[ 0.1113, -0.0354, -0.0191],
          [ 0.1353, -0.1835,  0.0038],
          [-0.0408,  0.0095, -0.1232]]],


        [[[ 0.0704,  0.1292,  0.0853],
          [-0.0999, -0.1439,  0.1977],
          [-0.1603,  0.0078, -0.0392]],

         [[-0.0355, -0.0960,  0.1414],
          [-0.0243, -0.1795,  0.0356],
          [-0.0133,  0.0742,  0.0410]],

         [[-0.1478, -0.1868,  0.1478],
          [ 0.1373, -0.1611, -0.0391],
          [-0.0145, -0.0537,  0.2630]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5557e-05,  5.8542e-06,  6.7844e-06],
          [ 8.7657e-06, -2.2660e-05, -9.3199e-06],
          [ 2.0514e-05, -1.1807e-05,  2.6734e-06]],

         [[ 5.6147e-07, -1.1760e-05,  2.2663e-05],
          [ 2.1115e-06, -4.9647e-06,  3.7453e-06],
          [ 5.0817e-06, -9.0322e-07, -5.3343e-06]],

         [[ 2.2857e-06, -7.7755e-06,  1.3510e-05],
          [ 1.1655e-05, -1.4679e-05,  1.3353e-05],
          [-8.3179e-06,  4.7592e-06, -1.8564e-05]]],


        [[[ 1.1037e-05, -7.5588e-06, -3.4356e-07],
          [ 1.4542e-05,  1.7731e-05,  1.7751e-05],
          [ 2.8752e-06, -5.0259e-06,  1.2495e-05]],

         [[-1.6582e-05, -1.9035e-05,  1.5534e-05],
          [ 2.5121e-06, -5.5702e-06,  9.5985e-06],
          [ 1.3554e-05, -1.9479e-05,  1.3818e-05]],

         [[ 9.9335e-06, -4.7134e-06, -1.2688e-06],
          [ 1.1164e-06,  3.1802e-06, -1.1964e-05],
          [-2.6203e-06, -1.4308e-05, -6.6834e-07]]],


        [[[-6.1875e-06,  9.8631e-06, -3.8748e-06],
          [-1.1547e-05,  7.1423e-06,  1.2266e-05],
          [ 1.2538e-05,  1.7390e-05,  4.8877e-07]],

         [[ 1.0361e-05,  1.3319e-05,  1.1201e-05],
          [ 7.6138e-06, -4.1592e-06, -3.9535e-06],
          [-4.0526e-06, -3.0409e-06,  1.4289e-05]],

         [[-4.9043e-06,  1.4329e-05,  7.0830e-06],
          [-4.2210e-06, -4.9123e-06, -6.3646e-06],
          [-3.7980e-06, -9.3189e-07,  4.4215e-06]]],


        ...,


        [[[ 2.2077e-05, -1.1877e-05, -1.1376e-05],
          [-4.2421e-06, -1.0000e-05, -9.8398e-06],
          [-1.0428e-05,  1.6462e-06,  4.9537e-06]],

         [[ 1.6914e-05,  1.8965e-05, -1.1827e-05],
          [ 2.1388e-05, -1.3754e-05, -1.1671e-05],
          [-7.3499e-06, -8.7048e-06, -4.2809e-06]],

         [[ 1.9339e-05,  1.0601e-05, -9.2854e-06],
          [-7.1635e-06, -1.5237e-05, -6.4409e-06],
          [ 5.3746e-06,  1.7597e-05, -1.3273e-05]]],


        [[[-1.2404e-05,  7.6339e-06, -1.6979e-05],
          [-1.4568e-05, -1.4666e-05,  1.1800e-05],
          [-1.6402e-05,  1.5109e-05, -1.2834e-05]],

         [[ 8.7488e-06, -1.2950e-05,  7.1113e-06],
          [ 1.5756e-05,  1.0354e-06, -8.5693e-06],
          [-1.5406e-05,  1.1670e-05, -1.9159e-05]],

         [[ 1.1135e-05, -3.5426e-06, -1.9059e-06],
          [ 1.3527e-05, -1.8348e-05,  3.8039e-07],
          [-4.0814e-06,  9.4963e-07, -1.2320e-05]]],


        [[[ 7.0390e-06,  1.2921e-05,  8.5316e-06],
          [-9.9853e-06, -1.4390e-05,  1.9767e-05],
          [-1.6029e-05,  7.8373e-07, -3.9232e-06]],

         [[-3.5508e-06, -9.6042e-06,  1.4143e-05],
          [-2.4302e-06, -1.7949e-05,  3.5625e-06],
          [-1.3308e-06,  7.4199e-06,  4.0994e-06]],

         [[-1.4780e-05, -1.8675e-05,  1.4784e-05],
          [ 1.3734e-05, -1.6111e-05, -3.9144e-06],
          [-1.4476e-06, -5.3736e-06,  2.6296e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0143]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.1675]], device='cuda:0')

Epoch: 39 | Batch_idx: 0 |  Loss: (0.8346) |  Loss2: (0.3553) | Acc: (84.00%) (108/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.8197) |  Loss2: (0.3552) | Acc: (84.00%) (1186/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.8471) |  Loss2: (0.3552) | Acc: (83.00%) (2241/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.8488) |  Loss2: (0.3551) | Acc: (83.00%) (3299/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.8485) |  Loss2: (0.3551) | Acc: (83.00%) (4360/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.8499) |  Loss2: (0.3551) | Acc: (83.00%) (5428/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.8472) |  Loss2: (0.3550) | Acc: (83.00%) (6498/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.8527) |  Loss2: (0.3550) | Acc: (83.00%) (7547/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.8549) |  Loss2: (0.3549) | Acc: (82.00%) (8599/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.8539) |  Loss2: (0.3549) | Acc: (83.00%) (9674/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.8566) |  Loss2: (0.3549) | Acc: (82.00%) (10725/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.8568) |  Loss2: (0.3548) | Acc: (82.00%) (11785/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.8559) |  Loss2: (0.3548) | Acc: (82.00%) (12842/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.8572) |  Loss2: (0.3548) | Acc: (82.00%) (13906/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.8556) |  Loss2: (0.3547) | Acc: (82.00%) (14969/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.8562) |  Loss2: (0.3547) | Acc: (82.00%) (16014/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.8553) |  Loss2: (0.3546) | Acc: (82.00%) (17074/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.8570) |  Loss2: (0.3546) | Acc: (82.00%) (18131/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.8569) |  Loss2: (0.3546) | Acc: (82.00%) (19203/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.8566) |  Loss2: (0.3545) | Acc: (82.00%) (20267/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.8562) |  Loss2: (0.3545) | Acc: (82.00%) (21338/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.8563) |  Loss2: (0.3544) | Acc: (82.00%) (22396/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.8560) |  Loss2: (0.3544) | Acc: (82.00%) (23434/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.8551) |  Loss2: (0.3544) | Acc: (82.00%) (24498/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.8541) |  Loss2: (0.3543) | Acc: (82.00%) (25564/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.8550) |  Loss2: (0.3543) | Acc: (82.00%) (26601/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.8543) |  Loss2: (0.3543) | Acc: (82.00%) (27662/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.8545) |  Loss2: (0.3542) | Acc: (82.00%) (28707/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.8554) |  Loss2: (0.3542) | Acc: (82.00%) (29744/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.8547) |  Loss2: (0.3541) | Acc: (82.00%) (30806/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.8536) |  Loss2: (0.3541) | Acc: (82.00%) (31877/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.8540) |  Loss2: (0.3541) | Acc: (82.00%) (32929/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.8535) |  Loss2: (0.3540) | Acc: (82.00%) (34001/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.8546) |  Loss2: (0.3540) | Acc: (82.00%) (35051/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.8539) |  Loss2: (0.3539) | Acc: (82.00%) (36116/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.8538) |  Loss2: (0.3539) | Acc: (82.00%) (37168/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.8537) |  Loss2: (0.3539) | Acc: (82.00%) (38227/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.8527) |  Loss2: (0.3538) | Acc: (82.00%) (39301/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.8531) |  Loss2: (0.3538) | Acc: (82.00%) (40343/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.8515) |  Loss2: (0.3537) | Acc: (82.00%) (41382/50000)
# TEST : Loss: (0.5500) | Acc: (81.00%) (8132/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1555,  0.0585,  0.0678],
          [ 0.0876, -0.2265, -0.0932],
          [ 0.2051, -0.1180,  0.0267]],

         [[ 0.0056, -0.1176,  0.2265],
          [ 0.0211, -0.0496,  0.0374],
          [ 0.0508, -0.0090, -0.0533]],

         [[ 0.0228, -0.0777,  0.1350],
          [ 0.1165, -0.1467,  0.1335],
          [-0.0831,  0.0476, -0.1856]]],


        [[[ 0.1103, -0.0756, -0.0034],
          [ 0.1454,  0.1772,  0.1774],
          [ 0.0287, -0.0502,  0.1249]],

         [[-0.1658, -0.1903,  0.1553],
          [ 0.0251, -0.0557,  0.0959],
          [ 0.1355, -0.1947,  0.1381]],

         [[ 0.0993, -0.0471, -0.0127],
          [ 0.0112,  0.0318, -0.1196],
          [-0.0262, -0.1430, -0.0067]]],


        [[[-0.0619,  0.0986, -0.0387],
          [-0.1154,  0.0714,  0.1226],
          [ 0.1253,  0.1738,  0.0049]],

         [[ 0.1036,  0.1331,  0.1120],
          [ 0.0761, -0.0416, -0.0395],
          [-0.0405, -0.0304,  0.1428]],

         [[-0.0490,  0.1432,  0.0708],
          [-0.0422, -0.0491, -0.0636],
          [-0.0380, -0.0093,  0.0442]]],


        ...,


        [[[ 0.2207, -0.1187, -0.1137],
          [-0.0424, -0.1000, -0.0984],
          [-0.1042,  0.0165,  0.0495]],

         [[ 0.1691,  0.1896, -0.1182],
          [ 0.2138, -0.1375, -0.1167],
          [-0.0735, -0.0870, -0.0428]],

         [[ 0.1933,  0.1060, -0.0928],
          [-0.0716, -0.1523, -0.0644],
          [ 0.0537,  0.1759, -0.1327]]],


        [[[-0.1240,  0.0763, -0.1697],
          [-0.1456, -0.1466,  0.1180],
          [-0.1640,  0.1510, -0.1283]],

         [[ 0.0875, -0.1294,  0.0711],
          [ 0.1575,  0.0103, -0.0857],
          [-0.1540,  0.1167, -0.1915]],

         [[ 0.1113, -0.0354, -0.0191],
          [ 0.1352, -0.1834,  0.0038],
          [-0.0408,  0.0095, -0.1232]]],


        [[[ 0.0704,  0.1292,  0.0853],
          [-0.0998, -0.1438,  0.1976],
          [-0.1602,  0.0078, -0.0392]],

         [[-0.0355, -0.0960,  0.1414],
          [-0.0243, -0.1794,  0.0356],
          [-0.0133,  0.0742,  0.0410]],

         [[-0.1477, -0.1867,  0.1478],
          [ 0.1373, -0.1610, -0.0391],
          [-0.0145, -0.0537,  0.2629]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5552e-05,  5.8518e-06,  6.7818e-06],
          [ 8.7622e-06, -2.2651e-05, -9.3161e-06],
          [ 2.0506e-05, -1.1802e-05,  2.6723e-06]],

         [[ 5.6125e-07, -1.1755e-05,  2.2654e-05],
          [ 2.1107e-06, -4.9628e-06,  3.7438e-06],
          [ 5.0797e-06, -9.0285e-07, -5.3322e-06]],

         [[ 2.2848e-06, -7.7726e-06,  1.3505e-05],
          [ 1.1651e-05, -1.4673e-05,  1.3347e-05],
          [-8.3147e-06,  4.7573e-06, -1.8557e-05]]],


        [[[ 1.1033e-05, -7.5559e-06, -3.4342e-07],
          [ 1.4536e-05,  1.7724e-05,  1.7744e-05],
          [ 2.8741e-06, -5.0241e-06,  1.2490e-05]],

         [[-1.6575e-05, -1.9027e-05,  1.5528e-05],
          [ 2.5112e-06, -5.5680e-06,  9.5947e-06],
          [ 1.3549e-05, -1.9471e-05,  1.3813e-05]],

         [[ 9.9297e-06, -4.7115e-06, -1.2683e-06],
          [ 1.1159e-06,  3.1788e-06, -1.1960e-05],
          [-2.6193e-06, -1.4302e-05, -6.6808e-07]]],


        [[[-6.1850e-06,  9.8593e-06, -3.8733e-06],
          [-1.1542e-05,  7.1393e-06,  1.2262e-05],
          [ 1.2534e-05,  1.7383e-05,  4.8858e-07]],

         [[ 1.0357e-05,  1.3313e-05,  1.1197e-05],
          [ 7.6109e-06, -4.1576e-06, -3.9519e-06],
          [-4.0510e-06, -3.0398e-06,  1.4284e-05]],

         [[-4.9024e-06,  1.4323e-05,  7.0801e-06],
          [-4.2194e-06, -4.9104e-06, -6.3620e-06],
          [-3.7965e-06, -9.3153e-07,  4.4198e-06]]],


        ...,


        [[[ 2.2069e-05, -1.1872e-05, -1.1371e-05],
          [-4.2404e-06, -9.9967e-06, -9.8361e-06],
          [-1.0424e-05,  1.6456e-06,  4.9518e-06]],

         [[ 1.6907e-05,  1.8958e-05, -1.1823e-05],
          [ 2.1380e-05, -1.3749e-05, -1.1666e-05],
          [-7.3469e-06, -8.7013e-06, -4.2793e-06]],

         [[ 1.9331e-05,  1.0597e-05, -9.2819e-06],
          [-7.1606e-06, -1.5232e-05, -6.4383e-06],
          [ 5.3725e-06,  1.7590e-05, -1.3268e-05]]],


        [[[-1.2399e-05,  7.6310e-06, -1.6973e-05],
          [-1.4562e-05, -1.4661e-05,  1.1795e-05],
          [-1.6396e-05,  1.5103e-05, -1.2829e-05]],

         [[ 8.7453e-06, -1.2945e-05,  7.1084e-06],
          [ 1.5750e-05,  1.0350e-06, -8.5660e-06],
          [-1.5400e-05,  1.1665e-05, -1.9151e-05]],

         [[ 1.1130e-05, -3.5412e-06, -1.9052e-06],
          [ 1.3522e-05, -1.8341e-05,  3.8025e-07],
          [-4.0798e-06,  9.4926e-07, -1.2315e-05]]],


        [[[ 7.0364e-06,  1.2916e-05,  8.5284e-06],
          [-9.9815e-06, -1.4384e-05,  1.9759e-05],
          [-1.6022e-05,  7.8344e-07, -3.9216e-06]],

         [[-3.5493e-06, -9.6004e-06,  1.4137e-05],
          [-2.4293e-06, -1.7942e-05,  3.5611e-06],
          [-1.3303e-06,  7.4170e-06,  4.0978e-06]],

         [[-1.4774e-05, -1.8668e-05,  1.4778e-05],
          [ 1.3729e-05, -1.6104e-05, -3.9128e-06],
          [-1.4470e-06, -5.3716e-06,  2.6286e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0029]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0614]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4815) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (1162/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5131) |  Loss2: (0.0000) | Acc: (82.00%) (2220/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (83.00%) (3294/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5151) |  Loss2: (0.0000) | Acc: (82.00%) (4342/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5150) |  Loss2: (0.0000) | Acc: (82.00%) (5395/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (6461/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.5084) |  Loss2: (0.0000) | Acc: (82.00%) (7527/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (8585/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.5143) |  Loss2: (0.0000) | Acc: (82.00%) (9623/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (82.00%) (10679/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.5134) |  Loss2: (0.0000) | Acc: (82.00%) (11729/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.5145) |  Loss2: (0.0000) | Acc: (82.00%) (12781/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.5163) |  Loss2: (0.0000) | Acc: (82.00%) (13816/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (14861/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (15886/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (16952/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (17993/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (19068/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (20147/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.5114) |  Loss2: (0.0000) | Acc: (82.00%) (21231/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (22311/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (23351/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.5094) |  Loss2: (0.0000) | Acc: (82.00%) (24399/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (25466/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.5100) |  Loss2: (0.0000) | Acc: (82.00%) (26496/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (27560/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (28609/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (29678/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (30745/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (31789/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (32854/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (33924/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.5073) |  Loss2: (0.0000) | Acc: (82.00%) (34953/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (35974/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.5101) |  Loss2: (0.0000) | Acc: (82.00%) (37023/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.5099) |  Loss2: (0.0000) | Acc: (82.00%) (38077/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.5106) |  Loss2: (0.0000) | Acc: (82.00%) (39124/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (40193/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (41219/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_040.pth.tar'
# TEST : Loss: (0.5991) | Acc: (80.00%) (8001/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1538,  0.0593,  0.0731],
          [ 0.0848, -0.2332, -0.0927],
          [ 0.2016, -0.1165,  0.0264]],

         [[ 0.0078, -0.1154,  0.2313],
          [ 0.0189, -0.0561,  0.0364],
          [ 0.0475, -0.0103, -0.0581]],

         [[ 0.0258, -0.0748,  0.1413],
          [ 0.1153, -0.1513,  0.1349],
          [-0.0859,  0.0468, -0.1878]]],


        [[[ 0.1074, -0.0764, -0.0032],
          [ 0.1435,  0.1766,  0.1779],
          [ 0.0261, -0.0514,  0.1250]],

         [[-0.1671, -0.1896,  0.1573],
          [ 0.0242, -0.0554,  0.0979],
          [ 0.1331, -0.1955,  0.1393]],

         [[ 0.0973, -0.0476, -0.0118],
          [ 0.0096,  0.0309, -0.1188],
          [-0.0289, -0.1449, -0.0068]]],


        [[[-0.0621,  0.0985, -0.0392],
          [-0.1157,  0.0715,  0.1224],
          [ 0.1247,  0.1737,  0.0048]],

         [[ 0.1035,  0.1333,  0.1118],
          [ 0.0758, -0.0413, -0.0396],
          [-0.0412, -0.0305,  0.1427]],

         [[-0.0480,  0.1443,  0.0715],
          [-0.0413, -0.0480, -0.0629],
          [-0.0373, -0.0085,  0.0449]]],


        ...,


        [[[ 0.2253, -0.1149, -0.1077],
          [-0.0408, -0.0994, -0.0959],
          [-0.1026,  0.0172,  0.0526]],

         [[ 0.1701,  0.1900, -0.1160],
          [ 0.2126, -0.1397, -0.1176],
          [-0.0744, -0.0889, -0.0430]],

         [[ 0.1942,  0.1066, -0.0906],
          [-0.0727, -0.1538, -0.0650],
          [ 0.0530,  0.1749, -0.1323]]],


        [[[-0.1230,  0.0771, -0.1681],
          [-0.1459, -0.1468,  0.1185],
          [-0.1648,  0.1504, -0.1285]],

         [[ 0.0872, -0.1297,  0.0716],
          [ 0.1566,  0.0097, -0.0855],
          [-0.1546,  0.1163, -0.1915]],

         [[ 0.1097, -0.0372, -0.0200],
          [ 0.1335, -0.1849,  0.0029],
          [-0.0418,  0.0087, -0.1240]]],


        [[[ 0.0694,  0.1289,  0.0865],
          [-0.0995, -0.1467,  0.1971],
          [-0.1634,  0.0030, -0.0438]],

         [[-0.0356, -0.0966,  0.1424],
          [-0.0235, -0.1824,  0.0344],
          [-0.0147,  0.0707,  0.0360]],

         [[-0.1489, -0.1874,  0.1495],
          [ 0.1371, -0.1636, -0.0391],
          [-0.0160, -0.0560,  0.2599]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0181, -0.0675, -0.0652],
          [-0.0146, -0.0621, -0.0827],
          [-0.0178, -0.0281, -0.0516]],

         [[-0.0820, -0.1386, -0.1400],
          [-0.0751, -0.1285, -0.1496],
          [-0.0715, -0.0896, -0.1091]],

         [[ 0.0184, -0.0426, -0.0451],
          [ 0.0264, -0.0377, -0.0551],
          [ 0.0268, -0.0067, -0.0183]]],


        [[[-0.0064, -0.0027, -0.0107],
          [-0.0059,  0.0042,  0.0015],
          [-0.0197, -0.0054, -0.0049]],

         [[-0.0188, -0.0156, -0.0260],
          [-0.0082, -0.0012, -0.0067],
          [-0.0153, -0.0040, -0.0053]],

         [[-0.0309, -0.0266, -0.0338],
          [-0.0179, -0.0122, -0.0158],
          [-0.0177, -0.0090, -0.0103]]],


        [[[ 0.0007, -0.0018,  0.0003],
          [ 0.0046,  0.0023,  0.0023],
          [ 0.0056,  0.0028,  0.0014]],

         [[ 0.0019, -0.0003,  0.0010],
          [ 0.0062,  0.0049,  0.0045],
          [ 0.0060,  0.0039,  0.0023]],

         [[ 0.0063,  0.0042,  0.0043],
          [ 0.0121,  0.0108,  0.0092],
          [ 0.0114,  0.0094,  0.0068]]],


        ...,


        [[[ 0.0347,  0.0355,  0.0290],
          [ 0.0170,  0.0249,  0.0270],
          [ 0.0224,  0.0266,  0.0293]],

         [[ 0.0403,  0.0411,  0.0316],
          [ 0.0203,  0.0273,  0.0269],
          [ 0.0241,  0.0274,  0.0289]],

         [[ 0.0231,  0.0215,  0.0150],
          [ 0.0048,  0.0110,  0.0150],
          [ 0.0096,  0.0140,  0.0209]]],


        [[[ 0.0137,  0.0026, -0.0073],
          [ 0.0144,  0.0046, -0.0043],
          [ 0.0093,  0.0001, -0.0044]],

         [[ 0.0195,  0.0059, -0.0033],
          [ 0.0181,  0.0047, -0.0027],
          [ 0.0131,  0.0006, -0.0026]],

         [[ 0.0174,  0.0066,  0.0001],
          [ 0.0178,  0.0065,  0.0011],
          [ 0.0130,  0.0025,  0.0012]]],


        [[[-0.0303, -0.0070, -0.0330],
          [-0.0695, -0.0435, -0.0696],
          [-0.0558, -0.0329, -0.0455]],

         [[-0.0096,  0.0191, -0.0021],
          [-0.0447, -0.0130, -0.0314],
          [-0.0412, -0.0115, -0.0133]],

         [[ 0.0239,  0.0476,  0.0394],
          [-0.0026,  0.0244,  0.0144],
          [ 0.0042,  0.0304,  0.0337]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0032]], device='cuda:0')

percentage_weight_grad tensor([[-0.0000]], device='cuda:0')

Epoch: 41 | Batch_idx: 0 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (1179/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (2253/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4743) |  Loss2: (0.0000) | Acc: (83.00%) (3321/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4848) |  Loss2: (0.0000) | Acc: (83.00%) (4368/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4871) |  Loss2: (0.0000) | Acc: (83.00%) (5435/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4830) |  Loss2: (0.0000) | Acc: (83.00%) (6517/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (7582/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4826) |  Loss2: (0.0000) | Acc: (83.00%) (8662/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4793) |  Loss2: (0.0000) | Acc: (83.00%) (9752/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4829) |  Loss2: (0.0000) | Acc: (83.00%) (10800/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4844) |  Loss2: (0.0000) | Acc: (83.00%) (11866/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (12897/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4876) |  Loss2: (0.0000) | Acc: (83.00%) (13966/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4866) |  Loss2: (0.0000) | Acc: (83.00%) (15041/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4876) |  Loss2: (0.0000) | Acc: (83.00%) (16102/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4880) |  Loss2: (0.0000) | Acc: (83.00%) (17163/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4884) |  Loss2: (0.0000) | Acc: (83.00%) (18222/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4889) |  Loss2: (0.0000) | Acc: (83.00%) (19279/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (20324/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4890) |  Loss2: (0.0000) | Acc: (83.00%) (21410/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (83.00%) (22455/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (23505/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (24549/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4930) |  Loss2: (0.0000) | Acc: (83.00%) (25609/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4937) |  Loss2: (0.0000) | Acc: (82.00%) (26660/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (27746/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4908) |  Loss2: (0.0000) | Acc: (83.00%) (28840/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (83.00%) (29911/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (30967/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4887) |  Loss2: (0.0000) | Acc: (83.00%) (32041/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (33096/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4900) |  Loss2: (0.0000) | Acc: (83.00%) (34138/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (35216/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4902) |  Loss2: (0.0000) | Acc: (83.00%) (36272/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (83.00%) (37352/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4897) |  Loss2: (0.0000) | Acc: (83.00%) (38402/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (39470/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (83.00%) (40536/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4888) |  Loss2: (0.0000) | Acc: (83.00%) (41557/50000)
# TEST : Loss: (0.6187) | Acc: (79.00%) (7928/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1505,  0.0622,  0.0777],
          [ 0.0855, -0.2361, -0.0915],
          [ 0.2054, -0.1143,  0.0266]],

         [[ 0.0098, -0.1112,  0.2368],
          [ 0.0188, -0.0580,  0.0391],
          [ 0.0492, -0.0083, -0.0558]],

         [[ 0.0280, -0.0721,  0.1468],
          [ 0.1143, -0.1548,  0.1369],
          [-0.0878,  0.0445, -0.1878]]],


        [[[ 0.1076, -0.0769, -0.0036],
          [ 0.1437,  0.1766,  0.1789],
          [ 0.0259, -0.0518,  0.1249]],

         [[-0.1641, -0.1877,  0.1597],
          [ 0.0269, -0.0531,  0.1015],
          [ 0.1345, -0.1943,  0.1413]],

         [[ 0.0996, -0.0462, -0.0096],
          [ 0.0118,  0.0325, -0.1154],
          [-0.0277, -0.1442, -0.0049]]],


        [[[-0.0632,  0.0979, -0.0395],
          [-0.1167,  0.0710,  0.1222],
          [ 0.1243,  0.1736,  0.0050]],

         [[ 0.1025,  0.1330,  0.1118],
          [ 0.0747, -0.0418, -0.0397],
          [-0.0419, -0.0308,  0.1425]],

         [[-0.0482,  0.1446,  0.0721],
          [-0.0416, -0.0479, -0.0626],
          [-0.0374, -0.0083,  0.0453]]],


        ...,


        [[[ 0.2238, -0.1156, -0.1101],
          [-0.0418, -0.1001, -0.0980],
          [-0.1006,  0.0193,  0.0529]],

         [[ 0.1676,  0.1892, -0.1180],
          [ 0.2105, -0.1404, -0.1197],
          [-0.0737, -0.0878, -0.0434]],

         [[ 0.1935,  0.1069, -0.0920],
          [-0.0723, -0.1529, -0.0664],
          [ 0.0557,  0.1771, -0.1322]]],


        [[[-0.1231,  0.0775, -0.1673],
          [-0.1451, -0.1460,  0.1189],
          [-0.1644,  0.1505, -0.1298]],

         [[ 0.0868, -0.1295,  0.0721],
          [ 0.1572,  0.0103, -0.0851],
          [-0.1539,  0.1164, -0.1926]],

         [[ 0.1111, -0.0354, -0.0179],
          [ 0.1359, -0.1826,  0.0048],
          [-0.0399,  0.0102, -0.1237]]],


        [[[ 0.0670,  0.1248,  0.0838],
          [-0.1000, -0.1521,  0.1937],
          [-0.1621,  0.0013, -0.0454]],

         [[-0.0347, -0.0984,  0.1421],
          [-0.0194, -0.1833,  0.0357],
          [-0.0087,  0.0742,  0.0402]],

         [[-0.1492, -0.1902,  0.1488],
          [ 0.1410, -0.1632, -0.0360],
          [-0.0087, -0.0498,  0.2672]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1063,  0.1161,  0.1831],
          [ 0.0154,  0.0553,  0.1220],
          [-0.0229,  0.0162,  0.0895]],

         [[ 0.0455,  0.0488,  0.0870],
          [-0.0476, -0.0146,  0.0306],
          [-0.0978, -0.0622,  0.0118]],

         [[ 0.0174,  0.0247,  0.0534],
          [-0.0580, -0.0242,  0.0231],
          [-0.0938, -0.0550,  0.0168]]],


        [[[ 0.0124,  0.0081, -0.0092],
          [ 0.0152,  0.0101, -0.0082],
          [ 0.0296,  0.0230, -0.0026]],

         [[ 0.0034, -0.0019, -0.0209],
          [ 0.0034, -0.0001, -0.0215],
          [ 0.0144,  0.0108, -0.0165]],

         [[-0.0067, -0.0151, -0.0349],
          [-0.0109, -0.0169, -0.0382],
          [-0.0029, -0.0090, -0.0331]]],


        [[[ 0.0007,  0.0012,  0.0027],
          [ 0.0020,  0.0019,  0.0037],
          [ 0.0023,  0.0025,  0.0048]],

         [[-0.0049, -0.0032, -0.0015],
          [-0.0019, -0.0015,  0.0000],
          [-0.0001,  0.0004,  0.0024]],

         [[-0.0066, -0.0045, -0.0026],
          [-0.0026, -0.0017, -0.0002],
          [ 0.0005,  0.0012,  0.0029]]],


        ...,


        [[[ 0.1021,  0.0901,  0.0715],
          [ 0.1007,  0.0881,  0.0771],
          [ 0.0849,  0.0780,  0.0706]],

         [[ 0.0911,  0.0810,  0.0617],
          [ 0.0823,  0.0715,  0.0612],
          [ 0.0605,  0.0552,  0.0464]],

         [[ 0.1080,  0.0969,  0.0798],
          [ 0.0958,  0.0855,  0.0748],
          [ 0.0692,  0.0658,  0.0564]]],


        [[[-0.0087, -0.0043, -0.0042],
          [-0.0039, -0.0043, -0.0100],
          [ 0.0057,  0.0080, -0.0021]],

         [[-0.0108, -0.0081, -0.0064],
          [-0.0040, -0.0067, -0.0114],
          [ 0.0066,  0.0076, -0.0018]],

         [[-0.0022, -0.0008,  0.0015],
          [ 0.0080,  0.0045,  0.0008],
          [ 0.0188,  0.0202,  0.0126]]],


        [[[-0.0212, -0.0481, -0.0199],
          [ 0.0201,  0.0173,  0.0392],
          [ 0.0805,  0.0759,  0.0994]],

         [[ 0.0036, -0.0242,  0.0009],
          [ 0.0301,  0.0288,  0.0539],
          [ 0.0737,  0.0720,  0.1043]],

         [[ 0.0042, -0.0146,  0.0056],
          [ 0.0111,  0.0170,  0.0389],
          [ 0.0381,  0.0407,  0.0681]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0032]], device='cuda:0')

percentage_weight_grad tensor([[-0.0000]], device='cuda:0')

Epoch: 42 | Batch_idx: 0 |  Loss: (0.5916) |  Loss2: (0.0000) | Acc: (79.00%) (102/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4853) |  Loss2: (0.0000) | Acc: (83.00%) (1172/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4747) |  Loss2: (0.0000) | Acc: (83.00%) (2235/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4705) |  Loss2: (0.0000) | Acc: (83.00%) (3319/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4738) |  Loss2: (0.0000) | Acc: (83.00%) (4383/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (5461/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4706) |  Loss2: (0.0000) | Acc: (83.00%) (6527/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4675) |  Loss2: (0.0000) | Acc: (83.00%) (7607/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (8687/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4677) |  Loss2: (0.0000) | Acc: (83.00%) (9757/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4674) |  Loss2: (0.0000) | Acc: (83.00%) (10841/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4679) |  Loss2: (0.0000) | Acc: (83.00%) (11914/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4691) |  Loss2: (0.0000) | Acc: (83.00%) (12980/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4640) |  Loss2: (0.0000) | Acc: (83.00%) (14082/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4651) |  Loss2: (0.0000) | Acc: (83.00%) (15159/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4646) |  Loss2: (0.0000) | Acc: (84.00%) (16247/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (84.00%) (17325/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4630) |  Loss2: (0.0000) | Acc: (84.00%) (18401/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4644) |  Loss2: (0.0000) | Acc: (84.00%) (19486/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4664) |  Loss2: (0.0000) | Acc: (84.00%) (20548/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4669) |  Loss2: (0.0000) | Acc: (84.00%) (21640/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4695) |  Loss2: (0.0000) | Acc: (84.00%) (22689/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (83.00%) (23759/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4693) |  Loss2: (0.0000) | Acc: (84.00%) (24842/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4690) |  Loss2: (0.0000) | Acc: (84.00%) (25927/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4688) |  Loss2: (0.0000) | Acc: (84.00%) (27007/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4696) |  Loss2: (0.0000) | Acc: (84.00%) (28076/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4695) |  Loss2: (0.0000) | Acc: (84.00%) (29147/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4678) |  Loss2: (0.0000) | Acc: (84.00%) (30245/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4685) |  Loss2: (0.0000) | Acc: (84.00%) (31304/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4684) |  Loss2: (0.0000) | Acc: (84.00%) (32375/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4677) |  Loss2: (0.0000) | Acc: (84.00%) (33461/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (34512/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4680) |  Loss2: (0.0000) | Acc: (84.00%) (35601/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4669) |  Loss2: (0.0000) | Acc: (84.00%) (36701/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4662) |  Loss2: (0.0000) | Acc: (84.00%) (37782/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4665) |  Loss2: (0.0000) | Acc: (84.00%) (38834/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4655) |  Loss2: (0.0000) | Acc: (84.00%) (39923/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4657) |  Loss2: (0.0000) | Acc: (84.00%) (40987/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (84.00%) (42005/50000)
# TEST : Loss: (0.5577) | Acc: (81.00%) (8124/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1494,  0.0654,  0.0828],
          [ 0.0868, -0.2392, -0.0927],
          [ 0.2103, -0.1109,  0.0267]],

         [[ 0.0066, -0.1103,  0.2390],
          [ 0.0153, -0.0639,  0.0345],
          [ 0.0488, -0.0092, -0.0599]],

         [[ 0.0241, -0.0708,  0.1504],
          [ 0.1109, -0.1585,  0.1362],
          [-0.0876,  0.0454, -0.1875]]],


        [[[ 0.1053, -0.0779, -0.0032],
          [ 0.1419,  0.1766,  0.1809],
          [ 0.0224, -0.0529,  0.1245]],

         [[-0.1656, -0.1881,  0.1600],
          [ 0.0260, -0.0526,  0.1036],
          [ 0.1315, -0.1951,  0.1413]],

         [[ 0.0989, -0.0467, -0.0093],
          [ 0.0111,  0.0326, -0.1136],
          [-0.0299, -0.1450, -0.0047]]],


        [[[-0.0641,  0.0972, -0.0399],
          [-0.1179,  0.0702,  0.1219],
          [ 0.1229,  0.1727,  0.0047]],

         [[ 0.1022,  0.1333,  0.1125],
          [ 0.0738, -0.0420, -0.0392],
          [-0.0431, -0.0313,  0.1425]],

         [[-0.0479,  0.1452,  0.0730],
          [-0.0420, -0.0479, -0.0620],
          [-0.0381, -0.0086,  0.0453]]],


        ...,


        [[[ 0.2248, -0.1148, -0.1085],
          [-0.0423, -0.1004, -0.0961],
          [-0.1017,  0.0192,  0.0557]],

         [[ 0.1694,  0.1903, -0.1166],
          [ 0.2114, -0.1396, -0.1177],
          [-0.0730, -0.0869, -0.0405]],

         [[ 0.1958,  0.1085, -0.0909],
          [-0.0703, -0.1515, -0.0647],
          [ 0.0576,  0.1785, -0.1298]]],


        [[[-0.1222,  0.0789, -0.1665],
          [-0.1445, -0.1449,  0.1191],
          [-0.1646,  0.1508, -0.1303]],

         [[ 0.0863, -0.1292,  0.0718],
          [ 0.1563,  0.0101, -0.0858],
          [-0.1554,  0.1157, -0.1940]],

         [[ 0.1108, -0.0348, -0.0176],
          [ 0.1357, -0.1819,  0.0051],
          [-0.0406,  0.0103, -0.1241]]],


        [[[ 0.0686,  0.1282,  0.0894],
          [-0.0984, -0.1518,  0.1980],
          [-0.1643, -0.0025, -0.0481]],

         [[-0.0330, -0.0961,  0.1459],
          [-0.0170, -0.1826,  0.0403],
          [-0.0093,  0.0723,  0.0389]],

         [[-0.1474, -0.1882,  0.1528],
          [ 0.1425, -0.1629, -0.0315],
          [-0.0098, -0.0513,  0.2661]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0105,  0.0353,  0.0433],
          [-0.0665, -0.0114, -0.0338],
          [-0.1167, -0.0832, -0.1090]],

         [[-0.0084,  0.0192,  0.0163],
          [-0.0544, -0.0216, -0.0602],
          [-0.0943, -0.0855, -0.1297]],

         [[ 0.1172,  0.1288,  0.1035],
          [ 0.0987,  0.1107,  0.0582],
          [ 0.0715,  0.0626,  0.0046]]],


        [[[-0.0163, -0.0120, -0.0041],
          [-0.0181, -0.0152, -0.0070],
          [-0.0170, -0.0152, -0.0081]],

         [[-0.0157, -0.0112, -0.0017],
          [-0.0165, -0.0130, -0.0029],
          [-0.0136, -0.0113, -0.0026]],

         [[-0.0323, -0.0286, -0.0175],
          [-0.0324, -0.0293, -0.0170],
          [-0.0283, -0.0251, -0.0147]]],


        [[[ 0.0003,  0.0016,  0.0016],
          [-0.0004,  0.0013,  0.0005],
          [-0.0037, -0.0004, -0.0006]],

         [[ 0.0004,  0.0012,  0.0012],
          [ 0.0005,  0.0014, -0.0000],
          [-0.0024,  0.0002, -0.0007]],

         [[ 0.0003,  0.0000, -0.0001],
          [ 0.0009,  0.0003, -0.0015],
          [-0.0011, -0.0002, -0.0020]]],


        ...,


        [[[-0.0373, -0.0476, -0.0390],
          [-0.0126, -0.0219, -0.0261],
          [-0.0130, -0.0236, -0.0387]],

         [[-0.0353, -0.0454, -0.0371],
          [-0.0085, -0.0205, -0.0264],
          [-0.0083, -0.0212, -0.0384]],

         [[-0.0257, -0.0295, -0.0180],
          [-0.0035, -0.0118, -0.0126],
          [-0.0030, -0.0147, -0.0259]]],


        [[[ 0.0173,  0.0140,  0.0111],
          [ 0.0184,  0.0160,  0.0078],
          [ 0.0032,  0.0042, -0.0018]],

         [[ 0.0052,  0.0004, -0.0020],
          [ 0.0043,  0.0016, -0.0051],
          [-0.0113, -0.0111, -0.0156]],

         [[ 0.0137,  0.0095,  0.0085],
          [ 0.0138,  0.0107,  0.0047],
          [ 0.0014, -0.0008, -0.0064]]],


        [[[-0.0873, -0.0695, -0.0767],
          [-0.0843, -0.0818, -0.0853],
          [-0.1083, -0.1365, -0.1184]],

         [[-0.0744, -0.0582, -0.0721],
          [-0.0517, -0.0549, -0.0658],
          [-0.0551, -0.0886, -0.0845]],

         [[-0.0035,  0.0158,  0.0064],
          [ 0.0103,  0.0147,  0.0107],
          [ 0.0091, -0.0127, -0.0019]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0032]], device='cuda:0')

percentage_weight_grad tensor([[-0.0000]], device='cuda:0')

Epoch: 43 | Batch_idx: 0 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (85.00%) (1197/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (85.00%) (2299/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4281) |  Loss2: (0.0000) | Acc: (85.00%) (3394/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (85.00%) (4468/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (5574/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (85.00%) (6644/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (85.00%) (7739/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (85.00%) (8836/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (9896/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (10964/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4460) |  Loss2: (0.0000) | Acc: (84.00%) (12029/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (13119/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (14190/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4478) |  Loss2: (0.0000) | Acc: (84.00%) (15266/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (16336/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (17405/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (18481/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (19558/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (20626/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4515) |  Loss2: (0.0000) | Acc: (84.00%) (21728/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4513) |  Loss2: (0.0000) | Acc: (84.00%) (22810/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (23887/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (24961/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4511) |  Loss2: (0.0000) | Acc: (84.00%) (26054/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (27121/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4511) |  Loss2: (0.0000) | Acc: (84.00%) (28219/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (29291/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (30372/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (31433/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4534) |  Loss2: (0.0000) | Acc: (84.00%) (32511/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4525) |  Loss2: (0.0000) | Acc: (84.00%) (33605/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (34677/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (35750/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4531) |  Loss2: (0.0000) | Acc: (84.00%) (36830/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4526) |  Loss2: (0.0000) | Acc: (84.00%) (37913/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (84.00%) (38973/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (40059/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4527) |  Loss2: (0.0000) | Acc: (84.00%) (41148/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4529) |  Loss2: (0.0000) | Acc: (84.00%) (42180/50000)
# TEST : Loss: (0.6960) | Acc: (76.00%) (7669/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1491,  0.0614,  0.0816],
          [ 0.0869, -0.2442, -0.0936],
          [ 0.2111, -0.1109,  0.0264]],

         [[ 0.0084, -0.1118,  0.2407],
          [ 0.0166, -0.0672,  0.0357],
          [ 0.0498, -0.0097, -0.0603]],

         [[ 0.0230, -0.0742,  0.1513],
          [ 0.1086, -0.1649,  0.1359],
          [-0.0898,  0.0415, -0.1882]]],


        [[[ 0.1033, -0.0784, -0.0041],
          [ 0.1409,  0.1766,  0.1814],
          [ 0.0211, -0.0531,  0.1245]],

         [[-0.1667, -0.1884,  0.1592],
          [ 0.0255, -0.0522,  0.1043],
          [ 0.1306, -0.1947,  0.1419]],

         [[ 0.0971, -0.0482, -0.0113],
          [ 0.0099,  0.0314, -0.1142],
          [-0.0315, -0.1461, -0.0055]]],


        [[[-0.0648,  0.0969, -0.0399],
          [-0.1185,  0.0701,  0.1215],
          [ 0.1223,  0.1727,  0.0043]],

         [[ 0.1020,  0.1335,  0.1128],
          [ 0.0737, -0.0415, -0.0390],
          [-0.0432, -0.0309,  0.1424]],

         [[-0.0476,  0.1458,  0.0735],
          [-0.0414, -0.0470, -0.0616],
          [-0.0375, -0.0076,  0.0456]]],


        ...,


        [[[ 0.2228, -0.1159, -0.1105],
          [-0.0431, -0.1017, -0.0986],
          [-0.1017,  0.0183,  0.0537]],

         [[ 0.1682,  0.1902, -0.1174],
          [ 0.2115, -0.1396, -0.1189],
          [-0.0719, -0.0863, -0.0408]],

         [[ 0.1937,  0.1073, -0.0926],
          [-0.0714, -0.1527, -0.0670],
          [ 0.0570,  0.1775, -0.1312]]],


        [[[-0.1207,  0.0799, -0.1651],
          [-0.1443, -0.1455,  0.1189],
          [-0.1631,  0.1516, -0.1300]],

         [[ 0.0875, -0.1288,  0.0724],
          [ 0.1564,  0.0088, -0.0864],
          [-0.1541,  0.1160, -0.1942]],

         [[ 0.1112, -0.0354, -0.0179],
          [ 0.1352, -0.1836,  0.0040],
          [-0.0399,  0.0102, -0.1246]]],


        [[[ 0.0711,  0.1314,  0.0903],
          [-0.0991, -0.1522,  0.1971],
          [-0.1641, -0.0020, -0.0496]],

         [[-0.0329, -0.0957,  0.1450],
          [-0.0191, -0.1838,  0.0393],
          [-0.0097,  0.0733,  0.0384]],

         [[-0.1508, -0.1908,  0.1496],
          [ 0.1383, -0.1653, -0.0325],
          [-0.0111, -0.0500,  0.2669]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1037,  0.0975,  0.0981],
          [ 0.1082,  0.0841,  0.0883],
          [ 0.1414,  0.1224,  0.1413]],

         [[ 0.0653,  0.0563,  0.0509],
          [ 0.0613,  0.0363,  0.0308],
          [ 0.0861,  0.0640,  0.0733]],

         [[ 0.0215,  0.0064, -0.0188],
          [ 0.0020, -0.0267, -0.0525],
          [ 0.0255, -0.0034, -0.0167]]],


        [[[-0.0019, -0.0072, -0.0338],
          [ 0.0013,  0.0026, -0.0240],
          [ 0.0014,  0.0071, -0.0094]],

         [[-0.0311, -0.0358, -0.0619],
          [-0.0256, -0.0240, -0.0503],
          [-0.0238, -0.0183, -0.0357]],

         [[-0.0429, -0.0479, -0.0732],
          [-0.0411, -0.0420, -0.0693],
          [-0.0389, -0.0356, -0.0557]]],


        [[[-0.0046, -0.0034, -0.0024],
          [-0.0072, -0.0047, -0.0036],
          [-0.0055, -0.0030, -0.0029]],

         [[-0.0010,  0.0005,  0.0017],
          [-0.0047, -0.0019, -0.0009],
          [-0.0034, -0.0005,  0.0000]],

         [[ 0.0132,  0.0146,  0.0163],
          [ 0.0102,  0.0126,  0.0139],
          [ 0.0110,  0.0139,  0.0146]]],


        ...,


        [[[ 0.0569,  0.0483,  0.0384],
          [ 0.0545,  0.0519,  0.0478],
          [ 0.0331,  0.0427,  0.0505]],

         [[ 0.0433,  0.0319,  0.0215],
          [ 0.0326,  0.0279,  0.0237],
          [ 0.0034,  0.0116,  0.0201]],

         [[ 0.0609,  0.0495,  0.0432],
          [ 0.0449,  0.0406,  0.0427],
          [ 0.0101,  0.0190,  0.0331]]],


        [[[-0.0052, -0.0120, -0.0075],
          [-0.0034, -0.0094, -0.0019],
          [-0.0049, -0.0125, -0.0044]],

         [[ 0.0078,  0.0025,  0.0095],
          [ 0.0064,  0.0019,  0.0130],
          [ 0.0032, -0.0028,  0.0091]],

         [[ 0.0141,  0.0106,  0.0187],
          [ 0.0072,  0.0035,  0.0154],
          [ 0.0040, -0.0009,  0.0115]]],


        [[[ 0.0450,  0.0373,  0.0237],
          [ 0.0563,  0.0766,  0.0814],
          [ 0.0545,  0.0889,  0.1118]],

         [[ 0.0303,  0.0282,  0.0238],
          [ 0.0411,  0.0630,  0.0717],
          [ 0.0310,  0.0648,  0.0894]],

         [[ 0.0392,  0.0498,  0.0487],
          [ 0.0568,  0.0822,  0.0881],
          [ 0.0588,  0.0908,  0.1105]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0032]], device='cuda:0')

percentage_weight_grad tensor([[-0.0000]], device='cuda:0')

Epoch: 44 | Batch_idx: 0 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4627) |  Loss2: (0.0000) | Acc: (83.00%) (1174/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (2271/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4464) |  Loss2: (0.0000) | Acc: (84.00%) (3355/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4483) |  Loss2: (0.0000) | Acc: (84.00%) (4435/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4494) |  Loss2: (0.0000) | Acc: (84.00%) (5521/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (84.00%) (6617/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (7718/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (8791/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (9895/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (10986/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4399) |  Loss2: (0.0000) | Acc: (84.00%) (12060/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (13140/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (14225/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (15331/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (16420/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4432) |  Loss2: (0.0000) | Acc: (84.00%) (17495/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (18575/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (19678/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (20766/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (21857/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (22918/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (24003/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (25071/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (26147/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (27242/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (28342/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (29425/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (30497/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4454) |  Loss2: (0.0000) | Acc: (84.00%) (31566/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4460) |  Loss2: (0.0000) | Acc: (84.00%) (32643/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (33744/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (34817/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (35922/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (37006/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (38086/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (39159/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (40276/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (41357/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4437) |  Loss2: (0.0000) | Acc: (84.00%) (42387/50000)
# TEST : Loss: (0.5701) | Acc: (80.00%) (8061/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1482,  0.0625,  0.0840],
          [ 0.0900, -0.2453, -0.0935],
          [ 0.2163, -0.1111,  0.0231]],

         [[ 0.0080, -0.1107,  0.2423],
          [ 0.0187, -0.0679,  0.0369],
          [ 0.0530, -0.0105, -0.0626]],

         [[ 0.0233, -0.0736,  0.1520],
          [ 0.1102, -0.1666,  0.1363],
          [-0.0886,  0.0386, -0.1909]]],


        [[[ 0.1005, -0.0800, -0.0046],
          [ 0.1388,  0.1754,  0.1818],
          [ 0.0190, -0.0545,  0.1243]],

         [[-0.1687, -0.1892,  0.1596],
          [ 0.0241, -0.0529,  0.1055],
          [ 0.1294, -0.1953,  0.1427]],

         [[ 0.0947, -0.0495, -0.0115],
          [ 0.0081,  0.0302, -0.1136],
          [-0.0326, -0.1469, -0.0050]]],


        [[[-0.0654,  0.0965, -0.0401],
          [-0.1191,  0.0697,  0.1214],
          [ 0.1214,  0.1722,  0.0043]],

         [[ 0.1019,  0.1336,  0.1130],
          [ 0.0733, -0.0416, -0.0389],
          [-0.0439, -0.0312,  0.1423]],

         [[-0.0475,  0.1458,  0.0736],
          [-0.0416, -0.0471, -0.0617],
          [-0.0379, -0.0080,  0.0451]]],


        ...,


        [[[ 0.2255, -0.1141, -0.1095],
          [-0.0423, -0.1011, -0.0976],
          [-0.0999,  0.0199,  0.0552]],

         [[ 0.1681,  0.1903, -0.1175],
          [ 0.2100, -0.1403, -0.1195],
          [-0.0726, -0.0867, -0.0416]],

         [[ 0.1929,  0.1072, -0.0924],
          [-0.0729, -0.1532, -0.0676],
          [ 0.0566,  0.1773, -0.1318]]],


        [[[-0.1209,  0.0807, -0.1634],
          [-0.1454, -0.1454,  0.1198],
          [-0.1641,  0.1516, -0.1298]],

         [[ 0.0871, -0.1281,  0.0737],
          [ 0.1550,  0.0086, -0.0858],
          [-0.1553,  0.1154, -0.1945]],

         [[ 0.1125, -0.0333, -0.0152],
          [ 0.1357, -0.1822,  0.0059],
          [-0.0395,  0.0108, -0.1239]]],


        [[[ 0.0699,  0.1329,  0.0946],
          [-0.1004, -0.1547,  0.1963],
          [-0.1661, -0.0057, -0.0545]],

         [[-0.0334, -0.0954,  0.1484],
          [-0.0195, -0.1864,  0.0386],
          [-0.0109,  0.0697,  0.0335]],

         [[-0.1482, -0.1877,  0.1545],
          [ 0.1405, -0.1653, -0.0307],
          [-0.0100, -0.0509,  0.2648]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0413,  0.0473,  0.0550],
          [ 0.0411,  0.0458,  0.0451],
          [ 0.0323,  0.0392,  0.0321]],

         [[ 0.0309,  0.0434,  0.0493],
          [ 0.0229,  0.0368,  0.0409],
          [ 0.0167,  0.0294,  0.0301]],

         [[ 0.0130,  0.0199,  0.0296],
          [ 0.0091,  0.0194,  0.0261],
          [ 0.0044,  0.0154,  0.0185]]],


        [[[ 0.0033,  0.0102,  0.0215],
          [ 0.0027,  0.0062,  0.0149],
          [ 0.0130,  0.0149,  0.0217]],

         [[ 0.0108,  0.0209,  0.0360],
          [ 0.0102,  0.0182,  0.0312],
          [ 0.0217,  0.0280,  0.0380]],

         [[ 0.0231,  0.0315,  0.0465],
          [ 0.0230,  0.0303,  0.0431],
          [ 0.0342,  0.0405,  0.0502]]],


        [[[ 0.0040,  0.0035,  0.0045],
          [ 0.0011, -0.0003,  0.0004],
          [-0.0015, -0.0030, -0.0024]],

         [[ 0.0014,  0.0012,  0.0024],
          [-0.0020, -0.0032, -0.0021],
          [-0.0047, -0.0059, -0.0049]],

         [[-0.0032, -0.0031, -0.0015],
          [-0.0063, -0.0074, -0.0061],
          [-0.0081, -0.0094, -0.0084]]],


        ...,


        [[[ 0.0206,  0.0173,  0.0141],
          [ 0.0179,  0.0160,  0.0097],
          [ 0.0195,  0.0238,  0.0153]],

         [[ 0.0180,  0.0167,  0.0168],
          [ 0.0178,  0.0166,  0.0113],
          [ 0.0237,  0.0268,  0.0185]],

         [[ 0.0167,  0.0131,  0.0110],
          [ 0.0158,  0.0142,  0.0069],
          [ 0.0198,  0.0227,  0.0118]]],


        [[[ 0.0012, -0.0010, -0.0033],
          [ 0.0055,  0.0000, -0.0018],
          [ 0.0122,  0.0093,  0.0046]],

         [[-0.0043, -0.0066, -0.0080],
          [-0.0008, -0.0055, -0.0064],
          [ 0.0046,  0.0022, -0.0012]],

         [[-0.0031, -0.0073, -0.0104],
          [-0.0001, -0.0074, -0.0099],
          [ 0.0044, -0.0012, -0.0060]]],


        [[[ 0.0700,  0.0588,  0.0529],
          [ 0.0410,  0.0195,  0.0168],
          [ 0.0262,  0.0058, -0.0009]],

         [[ 0.0670,  0.0541,  0.0527],
          [ 0.0421,  0.0178,  0.0203],
          [ 0.0297,  0.0073,  0.0051]],

         [[ 0.0426,  0.0315,  0.0326],
          [ 0.0191, -0.0021,  0.0022],
          [ 0.0095, -0.0111, -0.0124]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[-0.0032]], device='cuda:0')

percentage_weight_grad tensor([[-0.0000]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.7897) |  Loss2: (0.3522) | Acc: (87.00%) (112/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.7912) |  Loss2: (0.3522) | Acc: (85.00%) (1197/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.8501) |  Loss2: (0.3521) | Acc: (82.00%) (2216/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.8522) |  Loss2: (0.3521) | Acc: (82.00%) (3266/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.8683) |  Loss2: (0.3521) | Acc: (81.00%) (4282/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.8825) |  Loss2: (0.3520) | Acc: (81.00%) (5300/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.8954) |  Loss2: (0.3520) | Acc: (80.00%) (6316/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.8959) |  Loss2: (0.3520) | Acc: (80.00%) (7345/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.8926) |  Loss2: (0.3519) | Acc: (80.00%) (8395/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.8931) |  Loss2: (0.3519) | Acc: (81.00%) (9441/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.8943) |  Loss2: (0.3518) | Acc: (81.00%) (10476/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.8976) |  Loss2: (0.3518) | Acc: (80.00%) (11495/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.8976) |  Loss2: (0.3517) | Acc: (80.00%) (12528/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.8958) |  Loss2: (0.3517) | Acc: (80.00%) (13578/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.8985) |  Loss2: (0.3517) | Acc: (81.00%) (14620/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.8977) |  Loss2: (0.3516) | Acc: (81.00%) (15666/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.8957) |  Loss2: (0.3516) | Acc: (81.00%) (16732/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.8948) |  Loss2: (0.3515) | Acc: (81.00%) (17784/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.8910) |  Loss2: (0.3515) | Acc: (81.00%) (18845/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.8896) |  Loss2: (0.3514) | Acc: (81.00%) (19885/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.8893) |  Loss2: (0.3514) | Acc: (81.00%) (20934/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.8897) |  Loss2: (0.3513) | Acc: (81.00%) (21972/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.8877) |  Loss2: (0.3513) | Acc: (81.00%) (23029/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.8852) |  Loss2: (0.3513) | Acc: (81.00%) (24094/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.8848) |  Loss2: (0.3512) | Acc: (81.00%) (25131/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.8839) |  Loss2: (0.3512) | Acc: (81.00%) (26196/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.8828) |  Loss2: (0.3511) | Acc: (81.00%) (27258/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.8818) |  Loss2: (0.3511) | Acc: (81.00%) (28317/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.8815) |  Loss2: (0.3510) | Acc: (81.00%) (29373/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.8803) |  Loss2: (0.3510) | Acc: (81.00%) (30433/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.8780) |  Loss2: (0.3509) | Acc: (81.00%) (31499/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.8774) |  Loss2: (0.3509) | Acc: (81.00%) (32548/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.8753) |  Loss2: (0.3508) | Acc: (81.00%) (33612/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.8741) |  Loss2: (0.3508) | Acc: (81.00%) (34685/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.8745) |  Loss2: (0.3508) | Acc: (81.00%) (35728/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.8736) |  Loss2: (0.3507) | Acc: (81.00%) (36792/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.8729) |  Loss2: (0.3507) | Acc: (81.00%) (37845/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.8726) |  Loss2: (0.3506) | Acc: (81.00%) (38894/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.8726) |  Loss2: (0.3506) | Acc: (81.00%) (39939/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.8718) |  Loss2: (0.3505) | Acc: (81.00%) (40965/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_045.pth.tar'
# TEST : Loss: (0.5637) | Acc: (80.00%) (8062/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1481,  0.0626,  0.0837],
          [ 0.0896, -0.2456, -0.0943],
          [ 0.2159, -0.1117,  0.0219]],

         [[ 0.0072, -0.1113,  0.2412],
          [ 0.0178, -0.0688,  0.0355],
          [ 0.0522, -0.0116, -0.0642]],

         [[ 0.0228, -0.0742,  0.1508],
          [ 0.1096, -0.1674,  0.1350],
          [-0.0891,  0.0375, -0.1923]]],


        [[[ 0.1004, -0.0800, -0.0048],
          [ 0.1387,  0.1753,  0.1816],
          [ 0.0189, -0.0547,  0.1241]],

         [[-0.1686, -0.1891,  0.1594],
          [ 0.0241, -0.0529,  0.1053],
          [ 0.1292, -0.1954,  0.1424]],

         [[ 0.0945, -0.0497, -0.0118],
          [ 0.0080,  0.0300, -0.1138],
          [-0.0328, -0.1471, -0.0053]]],


        [[[-0.0654,  0.0965, -0.0402],
          [-0.1191,  0.0697,  0.1213],
          [ 0.1213,  0.1722,  0.0043]],

         [[ 0.1018,  0.1336,  0.1129],
          [ 0.0733, -0.0415, -0.0390],
          [-0.0439, -0.0312,  0.1422]],

         [[-0.0474,  0.1459,  0.0736],
          [-0.0415, -0.0470, -0.0616],
          [-0.0377, -0.0079,  0.0452]]],


        ...,


        [[[ 0.2255, -0.1141, -0.1097],
          [-0.0422, -0.1010, -0.0976],
          [-0.0996,  0.0201,  0.0553]],

         [[ 0.1683,  0.1903, -0.1174],
          [ 0.2101, -0.1401, -0.1194],
          [-0.0723, -0.0865, -0.0414]],

         [[ 0.1928,  0.1071, -0.0924],
          [-0.0729, -0.1532, -0.0675],
          [ 0.0565,  0.1773, -0.1317]]],


        [[[-0.1208,  0.0808, -0.1632],
          [-0.1454, -0.1453,  0.1198],
          [-0.1642,  0.1515, -0.1298]],

         [[ 0.0872, -0.1279,  0.0739],
          [ 0.1550,  0.0087, -0.0857],
          [-0.1553,  0.1153, -0.1945]],

         [[ 0.1125, -0.0332, -0.0152],
          [ 0.1356, -0.1821,  0.0059],
          [-0.0396,  0.0108, -0.1239]]],


        [[[ 0.0694,  0.1323,  0.0940],
          [-0.1006, -0.1551,  0.1957],
          [-0.1665, -0.0064, -0.0553]],

         [[-0.0343, -0.0964,  0.1474],
          [-0.0203, -0.1873,  0.0378],
          [-0.0120,  0.0684,  0.0323]],

         [[-0.1487, -0.1883,  0.1539],
          [ 0.1399, -0.1657, -0.0311],
          [-0.0108, -0.0517,  0.2641]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4808e-05,  6.2632e-06,  8.3722e-06],
          [ 8.9646e-06, -2.4560e-05, -9.4253e-06],
          [ 2.1594e-05, -1.1173e-05,  2.1893e-06]],

         [[ 7.1677e-07, -1.1128e-05,  2.4117e-05],
          [ 1.7778e-06, -6.8797e-06,  3.5520e-06],
          [ 5.2235e-06, -1.1569e-06, -6.4175e-06]],

         [[ 2.2780e-06, -7.4154e-06,  1.5076e-05],
          [ 1.0960e-05, -1.6741e-05,  1.3496e-05],
          [-8.9146e-06,  3.7546e-06, -1.9228e-05]]],


        [[[ 1.0039e-05, -8.0019e-06, -4.7933e-07],
          [ 1.3873e-05,  1.7531e-05,  1.8165e-05],
          [ 1.8874e-06, -5.4670e-06,  1.2409e-05]],

         [[-1.6860e-05, -1.8912e-05,  1.5936e-05],
          [ 2.4093e-06, -5.2897e-06,  1.0530e-05],
          [ 1.2922e-05, -1.9540e-05,  1.4237e-05]],

         [[ 9.4509e-06, -4.9652e-06, -1.1783e-06],
          [ 8.0049e-07,  3.0017e-06, -1.1378e-05],
          [-3.2829e-06, -1.4711e-05, -5.3394e-07]]],


        [[[-6.5414e-06,  9.6472e-06, -4.0159e-06],
          [-1.1910e-05,  6.9721e-06,  1.2133e-05],
          [ 1.2133e-05,  1.7220e-05,  4.3037e-07]],

         [[ 1.0179e-05,  1.3360e-05,  1.1289e-05],
          [ 7.3287e-06, -4.1521e-06, -3.8960e-06],
          [-4.3869e-06, -3.1179e-06,  1.4219e-05]],

         [[-4.7448e-06,  1.4589e-05,  7.3617e-06],
          [-4.1538e-06, -4.6997e-06, -6.1634e-06],
          [-3.7730e-06, -7.8541e-07,  4.5174e-06]]],


        ...,


        [[[ 2.2545e-05, -1.1413e-05, -1.0966e-05],
          [-4.2189e-06, -1.0099e-05, -9.7562e-06],
          [-9.9564e-06,  2.0116e-06,  5.5325e-06]],

         [[ 1.6825e-05,  1.9033e-05, -1.1743e-05],
          [ 2.1007e-05, -1.4012e-05, -1.1936e-05],
          [-7.2309e-06, -8.6463e-06, -4.1398e-06]],

         [[ 1.9278e-05,  1.0713e-05, -9.2408e-06],
          [-7.2944e-06, -1.5319e-05, -6.7514e-06],
          [ 5.6521e-06,  1.7730e-05, -1.3167e-05]]],


        [[[-1.2077e-05,  8.0756e-06, -1.6324e-05],
          [-1.4536e-05, -1.4526e-05,  1.1984e-05],
          [-1.6417e-05,  1.5147e-05, -1.2979e-05]],

         [[ 8.7175e-06, -1.2789e-05,  7.3885e-06],
          [ 1.5499e-05,  8.6856e-07, -8.5662e-06],
          [-1.5531e-05,  1.1531e-05, -1.9449e-05]],

         [[ 1.1252e-05, -3.3213e-06, -1.5157e-06],
          [ 1.3563e-05, -1.8208e-05,  5.9390e-07],
          [-3.9590e-06,  1.0796e-06, -1.2392e-05]]],


        [[[ 6.9363e-06,  1.3231e-05,  9.4025e-06],
          [-1.0062e-05, -1.5515e-05,  1.9574e-05],
          [-1.6649e-05, -6.3953e-07, -5.5289e-06]],

         [[-3.4305e-06, -9.6356e-06,  1.4744e-05],
          [-2.0340e-06, -1.8730e-05,  3.7763e-06],
          [-1.2043e-06,  6.8403e-06,  3.2257e-06]],

         [[-1.4872e-05, -1.8827e-05,  1.5388e-05],
          [ 1.3988e-05, -1.6570e-05, -3.1083e-06],
          [-1.0831e-06, -5.1668e-06,  2.6412e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1781]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0021]], device='cuda:0')

Epoch: 46 | Batch_idx: 0 |  Loss: (0.9209) |  Loss2: (0.3487) | Acc: (78.00%) (101/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.8922) |  Loss2: (0.3487) | Acc: (80.00%) (1137/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.8537) |  Loss2: (0.3486) | Acc: (82.00%) (2212/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.8351) |  Loss2: (0.3486) | Acc: (82.00%) (3293/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.8341) |  Loss2: (0.3485) | Acc: (83.00%) (4356/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.8446) |  Loss2: (0.3485) | Acc: (82.00%) (5400/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.8450) |  Loss2: (0.3484) | Acc: (82.00%) (6464/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.8436) |  Loss2: (0.3484) | Acc: (82.00%) (7542/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.8426) |  Loss2: (0.3484) | Acc: (82.00%) (8593/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.8376) |  Loss2: (0.3483) | Acc: (83.00%) (9676/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.8379) |  Loss2: (0.3483) | Acc: (83.00%) (10743/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.8367) |  Loss2: (0.3482) | Acc: (83.00%) (11802/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.8339) |  Loss2: (0.3482) | Acc: (83.00%) (12887/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.8313) |  Loss2: (0.3482) | Acc: (83.00%) (13975/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.8287) |  Loss2: (0.3481) | Acc: (83.00%) (15070/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.8269) |  Loss2: (0.3481) | Acc: (83.00%) (16146/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.8272) |  Loss2: (0.3480) | Acc: (83.00%) (17205/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.8274) |  Loss2: (0.3480) | Acc: (83.00%) (18268/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.8271) |  Loss2: (0.3480) | Acc: (83.00%) (19342/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.8272) |  Loss2: (0.3479) | Acc: (83.00%) (20404/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.8268) |  Loss2: (0.3479) | Acc: (83.00%) (21468/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.8257) |  Loss2: (0.3478) | Acc: (83.00%) (22544/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.8233) |  Loss2: (0.3478) | Acc: (83.00%) (23623/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.8243) |  Loss2: (0.3478) | Acc: (83.00%) (24685/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.8230) |  Loss2: (0.3477) | Acc: (83.00%) (25754/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.8238) |  Loss2: (0.3477) | Acc: (83.00%) (26820/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.8233) |  Loss2: (0.3476) | Acc: (83.00%) (27900/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.8241) |  Loss2: (0.3476) | Acc: (83.00%) (28956/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.8242) |  Loss2: (0.3476) | Acc: (83.00%) (30033/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.8238) |  Loss2: (0.3475) | Acc: (83.00%) (31118/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.8232) |  Loss2: (0.3475) | Acc: (83.00%) (32189/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.8221) |  Loss2: (0.3474) | Acc: (83.00%) (33286/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.8201) |  Loss2: (0.3474) | Acc: (83.00%) (34386/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.8202) |  Loss2: (0.3473) | Acc: (83.00%) (35454/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.8195) |  Loss2: (0.3473) | Acc: (83.00%) (36536/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.8178) |  Loss2: (0.3473) | Acc: (83.00%) (37636/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.8186) |  Loss2: (0.3472) | Acc: (83.00%) (38684/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.8184) |  Loss2: (0.3472) | Acc: (83.00%) (39752/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.8176) |  Loss2: (0.3471) | Acc: (83.00%) (40835/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.8174) |  Loss2: (0.3471) | Acc: (83.00%) (41874/50000)
# TEST : Loss: (0.5315) | Acc: (81.00%) (8169/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1480,  0.0626,  0.0837],
          [ 0.0896, -0.2455, -0.0942],
          [ 0.2159, -0.1117,  0.0219]],

         [[ 0.0072, -0.1112,  0.2411],
          [ 0.0178, -0.0688,  0.0355],
          [ 0.0522, -0.0116, -0.0641]],

         [[ 0.0228, -0.0741,  0.1507],
          [ 0.1096, -0.1673,  0.1349],
          [-0.0891,  0.0375, -0.1922]]],


        [[[ 0.1003, -0.0800, -0.0048],
          [ 0.1387,  0.1752,  0.1816],
          [ 0.0189, -0.0546,  0.1240]],

         [[-0.1685, -0.1890,  0.1593],
          [ 0.0241, -0.0529,  0.1053],
          [ 0.1292, -0.1953,  0.1423]],

         [[ 0.0945, -0.0496, -0.0118],
          [ 0.0080,  0.0300, -0.1137],
          [-0.0328, -0.1471, -0.0053]]],


        [[[-0.0654,  0.0964, -0.0401],
          [-0.1191,  0.0697,  0.1213],
          [ 0.1213,  0.1721,  0.0043]],

         [[ 0.1018,  0.1336,  0.1128],
          [ 0.0733, -0.0415, -0.0389],
          [-0.0439, -0.0312,  0.1421]],

         [[-0.0474,  0.1458,  0.0736],
          [-0.0415, -0.0470, -0.0616],
          [-0.0377, -0.0079,  0.0452]]],


        ...,


        [[[ 0.2254, -0.1141, -0.1096],
          [-0.0422, -0.1009, -0.0975],
          [-0.0995,  0.0201,  0.0553]],

         [[ 0.1682,  0.1903, -0.1174],
          [ 0.2100, -0.1401, -0.1193],
          [-0.0723, -0.0864, -0.0414]],

         [[ 0.1927,  0.1071, -0.0924],
          [-0.0729, -0.1531, -0.0675],
          [ 0.0565,  0.1772, -0.1316]]],


        [[[-0.1207,  0.0807, -0.1632],
          [-0.1453, -0.1452,  0.1198],
          [-0.1641,  0.1514, -0.1297]],

         [[ 0.0871, -0.1278,  0.0739],
          [ 0.1549,  0.0087, -0.0856],
          [-0.1553,  0.1153, -0.1944]],

         [[ 0.1125, -0.0332, -0.0152],
          [ 0.1356, -0.1820,  0.0059],
          [-0.0396,  0.0108, -0.1239]]],


        [[[ 0.0693,  0.1323,  0.0940],
          [-0.1006, -0.1551,  0.1957],
          [-0.1664, -0.0064, -0.0553]],

         [[-0.0343, -0.0963,  0.1474],
          [-0.0203, -0.1872,  0.0377],
          [-0.0120,  0.0684,  0.0322]],

         [[-0.1487, -0.1882,  0.1538],
          [ 0.1398, -0.1656, -0.0311],
          [-0.0108, -0.0516,  0.2640]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4802e-05,  6.2608e-06,  8.3690e-06],
          [ 8.9611e-06, -2.4550e-05, -9.4215e-06],
          [ 2.1585e-05, -1.1168e-05,  2.1884e-06]],

         [[ 7.1649e-07, -1.1124e-05,  2.4108e-05],
          [ 1.7771e-06, -6.8771e-06,  3.5505e-06],
          [ 5.2215e-06, -1.1565e-06, -6.4149e-06]],

         [[ 2.2771e-06, -7.4125e-06,  1.5070e-05],
          [ 1.0956e-05, -1.6734e-05,  1.3491e-05],
          [-8.9111e-06,  3.7531e-06, -1.9221e-05]]],


        [[[ 1.0035e-05, -7.9986e-06, -4.7915e-07],
          [ 1.3868e-05,  1.7524e-05,  1.8158e-05],
          [ 1.8867e-06, -5.4648e-06,  1.2404e-05]],

         [[-1.6854e-05, -1.8905e-05,  1.5929e-05],
          [ 2.4084e-06, -5.2877e-06,  1.0526e-05],
          [ 1.2917e-05, -1.9532e-05,  1.4232e-05]],

         [[ 9.4471e-06, -4.9633e-06, -1.1779e-06],
          [ 8.0016e-07,  3.0005e-06, -1.1373e-05],
          [-3.2816e-06, -1.4705e-05, -5.3374e-07]]],


        [[[-6.5388e-06,  9.6435e-06, -4.0143e-06],
          [-1.1906e-05,  6.9695e-06,  1.2128e-05],
          [ 1.2128e-05,  1.7213e-05,  4.3021e-07]],

         [[ 1.0175e-05,  1.3355e-05,  1.1284e-05],
          [ 7.3258e-06, -4.1505e-06, -3.8945e-06],
          [-4.3851e-06, -3.1166e-06,  1.4214e-05]],

         [[-4.7429e-06,  1.4583e-05,  7.3588e-06],
          [-4.1522e-06, -4.6978e-06, -6.1610e-06],
          [-3.7716e-06, -7.8512e-07,  4.5156e-06]]],


        ...,


        [[[ 2.2537e-05, -1.1409e-05, -1.0961e-05],
          [-4.2173e-06, -1.0095e-05, -9.7524e-06],
          [-9.9526e-06,  2.0108e-06,  5.5303e-06]],

         [[ 1.6819e-05,  1.9025e-05, -1.1739e-05],
          [ 2.0999e-05, -1.4007e-05, -1.1931e-05],
          [-7.2280e-06, -8.6428e-06, -4.1382e-06]],

         [[ 1.9270e-05,  1.0708e-05, -9.2373e-06],
          [-7.2915e-06, -1.5313e-05, -6.7488e-06],
          [ 5.6499e-06,  1.7723e-05, -1.3161e-05]]],


        [[[-1.2072e-05,  8.0724e-06, -1.6318e-05],
          [-1.4530e-05, -1.4520e-05,  1.1979e-05],
          [-1.6411e-05,  1.5141e-05, -1.2973e-05]],

         [[ 8.7140e-06, -1.2784e-05,  7.3855e-06],
          [ 1.5493e-05,  8.6823e-07, -8.5630e-06],
          [-1.5526e-05,  1.1527e-05, -1.9441e-05]],

         [[ 1.1247e-05, -3.3200e-06, -1.5151e-06],
          [ 1.3558e-05, -1.8201e-05,  5.9366e-07],
          [-3.9574e-06,  1.0792e-06, -1.2387e-05]]],


        [[[ 6.9336e-06,  1.3226e-05,  9.3987e-06],
          [-1.0058e-05, -1.5509e-05,  1.9566e-05],
          [-1.6643e-05, -6.3927e-07, -5.5267e-06]],

         [[-3.4292e-06, -9.6318e-06,  1.4738e-05],
          [-2.0332e-06, -1.8723e-05,  3.7749e-06],
          [-1.2038e-06,  6.8377e-06,  3.2244e-06]],

         [[-1.4866e-05, -1.8819e-05,  1.5382e-05],
          [ 1.3983e-05, -1.6564e-05, -3.1071e-06],
          [-1.0827e-06, -5.1648e-06,  2.6402e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.2017]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0694]], device='cuda:0')

Epoch: 47 | Batch_idx: 0 |  Loss: (0.8527) |  Loss2: (0.3455) | Acc: (84.00%) (108/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.8228) |  Loss2: (0.3454) | Acc: (84.00%) (1183/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.8254) |  Loss2: (0.3454) | Acc: (83.00%) (2249/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.8281) |  Loss2: (0.3454) | Acc: (83.00%) (3321/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.8273) |  Loss2: (0.3453) | Acc: (83.00%) (4399/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.8113) |  Loss2: (0.3453) | Acc: (84.00%) (5490/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.8078) |  Loss2: (0.3453) | Acc: (84.00%) (6567/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.8036) |  Loss2: (0.3452) | Acc: (84.00%) (7668/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.8001) |  Loss2: (0.3452) | Acc: (84.00%) (8761/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.8042) |  Loss2: (0.3451) | Acc: (84.00%) (9842/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.8031) |  Loss2: (0.3451) | Acc: (84.00%) (10922/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.8032) |  Loss2: (0.3451) | Acc: (84.00%) (11991/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.8025) |  Loss2: (0.3450) | Acc: (84.00%) (13074/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.8008) |  Loss2: (0.3450) | Acc: (84.00%) (14155/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.7992) |  Loss2: (0.3449) | Acc: (84.00%) (15234/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.7978) |  Loss2: (0.3449) | Acc: (84.00%) (16318/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.7988) |  Loss2: (0.3449) | Acc: (84.00%) (17388/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.7971) |  Loss2: (0.3448) | Acc: (84.00%) (18487/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.7960) |  Loss2: (0.3448) | Acc: (84.00%) (19566/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.7966) |  Loss2: (0.3447) | Acc: (84.00%) (20648/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.7992) |  Loss2: (0.3447) | Acc: (84.00%) (21710/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.7985) |  Loss2: (0.3447) | Acc: (84.00%) (22793/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.7994) |  Loss2: (0.3446) | Acc: (84.00%) (23857/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.7992) |  Loss2: (0.3446) | Acc: (84.00%) (24934/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.7991) |  Loss2: (0.3445) | Acc: (84.00%) (26023/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.7991) |  Loss2: (0.3445) | Acc: (84.00%) (27102/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.7997) |  Loss2: (0.3445) | Acc: (84.00%) (28177/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.7998) |  Loss2: (0.3444) | Acc: (84.00%) (29266/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.7993) |  Loss2: (0.3444) | Acc: (84.00%) (30353/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.7999) |  Loss2: (0.3443) | Acc: (84.00%) (31422/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.7994) |  Loss2: (0.3443) | Acc: (84.00%) (32512/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.7998) |  Loss2: (0.3442) | Acc: (84.00%) (33587/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.7991) |  Loss2: (0.3442) | Acc: (84.00%) (34689/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.7989) |  Loss2: (0.3442) | Acc: (84.00%) (35762/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.7991) |  Loss2: (0.3441) | Acc: (84.00%) (36826/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.7992) |  Loss2: (0.3441) | Acc: (84.00%) (37900/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.7984) |  Loss2: (0.3440) | Acc: (84.00%) (38989/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.7978) |  Loss2: (0.3440) | Acc: (84.00%) (40074/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.7978) |  Loss2: (0.3440) | Acc: (84.00%) (41142/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.7979) |  Loss2: (0.3439) | Acc: (84.00%) (42173/50000)
# TEST : Loss: (0.5196) | Acc: (81.00%) (8190/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1480,  0.0626,  0.0837],
          [ 0.0896, -0.2454, -0.0942],
          [ 0.2158, -0.1116,  0.0219]],

         [[ 0.0072, -0.1112,  0.2410],
          [ 0.0178, -0.0687,  0.0355],
          [ 0.0522, -0.0116, -0.0641]],

         [[ 0.0228, -0.0741,  0.1506],
          [ 0.1095, -0.1673,  0.1349],
          [-0.0891,  0.0375, -0.1921]]],


        [[[ 0.1003, -0.0800, -0.0048],
          [ 0.1386,  0.1752,  0.1815],
          [ 0.0189, -0.0546,  0.1240]],

         [[-0.1685, -0.1890,  0.1592],
          [ 0.0241, -0.0529,  0.1052],
          [ 0.1291, -0.1952,  0.1423]],

         [[ 0.0944, -0.0496, -0.0118],
          [ 0.0080,  0.0300, -0.1137],
          [-0.0328, -0.1470, -0.0053]]],


        [[[-0.0654,  0.0964, -0.0401],
          [-0.1190,  0.0697,  0.1212],
          [ 0.1212,  0.1721,  0.0043]],

         [[ 0.1017,  0.1335,  0.1128],
          [ 0.0732, -0.0415, -0.0389],
          [-0.0438, -0.0312,  0.1421]],

         [[-0.0474,  0.1458,  0.0736],
          [-0.0415, -0.0470, -0.0616],
          [-0.0377, -0.0078,  0.0451]]],


        ...,


        [[[ 0.2253, -0.1140, -0.1096],
          [-0.0422, -0.1009, -0.0975],
          [-0.0995,  0.0201,  0.0553]],

         [[ 0.1681,  0.1902, -0.1173],
          [ 0.2099, -0.1400, -0.1193],
          [-0.0723, -0.0864, -0.0414]],

         [[ 0.1926,  0.1070, -0.0923],
          [-0.0729, -0.1531, -0.0675],
          [ 0.0565,  0.1772, -0.1316]]],


        [[[-0.1207,  0.0807, -0.1631],
          [-0.1452, -0.1451,  0.1197],
          [-0.1640,  0.1514, -0.1297]],

         [[ 0.0871, -0.1278,  0.0738],
          [ 0.1549,  0.0087, -0.0856],
          [-0.1552,  0.1152, -0.1943]],

         [[ 0.1124, -0.0332, -0.0151],
          [ 0.1355, -0.1819,  0.0059],
          [-0.0396,  0.0108, -0.1238]]],


        [[[ 0.0693,  0.1322,  0.0939],
          [-0.1005, -0.1550,  0.1956],
          [-0.1664, -0.0064, -0.0552]],

         [[-0.0343, -0.0963,  0.1473],
          [-0.0203, -0.1872,  0.0377],
          [-0.0120,  0.0684,  0.0322]],

         [[-0.1486, -0.1881,  0.1538],
          [ 0.1398, -0.1656, -0.0311],
          [-0.0108, -0.0516,  0.2639]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4796e-05,  6.2585e-06,  8.3658e-06],
          [ 8.9577e-06, -2.4541e-05, -9.4177e-06],
          [ 2.1577e-05, -1.1164e-05,  2.1876e-06]],

         [[ 7.1622e-07, -1.1120e-05,  2.4098e-05],
          [ 1.7764e-06, -6.8744e-06,  3.5491e-06],
          [ 5.2194e-06, -1.1560e-06, -6.4123e-06]],

         [[ 2.2762e-06, -7.4096e-06,  1.5064e-05],
          [ 1.0951e-05, -1.6728e-05,  1.3486e-05],
          [-8.9076e-06,  3.7517e-06, -1.9213e-05]]],


        [[[ 1.0031e-05, -7.9954e-06, -4.7897e-07],
          [ 1.3863e-05,  1.7517e-05,  1.8151e-05],
          [ 1.8860e-06, -5.4627e-06,  1.2399e-05]],

         [[-1.6847e-05, -1.8897e-05,  1.5923e-05],
          [ 2.4074e-06, -5.2856e-06,  1.0522e-05],
          [ 1.2912e-05, -1.9524e-05,  1.4226e-05]],

         [[ 9.4434e-06, -4.9614e-06, -1.1774e-06],
          [ 7.9983e-07,  2.9993e-06, -1.1369e-05],
          [-3.2803e-06, -1.4699e-05, -5.3354e-07]]],


        [[[-6.5361e-06,  9.6397e-06, -4.0127e-06],
          [-1.1901e-05,  6.9668e-06,  1.2123e-05],
          [ 1.2123e-05,  1.7206e-05,  4.3005e-07]],

         [[ 1.0171e-05,  1.3350e-05,  1.1280e-05],
          [ 7.3229e-06, -4.1489e-06, -3.8930e-06],
          [-4.3834e-06, -3.1154e-06,  1.4208e-05]],

         [[-4.7410e-06,  1.4577e-05,  7.3559e-06],
          [-4.1506e-06, -4.6959e-06, -6.1585e-06],
          [-3.7701e-06, -7.8483e-07,  4.5139e-06]]],


        ...,


        [[[ 2.2528e-05, -1.1404e-05, -1.0957e-05],
          [-4.2157e-06, -1.0090e-05, -9.7486e-06],
          [-9.9489e-06,  2.0100e-06,  5.5281e-06]],

         [[ 1.6812e-05,  1.9018e-05, -1.1734e-05],
          [ 2.0991e-05, -1.4001e-05, -1.1927e-05],
          [-7.2251e-06, -8.6393e-06, -4.1366e-06]],

         [[ 1.9262e-05,  1.0704e-05, -9.2338e-06],
          [-7.2886e-06, -1.5307e-05, -6.7461e-06],
          [ 5.6477e-06,  1.7716e-05, -1.3156e-05]]],


        [[[-1.2067e-05,  8.0692e-06, -1.6311e-05],
          [-1.4524e-05, -1.4514e-05,  1.1975e-05],
          [-1.6405e-05,  1.5135e-05, -1.2968e-05]],

         [[ 8.7105e-06, -1.2779e-05,  7.3826e-06],
          [ 1.5488e-05,  8.6790e-07, -8.5597e-06],
          [-1.5520e-05,  1.1522e-05, -1.9433e-05]],

         [[ 1.1243e-05, -3.3187e-06, -1.5145e-06],
          [ 1.3552e-05, -1.8194e-05,  5.9342e-07],
          [-3.9558e-06,  1.0787e-06, -1.2383e-05]]],


        [[[ 6.9310e-06,  1.3221e-05,  9.3949e-06],
          [-1.0054e-05, -1.5503e-05,  1.9559e-05],
          [-1.6637e-05, -6.3902e-07, -5.5245e-06]],

         [[-3.4279e-06, -9.6280e-06,  1.4733e-05],
          [-2.0324e-06, -1.8715e-05,  3.7734e-06],
          [-1.2034e-06,  6.8351e-06,  3.2231e-06]],

         [[-1.4861e-05, -1.8812e-05,  1.5376e-05],
          [ 1.3978e-05, -1.6558e-05, -3.1058e-06],
          [-1.0823e-06, -5.1627e-06,  2.6391e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.2050]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0339]], device='cuda:0')

Epoch: 48 | Batch_idx: 0 |  Loss: (0.7147) |  Loss2: (0.3423) | Acc: (85.00%) (110/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.7803) |  Loss2: (0.3422) | Acc: (85.00%) (1203/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.7883) |  Loss2: (0.3422) | Acc: (84.00%) (2282/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.7968) |  Loss2: (0.3422) | Acc: (84.00%) (3354/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.7889) |  Loss2: (0.3421) | Acc: (84.00%) (4443/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.7940) |  Loss2: (0.3421) | Acc: (84.00%) (5503/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.7914) |  Loss2: (0.3420) | Acc: (84.00%) (6579/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.7899) |  Loss2: (0.3420) | Acc: (84.00%) (7652/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.7850) |  Loss2: (0.3419) | Acc: (84.00%) (8741/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.7825) |  Loss2: (0.3419) | Acc: (84.00%) (9834/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.7842) |  Loss2: (0.3418) | Acc: (84.00%) (10908/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.7876) |  Loss2: (0.3418) | Acc: (84.00%) (11982/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.7880) |  Loss2: (0.3418) | Acc: (84.00%) (13058/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.7870) |  Loss2: (0.3417) | Acc: (84.00%) (14138/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.7889) |  Loss2: (0.3417) | Acc: (84.00%) (15214/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.7856) |  Loss2: (0.3416) | Acc: (84.00%) (16318/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.7878) |  Loss2: (0.3416) | Acc: (84.00%) (17382/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.7866) |  Loss2: (0.3415) | Acc: (84.00%) (18473/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.7863) |  Loss2: (0.3415) | Acc: (84.00%) (19562/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.7871) |  Loss2: (0.3415) | Acc: (84.00%) (20636/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.7874) |  Loss2: (0.3414) | Acc: (84.00%) (21712/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.7884) |  Loss2: (0.3414) | Acc: (84.00%) (22785/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.7881) |  Loss2: (0.3413) | Acc: (84.00%) (23873/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.7904) |  Loss2: (0.3413) | Acc: (84.00%) (24933/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.7928) |  Loss2: (0.3413) | Acc: (84.00%) (25986/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.7911) |  Loss2: (0.3412) | Acc: (84.00%) (27080/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.7905) |  Loss2: (0.3412) | Acc: (84.00%) (28168/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.7899) |  Loss2: (0.3411) | Acc: (84.00%) (29259/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.7900) |  Loss2: (0.3411) | Acc: (84.00%) (30346/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.7896) |  Loss2: (0.3411) | Acc: (84.00%) (31435/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.7892) |  Loss2: (0.3410) | Acc: (84.00%) (32520/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.7883) |  Loss2: (0.3410) | Acc: (84.00%) (33615/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.7883) |  Loss2: (0.3409) | Acc: (84.00%) (34692/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.7882) |  Loss2: (0.3409) | Acc: (84.00%) (35762/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.7880) |  Loss2: (0.3409) | Acc: (84.00%) (36831/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.7881) |  Loss2: (0.3408) | Acc: (84.00%) (37913/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.7873) |  Loss2: (0.3408) | Acc: (84.00%) (39021/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.7872) |  Loss2: (0.3407) | Acc: (84.00%) (40096/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.7878) |  Loss2: (0.3407) | Acc: (84.00%) (41159/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.7866) |  Loss2: (0.3407) | Acc: (84.00%) (42212/50000)
# TEST : Loss: (0.5085) | Acc: (82.00%) (8237/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1479,  0.0626,  0.0836],
          [ 0.0895, -0.2453, -0.0941],
          [ 0.2157, -0.1116,  0.0219]],

         [[ 0.0072, -0.1112,  0.2409],
          [ 0.0178, -0.0687,  0.0355],
          [ 0.0522, -0.0116, -0.0641]],

         [[ 0.0228, -0.0741,  0.1506],
          [ 0.1095, -0.1672,  0.1348],
          [-0.0890,  0.0375, -0.1921]]],


        [[[ 0.1003, -0.0799, -0.0048],
          [ 0.1386,  0.1751,  0.1814],
          [ 0.0189, -0.0546,  0.1239]],

         [[-0.1684, -0.1889,  0.1592],
          [ 0.0241, -0.0528,  0.1052],
          [ 0.1291, -0.1952,  0.1422]],

         [[ 0.0944, -0.0496, -0.0118],
          [ 0.0080,  0.0300, -0.1136],
          [-0.0328, -0.1469, -0.0053]]],


        [[[-0.0653,  0.0964, -0.0401],
          [-0.1190,  0.0696,  0.1212],
          [ 0.1212,  0.1720,  0.0043]],

         [[ 0.1017,  0.1334,  0.1128],
          [ 0.0732, -0.0415, -0.0389],
          [-0.0438, -0.0311,  0.1420]],

         [[-0.0474,  0.1457,  0.0735],
          [-0.0415, -0.0469, -0.0616],
          [-0.0377, -0.0078,  0.0451]]],


        ...,


        [[[ 0.2252, -0.1140, -0.1095],
          [-0.0421, -0.1009, -0.0974],
          [-0.0995,  0.0201,  0.0553]],

         [[ 0.1681,  0.1901, -0.1173],
          [ 0.2098, -0.1400, -0.1192],
          [-0.0722, -0.0864, -0.0414]],

         [[ 0.1925,  0.1070, -0.0923],
          [-0.0729, -0.1530, -0.0674],
          [ 0.0565,  0.1771, -0.1315]]],


        [[[-0.1206,  0.0807, -0.1630],
          [-0.1452, -0.1451,  0.1197],
          [-0.1640,  0.1513, -0.1296]],

         [[ 0.0871, -0.1277,  0.0738],
          [ 0.1548,  0.0087, -0.0856],
          [-0.1551,  0.1152, -0.1943]],

         [[ 0.1124, -0.0332, -0.0151],
          [ 0.1355, -0.1819,  0.0059],
          [-0.0395,  0.0108, -0.1238]]],


        [[[ 0.0693,  0.1322,  0.0939],
          [-0.1005, -0.1550,  0.1955],
          [-0.1663, -0.0064, -0.0552]],

         [[-0.0343, -0.0962,  0.1473],
          [-0.0203, -0.1871,  0.0377],
          [-0.0120,  0.0683,  0.0322]],

         [[-0.1485, -0.1880,  0.1537],
          [ 0.1397, -0.1655, -0.0310],
          [-0.0108, -0.0516,  0.2638]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4790e-05,  6.2562e-06,  8.3626e-06],
          [ 8.9542e-06, -2.4532e-05, -9.4140e-06],
          [ 2.1569e-05, -1.1160e-05,  2.1867e-06]],

         [[ 7.1595e-07, -1.1115e-05,  2.4089e-05],
          [ 1.7756e-06, -6.8718e-06,  3.5476e-06],
          [ 5.2174e-06, -1.1556e-06, -6.4097e-06]],

         [[ 2.2754e-06, -7.4067e-06,  1.5058e-05],
          [ 1.0947e-05, -1.6721e-05,  1.3480e-05],
          [-8.9041e-06,  3.7502e-06, -1.9205e-05]]],


        [[[ 1.0027e-05, -7.9922e-06, -4.7878e-07],
          [ 1.3858e-05,  1.7510e-05,  1.8144e-05],
          [ 1.8852e-06, -5.4605e-06,  1.2394e-05]],

         [[-1.6841e-05, -1.8890e-05,  1.5916e-05],
          [ 2.4065e-06, -5.2836e-06,  1.0518e-05],
          [ 1.2907e-05, -1.9517e-05,  1.4220e-05]],

         [[ 9.4396e-06, -4.9595e-06, -1.1769e-06],
          [ 7.9950e-07,  2.9982e-06, -1.1365e-05],
          [-3.2790e-06, -1.4694e-05, -5.3334e-07]]],


        [[[-6.5335e-06,  9.6359e-06, -4.0111e-06],
          [-1.1896e-05,  6.9642e-06,  1.2119e-05],
          [ 1.2119e-05,  1.7199e-05,  4.2988e-07]],

         [[ 1.0167e-05,  1.3345e-05,  1.1276e-05],
          [ 7.3200e-06, -4.1473e-06, -3.8916e-06],
          [-4.3816e-06, -3.1142e-06,  1.4202e-05]],

         [[-4.7391e-06,  1.4572e-05,  7.3530e-06],
          [-4.1490e-06, -4.6940e-06, -6.1560e-06],
          [-3.7686e-06, -7.8454e-07,  4.5121e-06]]],


        ...,


        [[[ 2.2519e-05, -1.1400e-05, -1.0952e-05],
          [-4.2141e-06, -1.0086e-05, -9.7449e-06],
          [-9.9451e-06,  2.0092e-06,  5.5259e-06]],

         [[ 1.6806e-05,  1.9010e-05, -1.1729e-05],
          [ 2.0983e-05, -1.3996e-05, -1.1922e-05],
          [-7.2222e-06, -8.6358e-06, -4.1350e-06]],

         [[ 1.9255e-05,  1.0700e-05, -9.2303e-06],
          [-7.2857e-06, -1.5301e-05, -6.7435e-06],
          [ 5.6455e-06,  1.7709e-05, -1.3151e-05]]],


        [[[-1.2063e-05,  8.0660e-06, -1.6305e-05],
          [-1.4518e-05, -1.4508e-05,  1.1970e-05],
          [-1.6398e-05,  1.5129e-05, -1.2963e-05]],

         [[ 8.7070e-06, -1.2774e-05,  7.3797e-06],
          [ 1.5482e-05,  8.6757e-07, -8.5565e-06],
          [-1.5514e-05,  1.1518e-05, -1.9426e-05]],

         [[ 1.1238e-05, -3.3174e-06, -1.5139e-06],
          [ 1.3547e-05, -1.8187e-05,  5.9319e-07],
          [-3.9542e-06,  1.0783e-06, -1.2378e-05]]],


        [[[ 6.9284e-06,  1.3215e-05,  9.3911e-06],
          [-1.0050e-05, -1.5497e-05,  1.9551e-05],
          [-1.6630e-05, -6.3876e-07, -5.5223e-06]],

         [[-3.4266e-06, -9.6242e-06,  1.4727e-05],
          [-2.0316e-06, -1.8708e-05,  3.7720e-06],
          [-1.2029e-06,  6.8325e-06,  3.2218e-06]],

         [[-1.4855e-05, -1.8804e-05,  1.5370e-05],
          [ 1.3973e-05, -1.6551e-05, -3.1046e-06],
          [-1.0818e-06, -5.1607e-06,  2.6381e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1990]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0300]], device='cuda:0')

Epoch: 49 | Batch_idx: 0 |  Loss: (0.8375) |  Loss2: (0.3391) | Acc: (84.00%) (108/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.7978) |  Loss2: (0.3391) | Acc: (84.00%) (1187/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.7876) |  Loss2: (0.3390) | Acc: (85.00%) (2285/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.7783) |  Loss2: (0.3390) | Acc: (85.00%) (3376/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.7782) |  Loss2: (0.3390) | Acc: (85.00%) (4472/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.7772) |  Loss2: (0.3389) | Acc: (85.00%) (5565/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.7756) |  Loss2: (0.3389) | Acc: (85.00%) (6644/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.7736) |  Loss2: (0.3389) | Acc: (85.00%) (7743/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.7714) |  Loss2: (0.3388) | Acc: (85.00%) (8826/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.7697) |  Loss2: (0.3388) | Acc: (85.00%) (9931/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.7708) |  Loss2: (0.3387) | Acc: (85.00%) (11009/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.7713) |  Loss2: (0.3387) | Acc: (85.00%) (12089/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.7722) |  Loss2: (0.3386) | Acc: (85.00%) (13172/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.7728) |  Loss2: (0.3386) | Acc: (85.00%) (14253/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.7739) |  Loss2: (0.3386) | Acc: (85.00%) (15344/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.7755) |  Loss2: (0.3385) | Acc: (84.00%) (16414/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.7745) |  Loss2: (0.3385) | Acc: (84.00%) (17513/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.7745) |  Loss2: (0.3384) | Acc: (84.00%) (18591/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.7766) |  Loss2: (0.3384) | Acc: (84.00%) (19651/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.7775) |  Loss2: (0.3384) | Acc: (84.00%) (20731/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.7759) |  Loss2: (0.3383) | Acc: (84.00%) (21829/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.7762) |  Loss2: (0.3383) | Acc: (84.00%) (22913/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.7757) |  Loss2: (0.3382) | Acc: (84.00%) (23992/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.7768) |  Loss2: (0.3382) | Acc: (84.00%) (25074/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.7744) |  Loss2: (0.3382) | Acc: (84.00%) (26174/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.7746) |  Loss2: (0.3381) | Acc: (84.00%) (27259/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.7721) |  Loss2: (0.3381) | Acc: (84.00%) (28375/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.7721) |  Loss2: (0.3380) | Acc: (84.00%) (29456/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.7714) |  Loss2: (0.3380) | Acc: (84.00%) (30549/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.7713) |  Loss2: (0.3380) | Acc: (84.00%) (31646/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.7736) |  Loss2: (0.3379) | Acc: (84.00%) (32679/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.7730) |  Loss2: (0.3379) | Acc: (84.00%) (33779/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.7733) |  Loss2: (0.3379) | Acc: (84.00%) (34875/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.7732) |  Loss2: (0.3378) | Acc: (84.00%) (35964/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.7729) |  Loss2: (0.3378) | Acc: (84.00%) (37043/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.7734) |  Loss2: (0.3377) | Acc: (84.00%) (38130/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.7738) |  Loss2: (0.3377) | Acc: (84.00%) (39216/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.7736) |  Loss2: (0.3377) | Acc: (84.00%) (40304/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.7743) |  Loss2: (0.3376) | Acc: (84.00%) (41381/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.7735) |  Loss2: (0.3376) | Acc: (84.00%) (42446/50000)
# TEST : Loss: (0.5064) | Acc: (82.00%) (8248/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1478,  0.0625,  0.0836],
          [ 0.0895, -0.2452, -0.0941],
          [ 0.2156, -0.1116,  0.0219]],

         [[ 0.0072, -0.1111,  0.2408],
          [ 0.0177, -0.0687,  0.0355],
          [ 0.0522, -0.0116, -0.0641]],

         [[ 0.0227, -0.0740,  0.1505],
          [ 0.1094, -0.1671,  0.1348],
          [-0.0890,  0.0375, -0.1920]]],


        [[[ 0.1002, -0.0799, -0.0048],
          [ 0.1385,  0.1750,  0.1814],
          [ 0.0188, -0.0546,  0.1239]],

         [[-0.1683, -0.1888,  0.1591],
          [ 0.0241, -0.0528,  0.1051],
          [ 0.1290, -0.1951,  0.1421]],

         [[ 0.0944, -0.0496, -0.0118],
          [ 0.0080,  0.0300, -0.1136],
          [-0.0328, -0.1469, -0.0053]]],


        [[[-0.0653,  0.0963, -0.0401],
          [-0.1189,  0.0696,  0.1211],
          [ 0.1211,  0.1719,  0.0043]],

         [[ 0.1016,  0.1334,  0.1127],
          [ 0.0732, -0.0415, -0.0389],
          [-0.0438, -0.0311,  0.1420]],

         [[-0.0474,  0.1457,  0.0735],
          [-0.0415, -0.0469, -0.0615],
          [-0.0377, -0.0078,  0.0451]]],


        ...,


        [[[ 0.2251, -0.1140, -0.1095],
          [-0.0421, -0.1008, -0.0974],
          [-0.0994,  0.0201,  0.0552]],

         [[ 0.1680,  0.1900, -0.1172],
          [ 0.2097, -0.1399, -0.1192],
          [-0.0722, -0.0863, -0.0413]],

         [[ 0.1925,  0.1070, -0.0923],
          [-0.0728, -0.1530, -0.0674],
          [ 0.0564,  0.1770, -0.1315]]],


        [[[-0.1206,  0.0806, -0.1630],
          [-0.1451, -0.1450,  0.1197],
          [-0.1639,  0.1512, -0.1296]],

         [[ 0.0870, -0.1277,  0.0738],
          [ 0.1548,  0.0087, -0.0855],
          [-0.1551,  0.1151, -0.1942]],

         [[ 0.1123, -0.0332, -0.0151],
          [ 0.1354, -0.1818,  0.0059],
          [-0.0395,  0.0108, -0.1237]]],


        [[[ 0.0693,  0.1321,  0.0939],
          [-0.1005, -0.1549,  0.1954],
          [-0.1662, -0.0064, -0.0552]],

         [[-0.0343, -0.0962,  0.1472],
          [-0.0203, -0.1870,  0.0377],
          [-0.0120,  0.0683,  0.0322]],

         [[-0.1485, -0.1880,  0.1536],
          [ 0.1397, -0.1654, -0.0310],
          [-0.0108, -0.0516,  0.2637]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4784e-05,  6.2538e-06,  8.3594e-06],
          [ 8.9507e-06, -2.4522e-05, -9.4102e-06],
          [ 2.1561e-05, -1.1155e-05,  2.1858e-06]],

         [[ 7.1567e-07, -1.1111e-05,  2.4080e-05],
          [ 1.7749e-06, -6.8692e-06,  3.5461e-06],
          [ 5.2153e-06, -1.1552e-06, -6.4070e-06]],

         [[ 2.2745e-06, -7.4038e-06,  1.5052e-05],
          [ 1.0943e-05, -1.6715e-05,  1.3475e-05],
          [-8.9007e-06,  3.7488e-06, -1.9198e-05]]],


        [[[ 1.0023e-05, -7.9890e-06, -4.7860e-07],
          [ 1.3852e-05,  1.7503e-05,  1.8137e-05],
          [ 1.8845e-06, -5.4583e-06,  1.2389e-05]],

         [[-1.6835e-05, -1.8882e-05,  1.5910e-05],
          [ 2.4056e-06, -5.2816e-06,  1.0513e-05],
          [ 1.2901e-05, -1.9509e-05,  1.4214e-05]],

         [[ 9.4358e-06, -4.9576e-06, -1.1764e-06],
          [ 7.9917e-07,  2.9970e-06, -1.1360e-05],
          [-3.2777e-06, -1.4688e-05, -5.3314e-07]]],


        [[[-6.5309e-06,  9.6321e-06, -4.0095e-06],
          [-1.1892e-05,  6.9616e-06,  1.2114e-05],
          [ 1.2114e-05,  1.7192e-05,  4.2972e-07]],

         [[ 1.0163e-05,  1.3339e-05,  1.1271e-05],
          [ 7.3171e-06, -4.1457e-06, -3.8901e-06],
          [-4.3799e-06, -3.1129e-06,  1.4196e-05]],

         [[-4.7372e-06,  1.4566e-05,  7.3500e-06],
          [-4.1474e-06, -4.6921e-06, -6.1535e-06],
          [-3.7672e-06, -7.8425e-07,  4.5104e-06]]],


        ...,


        [[[ 2.2511e-05, -1.1396e-05, -1.0948e-05],
          [-4.2125e-06, -1.0082e-05, -9.7411e-06],
          [-9.9413e-06,  2.0084e-06,  5.5237e-06]],

         [[ 1.6800e-05,  1.9002e-05, -1.1725e-05],
          [ 2.0975e-05, -1.3991e-05, -1.1917e-05],
          [-7.2193e-06, -8.6323e-06, -4.1334e-06]],

         [[ 1.9247e-05,  1.0696e-05, -9.2268e-06],
          [-7.2828e-06, -1.5295e-05, -6.7409e-06],
          [ 5.6433e-06,  1.7702e-05, -1.3146e-05]]],


        [[[-1.2058e-05,  8.0628e-06, -1.6298e-05],
          [-1.4512e-05, -1.4502e-05,  1.1965e-05],
          [-1.6392e-05,  1.5124e-05, -1.2958e-05]],

         [[ 8.7035e-06, -1.2768e-05,  7.3768e-06],
          [ 1.5476e-05,  8.6725e-07, -8.5533e-06],
          [-1.5508e-05,  1.1514e-05, -1.9418e-05]],

         [[ 1.1234e-05, -3.3161e-06, -1.5134e-06],
          [ 1.3542e-05, -1.8180e-05,  5.9295e-07],
          [-3.9526e-06,  1.0779e-06, -1.2373e-05]]],


        [[[ 6.9258e-06,  1.3210e-05,  9.3873e-06],
          [-1.0047e-05, -1.5491e-05,  1.9543e-05],
          [-1.6624e-05, -6.3851e-07, -5.5201e-06]],

         [[-3.4253e-06, -9.6205e-06,  1.4721e-05],
          [-2.0308e-06, -1.8700e-05,  3.7705e-06],
          [-1.2024e-06,  6.8298e-06,  3.2205e-06]],

         [[-1.4849e-05, -1.8796e-05,  1.5365e-05],
          [ 1.3967e-05, -1.6545e-05, -3.1033e-06],
          [-1.0814e-06, -5.1586e-06,  2.6370e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1905]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0239]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (84.00%) (1196/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (84.00%) (2283/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (85.00%) (3388/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4388) |  Loss2: (0.0000) | Acc: (85.00%) (4469/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (85.00%) (5556/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (6630/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (7721/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (8783/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4444) |  Loss2: (0.0000) | Acc: (84.00%) (9859/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (10946/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (12023/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (13112/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (14211/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (15312/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (16413/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (17484/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (84.00%) (18579/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (19686/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (20780/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (84.00%) (21859/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (22937/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (84.00%) (24038/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (25128/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (85.00%) (26221/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (84.00%) (27304/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (28425/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (29527/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4324) |  Loss2: (0.0000) | Acc: (85.00%) (30622/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (31714/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (32809/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (33905/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4325) |  Loss2: (0.0000) | Acc: (85.00%) (34979/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (36053/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (85.00%) (37147/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (85.00%) (38243/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (39328/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4336) |  Loss2: (0.0000) | Acc: (85.00%) (40412/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4330) |  Loss2: (0.0000) | Acc: (85.00%) (41501/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (85.00%) (42520/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_050.pth.tar'
# TEST : Loss: (0.5590) | Acc: (81.00%) (8166/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1449,  0.0618,  0.0848],
          [ 0.0986, -0.2462, -0.0963],
          [ 0.2263, -0.1027,  0.0215]],

         [[ 0.0063, -0.1125,  0.2399],
          [ 0.0216, -0.0710,  0.0322],
          [ 0.0554, -0.0067, -0.0663]],

         [[ 0.0213, -0.0748,  0.1517],
          [ 0.1140, -0.1658,  0.1370],
          [-0.0848,  0.0451, -0.1878]]],


        [[[ 0.0971, -0.0829, -0.0061],
          [ 0.1373,  0.1735,  0.1812],
          [ 0.0163, -0.0580,  0.1218]],

         [[-0.1688, -0.1898,  0.1598],
          [ 0.0255, -0.0522,  0.1073],
          [ 0.1286, -0.1966,  0.1421]],

         [[ 0.0944, -0.0502, -0.0102],
          [ 0.0100,  0.0310, -0.1108],
          [-0.0320, -0.1474, -0.0045]]],


        [[[-0.0665,  0.0956, -0.0404],
          [-0.1199,  0.0692,  0.1212],
          [ 0.1199,  0.1712,  0.0040]],

         [[ 0.1014,  0.1336,  0.1132],
          [ 0.0729, -0.0410, -0.0381],
          [-0.0444, -0.0313,  0.1421]],

         [[-0.0473,  0.1460,  0.0741],
          [-0.0416, -0.0465, -0.0608],
          [-0.0382, -0.0080,  0.0453]]],


        ...,


        [[[ 0.2250, -0.1135, -0.1099],
          [-0.0425, -0.1016, -0.0979],
          [-0.0982,  0.0211,  0.0566]],

         [[ 0.1682,  0.1912, -0.1168],
          [ 0.2099, -0.1395, -0.1189],
          [-0.0704, -0.0845, -0.0396]],

         [[ 0.1911,  0.1065, -0.0934],
          [-0.0742, -0.1540, -0.0690],
          [ 0.0564,  0.1772, -0.1317]]],


        [[[-0.1193,  0.0819, -0.1612],
          [-0.1447, -0.1450,  0.1201],
          [-0.1630,  0.1518, -0.1294]],

         [[ 0.0868, -0.1281,  0.0733],
          [ 0.1537,  0.0069, -0.0870],
          [-0.1555,  0.1143, -0.1955]],

         [[ 0.1129, -0.0326, -0.0145],
          [ 0.1352, -0.1823,  0.0059],
          [-0.0395,  0.0106, -0.1240]]],


        [[[ 0.0725,  0.1339,  0.0951],
          [-0.0951, -0.1533,  0.2005],
          [-0.1617, -0.0027, -0.0488]],

         [[-0.0317, -0.0957,  0.1476],
          [-0.0147, -0.1850,  0.0436],
          [-0.0064,  0.0738,  0.0403]],

         [[-0.1492, -0.1901,  0.1514],
          [ 0.1410, -0.1664, -0.0283],
          [-0.0098, -0.0500,  0.2680]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1764, -0.2087, -0.2311],
          [-0.2159, -0.2046, -0.2174],
          [-0.2279, -0.2199, -0.2178]],

         [[-0.1685, -0.1889, -0.2005],
          [-0.2193, -0.1967, -0.1958],
          [-0.2391, -0.2169, -0.2015]],

         [[-0.1126, -0.1197, -0.1271],
          [-0.1789, -0.1504, -0.1467],
          [-0.2083, -0.1795, -0.1634]]],


        [[[ 0.0075,  0.0088,  0.0082],
          [ 0.0029,  0.0010, -0.0001],
          [ 0.0003, -0.0023, -0.0031]],

         [[ 0.0071,  0.0082,  0.0076],
          [ 0.0043,  0.0023,  0.0002],
          [ 0.0011, -0.0011, -0.0009]],

         [[ 0.0048,  0.0022,  0.0007],
          [ 0.0053, -0.0004, -0.0040],
          [ 0.0026, -0.0022, -0.0026]]],


        [[[ 0.0036,  0.0037,  0.0027],
          [ 0.0020,  0.0038,  0.0024],
          [ 0.0005,  0.0028,  0.0027]],

         [[ 0.0005, -0.0001, -0.0023],
          [-0.0009, -0.0003, -0.0030],
          [-0.0021, -0.0016, -0.0029]],

         [[-0.0020, -0.0043, -0.0073],
          [-0.0026, -0.0042, -0.0077],
          [-0.0035, -0.0049, -0.0068]]],


        ...,


        [[[ 0.0228,  0.0216,  0.0169],
          [ 0.0262,  0.0245,  0.0197],
          [ 0.0404,  0.0383,  0.0307]],

         [[ 0.0252,  0.0201,  0.0123],
          [ 0.0292,  0.0238,  0.0178],
          [ 0.0418,  0.0367,  0.0294]],

         [[ 0.0307,  0.0203,  0.0059],
          [ 0.0339,  0.0246,  0.0129],
          [ 0.0451,  0.0378,  0.0264]]],


        [[[-0.0121, -0.0097, -0.0114],
          [-0.0051, -0.0012, -0.0047],
          [-0.0021,  0.0016, -0.0006]],

         [[-0.0138, -0.0116, -0.0113],
          [-0.0060, -0.0023, -0.0037],
          [-0.0011,  0.0018,  0.0012]],

         [[-0.0074, -0.0048, -0.0044],
          [ 0.0016,  0.0049,  0.0036],
          [ 0.0067,  0.0087,  0.0082]]],


        [[[-0.0506, -0.0622, -0.0673],
          [-0.0606, -0.0607, -0.0539],
          [-0.0845, -0.0760, -0.0595]],

         [[-0.0033, -0.0079, -0.0135],
          [-0.0054,  0.0007,  0.0058],
          [-0.0204, -0.0087,  0.0056]],

         [[ 0.0370,  0.0355,  0.0265],
          [ 0.0376,  0.0474,  0.0470],
          [ 0.0279,  0.0410,  0.0486]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1890]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 51 | Batch_idx: 0 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (1217/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (2315/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4237) |  Loss2: (0.0000) | Acc: (85.00%) (3387/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (4462/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (5564/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4147) |  Loss2: (0.0000) | Acc: (85.00%) (6673/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4066) |  Loss2: (0.0000) | Acc: (85.00%) (7796/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4042) |  Loss2: (0.0000) | Acc: (85.00%) (8910/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (85.00%) (10017/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (85.00%) (11101/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (12190/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4092) |  Loss2: (0.0000) | Acc: (85.00%) (13281/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (14374/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (15463/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (16553/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (17672/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (18754/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (19846/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (20949/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (22035/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (23132/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (24227/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (25313/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (26414/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4160) |  Loss2: (0.0000) | Acc: (85.00%) (27517/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (28598/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (29704/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (30787/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4180) |  Loss2: (0.0000) | Acc: (85.00%) (31872/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (33011/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (34085/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (35160/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4178) |  Loss2: (0.0000) | Acc: (85.00%) (36246/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4191) |  Loss2: (0.0000) | Acc: (85.00%) (37329/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (38452/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (39543/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (40639/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4189) |  Loss2: (0.0000) | Acc: (85.00%) (41703/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (42758/50000)
# TEST : Loss: (0.5735) | Acc: (81.00%) (8136/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1473,  0.0595,  0.0845],
          [ 0.0991, -0.2490, -0.0972],
          [ 0.2316, -0.0996,  0.0215]],

         [[ 0.0084, -0.1096,  0.2425],
          [ 0.0256, -0.0706,  0.0334],
          [ 0.0616, -0.0030, -0.0661]],

         [[ 0.0188, -0.0749,  0.1539],
          [ 0.1123, -0.1683,  0.1388],
          [-0.0866,  0.0434, -0.1880]]],


        [[[ 0.0969, -0.0842, -0.0071],
          [ 0.1376,  0.1714,  0.1792],
          [ 0.0160, -0.0604,  0.1188]],

         [[-0.1687, -0.1915,  0.1587],
          [ 0.0257, -0.0547,  0.1052],
          [ 0.1276, -0.1999,  0.1389]],

         [[ 0.0944, -0.0520, -0.0112],
          [ 0.0101,  0.0283, -0.1127],
          [-0.0325, -0.1505, -0.0074]]],


        [[[-0.0669,  0.0951, -0.0406],
          [-0.1203,  0.0688,  0.1208],
          [ 0.1195,  0.1708,  0.0036]],

         [[ 0.1016,  0.1338,  0.1134],
          [ 0.0728, -0.0411, -0.0382],
          [-0.0445, -0.0314,  0.1418]],

         [[-0.0470,  0.1463,  0.0744],
          [-0.0414, -0.0464, -0.0608],
          [-0.0381, -0.0079,  0.0452]]],


        ...,


        [[[ 0.2271, -0.1125, -0.1092],
          [-0.0404, -0.1000, -0.0957],
          [-0.0969,  0.0230,  0.0588]],

         [[ 0.1703,  0.1921, -0.1163],
          [ 0.2123, -0.1380, -0.1171],
          [-0.0695, -0.0834, -0.0381]],

         [[ 0.1919,  0.1060, -0.0946],
          [-0.0723, -0.1531, -0.0686],
          [ 0.0569,  0.1778, -0.1311]]],


        [[[-0.1183,  0.0833, -0.1594],
          [-0.1438, -0.1443,  0.1209],
          [-0.1623,  0.1512, -0.1305]],

         [[ 0.0863, -0.1279,  0.0739],
          [ 0.1527,  0.0059, -0.0876],
          [-0.1566,  0.1123, -0.1977]],

         [[ 0.1123, -0.0323, -0.0137],
          [ 0.1342, -0.1830,  0.0054],
          [-0.0406,  0.0090, -0.1256]]],


        [[[ 0.0734,  0.1325,  0.0948],
          [-0.0917, -0.1538,  0.2014],
          [-0.1609, -0.0038, -0.0499]],

         [[-0.0318, -0.0994,  0.1445],
          [-0.0129, -0.1875,  0.0426],
          [-0.0068,  0.0720,  0.0383]],

         [[-0.1514, -0.1960,  0.1464],
          [ 0.1409, -0.1697, -0.0293],
          [-0.0107, -0.0509,  0.2677]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1265, -0.1430, -0.1799],
          [-0.1637, -0.1859, -0.1922],
          [-0.1906, -0.1998, -0.1770]],

         [[-0.0598, -0.0739, -0.1157],
          [-0.0879, -0.1072, -0.1222],
          [-0.1243, -0.1299, -0.1153]],

         [[-0.0006, -0.0024, -0.0425],
          [-0.0256, -0.0344, -0.0439],
          [-0.0773, -0.0692, -0.0422]]],


        [[[-0.0272, -0.0189, -0.0017],
          [-0.0310, -0.0193, -0.0001],
          [-0.0461, -0.0332, -0.0115]],

         [[-0.0184, -0.0122,  0.0074],
          [-0.0203, -0.0086,  0.0126],
          [-0.0292, -0.0156,  0.0075]],

         [[-0.0116, -0.0058,  0.0127],
          [-0.0195, -0.0088,  0.0117],
          [-0.0279, -0.0150,  0.0058]]],


        [[[-0.0027, -0.0028,  0.0013],
          [-0.0019, -0.0014,  0.0016],
          [ 0.0010,  0.0014,  0.0025]],

         [[-0.0038, -0.0042,  0.0000],
          [-0.0037, -0.0036, -0.0003],
          [-0.0008, -0.0007,  0.0012]],

         [[-0.0021, -0.0017,  0.0026],
          [-0.0027, -0.0021,  0.0019],
          [-0.0009, -0.0004,  0.0025]]],


        ...,


        [[[ 0.0667,  0.0567,  0.0538],
          [ 0.0640,  0.0516,  0.0559],
          [ 0.0531,  0.0489,  0.0571]],

         [[ 0.0688,  0.0577,  0.0537],
          [ 0.0616,  0.0488,  0.0517],
          [ 0.0516,  0.0468,  0.0522]],

         [[ 0.0588,  0.0502,  0.0466],
          [ 0.0509,  0.0400,  0.0409],
          [ 0.0432,  0.0373,  0.0405]]],


        [[[ 0.0075,  0.0000,  0.0095],
          [ 0.0040, -0.0018,  0.0038],
          [ 0.0066,  0.0027,  0.0050]],

         [[ 0.0039, -0.0052,  0.0028],
          [-0.0008, -0.0088, -0.0044],
          [ 0.0025, -0.0042, -0.0026]],

         [[-0.0029, -0.0105, -0.0041],
          [-0.0080, -0.0142, -0.0107],
          [-0.0042, -0.0086, -0.0072]]],


        [[[ 0.0161,  0.0386,  0.0362],
          [ 0.0388,  0.0564,  0.0538],
          [ 0.0573,  0.0630,  0.0486]],

         [[-0.0205, -0.0008, -0.0090],
          [ 0.0117,  0.0270,  0.0170],
          [ 0.0338,  0.0398,  0.0201]],

         [[-0.0775, -0.0630, -0.0716],
          [-0.0430, -0.0313, -0.0448],
          [-0.0152, -0.0138, -0.0371]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1890]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 52 | Batch_idx: 0 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (1211/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (2331/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (85.00%) (3410/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (85.00%) (4512/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (5627/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (6710/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4094) |  Loss2: (0.0000) | Acc: (86.00%) (7820/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (86.00%) (8930/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (86.00%) (10020/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (86.00%) (11131/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4052) |  Loss2: (0.0000) | Acc: (86.00%) (12236/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (13349/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4034) |  Loss2: (0.0000) | Acc: (86.00%) (14445/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (86.00%) (15534/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (16613/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4101) |  Loss2: (0.0000) | Acc: (85.00%) (17706/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (18788/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (19907/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (21018/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (22124/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (85.00%) (23223/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (86.00%) (24335/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4065) |  Loss2: (0.0000) | Acc: (86.00%) (25432/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4069) |  Loss2: (0.0000) | Acc: (85.00%) (26527/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (27613/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (28685/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (29792/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (85.00%) (30903/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (32008/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (33101/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (34212/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (35305/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (36393/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (37478/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4102) |  Loss2: (0.0000) | Acc: (85.00%) (38581/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (85.00%) (39696/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (40805/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (41919/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (42969/50000)
# TEST : Loss: (0.4941) | Acc: (83.00%) (8355/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1486,  0.0571,  0.0856],
          [ 0.0988, -0.2537, -0.0964],
          [ 0.2351, -0.0976,  0.0234]],

         [[ 0.0061, -0.1100,  0.2443],
          [ 0.0234, -0.0748,  0.0346],
          [ 0.0607, -0.0035, -0.0653]],

         [[ 0.0194, -0.0732,  0.1586],
          [ 0.1133, -0.1699,  0.1428],
          [-0.0845,  0.0438, -0.1855]]],


        [[[ 0.0955, -0.0856, -0.0082],
          [ 0.1371,  0.1701,  0.1794],
          [ 0.0153, -0.0619,  0.1183]],

         [[-0.1688, -0.1924,  0.1573],
          [ 0.0261, -0.0558,  0.1049],
          [ 0.1275, -0.2014,  0.1378]],

         [[ 0.0954, -0.0520, -0.0118],
          [ 0.0113,  0.0279, -0.1125],
          [-0.0315, -0.1510, -0.0078]]],


        [[[-0.0674,  0.0947, -0.0405],
          [-0.1212,  0.0683,  0.1209],
          [ 0.1182,  0.1701,  0.0035]],

         [[ 0.1013,  0.1337,  0.1140],
          [ 0.0720, -0.0414, -0.0380],
          [-0.0459, -0.0321,  0.1416]],

         [[-0.0469,  0.1465,  0.0750],
          [-0.0419, -0.0466, -0.0606],
          [-0.0391, -0.0085,  0.0450]]],


        ...,


        [[[ 0.2252, -0.1144, -0.1113],
          [-0.0423, -0.1019, -0.0973],
          [-0.0960,  0.0233,  0.0593]],

         [[ 0.1699,  0.1920, -0.1170],
          [ 0.2118, -0.1381, -0.1177],
          [-0.0674, -0.0818, -0.0367]],

         [[ 0.1891,  0.1040, -0.0970],
          [-0.0747, -0.1548, -0.0708],
          [ 0.0571,  0.1776, -0.1315]]],


        [[[-0.1181,  0.0832, -0.1594],
          [-0.1431, -0.1440,  0.1211],
          [-0.1613,  0.1513, -0.1307]],

         [[ 0.0867, -0.1279,  0.0741],
          [ 0.1535,  0.0060, -0.0871],
          [-0.1557,  0.1123, -0.1977]],

         [[ 0.1126, -0.0324, -0.0136],
          [ 0.1347, -0.1831,  0.0056],
          [-0.0399,  0.0088, -0.1259]]],


        [[[ 0.0747,  0.1331,  0.0939],
          [-0.0929, -0.1567,  0.1986],
          [-0.1624, -0.0065, -0.0537]],

         [[-0.0330, -0.1003,  0.1429],
          [-0.0152, -0.1909,  0.0397],
          [-0.0080,  0.0703,  0.0353]],

         [[-0.1514, -0.1966,  0.1449],
          [ 0.1414, -0.1704, -0.0294],
          [-0.0083, -0.0484,  0.2693]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1740,  0.1513,  0.1593],
          [ 0.2149,  0.1816,  0.1816],
          [ 0.2842,  0.2322,  0.2143]],

         [[ 0.1260,  0.0835,  0.0778],
          [ 0.1789,  0.1227,  0.0982],
          [ 0.2475,  0.1734,  0.1294]],

         [[-0.0076, -0.0324, -0.0563],
          [ 0.0587,  0.0247, -0.0067],
          [ 0.1246,  0.0735,  0.0396]]],


        [[[-0.0008,  0.0065,  0.0099],
          [-0.0099, -0.0069,  0.0016],
          [-0.0135, -0.0164, -0.0080]],

         [[-0.0033,  0.0045,  0.0079],
          [-0.0088, -0.0044,  0.0042],
          [-0.0104, -0.0128, -0.0032]],

         [[ 0.0065,  0.0114,  0.0131],
          [-0.0029, -0.0007,  0.0063],
          [-0.0061, -0.0104, -0.0030]]],


        [[[-0.0077, -0.0045, -0.0019],
          [-0.0092, -0.0080, -0.0048],
          [-0.0083, -0.0094, -0.0072]],

         [[ 0.0019,  0.0043,  0.0062],
          [ 0.0004,  0.0010,  0.0037],
          [ 0.0009, -0.0002,  0.0017]],

         [[ 0.0111,  0.0139,  0.0154],
          [ 0.0099,  0.0109,  0.0133],
          [ 0.0106,  0.0095,  0.0109]]],


        ...,


        [[[-0.0562, -0.0450, -0.0371],
          [-0.0577, -0.0435, -0.0354],
          [-0.0331, -0.0259, -0.0218]],

         [[-0.0451, -0.0357, -0.0260],
          [-0.0434, -0.0312, -0.0227],
          [-0.0144, -0.0102, -0.0070]],

         [[-0.0130, -0.0179, -0.0143],
          [-0.0144, -0.0178, -0.0166],
          [ 0.0101,  0.0005, -0.0051]]],


        [[[ 0.0060,  0.0029, -0.0023],
          [ 0.0088,  0.0067, -0.0015],
          [ 0.0072,  0.0051, -0.0020]],

         [[-0.0029, -0.0092, -0.0131],
          [ 0.0015, -0.0031, -0.0092],
          [ 0.0026, -0.0012, -0.0056]],

         [[ 0.0023, -0.0043, -0.0079],
          [ 0.0053,  0.0007, -0.0042],
          [ 0.0043,  0.0011, -0.0018]]],


        [[[-0.0726, -0.0744, -0.0569],
          [-0.0637, -0.0690, -0.0711],
          [-0.0568, -0.0669, -0.0769]],

         [[-0.0862, -0.0912, -0.0873],
          [-0.0861, -0.1012, -0.1142],
          [-0.0903, -0.1119, -0.1251]],

         [[-0.1525, -0.1617, -0.1624],
          [-0.1714, -0.1948, -0.2051],
          [-0.1848, -0.2109, -0.2174]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1889]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 53 | Batch_idx: 0 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (85.00%) (1201/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (2314/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (3424/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (4547/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (5661/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (6783/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (7895/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3919) |  Loss2: (0.0000) | Acc: (86.00%) (9011/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (10103/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (11199/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (12289/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (13389/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (14510/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (15604/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (16719/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (17833/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (18952/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (20054/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (21159/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (22293/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (23392/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3940) |  Loss2: (0.0000) | Acc: (86.00%) (24487/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (25573/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (26681/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (27784/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (28910/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (30022/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (31143/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (32248/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (33357/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (34455/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (35568/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (36669/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3940) |  Loss2: (0.0000) | Acc: (86.00%) (37779/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (38886/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (39993/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (41081/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (42186/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (43267/50000)
# TEST : Loss: (0.5132) | Acc: (83.00%) (8313/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1474,  0.0597,  0.0868],
          [ 0.1008, -0.2577, -0.0995],
          [ 0.2358, -0.1018,  0.0175]],

         [[ 0.0069, -0.1075,  0.2430],
          [ 0.0259, -0.0778,  0.0307],
          [ 0.0607, -0.0073, -0.0704]],

         [[ 0.0232, -0.0680,  0.1612],
          [ 0.1163, -0.1723,  0.1400],
          [-0.0866,  0.0373, -0.1914]]],


        [[[ 0.0950, -0.0860, -0.0077],
          [ 0.1361,  0.1693,  0.1801],
          [ 0.0124, -0.0654,  0.1160]],

         [[-0.1674, -0.1919,  0.1586],
          [ 0.0265, -0.0561,  0.1059],
          [ 0.1255, -0.2048,  0.1356]],

         [[ 0.0980, -0.0502, -0.0089],
          [ 0.0134,  0.0288, -0.1103],
          [-0.0312, -0.1526, -0.0084]]],


        [[[-0.0684,  0.0941, -0.0406],
          [-0.1219,  0.0679,  0.1207],
          [ 0.1173,  0.1694,  0.0031]],

         [[ 0.1012,  0.1340,  0.1146],
          [ 0.0719, -0.0411, -0.0375],
          [-0.0462, -0.0323,  0.1415]],

         [[-0.0469,  0.1465,  0.0753],
          [-0.0420, -0.0465, -0.0604],
          [-0.0394, -0.0088,  0.0446]]],


        ...,


        [[[ 0.2243, -0.1148, -0.1131],
          [-0.0433, -0.1022, -0.0992],
          [-0.0950,  0.0246,  0.0588]],

         [[ 0.1681,  0.1912, -0.1189],
          [ 0.2107, -0.1384, -0.1199],
          [-0.0667, -0.0809, -0.0379]],

         [[ 0.1864,  0.1030, -0.0985],
          [-0.0755, -0.1549, -0.0723],
          [ 0.0587,  0.1791, -0.1317]]],


        [[[-0.1172,  0.0841, -0.1582],
          [-0.1436, -0.1443,  0.1208],
          [-0.1618,  0.1513, -0.1313]],

         [[ 0.0870, -0.1271,  0.0751],
          [ 0.1528,  0.0057, -0.0871],
          [-0.1560,  0.1125, -0.1979]],

         [[ 0.1128, -0.0318, -0.0125],
          [ 0.1341, -0.1831,  0.0058],
          [-0.0401,  0.0091, -0.1257]]],


        [[[ 0.0797,  0.1362,  0.0985],
          [-0.0921, -0.1592,  0.1992],
          [-0.1608, -0.0096, -0.0559]],

         [[-0.0286, -0.0969,  0.1479],
          [-0.0159, -0.1931,  0.0412],
          [-0.0074,  0.0682,  0.0344]],

         [[-0.1453, -0.1913,  0.1502],
          [ 0.1431, -0.1696, -0.0267],
          [-0.0044, -0.0462,  0.2712]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0089, -0.0147, -0.0543],
          [ 0.0006, -0.0190, -0.0554],
          [ 0.0045, -0.0068, -0.0468]],

         [[-0.0214, -0.0380, -0.0611],
          [-0.0300, -0.0481, -0.0746],
          [-0.0214, -0.0365, -0.0714]],

         [[-0.0994, -0.1120, -0.1315],
          [-0.1094, -0.1193, -0.1364],
          [-0.1029, -0.1068, -0.1230]]],


        [[[ 0.0149,  0.0172,  0.0213],
          [ 0.0143,  0.0182,  0.0199],
          [ 0.0237,  0.0320,  0.0310]],

         [[ 0.0128,  0.0110,  0.0107],
          [ 0.0103,  0.0113,  0.0083],
          [ 0.0189,  0.0252,  0.0201]],

         [[ 0.0089,  0.0151,  0.0253],
          [ 0.0104,  0.0190,  0.0266],
          [ 0.0192,  0.0315,  0.0341]]],


        [[[ 0.0092,  0.0075,  0.0051],
          [ 0.0075,  0.0061,  0.0033],
          [ 0.0036,  0.0027, -0.0006]],

         [[ 0.0016,  0.0005, -0.0021],
          [-0.0005, -0.0011, -0.0039],
          [-0.0048, -0.0050, -0.0083]],

         [[ 0.0025,  0.0012, -0.0015],
          [ 0.0003, -0.0005, -0.0031],
          [-0.0035, -0.0038, -0.0067]]],


        ...,


        [[[-0.0670, -0.0564, -0.0360],
          [-0.0620, -0.0520, -0.0369],
          [-0.0517, -0.0466, -0.0326]],

         [[-0.0563, -0.0494, -0.0388],
          [-0.0525, -0.0452, -0.0386],
          [-0.0394, -0.0377, -0.0332]],

         [[-0.0438, -0.0389, -0.0310],
          [-0.0398, -0.0334, -0.0296],
          [-0.0252, -0.0238, -0.0234]]],


        [[[-0.0094, -0.0126, -0.0109],
          [ 0.0015, -0.0018, -0.0026],
          [ 0.0102,  0.0039,  0.0038]],

         [[-0.0093, -0.0130, -0.0130],
          [-0.0020, -0.0060, -0.0080],
          [ 0.0026, -0.0044, -0.0052]],

         [[-0.0045, -0.0063, -0.0055],
          [ 0.0012, -0.0007, -0.0019],
          [ 0.0066,  0.0020,  0.0006]]],


        [[[ 0.0368,  0.0473,  0.0410],
          [ 0.0277,  0.0430,  0.0287],
          [ 0.0252,  0.0294,  0.0255]],

         [[ 0.0148,  0.0213,  0.0193],
          [ 0.0175,  0.0296,  0.0194],
          [ 0.0219,  0.0232,  0.0230]],

         [[-0.0149, -0.0073, -0.0029],
          [-0.0109, -0.0039, -0.0110],
          [-0.0107, -0.0162, -0.0172]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1888]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 54 | Batch_idx: 0 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (1227/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (2343/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3760) |  Loss2: (0.0000) | Acc: (86.00%) (3442/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (4567/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (5682/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (6782/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (7901/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (9013/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (86.00%) (10131/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (11240/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (12354/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (13457/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (14575/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (15696/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (16798/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (17914/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (19029/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (20106/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3809) |  Loss2: (0.0000) | Acc: (86.00%) (21231/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (22368/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (86.00%) (23472/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (24543/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (25664/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3813) |  Loss2: (0.0000) | Acc: (86.00%) (26792/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (27902/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (29009/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (30123/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (31257/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (32352/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (33444/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (34558/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (35657/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (36767/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (37860/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (38986/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (40102/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (41214/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3829) |  Loss2: (0.0000) | Acc: (86.00%) (42331/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (43397/50000)
# TEST : Loss: (0.5773) | Acc: (80.00%) (8099/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1551,  0.0526,  0.0859],
          [ 0.0966, -0.2656, -0.1010],
          [ 0.2350, -0.1022,  0.0166]],

         [[-0.0029, -0.1141,  0.2421],
          [ 0.0193, -0.0861,  0.0287],
          [ 0.0565, -0.0102, -0.0735]],

         [[ 0.0192, -0.0687,  0.1647],
          [ 0.1138, -0.1754,  0.1432],
          [-0.0877,  0.0383, -0.1878]]],


        [[[ 0.0918, -0.0881, -0.0091],
          [ 0.1335,  0.1678,  0.1799],
          [ 0.0087, -0.0674,  0.1162]],

         [[-0.1693, -0.1936,  0.1573],
          [ 0.0247, -0.0575,  0.1058],
          [ 0.1221, -0.2066,  0.1361]],

         [[ 0.0961, -0.0518, -0.0099],
          [ 0.0118,  0.0274, -0.1103],
          [-0.0334, -0.1541, -0.0080]]],


        [[[-0.0686,  0.0938, -0.0406],
          [-0.1220,  0.0681,  0.1211],
          [ 0.1172,  0.1696,  0.0035]],

         [[ 0.1009,  0.1337,  0.1145],
          [ 0.0717, -0.0410, -0.0371],
          [-0.0463, -0.0322,  0.1417]],

         [[-0.0469,  0.1464,  0.0752],
          [-0.0418, -0.0462, -0.0600],
          [-0.0391, -0.0084,  0.0450]]],


        ...,


        [[[ 0.2235, -0.1146, -0.1135],
          [-0.0458, -0.1043, -0.1010],
          [-0.0966,  0.0236,  0.0578]],

         [[ 0.1670,  0.1924, -0.1177],
          [ 0.2087, -0.1387, -0.1201],
          [-0.0675, -0.0803, -0.0374]],

         [[ 0.1876,  0.1058, -0.0959],
          [-0.0749, -0.1537, -0.0712],
          [ 0.0602,  0.1810, -0.1301]]],


        [[[-0.1159,  0.0850, -0.1570],
          [-0.1432, -0.1439,  0.1212],
          [-0.1622,  0.1515, -0.1313]],

         [[ 0.0876, -0.1261,  0.0765],
          [ 0.1528,  0.0061, -0.0865],
          [-0.1567,  0.1124, -0.1980]],

         [[ 0.1139, -0.0305, -0.0108],
          [ 0.1350, -0.1820,  0.0071],
          [-0.0400,  0.0096, -0.1255]]],


        [[[ 0.0823,  0.1376,  0.0991],
          [-0.0923, -0.1624,  0.1978],
          [-0.1577, -0.0097, -0.0563]],

         [[-0.0244, -0.0946,  0.1487],
          [-0.0152, -0.1952,  0.0406],
          [-0.0035,  0.0702,  0.0357]],

         [[-0.1460, -0.1947,  0.1455],
          [ 0.1399, -0.1749, -0.0300],
          [-0.0033, -0.0461,  0.2718]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1347, -0.1702, -0.2246],
          [-0.1278, -0.1521, -0.2114],
          [-0.1177, -0.1419, -0.1781]],

         [[-0.1050, -0.1458, -0.1959],
          [-0.0996, -0.1299, -0.1847],
          [-0.0951, -0.1200, -0.1519]],

         [[-0.1544, -0.1839, -0.2322],
          [-0.1528, -0.1714, -0.2188],
          [-0.1482, -0.1601, -0.1849]]],


        [[[-0.0263, -0.0244, -0.0085],
          [-0.0127, -0.0099,  0.0032],
          [-0.0078, -0.0023,  0.0051]],

         [[-0.0292, -0.0268, -0.0128],
          [-0.0120, -0.0089,  0.0016],
          [-0.0064, -0.0014,  0.0029]],

         [[-0.0041, -0.0021,  0.0102],
          [ 0.0116,  0.0162,  0.0252],
          [ 0.0115,  0.0191,  0.0235]]],


        [[[ 0.0001,  0.0002, -0.0004],
          [ 0.0029,  0.0025,  0.0017],
          [ 0.0061,  0.0058,  0.0053]],

         [[-0.0039, -0.0043, -0.0053],
          [-0.0019, -0.0028, -0.0039],
          [ 0.0016,  0.0007, -0.0000]],

         [[-0.0017, -0.0010, -0.0014],
          [ 0.0000,  0.0001,  0.0001],
          [ 0.0025,  0.0019,  0.0023]]],


        ...,


        [[[ 0.0111,  0.0110,  0.0079],
          [ 0.0226,  0.0161,  0.0010],
          [ 0.0255,  0.0256,  0.0081]],

         [[-0.0058, -0.0060, -0.0076],
          [ 0.0035, -0.0020, -0.0148],
          [ 0.0020,  0.0038, -0.0113]],

         [[-0.0008, -0.0024, -0.0062],
          [ 0.0119,  0.0052, -0.0080],
          [ 0.0118,  0.0119, -0.0035]]],


        [[[ 0.0001,  0.0023,  0.0036],
          [ 0.0002,  0.0003,  0.0007],
          [ 0.0023,  0.0046,  0.0034]],

         [[-0.0130, -0.0103, -0.0071],
          [-0.0143, -0.0130, -0.0105],
          [-0.0128, -0.0098, -0.0102]],

         [[-0.0039, -0.0007,  0.0010],
          [-0.0038, -0.0013,  0.0009],
          [-0.0037, -0.0001,  0.0004]]],


        [[[-0.0717, -0.0957, -0.0894],
          [-0.0563, -0.0893, -0.0849],
          [-0.1042, -0.1206, -0.1068]],

         [[-0.0200, -0.0415, -0.0406],
          [ 0.0021, -0.0308, -0.0372],
          [-0.0381, -0.0541, -0.0519]],

         [[-0.0787, -0.1077, -0.1081],
          [-0.0612, -0.1019, -0.1068],
          [-0.0956, -0.1193, -0.1148]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.1887]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.7687) |  Loss2: (0.3362) | Acc: (85.00%) (109/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.7494) |  Loss2: (0.3362) | Acc: (85.00%) (1203/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.7782) |  Loss2: (0.3361) | Acc: (84.00%) (2272/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.7846) |  Loss2: (0.3361) | Acc: (84.00%) (3343/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.8025) |  Loss2: (0.3360) | Acc: (83.00%) (4391/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.8123) |  Loss2: (0.3360) | Acc: (83.00%) (5450/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.8198) |  Loss2: (0.3360) | Acc: (83.00%) (6492/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.8253) |  Loss2: (0.3359) | Acc: (82.00%) (7531/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.8245) |  Loss2: (0.3359) | Acc: (82.00%) (8597/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.8263) |  Loss2: (0.3359) | Acc: (82.00%) (9646/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.8226) |  Loss2: (0.3358) | Acc: (82.00%) (10726/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.8218) |  Loss2: (0.3358) | Acc: (83.00%) (11807/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.8187) |  Loss2: (0.3357) | Acc: (83.00%) (12879/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.8134) |  Loss2: (0.3357) | Acc: (83.00%) (13970/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.8120) |  Loss2: (0.3357) | Acc: (83.00%) (15042/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.8069) |  Loss2: (0.3356) | Acc: (83.00%) (16151/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.8081) |  Loss2: (0.3356) | Acc: (83.00%) (17196/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.8070) |  Loss2: (0.3355) | Acc: (83.00%) (18276/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.8065) |  Loss2: (0.3355) | Acc: (83.00%) (19361/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.8059) |  Loss2: (0.3355) | Acc: (83.00%) (20430/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.8063) |  Loss2: (0.3354) | Acc: (83.00%) (21504/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.8058) |  Loss2: (0.3354) | Acc: (83.00%) (22578/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.8059) |  Loss2: (0.3353) | Acc: (83.00%) (23650/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.8064) |  Loss2: (0.3353) | Acc: (83.00%) (24721/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.8051) |  Loss2: (0.3352) | Acc: (83.00%) (25795/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.8043) |  Loss2: (0.3352) | Acc: (83.00%) (26866/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.8038) |  Loss2: (0.3352) | Acc: (83.00%) (27949/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.8029) |  Loss2: (0.3351) | Acc: (83.00%) (29031/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.8025) |  Loss2: (0.3351) | Acc: (83.00%) (30106/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.8034) |  Loss2: (0.3350) | Acc: (83.00%) (31171/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.8011) |  Loss2: (0.3350) | Acc: (83.00%) (32285/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.8003) |  Loss2: (0.3349) | Acc: (83.00%) (33373/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.8003) |  Loss2: (0.3349) | Acc: (83.00%) (34446/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.8001) |  Loss2: (0.3348) | Acc: (83.00%) (35531/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.7985) |  Loss2: (0.3348) | Acc: (83.00%) (36629/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.7979) |  Loss2: (0.3347) | Acc: (83.00%) (37713/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.7977) |  Loss2: (0.3347) | Acc: (83.00%) (38781/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.7974) |  Loss2: (0.3347) | Acc: (83.00%) (39851/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.7973) |  Loss2: (0.3346) | Acc: (83.00%) (40924/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.7972) |  Loss2: (0.3346) | Acc: (83.00%) (41964/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_055.pth.tar'
# TEST : Loss: (0.5489) | Acc: (81.00%) (8197/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1581,  0.0494,  0.0830],
          [ 0.0936, -0.2688, -0.1038],
          [ 0.2317, -0.1055,  0.0138]],

         [[-0.0066, -0.1179,  0.2383],
          [ 0.0156, -0.0900,  0.0251],
          [ 0.0526, -0.0142, -0.0770]],

         [[ 0.0164, -0.0714,  0.1621],
          [ 0.1111, -0.1781,  0.1408],
          [-0.0904,  0.0356, -0.1899]]],


        [[[ 0.0921, -0.0876, -0.0088],
          [ 0.1336,  0.1680,  0.1800],
          [ 0.0087, -0.0672,  0.1162]],

         [[-0.1689, -0.1930,  0.1576],
          [ 0.0249, -0.0571,  0.1060],
          [ 0.1221, -0.2063,  0.1363]],

         [[ 0.0962, -0.0515, -0.0097],
          [ 0.0118,  0.0275, -0.1103],
          [-0.0335, -0.1540, -0.0081]]],


        [[[-0.0686,  0.0937, -0.0406],
          [-0.1221,  0.0680,  0.1210],
          [ 0.1171,  0.1695,  0.0035]],

         [[ 0.1008,  0.1337,  0.1145],
          [ 0.0716, -0.0410, -0.0371],
          [-0.0463, -0.0322,  0.1416]],

         [[-0.0470,  0.1463,  0.0752],
          [-0.0419, -0.0462, -0.0600],
          [-0.0391, -0.0084,  0.0450]]],


        ...,


        [[[ 0.2241, -0.1140, -0.1129],
          [-0.0454, -0.1039, -0.1004],
          [-0.0961,  0.0240,  0.0584]],

         [[ 0.1678,  0.1931, -0.1169],
          [ 0.2094, -0.1381, -0.1193],
          [-0.0667, -0.0796, -0.0365]],

         [[ 0.1882,  0.1063, -0.0953],
          [-0.0744, -0.1532, -0.0707],
          [ 0.0608,  0.1815, -0.1294]]],


        [[[-0.1158,  0.0850, -0.1569],
          [-0.1432, -0.1437,  0.1213],
          [-0.1622,  0.1514, -0.1312]],

         [[ 0.0877, -0.1259,  0.0765],
          [ 0.1528,  0.0063, -0.0862],
          [-0.1566,  0.1125, -0.1978]],

         [[ 0.1139, -0.0305, -0.0109],
          [ 0.1350, -0.1819,  0.0071],
          [-0.0401,  0.0095, -0.1255]]],


        [[[ 0.0835,  0.1389,  0.1001],
          [-0.0913, -0.1613,  0.1985],
          [-0.1566, -0.0085, -0.0553]],

         [[-0.0241, -0.0943,  0.1487],
          [-0.0153, -0.1951,  0.0405],
          [-0.0035,  0.0704,  0.0358]],

         [[-0.1458, -0.1944,  0.1455],
          [ 0.1399, -0.1747, -0.0300],
          [-0.0030, -0.0456,  0.2719]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5806e-05,  4.9359e-06,  8.2986e-06],
          [ 9.3584e-06, -2.6876e-05, -1.0384e-05],
          [ 2.3171e-05, -1.0548e-05,  1.3794e-06]],

         [[-6.6138e-07, -1.1791e-05,  2.3833e-05],
          [ 1.5615e-06, -8.9998e-06,  2.5093e-06],
          [ 5.2570e-06, -1.4235e-06, -7.6953e-06]],

         [[ 1.6361e-06, -7.1414e-06,  1.6213e-05],
          [ 1.1112e-05, -1.7812e-05,  1.4078e-05],
          [-9.0393e-06,  3.5607e-06, -1.8985e-05]]],


        [[[ 9.2148e-06, -8.7643e-06, -8.7753e-07],
          [ 1.3364e-05,  1.6801e-05,  1.8000e-05],
          [ 8.6921e-07, -6.7181e-06,  1.1625e-05]],

         [[-1.6887e-05, -1.9301e-05,  1.5763e-05],
          [ 2.4876e-06, -5.7112e-06,  1.0604e-05],
          [ 1.2210e-05, -2.0631e-05,  1.3627e-05]],

         [[ 9.6168e-06, -5.1545e-06, -9.7392e-07],
          [ 1.1757e-06,  2.7501e-06, -1.1031e-05],
          [-3.3464e-06, -1.5401e-05, -8.1210e-07]]],


        [[[-6.8642e-06,  9.3747e-06, -4.0581e-06],
          [-1.2211e-05,  6.7993e-06,  1.2102e-05],
          [ 1.1709e-05,  1.6948e-05,  3.5066e-07]],

         [[ 1.0085e-05,  1.3365e-05,  1.1451e-05],
          [ 7.1608e-06, -4.0960e-06, -3.7053e-06],
          [-4.6348e-06, -3.2157e-06,  1.4160e-05]],

         [[-4.6969e-06,  1.4631e-05,  7.5210e-06],
          [-4.1874e-06, -4.6165e-06, -5.9979e-06],
          [-3.9137e-06, -8.3748e-07,  4.4994e-06]]],


        ...,


        [[[ 2.2409e-05, -1.1401e-05, -1.1295e-05],
          [-4.5373e-06, -1.0385e-05, -1.0037e-05],
          [-9.6097e-06,  2.4023e-06,  5.8437e-06]],

         [[ 1.6783e-05,  1.9312e-05, -1.1687e-05],
          [ 2.0936e-05, -1.3806e-05, -1.1931e-05],
          [-6.6746e-06, -7.9597e-06, -3.6537e-06]],

         [[ 1.8823e-05,  1.0631e-05, -9.5270e-06],
          [-7.4394e-06, -1.5318e-05, -7.0658e-06],
          [ 6.0755e-06,  1.8146e-05, -1.2938e-05]]],


        [[[-1.1583e-05,  8.5034e-06, -1.5694e-05],
          [-1.4318e-05, -1.4371e-05,  1.2133e-05],
          [-1.6220e-05,  1.5139e-05, -1.3124e-05]],

         [[ 8.7693e-06, -1.2593e-05,  7.6531e-06],
          [ 1.5282e-05,  6.2992e-07, -8.6228e-06],
          [-1.5664e-05,  1.1249e-05, -1.9783e-05]],

         [[ 1.1388e-05, -3.0514e-06, -1.0928e-06],
          [ 1.3498e-05, -1.8185e-05,  7.0743e-07],
          [-4.0081e-06,  9.4945e-07, -1.2552e-05]]],


        [[[ 8.3521e-06,  1.3889e-05,  1.0012e-05],
          [-9.1309e-06, -1.6126e-05,  1.9853e-05],
          [-1.5657e-05, -8.4989e-07, -5.5292e-06]],

         [[-2.4110e-06, -9.4336e-06,  1.4872e-05],
          [-1.5256e-06, -1.9513e-05,  4.0455e-06],
          [-3.4513e-07,  7.0417e-06,  3.5827e-06]],

         [[-1.4578e-05, -1.9440e-05,  1.4546e-05],
          [ 1.3994e-05, -1.7472e-05, -3.0039e-06],
          [-3.0265e-07, -4.5608e-06,  2.7193e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3034]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0665]], device='cuda:0')

Epoch: 56 | Batch_idx: 0 |  Loss: (0.6363) |  Loss2: (0.3327) | Acc: (89.00%) (114/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.7367) |  Loss2: (0.3326) | Acc: (86.00%) (1211/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.7331) |  Loss2: (0.3326) | Acc: (86.00%) (2318/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.7535) |  Loss2: (0.3326) | Acc: (85.00%) (3398/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.7603) |  Loss2: (0.3325) | Acc: (85.00%) (4482/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.7583) |  Loss2: (0.3325) | Acc: (85.00%) (5580/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.7673) |  Loss2: (0.3325) | Acc: (85.00%) (6666/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.7640) |  Loss2: (0.3324) | Acc: (85.00%) (7762/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.7680) |  Loss2: (0.3324) | Acc: (85.00%) (8838/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.7651) |  Loss2: (0.3323) | Acc: (85.00%) (9919/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.7639) |  Loss2: (0.3323) | Acc: (85.00%) (11012/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.7631) |  Loss2: (0.3322) | Acc: (85.00%) (12113/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.7634) |  Loss2: (0.3322) | Acc: (85.00%) (13199/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.7621) |  Loss2: (0.3321) | Acc: (85.00%) (14301/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.7639) |  Loss2: (0.3321) | Acc: (85.00%) (15365/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.7643) |  Loss2: (0.3320) | Acc: (85.00%) (16447/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.7622) |  Loss2: (0.3320) | Acc: (85.00%) (17549/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.7614) |  Loss2: (0.3320) | Acc: (85.00%) (18654/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.7614) |  Loss2: (0.3319) | Acc: (85.00%) (19737/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.7629) |  Loss2: (0.3319) | Acc: (85.00%) (20817/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.7614) |  Loss2: (0.3318) | Acc: (85.00%) (21918/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.7601) |  Loss2: (0.3318) | Acc: (85.00%) (23021/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.7590) |  Loss2: (0.3317) | Acc: (85.00%) (24129/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.7581) |  Loss2: (0.3317) | Acc: (85.00%) (25224/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.7569) |  Loss2: (0.3316) | Acc: (85.00%) (26322/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.7556) |  Loss2: (0.3316) | Acc: (85.00%) (27423/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.7544) |  Loss2: (0.3315) | Acc: (85.00%) (28535/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.7543) |  Loss2: (0.3315) | Acc: (85.00%) (29621/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.7543) |  Loss2: (0.3314) | Acc: (85.00%) (30719/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.7545) |  Loss2: (0.3314) | Acc: (85.00%) (31805/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.7548) |  Loss2: (0.3313) | Acc: (85.00%) (32882/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.7531) |  Loss2: (0.3313) | Acc: (85.00%) (34001/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.7525) |  Loss2: (0.3312) | Acc: (85.00%) (35100/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.7527) |  Loss2: (0.3311) | Acc: (85.00%) (36195/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.7520) |  Loss2: (0.3311) | Acc: (85.00%) (37288/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.7514) |  Loss2: (0.3310) | Acc: (85.00%) (38389/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.7519) |  Loss2: (0.3310) | Acc: (85.00%) (39480/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.7514) |  Loss2: (0.3309) | Acc: (85.00%) (40565/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.7512) |  Loss2: (0.3309) | Acc: (85.00%) (41654/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.7502) |  Loss2: (0.3308) | Acc: (85.00%) (42714/50000)
# TEST : Loss: (0.5151) | Acc: (83.00%) (8312/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1580,  0.0493,  0.0830],
          [ 0.0935, -0.2687, -0.1038],
          [ 0.2316, -0.1054,  0.0138]],

         [[-0.0066, -0.1179,  0.2382],
          [ 0.0156, -0.0900,  0.0251],
          [ 0.0525, -0.0142, -0.0769]],

         [[ 0.0164, -0.0714,  0.1621],
          [ 0.1111, -0.1781,  0.1407],
          [-0.0904,  0.0356, -0.1898]]],


        [[[ 0.0921, -0.0876, -0.0088],
          [ 0.1336,  0.1679,  0.1799],
          [ 0.0087, -0.0672,  0.1162]],

         [[-0.1688, -0.1929,  0.1576],
          [ 0.0249, -0.0571,  0.1060],
          [ 0.1221, -0.2062,  0.1362]],

         [[ 0.0961, -0.0515, -0.0097],
          [ 0.0118,  0.0275, -0.1103],
          [-0.0335, -0.1540, -0.0081]]],


        [[[-0.0686,  0.0937, -0.0406],
          [-0.1221,  0.0680,  0.1210],
          [ 0.1170,  0.1694,  0.0035]],

         [[ 0.1008,  0.1336,  0.1145],
          [ 0.0716, -0.0409, -0.0370],
          [-0.0463, -0.0321,  0.1415]],

         [[-0.0470,  0.1462,  0.0752],
          [-0.0419, -0.0461, -0.0600],
          [-0.0391, -0.0084,  0.0450]]],


        ...,


        [[[ 0.2240, -0.1140, -0.1129],
          [-0.0454, -0.1038, -0.1003],
          [-0.0961,  0.0240,  0.0584]],

         [[ 0.1678,  0.1930, -0.1168],
          [ 0.2093, -0.1380, -0.1193],
          [-0.0667, -0.0796, -0.0365]],

         [[ 0.1882,  0.1063, -0.0952],
          [-0.0744, -0.1531, -0.0706],
          [ 0.0607,  0.1814, -0.1293]]],


        [[[-0.1158,  0.0850, -0.1569],
          [-0.1431, -0.1436,  0.1213],
          [-0.1621,  0.1513, -0.1312]],

         [[ 0.0877, -0.1259,  0.0765],
          [ 0.1528,  0.0063, -0.0862],
          [-0.1566,  0.1124, -0.1978]],

         [[ 0.1138, -0.0305, -0.0109],
          [ 0.1349, -0.1818,  0.0071],
          [-0.0401,  0.0095, -0.1255]]],


        [[[ 0.0835,  0.1388,  0.1001],
          [-0.0913, -0.1612,  0.1984],
          [-0.1565, -0.0085, -0.0553]],

         [[-0.0241, -0.0943,  0.1487],
          [-0.0153, -0.1951,  0.0404],
          [-0.0034,  0.0704,  0.0358]],

         [[-0.1457, -0.1943,  0.1454],
          [ 0.1399, -0.1746, -0.0300],
          [-0.0030, -0.0456,  0.2718]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5799e-05,  4.9340e-06,  8.2954e-06],
          [ 9.3546e-06, -2.6865e-05, -1.0380e-05],
          [ 2.3162e-05, -1.0544e-05,  1.3789e-06]],

         [[-6.6112e-07, -1.1787e-05,  2.3824e-05],
          [ 1.5609e-06, -8.9963e-06,  2.5083e-06],
          [ 5.2550e-06, -1.4229e-06, -7.6924e-06]],

         [[ 1.6355e-06, -7.1385e-06,  1.6207e-05],
          [ 1.1108e-05, -1.7805e-05,  1.4073e-05],
          [-9.0358e-06,  3.5592e-06, -1.8978e-05]]],


        [[[ 9.2113e-06, -8.7608e-06, -8.7720e-07],
          [ 1.3359e-05,  1.6795e-05,  1.7993e-05],
          [ 8.6889e-07, -6.7155e-06,  1.1620e-05]],

         [[-1.6881e-05, -1.9293e-05,  1.5757e-05],
          [ 2.4866e-06, -5.7090e-06,  1.0600e-05],
          [ 1.2205e-05, -2.0623e-05,  1.3622e-05]],

         [[ 9.6130e-06, -5.1525e-06, -9.7355e-07],
          [ 1.1752e-06,  2.7490e-06, -1.1027e-05],
          [-3.3451e-06, -1.5395e-05, -8.1177e-07]]],


        [[[-6.8616e-06,  9.3709e-06, -4.0565e-06],
          [-1.2206e-05,  6.7967e-06,  1.2098e-05],
          [ 1.1704e-05,  1.6942e-05,  3.5053e-07]],

         [[ 1.0081e-05,  1.3360e-05,  1.1447e-05],
          [ 7.1579e-06, -4.0944e-06, -3.7038e-06],
          [-4.6331e-06, -3.2144e-06,  1.4154e-05]],

         [[-4.6950e-06,  1.4625e-05,  7.5180e-06],
          [-4.1858e-06, -4.6148e-06, -5.9956e-06],
          [-3.9121e-06, -8.3715e-07,  4.4976e-06]]],


        ...,


        [[[ 2.2401e-05, -1.1396e-05, -1.1290e-05],
          [-4.5355e-06, -1.0381e-05, -1.0033e-05],
          [-9.6059e-06,  2.4013e-06,  5.8413e-06]],

         [[ 1.6776e-05,  1.9304e-05, -1.1683e-05],
          [ 2.0927e-05, -1.3800e-05, -1.1927e-05],
          [-6.6720e-06, -7.9565e-06, -3.6523e-06]],

         [[ 1.8816e-05,  1.0627e-05, -9.5233e-06],
          [-7.4365e-06, -1.5312e-05, -7.0632e-06],
          [ 6.0732e-06,  1.8139e-05, -1.2933e-05]]],


        [[[-1.1579e-05,  8.5002e-06, -1.5688e-05],
          [-1.4312e-05, -1.4365e-05,  1.2128e-05],
          [-1.6214e-05,  1.5134e-05, -1.3119e-05]],

         [[ 8.7658e-06, -1.2588e-05,  7.6502e-06],
          [ 1.5276e-05,  6.2967e-07, -8.6193e-06],
          [-1.5658e-05,  1.1244e-05, -1.9776e-05]],

         [[ 1.1383e-05, -3.0503e-06, -1.0924e-06],
          [ 1.3493e-05, -1.8178e-05,  7.0716e-07],
          [-4.0065e-06,  9.4909e-07, -1.2547e-05]]],


        [[[ 8.3489e-06,  1.3883e-05,  1.0008e-05],
          [-9.1274e-06, -1.6119e-05,  1.9845e-05],
          [-1.5651e-05, -8.4956e-07, -5.5270e-06]],

         [[-2.4100e-06, -9.4298e-06,  1.4866e-05],
          [-1.5250e-06, -1.9505e-05,  4.0439e-06],
          [-3.4499e-07,  7.0390e-06,  3.5812e-06]],

         [[-1.4572e-05, -1.9432e-05,  1.4540e-05],
          [ 1.3988e-05, -1.7465e-05, -3.0027e-06],
          [-3.0253e-07, -4.5590e-06,  2.7182e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3372]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0164]], device='cuda:0')

Epoch: 57 | Batch_idx: 0 |  Loss: (0.8118) |  Loss2: (0.3289) | Acc: (85.00%) (109/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.7689) |  Loss2: (0.3288) | Acc: (84.00%) (1189/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.7549) |  Loss2: (0.3288) | Acc: (84.00%) (2284/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.7385) |  Loss2: (0.3287) | Acc: (85.00%) (3391/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.7486) |  Loss2: (0.3287) | Acc: (85.00%) (4471/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.7435) |  Loss2: (0.3286) | Acc: (85.00%) (5568/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.7389) |  Loss2: (0.3286) | Acc: (85.00%) (6680/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.7393) |  Loss2: (0.3286) | Acc: (85.00%) (7770/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.7436) |  Loss2: (0.3285) | Acc: (85.00%) (8846/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.7412) |  Loss2: (0.3285) | Acc: (85.00%) (9958/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.7411) |  Loss2: (0.3284) | Acc: (85.00%) (11059/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.7429) |  Loss2: (0.3284) | Acc: (85.00%) (12138/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.7411) |  Loss2: (0.3283) | Acc: (85.00%) (13245/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.7402) |  Loss2: (0.3283) | Acc: (85.00%) (14343/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.7405) |  Loss2: (0.3282) | Acc: (85.00%) (15437/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.7374) |  Loss2: (0.3282) | Acc: (85.00%) (16559/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.7388) |  Loss2: (0.3281) | Acc: (85.00%) (17655/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.7392) |  Loss2: (0.3281) | Acc: (85.00%) (18753/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.7363) |  Loss2: (0.3280) | Acc: (85.00%) (19872/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.7353) |  Loss2: (0.3280) | Acc: (85.00%) (20977/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.7340) |  Loss2: (0.3279) | Acc: (85.00%) (22092/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.7330) |  Loss2: (0.3279) | Acc: (85.00%) (23205/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.7303) |  Loss2: (0.3278) | Acc: (86.00%) (24329/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.7303) |  Loss2: (0.3278) | Acc: (86.00%) (25433/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.7305) |  Loss2: (0.3277) | Acc: (86.00%) (26535/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.7309) |  Loss2: (0.3277) | Acc: (86.00%) (27632/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.7293) |  Loss2: (0.3276) | Acc: (86.00%) (28737/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.7295) |  Loss2: (0.3276) | Acc: (86.00%) (29832/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.7293) |  Loss2: (0.3275) | Acc: (85.00%) (30931/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.7292) |  Loss2: (0.3275) | Acc: (85.00%) (32022/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.7286) |  Loss2: (0.3274) | Acc: (85.00%) (33130/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.7306) |  Loss2: (0.3274) | Acc: (85.00%) (34205/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.7306) |  Loss2: (0.3273) | Acc: (85.00%) (35296/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.7304) |  Loss2: (0.3273) | Acc: (85.00%) (36402/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.7303) |  Loss2: (0.3272) | Acc: (85.00%) (37507/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.7315) |  Loss2: (0.3272) | Acc: (85.00%) (38596/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.7307) |  Loss2: (0.3271) | Acc: (85.00%) (39707/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.7307) |  Loss2: (0.3271) | Acc: (85.00%) (40811/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.7303) |  Loss2: (0.3271) | Acc: (85.00%) (41922/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.7294) |  Loss2: (0.3270) | Acc: (86.00%) (43000/50000)
# TEST : Loss: (0.4953) | Acc: (83.00%) (8345/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1579,  0.0493,  0.0829],
          [ 0.0935, -0.2685, -0.1038],
          [ 0.2315, -0.1054,  0.0138]],

         [[-0.0066, -0.1178,  0.2381],
          [ 0.0156, -0.0899,  0.0251],
          [ 0.0525, -0.0142, -0.0769]],

         [[ 0.0163, -0.0714,  0.1620],
          [ 0.1110, -0.1780,  0.1407],
          [-0.0903,  0.0356, -0.1897]]],


        [[[ 0.0921, -0.0876, -0.0088],
          [ 0.1335,  0.1679,  0.1799],
          [ 0.0087, -0.0671,  0.1162]],

         [[-0.1687, -0.1929,  0.1575],
          [ 0.0249, -0.0571,  0.1060],
          [ 0.1220, -0.2061,  0.1362]],

         [[ 0.0961, -0.0515, -0.0097],
          [ 0.0117,  0.0275, -0.1102],
          [-0.0334, -0.1539, -0.0081]]],


        [[[-0.0686,  0.0937, -0.0405],
          [-0.1220,  0.0679,  0.1209],
          [ 0.1170,  0.1694,  0.0035]],

         [[ 0.1008,  0.1335,  0.1144],
          [ 0.0715, -0.0409, -0.0370],
          [-0.0463, -0.0321,  0.1415]],

         [[-0.0469,  0.1462,  0.0752],
          [-0.0418, -0.0461, -0.0599],
          [-0.0391, -0.0084,  0.0450]]],


        ...,


        [[[ 0.2239, -0.1139, -0.1129],
          [-0.0453, -0.1038, -0.1003],
          [-0.0960,  0.0240,  0.0584]],

         [[ 0.1677,  0.1930, -0.1168],
          [ 0.2092, -0.1380, -0.1192],
          [-0.0667, -0.0795, -0.0365]],

         [[ 0.1881,  0.1062, -0.0952],
          [-0.0743, -0.1531, -0.0706],
          [ 0.0607,  0.1813, -0.1293]]],


        [[[-0.1157,  0.0850, -0.1568],
          [-0.1431, -0.1436,  0.1212],
          [-0.1621,  0.1513, -0.1311]],

         [[ 0.0876, -0.1258,  0.0765],
          [ 0.1527,  0.0063, -0.0862],
          [-0.1565,  0.1124, -0.1977]],

         [[ 0.1138, -0.0305, -0.0109],
          [ 0.1349, -0.1817,  0.0071],
          [-0.0400,  0.0095, -0.1254]]],


        [[[ 0.0835,  0.1388,  0.1000],
          [-0.0912, -0.1611,  0.1984],
          [-0.1564, -0.0085, -0.0552]],

         [[-0.0241, -0.0943,  0.1486],
          [-0.0152, -0.1950,  0.0404],
          [-0.0034,  0.0704,  0.0358]],

         [[-0.1457, -0.1942,  0.1453],
          [ 0.1398, -0.1746, -0.0300],
          [-0.0030, -0.0456,  0.2717]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5793e-05,  4.9321e-06,  8.2922e-06],
          [ 9.3508e-06, -2.6855e-05, -1.0376e-05],
          [ 2.3153e-05, -1.0540e-05,  1.3783e-06]],

         [[-6.6087e-07, -1.1782e-05,  2.3814e-05],
          [ 1.5603e-06, -8.9928e-06,  2.5074e-06],
          [ 5.2529e-06, -1.4224e-06, -7.6895e-06]],

         [[ 1.6348e-06, -7.1355e-06,  1.6200e-05],
          [ 1.1104e-05, -1.7798e-05,  1.4068e-05],
          [-9.0323e-06,  3.5578e-06, -1.8970e-05]]],


        [[[ 9.2078e-06, -8.7573e-06, -8.7688e-07],
          [ 1.3353e-05,  1.6788e-05,  1.7986e-05],
          [ 8.6856e-07, -6.7129e-06,  1.1615e-05]],

         [[-1.6874e-05, -1.9286e-05,  1.5751e-05],
          [ 2.4857e-06, -5.7068e-06,  1.0596e-05],
          [ 1.2201e-05, -2.0615e-05,  1.3617e-05]],

         [[ 9.6092e-06, -5.1504e-06, -9.7319e-07],
          [ 1.1747e-06,  2.7479e-06, -1.1022e-05],
          [-3.3438e-06, -1.5389e-05, -8.1144e-07]]],


        [[[-6.8590e-06,  9.3671e-06, -4.0549e-06],
          [-1.2201e-05,  6.7941e-06,  1.2093e-05],
          [ 1.1699e-05,  1.6936e-05,  3.5039e-07]],

         [[ 1.0077e-05,  1.3355e-05,  1.1442e-05],
          [ 7.1550e-06, -4.0928e-06, -3.7024e-06],
          [-4.6313e-06, -3.2130e-06,  1.4149e-05]],

         [[-4.6931e-06,  1.4619e-05,  7.5151e-06],
          [-4.1842e-06, -4.6130e-06, -5.9933e-06],
          [-3.9106e-06, -8.3682e-07,  4.4959e-06]]],


        ...,


        [[[ 2.2392e-05, -1.1392e-05, -1.1286e-05],
          [-4.5338e-06, -1.0377e-05, -1.0029e-05],
          [-9.6021e-06,  2.4004e-06,  5.8390e-06]],

         [[ 1.6770e-05,  1.9297e-05, -1.1678e-05],
          [ 2.0919e-05, -1.3795e-05, -1.1922e-05],
          [-6.6693e-06, -7.9533e-06, -3.6508e-06]],

         [[ 1.8808e-05,  1.0623e-05, -9.5195e-06],
          [-7.4336e-06, -1.5306e-05, -7.0605e-06],
          [ 6.0709e-06,  1.8132e-05, -1.2928e-05]]],


        [[[-1.1574e-05,  8.4969e-06, -1.5681e-05],
          [-1.4307e-05, -1.4359e-05,  1.2124e-05],
          [-1.6207e-05,  1.5128e-05, -1.3114e-05]],

         [[ 8.7623e-06, -1.2584e-05,  7.6472e-06],
          [ 1.5271e-05,  6.2941e-07, -8.6158e-06],
          [-1.5651e-05,  1.1240e-05, -1.9768e-05]],

         [[ 1.1379e-05, -3.0491e-06, -1.0919e-06],
          [ 1.3488e-05, -1.8171e-05,  7.0689e-07],
          [-4.0049e-06,  9.4872e-07, -1.2543e-05]]],


        [[[ 8.3457e-06,  1.3878e-05,  1.0004e-05],
          [-9.1239e-06, -1.6113e-05,  1.9837e-05],
          [-1.5645e-05, -8.4924e-07, -5.5248e-06]],

         [[-2.4091e-06, -9.4260e-06,  1.4860e-05],
          [-1.5244e-06, -1.9498e-05,  4.0423e-06],
          [-3.4486e-07,  7.0364e-06,  3.5798e-06]],

         [[-1.4566e-05, -1.9425e-05,  1.4534e-05],
          [ 1.3983e-05, -1.7458e-05, -3.0016e-06],
          [-3.0241e-07, -4.5573e-06,  2.7172e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3611]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0639]], device='cuda:0')

Epoch: 58 | Batch_idx: 0 |  Loss: (0.7393) |  Loss2: (0.3251) | Acc: (87.00%) (112/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.7390) |  Loss2: (0.3251) | Acc: (86.00%) (1216/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.7355) |  Loss2: (0.3250) | Acc: (86.00%) (2312/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.7466) |  Loss2: (0.3250) | Acc: (85.00%) (3389/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.7437) |  Loss2: (0.3249) | Acc: (85.00%) (4487/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.7409) |  Loss2: (0.3249) | Acc: (85.00%) (5584/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.7377) |  Loss2: (0.3248) | Acc: (85.00%) (6688/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.7374) |  Loss2: (0.3248) | Acc: (85.00%) (7787/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.7379) |  Loss2: (0.3247) | Acc: (85.00%) (8888/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.7311) |  Loss2: (0.3247) | Acc: (85.00%) (10000/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.7312) |  Loss2: (0.3246) | Acc: (85.00%) (11102/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.7285) |  Loss2: (0.3246) | Acc: (86.00%) (12222/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.7274) |  Loss2: (0.3246) | Acc: (86.00%) (13322/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.7258) |  Loss2: (0.3245) | Acc: (86.00%) (14445/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.7251) |  Loss2: (0.3245) | Acc: (86.00%) (15554/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.7244) |  Loss2: (0.3244) | Acc: (86.00%) (16656/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.7236) |  Loss2: (0.3244) | Acc: (86.00%) (17764/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.7216) |  Loss2: (0.3243) | Acc: (86.00%) (18877/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.7211) |  Loss2: (0.3243) | Acc: (86.00%) (19984/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.7202) |  Loss2: (0.3243) | Acc: (86.00%) (21085/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.7205) |  Loss2: (0.3242) | Acc: (86.00%) (22168/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.7198) |  Loss2: (0.3242) | Acc: (86.00%) (23262/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.7183) |  Loss2: (0.3241) | Acc: (86.00%) (24382/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.7181) |  Loss2: (0.3241) | Acc: (86.00%) (25485/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.7182) |  Loss2: (0.3240) | Acc: (86.00%) (26593/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.7181) |  Loss2: (0.3240) | Acc: (86.00%) (27702/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.7179) |  Loss2: (0.3239) | Acc: (86.00%) (28815/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.7171) |  Loss2: (0.3239) | Acc: (86.00%) (29934/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.7164) |  Loss2: (0.3238) | Acc: (86.00%) (31045/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.7156) |  Loss2: (0.3238) | Acc: (86.00%) (32163/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.7147) |  Loss2: (0.3238) | Acc: (86.00%) (33268/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.7155) |  Loss2: (0.3237) | Acc: (86.00%) (34360/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.7166) |  Loss2: (0.3237) | Acc: (86.00%) (35446/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.7163) |  Loss2: (0.3236) | Acc: (86.00%) (36556/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.7163) |  Loss2: (0.3236) | Acc: (86.00%) (37660/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.7155) |  Loss2: (0.3235) | Acc: (86.00%) (38778/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.7149) |  Loss2: (0.3235) | Acc: (86.00%) (39892/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.7154) |  Loss2: (0.3234) | Acc: (86.00%) (40988/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.7149) |  Loss2: (0.3234) | Acc: (86.00%) (42109/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.7158) |  Loss2: (0.3233) | Acc: (86.00%) (43160/50000)
# TEST : Loss: (0.4917) | Acc: (83.00%) (8373/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1579,  0.0493,  0.0829],
          [ 0.0935, -0.2684, -0.1037],
          [ 0.2314, -0.1054,  0.0138]],

         [[-0.0066, -0.1178,  0.2380],
          [ 0.0156, -0.0899,  0.0251],
          [ 0.0525, -0.0142, -0.0769]],

         [[ 0.0163, -0.0713,  0.1619],
          [ 0.1110, -0.1779,  0.1406],
          [-0.0903,  0.0356, -0.1896]]],


        [[[ 0.0920, -0.0875, -0.0088],
          [ 0.1335,  0.1678,  0.1798],
          [ 0.0087, -0.0671,  0.1161]],

         [[-0.1687, -0.1928,  0.1574],
          [ 0.0248, -0.0570,  0.1059],
          [ 0.1220, -0.2061,  0.1361]],

         [[ 0.0961, -0.0515, -0.0097],
          [ 0.0117,  0.0275, -0.1102],
          [-0.0334, -0.1538, -0.0081]]],


        [[[-0.0686,  0.0936, -0.0405],
          [-0.1220,  0.0679,  0.1209],
          [ 0.1169,  0.1693,  0.0035]],

         [[ 0.1007,  0.1335,  0.1144],
          [ 0.0715, -0.0409, -0.0370],
          [-0.0463, -0.0321,  0.1414]],

         [[-0.0469,  0.1461,  0.0751],
          [-0.0418, -0.0461, -0.0599],
          [-0.0391, -0.0084,  0.0449]]],


        ...,


        [[[ 0.2238, -0.1139, -0.1128],
          [-0.0453, -0.1037, -0.1003],
          [-0.0960,  0.0240,  0.0584]],

         [[ 0.1676,  0.1929, -0.1167],
          [ 0.2091, -0.1379, -0.1192],
          [-0.0667, -0.0795, -0.0365]],

         [[ 0.1880,  0.1062, -0.0952],
          [-0.0743, -0.1530, -0.0706],
          [ 0.0607,  0.1812, -0.1292]]],


        [[[-0.1157,  0.0849, -0.1567],
          [-0.1430, -0.1435,  0.1212],
          [-0.1620,  0.1512, -0.1311]],

         [[ 0.0876, -0.1258,  0.0764],
          [ 0.1526,  0.0063, -0.0861],
          [-0.1564,  0.1124, -0.1976]],

         [[ 0.1137, -0.0305, -0.0109],
          [ 0.1348, -0.1816,  0.0071],
          [-0.0400,  0.0095, -0.1254]]],


        [[[ 0.0834,  0.1387,  0.1000],
          [-0.0912, -0.1611,  0.1983],
          [-0.1564, -0.0085, -0.0552]],

         [[-0.0241, -0.0942,  0.1485],
          [-0.0152, -0.1949,  0.0404],
          [-0.0034,  0.0703,  0.0358]],

         [[-0.1456, -0.1942,  0.1453],
          [ 0.1398, -0.1745, -0.0300],
          [-0.0030, -0.0456,  0.2716]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5787e-05,  4.9302e-06,  8.2890e-06],
          [ 9.3470e-06, -2.6844e-05, -1.0372e-05],
          [ 2.3143e-05, -1.0536e-05,  1.3778e-06]],

         [[-6.6061e-07, -1.1777e-05,  2.3805e-05],
          [ 1.5597e-06, -8.9893e-06,  2.5064e-06],
          [ 5.2509e-06, -1.4218e-06, -7.6866e-06]],

         [[ 1.6342e-06, -7.1326e-06,  1.6194e-05],
          [ 1.1099e-05, -1.7791e-05,  1.4063e-05],
          [-9.0288e-06,  3.5563e-06, -1.8962e-05]]],


        [[[ 9.2043e-06, -8.7538e-06, -8.7655e-07],
          [ 1.3348e-05,  1.6782e-05,  1.7979e-05],
          [ 8.6823e-07, -6.7102e-06,  1.1611e-05]],

         [[-1.6868e-05, -1.9278e-05,  1.5744e-05],
          [ 2.4847e-06, -5.7047e-06,  1.0592e-05],
          [ 1.2196e-05, -2.0606e-05,  1.3611e-05]],

         [[ 9.6054e-06, -5.1484e-06, -9.7282e-07],
          [ 1.1743e-06,  2.7468e-06, -1.1018e-05],
          [-3.3424e-06, -1.5383e-05, -8.1111e-07]]],


        [[[-6.8564e-06,  9.3633e-06, -4.0533e-06],
          [-1.2197e-05,  6.7914e-06,  1.2088e-05],
          [ 1.1695e-05,  1.6929e-05,  3.5025e-07]],

         [[ 1.0073e-05,  1.3349e-05,  1.1438e-05],
          [ 7.1521e-06, -4.0912e-06, -3.7009e-06],
          [-4.6296e-06, -3.2117e-06,  1.4144e-05]],

         [[-4.6912e-06,  1.4613e-05,  7.5122e-06],
          [-4.1826e-06, -4.6113e-06, -5.9909e-06],
          [-3.9091e-06, -8.3649e-07,  4.4941e-06]]],


        ...,


        [[[ 2.2383e-05, -1.1388e-05, -1.1282e-05],
          [-4.5320e-06, -1.0373e-05, -1.0025e-05],
          [-9.5983e-06,  2.3994e-06,  5.8367e-06]],

         [[ 1.6764e-05,  1.9289e-05, -1.1673e-05],
          [ 2.0911e-05, -1.3790e-05, -1.1917e-05],
          [-6.6667e-06, -7.9501e-06, -3.6494e-06]],

         [[ 1.8800e-05,  1.0619e-05, -9.5157e-06],
          [-7.4307e-06, -1.5300e-05, -7.0579e-06],
          [ 6.0685e-06,  1.8125e-05, -1.2922e-05]]],


        [[[-1.1569e-05,  8.4937e-06, -1.5675e-05],
          [-1.4301e-05, -1.4353e-05,  1.2119e-05],
          [-1.6201e-05,  1.5122e-05, -1.3109e-05]],

         [[ 8.7588e-06, -1.2579e-05,  7.6443e-06],
          [ 1.5265e-05,  6.2916e-07, -8.6123e-06],
          [-1.5645e-05,  1.1236e-05, -1.9761e-05]],

         [[ 1.1374e-05, -3.0479e-06, -1.0915e-06],
          [ 1.3483e-05, -1.8164e-05,  7.0662e-07],
          [-4.0033e-06,  9.4836e-07, -1.2538e-05]]],


        [[[ 8.3425e-06,  1.3873e-05,  1.0000e-05],
          [-9.1204e-06, -1.6106e-05,  1.9830e-05],
          [-1.5639e-05, -8.4891e-07, -5.5226e-06]],

         [[-2.4081e-06, -9.4222e-06,  1.4854e-05],
          [-1.5239e-06, -1.9490e-05,  4.0407e-06],
          [-3.4472e-07,  7.0338e-06,  3.5783e-06]],

         [[-1.4560e-05, -1.9417e-05,  1.4529e-05],
          [ 1.3978e-05, -1.7451e-05, -3.0004e-06],
          [-3.0230e-07, -4.5556e-06,  2.7161e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3685]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0192]], device='cuda:0')

Epoch: 59 | Batch_idx: 0 |  Loss: (0.6981) |  Loss2: (0.3215) | Acc: (89.00%) (114/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.7188) |  Loss2: (0.3215) | Acc: (86.00%) (1218/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.7074) |  Loss2: (0.3214) | Acc: (86.00%) (2330/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.6995) |  Loss2: (0.3214) | Acc: (87.00%) (3454/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.6964) |  Loss2: (0.3213) | Acc: (87.00%) (4573/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.6936) |  Loss2: (0.3213) | Acc: (87.00%) (5692/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.6993) |  Loss2: (0.3212) | Acc: (86.00%) (6789/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.6969) |  Loss2: (0.3212) | Acc: (87.00%) (7916/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.7017) |  Loss2: (0.3212) | Acc: (86.00%) (9004/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.7066) |  Loss2: (0.3211) | Acc: (86.00%) (10096/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.7063) |  Loss2: (0.3211) | Acc: (86.00%) (11199/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.7054) |  Loss2: (0.3210) | Acc: (86.00%) (12324/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.7053) |  Loss2: (0.3210) | Acc: (86.00%) (13439/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.7076) |  Loss2: (0.3209) | Acc: (86.00%) (14534/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.7093) |  Loss2: (0.3209) | Acc: (86.00%) (15642/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.7109) |  Loss2: (0.3208) | Acc: (86.00%) (16737/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.7105) |  Loss2: (0.3208) | Acc: (86.00%) (17847/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.7079) |  Loss2: (0.3208) | Acc: (86.00%) (18977/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.7071) |  Loss2: (0.3207) | Acc: (86.00%) (20092/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.7082) |  Loss2: (0.3207) | Acc: (86.00%) (21189/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.7080) |  Loss2: (0.3206) | Acc: (86.00%) (22293/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.7065) |  Loss2: (0.3206) | Acc: (86.00%) (23400/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.7042) |  Loss2: (0.3205) | Acc: (86.00%) (24536/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.7037) |  Loss2: (0.3205) | Acc: (86.00%) (25646/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.7041) |  Loss2: (0.3205) | Acc: (86.00%) (26761/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.7045) |  Loss2: (0.3204) | Acc: (86.00%) (27864/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.7051) |  Loss2: (0.3204) | Acc: (86.00%) (28968/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.7049) |  Loss2: (0.3203) | Acc: (86.00%) (30082/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.7065) |  Loss2: (0.3203) | Acc: (86.00%) (31169/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.7058) |  Loss2: (0.3202) | Acc: (86.00%) (32287/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.7062) |  Loss2: (0.3202) | Acc: (86.00%) (33389/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.7056) |  Loss2: (0.3201) | Acc: (86.00%) (34496/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.7050) |  Loss2: (0.3201) | Acc: (86.00%) (35611/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.7061) |  Loss2: (0.3201) | Acc: (86.00%) (36701/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.7057) |  Loss2: (0.3200) | Acc: (86.00%) (37822/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.7056) |  Loss2: (0.3200) | Acc: (86.00%) (38928/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.7064) |  Loss2: (0.3199) | Acc: (86.00%) (40030/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.7065) |  Loss2: (0.3199) | Acc: (86.00%) (41132/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.7062) |  Loss2: (0.3199) | Acc: (86.00%) (42236/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.7066) |  Loss2: (0.3198) | Acc: (86.00%) (43283/50000)
# TEST : Loss: (0.4864) | Acc: (83.00%) (8347/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1578,  0.0493,  0.0829],
          [ 0.0934, -0.2683, -0.1037],
          [ 0.2313, -0.1053,  0.0138]],

         [[-0.0066, -0.1177,  0.2380],
          [ 0.0156, -0.0899,  0.0251],
          [ 0.0525, -0.0142, -0.0768]],

         [[ 0.0163, -0.0713,  0.1619],
          [ 0.1109, -0.1778,  0.1406],
          [-0.0903,  0.0355, -0.1895]]],


        [[[ 0.0920, -0.0875, -0.0088],
          [ 0.1334,  0.1678,  0.1797],
          [ 0.0087, -0.0671,  0.1161]],

         [[-0.1686, -0.1927,  0.1574],
          [ 0.0248, -0.0570,  0.1059],
          [ 0.1219, -0.2060,  0.1361]],

         [[ 0.0960, -0.0515, -0.0097],
          [ 0.0117,  0.0275, -0.1101],
          [-0.0334, -0.1538, -0.0081]]],


        [[[-0.0685,  0.0936, -0.0405],
          [-0.1219,  0.0679,  0.1208],
          [ 0.1169,  0.1692,  0.0035]],

         [[ 0.1007,  0.1334,  0.1143],
          [ 0.0715, -0.0409, -0.0370],
          [-0.0463, -0.0321,  0.1414]],

         [[-0.0469,  0.1461,  0.0751],
          [-0.0418, -0.0461, -0.0599],
          [-0.0391, -0.0084,  0.0449]]],


        ...,


        [[[ 0.2237, -0.1138, -0.1128],
          [-0.0453, -0.1037, -0.1002],
          [-0.0959,  0.0240,  0.0583]],

         [[ 0.1676,  0.1928, -0.1167],
          [ 0.2090, -0.1378, -0.1191],
          [-0.0666, -0.0795, -0.0365]],

         [[ 0.1879,  0.1062, -0.0951],
          [-0.0743, -0.1529, -0.0706],
          [ 0.0607,  0.1812, -0.1292]]],


        [[[-0.1156,  0.0849, -0.1567],
          [-0.1429, -0.1435,  0.1211],
          [-0.1619,  0.1512, -0.1310]],

         [[ 0.0876, -0.1257,  0.0764],
          [ 0.1526,  0.0063, -0.0861],
          [-0.1564,  0.1123, -0.1975]],

         [[ 0.1137, -0.0305, -0.0109],
          [ 0.1348, -0.1816,  0.0071],
          [-0.0400,  0.0095, -0.1253]]],


        [[[ 0.0834,  0.1387,  0.1000],
          [-0.0912, -0.1610,  0.1982],
          [-0.1563, -0.0085, -0.0552]],

         [[-0.0241, -0.0942,  0.1485],
          [-0.0152, -0.1948,  0.0404],
          [-0.0034,  0.0703,  0.0358]],

         [[-0.1455, -0.1941,  0.1452],
          [ 0.1397, -0.1744, -0.0300],
          [-0.0030, -0.0455,  0.2715]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5780e-05,  4.9283e-06,  8.2858e-06],
          [ 9.3432e-06, -2.6834e-05, -1.0368e-05],
          [ 2.3134e-05, -1.0532e-05,  1.3772e-06]],

         [[-6.6036e-07, -1.1773e-05,  2.3796e-05],
          [ 1.5590e-06, -8.9858e-06,  2.5055e-06],
          [ 5.2489e-06, -1.4213e-06, -7.6836e-06]],

         [[ 1.6335e-06, -7.1297e-06,  1.6188e-05],
          [ 1.1095e-05, -1.7784e-05,  1.4057e-05],
          [-9.0253e-06,  3.5548e-06, -1.8955e-05]]],


        [[[ 9.2008e-06, -8.7503e-06, -8.7622e-07],
          [ 1.3343e-05,  1.6775e-05,  1.7972e-05],
          [ 8.6790e-07, -6.7076e-06,  1.1606e-05]],

         [[-1.6861e-05, -1.9271e-05,  1.5738e-05],
          [ 2.4838e-06, -5.7025e-06,  1.0588e-05],
          [ 1.2191e-05, -2.0598e-05,  1.3606e-05]],

         [[ 9.6016e-06, -5.1464e-06, -9.7246e-07],
          [ 1.1738e-06,  2.7457e-06, -1.1014e-05],
          [-3.3411e-06, -1.5378e-05, -8.1079e-07]]],


        [[[-6.8538e-06,  9.3595e-06, -4.0517e-06],
          [-1.2192e-05,  6.7888e-06,  1.2084e-05],
          [ 1.1690e-05,  1.6923e-05,  3.5012e-07]],

         [[ 1.0069e-05,  1.3344e-05,  1.1434e-05],
          [ 7.1492e-06, -4.0896e-06, -3.6995e-06],
          [-4.6278e-06, -3.2104e-06,  1.4138e-05]],

         [[-4.6893e-06,  1.4607e-05,  7.5093e-06],
          [-4.1810e-06, -4.6095e-06, -5.9886e-06],
          [-3.9077e-06, -8.3617e-07,  4.4924e-06]]],


        ...,


        [[[ 2.2374e-05, -1.1383e-05, -1.1277e-05],
          [-4.5303e-06, -1.0369e-05, -1.0022e-05],
          [-9.5945e-06,  2.3985e-06,  5.8343e-06]],

         [[ 1.6757e-05,  1.9281e-05, -1.1669e-05],
          [ 2.0903e-05, -1.3785e-05, -1.1913e-05],
          [-6.6641e-06, -7.9469e-06, -3.6479e-06]],

         [[ 1.8793e-05,  1.0615e-05, -9.5119e-06],
          [-7.4278e-06, -1.5294e-05, -7.0553e-06],
          [ 6.0662e-06,  1.8118e-05, -1.2917e-05]]],


        [[[-1.1565e-05,  8.4905e-06, -1.5668e-05],
          [-1.4295e-05, -1.4347e-05,  1.2114e-05],
          [-1.6195e-05,  1.5116e-05, -1.3103e-05]],

         [[ 8.7553e-06, -1.2574e-05,  7.6414e-06],
          [ 1.5259e-05,  6.2890e-07, -8.6088e-06],
          [-1.5639e-05,  1.1231e-05, -1.9753e-05]],

         [[ 1.1370e-05, -3.0468e-06, -1.0911e-06],
          [ 1.3477e-05, -1.8157e-05,  7.0634e-07],
          [-4.0017e-06,  9.4799e-07, -1.2533e-05]]],


        [[[ 8.3393e-06,  1.3868e-05,  9.9964e-06],
          [-9.1169e-06, -1.6100e-05,  1.9822e-05],
          [-1.5633e-05, -8.4858e-07, -5.5204e-06]],

         [[-2.4072e-06, -9.4184e-06,  1.4849e-05],
          [-1.5233e-06, -1.9483e-05,  4.0391e-06],
          [-3.4458e-07,  7.0312e-06,  3.5768e-06]],

         [[-1.4554e-05, -1.9409e-05,  1.4523e-05],
          [ 1.3973e-05, -1.7444e-05, -2.9992e-06],
          [-3.0218e-07, -4.5538e-06,  2.7151e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3742]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0084]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (1236/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (2356/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (3482/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (4608/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (5741/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (6846/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (7960/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (9051/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3756) |  Loss2: (0.0000) | Acc: (87.00%) (10151/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (87.00%) (11250/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (12361/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (13475/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (14572/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (15663/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (16776/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (17876/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (18999/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (20113/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (21224/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (22335/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (23439/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (86.00%) (24565/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (25695/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (86.00%) (26815/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3781) |  Loss2: (0.0000) | Acc: (86.00%) (27931/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (29037/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (30139/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (31258/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (32362/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (33476/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (34582/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (35687/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (36805/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (37906/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (86.00%) (39035/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (86.00%) (40158/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (86.00%) (41286/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (42381/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (86.00%) (43437/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_060.pth.tar'
# TEST : Loss: (0.5275) | Acc: (82.00%) (8266/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1493,  0.0570,  0.0912],
          [ 0.1018, -0.2625, -0.0962],
          [ 0.2416, -0.0958,  0.0210]],

         [[ 0.0017, -0.1096,  0.2482],
          [ 0.0236, -0.0840,  0.0341],
          [ 0.0616, -0.0051, -0.0694]],

         [[ 0.0214, -0.0658,  0.1695],
          [ 0.1156, -0.1738,  0.1479],
          [-0.0854,  0.0412, -0.1849]]],


        [[[ 0.0918, -0.0868, -0.0080],
          [ 0.1343,  0.1690,  0.1806],
          [ 0.0100, -0.0645,  0.1173]],

         [[-0.1691, -0.1924,  0.1574],
          [ 0.0251, -0.0567,  0.1061],
          [ 0.1224, -0.2046,  0.1366]],

         [[ 0.0960, -0.0507, -0.0089],
          [ 0.0117,  0.0274, -0.1097],
          [-0.0331, -0.1529, -0.0074]]],


        [[[-0.0685,  0.0936, -0.0404],
          [-0.1218,  0.0681,  0.1211],
          [ 0.1165,  0.1691,  0.0035]],

         [[ 0.1008,  0.1335,  0.1143],
          [ 0.0715, -0.0406, -0.0369],
          [-0.0467, -0.0323,  0.1411]],

         [[-0.0467,  0.1460,  0.0750],
          [-0.0417, -0.0459, -0.0599],
          [-0.0394, -0.0086,  0.0446]]],


        ...,


        [[[ 0.2233, -0.1154, -0.1139],
          [-0.0464, -0.1054, -0.1008],
          [-0.0963,  0.0242,  0.0596]],

         [[ 0.1666,  0.1914, -0.1172],
          [ 0.2074, -0.1399, -0.1202],
          [-0.0679, -0.0801, -0.0360]],

         [[ 0.1862,  0.1042, -0.0959],
          [-0.0761, -0.1556, -0.0724],
          [ 0.0590,  0.1796, -0.1296]]],


        [[[-0.1148,  0.0850, -0.1567],
          [-0.1425, -0.1435,  0.1209],
          [-0.1602,  0.1525, -0.1309]],

         [[ 0.0868, -0.1270,  0.0752],
          [ 0.1513,  0.0049, -0.0872],
          [-0.1563,  0.1124, -0.1983]],

         [[ 0.1135, -0.0311, -0.0115],
          [ 0.1339, -0.1824,  0.0065],
          [-0.0398,  0.0095, -0.1259]]],


        [[[ 0.0820,  0.1399,  0.1002],
          [-0.0951, -0.1639,  0.1983],
          [-0.1574, -0.0078, -0.0548]],

         [[-0.0228, -0.0913,  0.1493],
          [-0.0149, -0.1949,  0.0415],
          [-0.0007,  0.0747,  0.0380]],

         [[-0.1458, -0.1916,  0.1463],
          [ 0.1385, -0.1750, -0.0288],
          [-0.0015, -0.0416,  0.2745]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1207, -0.1725, -0.1654],
          [-0.0963, -0.1414, -0.1557],
          [-0.0895, -0.1159, -0.1250]],

         [[-0.0926, -0.1521, -0.1480],
          [-0.0581, -0.1074, -0.1226],
          [-0.0402, -0.0696, -0.0770]],

         [[-0.0837, -0.1363, -0.1383],
          [-0.0508, -0.0921, -0.1070],
          [-0.0372, -0.0593, -0.0621]]],


        [[[ 0.0048, -0.0032, -0.0009],
          [ 0.0039, -0.0014, -0.0017],
          [ 0.0027,  0.0018, -0.0022]],

         [[ 0.0042, -0.0021,  0.0016],
          [ 0.0043,  0.0010,  0.0003],
          [ 0.0044,  0.0047, -0.0005]],

         [[ 0.0019, -0.0018,  0.0029],
          [-0.0004, -0.0012, -0.0009],
          [-0.0013,  0.0016, -0.0015]]],


        [[[-0.0116, -0.0096, -0.0061],
          [-0.0085, -0.0063, -0.0046],
          [-0.0037, -0.0017, -0.0024]],

         [[-0.0041, -0.0014,  0.0026],
          [-0.0021,  0.0011,  0.0037],
          [ 0.0005,  0.0043,  0.0052]],

         [[ 0.0039,  0.0068,  0.0104],
          [ 0.0041,  0.0077,  0.0107],
          [ 0.0051,  0.0092,  0.0108]]],


        ...,


        [[[-0.0052, -0.0024, -0.0188],
          [ 0.0219,  0.0118, -0.0041],
          [ 0.0412,  0.0236,  0.0079]],

         [[-0.0070, -0.0046, -0.0216],
          [ 0.0149,  0.0042, -0.0097],
          [ 0.0378,  0.0194,  0.0048]],

         [[ 0.0085,  0.0082, -0.0082],
          [ 0.0282,  0.0178,  0.0053],
          [ 0.0464,  0.0309,  0.0196]]],


        [[[-0.0135, -0.0175, -0.0211],
          [-0.0049, -0.0071, -0.0055],
          [ 0.0086,  0.0051,  0.0105]],

         [[-0.0154, -0.0186, -0.0226],
          [-0.0078, -0.0095, -0.0086],
          [ 0.0023, -0.0011,  0.0039]],

         [[-0.0132, -0.0160, -0.0206],
          [-0.0086, -0.0101, -0.0098],
          [-0.0018, -0.0047, -0.0008]]],


        [[[-0.0330, -0.0500, -0.0298],
          [ 0.0115, -0.0028,  0.0025],
          [ 0.0486,  0.0448,  0.0280]],

         [[-0.0422, -0.0544, -0.0328],
          [-0.0172, -0.0240, -0.0106],
          [ 0.0056,  0.0095,  0.0039]],

         [[-0.0905, -0.1056, -0.0858],
          [-0.0691, -0.0763, -0.0637],
          [-0.0527, -0.0455, -0.0459]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3730]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 61 | Batch_idx: 0 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (86.00%) (1221/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (3467/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (4598/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (5725/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (6812/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (7954/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (9083/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3584) |  Loss2: (0.0000) | Acc: (87.00%) (10199/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (11319/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (12451/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (13570/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (14673/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (15818/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (16926/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (18048/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (19165/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (20274/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (21357/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (22493/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (23619/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (24719/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (25826/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (26927/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (28057/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3693) |  Loss2: (0.0000) | Acc: (87.00%) (29182/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (30313/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (31437/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (32543/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (33679/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (34799/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (35893/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (37022/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3693) |  Loss2: (0.0000) | Acc: (87.00%) (38132/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (39264/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (40396/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (41528/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (42639/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (43732/50000)
# TEST : Loss: (0.5280) | Acc: (82.00%) (8264/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1522,  0.0546,  0.0891],
          [ 0.0971, -0.2680, -0.1003],
          [ 0.2374, -0.0971,  0.0187]],

         [[ 0.0012, -0.1092,  0.2484],
          [ 0.0205, -0.0871,  0.0323],
          [ 0.0565, -0.0073, -0.0713]],

         [[ 0.0216, -0.0639,  0.1724],
          [ 0.1124, -0.1768,  0.1471],
          [-0.0915,  0.0377, -0.1866]]],


        [[[ 0.0924, -0.0844, -0.0060],
          [ 0.1348,  0.1707,  0.1820],
          [ 0.0112, -0.0628,  0.1182]],

         [[-0.1692, -0.1910,  0.1582],
          [ 0.0252, -0.0554,  0.1072],
          [ 0.1233, -0.2031,  0.1375]],

         [[ 0.0954, -0.0505, -0.0093],
          [ 0.0118,  0.0277, -0.1095],
          [-0.0319, -0.1518, -0.0070]]],


        [[[-0.0692,  0.0934, -0.0403],
          [-0.1227,  0.0679,  0.1210],
          [ 0.1157,  0.1688,  0.0034]],

         [[ 0.1001,  0.1334,  0.1145],
          [ 0.0706, -0.0409, -0.0370],
          [-0.0475, -0.0327,  0.1408]],

         [[-0.0475,  0.1457,  0.0747],
          [-0.0427, -0.0466, -0.0605],
          [-0.0403, -0.0092,  0.0439]]],


        ...,


        [[[ 0.2236, -0.1142, -0.1135],
          [-0.0471, -0.1055, -0.0999],
          [-0.0983,  0.0226,  0.0589]],

         [[ 0.1680,  0.1935, -0.1158],
          [ 0.2082, -0.1388, -0.1187],
          [-0.0679, -0.0804, -0.0357]],

         [[ 0.1863,  0.1048, -0.0956],
          [-0.0762, -0.1557, -0.0722],
          [ 0.0580,  0.1780, -0.1309]]],


        [[[-0.1120,  0.0865, -0.1557],
          [-0.1410, -0.1426,  0.1212],
          [-0.1595,  0.1526, -0.1316]],

         [[ 0.0896, -0.1252,  0.0762],
          [ 0.1529,  0.0060, -0.0866],
          [-0.1556,  0.1127, -0.1988]],

         [[ 0.1171, -0.0287, -0.0096],
          [ 0.1369, -0.1803,  0.0081],
          [-0.0378,  0.0109, -0.1253]]],


        [[[ 0.0788,  0.1370,  0.0991],
          [-0.0946, -0.1653,  0.2001],
          [-0.1595, -0.0103, -0.0548]],

         [[-0.0244, -0.0935,  0.1478],
          [-0.0133, -0.1957,  0.0426],
          [-0.0012,  0.0742,  0.0386]],

         [[-0.1497, -0.1959,  0.1422],
          [ 0.1368, -0.1783, -0.0305],
          [-0.0047, -0.0433,  0.2739]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0047, -0.0053, -0.0020],
          [-0.0002,  0.0031,  0.0085],
          [-0.0265, -0.0281, -0.0168]],

         [[ 0.0435,  0.0436,  0.0558],
          [ 0.0339,  0.0491,  0.0640],
          [ 0.0113,  0.0161,  0.0386]],

         [[ 0.0796,  0.0809,  0.0943],
          [ 0.0817,  0.0925,  0.1048],
          [ 0.0711,  0.0712,  0.0847]]],


        [[[ 0.0249,  0.0235,  0.0230],
          [ 0.0241,  0.0149,  0.0071],
          [ 0.0159,  0.0047, -0.0068]],

         [[ 0.0302,  0.0276,  0.0248],
          [ 0.0320,  0.0200,  0.0077],
          [ 0.0241,  0.0090, -0.0080]],

         [[ 0.0284,  0.0253,  0.0210],
          [ 0.0300,  0.0181,  0.0060],
          [ 0.0239,  0.0077, -0.0087]]],


        [[[ 0.0060,  0.0023,  0.0039],
          [ 0.0076,  0.0029,  0.0038],
          [ 0.0071,  0.0031,  0.0025]],

         [[ 0.0017, -0.0017, -0.0001],
          [ 0.0032, -0.0013, -0.0005],
          [ 0.0029, -0.0010, -0.0017]],

         [[-0.0017, -0.0047, -0.0029],
          [-0.0000, -0.0041, -0.0031],
          [-0.0005, -0.0041, -0.0043]]],


        ...,


        [[[ 0.0699,  0.0513,  0.0359],
          [ 0.0678,  0.0505,  0.0415],
          [ 0.0846,  0.0701,  0.0596]],

         [[ 0.0359,  0.0236,  0.0124],
          [ 0.0311,  0.0179,  0.0126],
          [ 0.0456,  0.0329,  0.0250]],

         [[ 0.0195,  0.0138,  0.0055],
          [ 0.0110,  0.0040,  0.0023],
          [ 0.0191,  0.0126,  0.0088]]],


        [[[ 0.0163,  0.0116,  0.0034],
          [ 0.0118,  0.0071,  0.0037],
          [ 0.0081,  0.0065,  0.0003]],

         [[ 0.0153,  0.0116,  0.0013],
          [ 0.0083,  0.0048, -0.0004],
          [ 0.0037,  0.0025, -0.0049]],

         [[ 0.0134,  0.0117,  0.0017],
          [ 0.0078,  0.0052, -0.0001],
          [ 0.0033,  0.0022, -0.0053]]],


        [[[-0.0782, -0.0573, -0.0631],
          [-0.0799, -0.0583, -0.0597],
          [-0.0470, -0.0352, -0.0471]],

         [[-0.0418, -0.0162, -0.0211],
          [-0.0454, -0.0232, -0.0263],
          [-0.0211, -0.0093, -0.0219]],

         [[-0.0381, -0.0106, -0.0085],
          [-0.0365, -0.0099, -0.0063],
          [-0.0162,  0.0026, -0.0035]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3728]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 62 | Batch_idx: 0 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (2375/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3485) |  Loss2: (0.0000) | Acc: (87.00%) (3485/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3482) |  Loss2: (0.0000) | Acc: (87.00%) (4613/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (5761/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (6873/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (7980/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (9105/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (10230/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (11358/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (12481/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (13598/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3535) |  Loss2: (0.0000) | Acc: (87.00%) (14730/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (87.00%) (15849/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (16961/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3540) |  Loss2: (0.0000) | Acc: (87.00%) (18089/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (19226/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (20333/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (21465/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (22596/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (23727/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (24834/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (87.00%) (25949/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (27080/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3518) |  Loss2: (0.0000) | Acc: (87.00%) (28211/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29324/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3525) |  Loss2: (0.0000) | Acc: (87.00%) (30457/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (31581/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (32698/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (33808/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (34921/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (87.00%) (36056/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (37182/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3545) |  Loss2: (0.0000) | Acc: (87.00%) (38287/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (39424/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (40546/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (41668/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (42805/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3548) |  Loss2: (0.0000) | Acc: (87.00%) (43890/50000)
# TEST : Loss: (0.4879) | Acc: (83.00%) (8325/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1502,  0.0580,  0.0932],
          [ 0.0991, -0.2683, -0.0978],
          [ 0.2403, -0.0947,  0.0205]],

         [[-0.0007, -0.1071,  0.2526],
          [ 0.0191, -0.0879,  0.0358],
          [ 0.0569, -0.0056, -0.0686]],

         [[ 0.0157, -0.0659,  0.1737],
          [ 0.1073, -0.1810,  0.1491],
          [-0.0948,  0.0361, -0.1841]]],


        [[[ 0.0929, -0.0839, -0.0046],
          [ 0.1342,  0.1705,  0.1826],
          [ 0.0097, -0.0635,  0.1187]],

         [[-0.1672, -0.1897,  0.1595],
          [ 0.0259, -0.0548,  0.1077],
          [ 0.1227, -0.2031,  0.1384]],

         [[ 0.0963, -0.0497, -0.0084],
          [ 0.0117,  0.0278, -0.1096],
          [-0.0330, -0.1524, -0.0072]]],


        [[[-0.0698,  0.0928, -0.0404],
          [-0.1230,  0.0676,  0.1208],
          [ 0.1157,  0.1687,  0.0034]],

         [[ 0.0999,  0.1333,  0.1146],
          [ 0.0707, -0.0407, -0.0367],
          [-0.0472, -0.0323,  0.1410]],

         [[-0.0480,  0.1453,  0.0748],
          [-0.0430, -0.0467, -0.0604],
          [-0.0404, -0.0091,  0.0439]]],


        ...,


        [[[ 0.2277, -0.1112, -0.1108],
          [-0.0415, -0.1025, -0.0971],
          [-0.0920,  0.0264,  0.0615]],

         [[ 0.1699,  0.1948, -0.1150],
          [ 0.2113, -0.1381, -0.1188],
          [-0.0640, -0.0792, -0.0360]],

         [[ 0.1875,  0.1060, -0.0946],
          [-0.0735, -0.1548, -0.0722],
          [ 0.0616,  0.1796, -0.1306]]],


        [[[-0.1121,  0.0865, -0.1550],
          [-0.1421, -0.1432,  0.1218],
          [-0.1598,  0.1527, -0.1306]],

         [[ 0.0892, -0.1254,  0.0765],
          [ 0.1515,  0.0048, -0.0867],
          [-0.1561,  0.1123, -0.1985]],

         [[ 0.1165, -0.0292, -0.0093],
          [ 0.1359, -0.1812,  0.0081],
          [-0.0381,  0.0105, -0.1251]]],


        [[[ 0.0820,  0.1407,  0.1028],
          [-0.0918, -0.1635,  0.2037],
          [-0.1610, -0.0125, -0.0552]],

         [[-0.0221, -0.0922,  0.1482],
          [-0.0102, -0.1940,  0.0444],
          [-0.0009,  0.0736,  0.0378]],

         [[-0.1487, -0.1942,  0.1437],
          [ 0.1407, -0.1748, -0.0267],
          [-0.0002, -0.0392,  0.2778]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0133,  0.0245,  0.0027],
          [-0.0203,  0.0099, -0.0193],
          [-0.0281, -0.0108, -0.0407]],

         [[-0.0360,  0.0106, -0.0078],
          [-0.0561, -0.0179, -0.0416],
          [-0.0812, -0.0560, -0.0764]],

         [[-0.0282,  0.0208,  0.0134],
          [-0.0441, -0.0042, -0.0159],
          [-0.0690, -0.0436, -0.0523]]],


        [[[-0.0206, -0.0306, -0.0225],
          [-0.0183, -0.0225, -0.0118],
          [-0.0210, -0.0220, -0.0106]],

         [[-0.0173, -0.0276, -0.0214],
          [-0.0111, -0.0170, -0.0104],
          [-0.0121, -0.0162, -0.0101]],

         [[-0.0159, -0.0244, -0.0161],
          [-0.0110, -0.0164, -0.0078],
          [-0.0118, -0.0169, -0.0096]]],


        [[[ 0.0025,  0.0021, -0.0003],
          [ 0.0007,  0.0001, -0.0017],
          [-0.0007, -0.0013, -0.0015]],

         [[ 0.0017,  0.0015, -0.0006],
          [-0.0009, -0.0013, -0.0026],
          [-0.0027, -0.0031, -0.0029]],

         [[-0.0002,  0.0002, -0.0013],
          [-0.0025, -0.0023, -0.0030],
          [-0.0036, -0.0039, -0.0032]]],


        ...,


        [[[ 0.0335,  0.0305,  0.0233],
          [ 0.0301,  0.0302,  0.0270],
          [ 0.0190,  0.0204,  0.0135]],

         [[ 0.0359,  0.0346,  0.0262],
          [ 0.0318,  0.0330,  0.0284],
          [ 0.0223,  0.0228,  0.0145]],

         [[ 0.0144,  0.0150,  0.0079],
          [ 0.0099,  0.0119,  0.0082],
          [-0.0027, -0.0019, -0.0073]]],


        [[[ 0.0044,  0.0031,  0.0042],
          [ 0.0054,  0.0006,  0.0014],
          [ 0.0012, -0.0006,  0.0002]],

         [[ 0.0007,  0.0007,  0.0033],
          [ 0.0025, -0.0009,  0.0001],
          [-0.0012, -0.0027, -0.0027]],

         [[ 0.0037,  0.0038,  0.0060],
          [ 0.0049,  0.0014,  0.0018],
          [ 0.0009, -0.0007, -0.0017]]],


        [[[ 0.1573,  0.1569,  0.1646],
          [ 0.1480,  0.1490,  0.1391],
          [ 0.1196,  0.1108,  0.1010]],

         [[ 0.1454,  0.1554,  0.1748],
          [ 0.1361,  0.1446,  0.1406],
          [ 0.1088,  0.1032,  0.0952]],

         [[ 0.1490,  0.1585,  0.1812],
          [ 0.1373,  0.1452,  0.1478],
          [ 0.1049,  0.1011,  0.1005]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3727]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 63 | Batch_idx: 0 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (1250/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (2395/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (3536/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (89.00%) (4671/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (89.00%) (5817/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (6940/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (8080/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (9211/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (10352/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3239) |  Loss2: (0.0000) | Acc: (88.00%) (11499/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (88.00%) (12634/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (13733/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (14860/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3330) |  Loss2: (0.0000) | Acc: (88.00%) (15977/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (17108/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (18206/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (19331/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (88.00%) (20458/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (21580/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (22692/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (23814/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (24957/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (26090/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (27222/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (28348/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (29487/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (30612/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (31737/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (32862/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (33987/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (35111/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (36248/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (37374/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (38512/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (39637/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (40763/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (41876/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (42999/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (88.00%) (44063/50000)
# TEST : Loss: (0.6414) | Acc: (80.00%) (8022/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1495,  0.0563,  0.0907],
          [ 0.1015, -0.2705, -0.0979],
          [ 0.2431, -0.0935,  0.0194]],

         [[-0.0003, -0.1065,  0.2514],
          [ 0.0211, -0.0876,  0.0377],
          [ 0.0579, -0.0047, -0.0695]],

         [[ 0.0170, -0.0634,  0.1757],
          [ 0.1085, -0.1806,  0.1517],
          [-0.0962,  0.0352, -0.1854]]],


        [[[ 0.0939, -0.0835, -0.0046],
          [ 0.1347,  0.1705,  0.1823],
          [ 0.0092, -0.0642,  0.1181]],

         [[-0.1647, -0.1884,  0.1608],
          [ 0.0277, -0.0538,  0.1089],
          [ 0.1231, -0.2027,  0.1393]],

         [[ 0.0985, -0.0490, -0.0076],
          [ 0.0127,  0.0279, -0.1091],
          [-0.0330, -0.1529, -0.0071]]],


        [[[-0.0699,  0.0925, -0.0407],
          [-0.1230,  0.0675,  0.1207],
          [ 0.1157,  0.1688,  0.0036]],

         [[ 0.0999,  0.1331,  0.1145],
          [ 0.0708, -0.0406, -0.0366],
          [-0.0470, -0.0321,  0.1412]],

         [[-0.0477,  0.1453,  0.0748],
          [-0.0427, -0.0464, -0.0602],
          [-0.0402, -0.0089,  0.0443]]],


        ...,


        [[[ 0.2278, -0.1121, -0.1116],
          [-0.0411, -0.1035, -0.0982],
          [-0.0912,  0.0257,  0.0615]],

         [[ 0.1688,  0.1935, -0.1156],
          [ 0.2110, -0.1390, -0.1194],
          [-0.0639, -0.0802, -0.0359]],

         [[ 0.1881,  0.1061, -0.0943],
          [-0.0720, -0.1546, -0.0722],
          [ 0.0638,  0.1803, -0.1295]]],


        [[[-0.1100,  0.0877, -0.1535],
          [-0.1416, -0.1430,  0.1222],
          [-0.1597,  0.1529, -0.1305]],

         [[ 0.0908, -0.1246,  0.0775],
          [ 0.1515,  0.0046, -0.0865],
          [-0.1564,  0.1122, -0.1984]],

         [[ 0.1170, -0.0292, -0.0090],
          [ 0.1354, -0.1817,  0.0078],
          [-0.0389,  0.0098, -0.1259]]],


        [[[ 0.0834,  0.1429,  0.1050],
          [-0.0927, -0.1665,  0.2026],
          [-0.1628, -0.0134, -0.0575]],

         [[-0.0205, -0.0904,  0.1493],
          [-0.0112, -0.1966,  0.0434],
          [-0.0034,  0.0732,  0.0359]],

         [[-0.1478, -0.1928,  0.1437],
          [ 0.1402, -0.1762, -0.0273],
          [-0.0016, -0.0377,  0.2774]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1043,  0.0712,  0.0658],
          [ 0.1625,  0.1295,  0.0898],
          [ 0.1536,  0.1292,  0.1071]],

         [[ 0.1187,  0.1034,  0.1079],
          [ 0.1617,  0.1429,  0.1182],
          [ 0.1618,  0.1396,  0.1293]],

         [[ 0.1029,  0.0995,  0.1096],
          [ 0.1288,  0.1161,  0.1026],
          [ 0.1398,  0.1172,  0.1088]]],


        [[[ 0.0142,  0.0191,  0.0146],
          [ 0.0094,  0.0152,  0.0081],
          [ 0.0036,  0.0116,  0.0046]],

         [[ 0.0047,  0.0111,  0.0078],
          [ 0.0008,  0.0070,  0.0010],
          [-0.0036,  0.0027, -0.0042]],

         [[ 0.0090,  0.0118,  0.0072],
          [ 0.0068,  0.0105,  0.0031],
          [ 0.0051,  0.0097,  0.0013]]],


        [[[ 0.0040,  0.0013,  0.0016],
          [ 0.0054,  0.0017,  0.0009],
          [ 0.0050,  0.0031,  0.0037]],

         [[ 0.0024, -0.0015, -0.0009],
          [ 0.0031, -0.0018, -0.0023],
          [ 0.0016, -0.0015, -0.0007]],

         [[-0.0028, -0.0070, -0.0061],
          [-0.0016, -0.0068, -0.0075],
          [-0.0022, -0.0059, -0.0058]]],


        ...,


        [[[-0.0695, -0.0654, -0.0470],
          [-0.0665, -0.0684, -0.0510],
          [-0.0662, -0.0632, -0.0492]],

         [[-0.0784, -0.0791, -0.0560],
          [-0.0767, -0.0862, -0.0626],
          [-0.0815, -0.0839, -0.0624]],

         [[-0.0900, -0.0912, -0.0637],
          [-0.0858, -0.0948, -0.0669],
          [-0.0903, -0.0917, -0.0668]]],


        [[[-0.0061, -0.0038, -0.0038],
          [-0.0097, -0.0036, -0.0010],
          [-0.0068, -0.0031,  0.0033]],

         [[-0.0076, -0.0072, -0.0075],
          [-0.0098, -0.0039, -0.0021],
          [-0.0064, -0.0042,  0.0015]],

         [[-0.0166, -0.0154, -0.0153],
          [-0.0158, -0.0087, -0.0064],
          [-0.0107, -0.0083, -0.0020]]],


        [[[ 0.0767,  0.0949,  0.0973],
          [ 0.0719,  0.0915,  0.0900],
          [ 0.0557,  0.0651,  0.0827]],

         [[ 0.1016,  0.1133,  0.1084],
          [ 0.0968,  0.1106,  0.1020],
          [ 0.0761,  0.0796,  0.0898]],

         [[ 0.0962,  0.1052,  0.1050],
          [ 0.0879,  0.1013,  0.0971],
          [ 0.0644,  0.0736,  0.0867]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3725]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 64 | Batch_idx: 0 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (2384/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (3515/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (4660/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (5795/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (6918/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (8050/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (9177/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (10302/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (11443/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (12551/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (13672/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (14818/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (15952/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (17080/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (18216/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (19322/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (20452/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (21608/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (22737/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (23880/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (24996/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (26118/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (27251/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (28401/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (29534/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (30667/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (31802/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (32935/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (34080/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (35216/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (36346/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (37466/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (38620/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (39760/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (40877/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (42007/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (43146/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (44230/50000)
# TEST : Loss: (0.5362) | Acc: (82.00%) (8281/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1505,  0.0569,  0.0945],
          [ 0.0980, -0.2754, -0.0971],
          [ 0.2423, -0.0947,  0.0176]],

         [[-0.0055, -0.1097,  0.2500],
          [ 0.0154, -0.0949,  0.0348],
          [ 0.0577, -0.0058, -0.0722]],

         [[ 0.0132, -0.0655,  0.1748],
          [ 0.1056, -0.1843,  0.1515],
          [-0.0935,  0.0374, -0.1842]]],


        [[[ 0.0935, -0.0841, -0.0064],
          [ 0.1348,  0.1706,  0.1818],
          [ 0.0085, -0.0651,  0.1178]],

         [[-0.1641, -0.1883,  0.1598],
          [ 0.0285, -0.0533,  0.1089],
          [ 0.1227, -0.2035,  0.1391]],

         [[ 0.0985, -0.0491, -0.0081],
          [ 0.0127,  0.0278, -0.1089],
          [-0.0337, -0.1540, -0.0072]]],


        [[[-0.0703,  0.0923, -0.0408],
          [-0.1234,  0.0672,  0.1204],
          [ 0.1154,  0.1686,  0.0036]],

         [[ 0.0998,  0.1331,  0.1144],
          [ 0.0706, -0.0406, -0.0367],
          [-0.0469, -0.0319,  0.1412]],

         [[-0.0475,  0.1455,  0.0749],
          [-0.0427, -0.0463, -0.0603],
          [-0.0400, -0.0087,  0.0444]]],


        ...,


        [[[ 0.2268, -0.1133, -0.1121],
          [-0.0417, -0.1041, -0.0971],
          [-0.0895,  0.0273,  0.0639]],

         [[ 0.1687,  0.1928, -0.1157],
          [ 0.2107, -0.1399, -0.1192],
          [-0.0621, -0.0796, -0.0350]],

         [[ 0.1891,  0.1063, -0.0944],
          [-0.0717, -0.1555, -0.0727],
          [ 0.0649,  0.1797, -0.1301]]],


        [[[-0.1090,  0.0883, -0.1527],
          [-0.1403, -0.1422,  0.1225],
          [-0.1587,  0.1527, -0.1313]],

         [[ 0.0925, -0.1231,  0.0786],
          [ 0.1535,  0.0060, -0.0859],
          [-0.1546,  0.1128, -0.1989]],

         [[ 0.1184, -0.0279, -0.0084],
          [ 0.1374, -0.1802,  0.0082],
          [-0.0375,  0.0102, -0.1266]]],


        [[[ 0.0772,  0.1381,  0.1031],
          [-0.0952, -0.1689,  0.2050],
          [-0.1647, -0.0140, -0.0576]],

         [[-0.0246, -0.0939,  0.1474],
          [-0.0119, -0.1984,  0.0443],
          [-0.0032,  0.0739,  0.0339]],

         [[-0.1510, -0.1971,  0.1395],
          [ 0.1394, -0.1783, -0.0276],
          [-0.0013, -0.0364,  0.2763]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0441, -0.0947, -0.1284],
          [-0.0254, -0.0768, -0.0938],
          [-0.0247, -0.0844, -0.0835]],

         [[-0.0354, -0.0802, -0.1146],
          [-0.0248, -0.0663, -0.0776],
          [-0.0283, -0.0731, -0.0652]],

         [[-0.0389, -0.0609, -0.0879],
          [-0.0266, -0.0490, -0.0592],
          [-0.0279, -0.0573, -0.0540]]],


        [[[-0.0326, -0.0319, -0.0165],
          [-0.0342, -0.0253, -0.0122],
          [-0.0307, -0.0269, -0.0208]],

         [[-0.0308, -0.0310, -0.0176],
          [-0.0396, -0.0308, -0.0166],
          [-0.0385, -0.0332, -0.0255]],

         [[-0.0300, -0.0311, -0.0202],
          [-0.0400, -0.0336, -0.0221],
          [-0.0413, -0.0375, -0.0312]]],


        [[[-0.0036,  0.0010,  0.0028],
          [-0.0059, -0.0010,  0.0008],
          [-0.0065, -0.0029, -0.0008]],

         [[-0.0057, -0.0009,  0.0015],
          [-0.0089, -0.0038, -0.0013],
          [-0.0108, -0.0069, -0.0041]],

         [[-0.0054, -0.0012,  0.0015],
          [-0.0081, -0.0038, -0.0009],
          [-0.0097, -0.0068, -0.0038]]],


        ...,


        [[[ 0.0243,  0.0184,  0.0096],
          [ 0.0280,  0.0166,  0.0092],
          [ 0.0261,  0.0132, -0.0020]],

         [[-0.0145, -0.0100, -0.0139],
          [-0.0067, -0.0095, -0.0122],
          [-0.0009, -0.0084, -0.0184]],

         [[-0.0156, -0.0151, -0.0219],
          [-0.0148, -0.0199, -0.0247],
          [-0.0074, -0.0164, -0.0273]]],


        [[[ 0.0077,  0.0018, -0.0044],
          [ 0.0108,  0.0032,  0.0002],
          [ 0.0081,  0.0027, -0.0009]],

         [[ 0.0007, -0.0051, -0.0077],
          [ 0.0010, -0.0047, -0.0043],
          [-0.0009, -0.0041, -0.0046]],

         [[ 0.0008, -0.0058, -0.0083],
          [ 0.0005, -0.0046, -0.0037],
          [-0.0006, -0.0039, -0.0032]]],


        [[[ 0.0080,  0.0315,  0.0208],
          [ 0.0088,  0.0433,  0.0341],
          [-0.0036,  0.0295,  0.0284]],

         [[-0.0267, -0.0042, -0.0132],
          [-0.0234,  0.0136,  0.0058],
          [-0.0385, -0.0022, -0.0043]],

         [[-0.0372, -0.0172, -0.0171],
          [-0.0351, -0.0093, -0.0113],
          [-0.0485, -0.0261, -0.0214]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.3724]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.6072) |  Loss2: (0.3183) | Acc: (88.00%) (113/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.6566) |  Loss2: (0.3183) | Acc: (88.00%) (1242/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.7216) |  Loss2: (0.3182) | Acc: (86.00%) (2319/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.7463) |  Loss2: (0.3182) | Acc: (85.00%) (3389/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.7626) |  Loss2: (0.3181) | Acc: (84.00%) (4444/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.7743) |  Loss2: (0.3181) | Acc: (84.00%) (5511/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.7826) |  Loss2: (0.3180) | Acc: (84.00%) (6570/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.7881) |  Loss2: (0.3179) | Acc: (83.00%) (7622/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.7844) |  Loss2: (0.3179) | Acc: (84.00%) (8718/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.7795) |  Loss2: (0.3178) | Acc: (84.00%) (9811/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.7853) |  Loss2: (0.3177) | Acc: (84.00%) (10869/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.7852) |  Loss2: (0.3177) | Acc: (84.00%) (11946/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.7814) |  Loss2: (0.3176) | Acc: (84.00%) (13041/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.7808) |  Loss2: (0.3175) | Acc: (84.00%) (14107/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.7852) |  Loss2: (0.3175) | Acc: (83.00%) (15158/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.7855) |  Loss2: (0.3174) | Acc: (83.00%) (16224/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.7838) |  Loss2: (0.3174) | Acc: (84.00%) (17317/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.7809) |  Loss2: (0.3173) | Acc: (84.00%) (18418/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.7767) |  Loss2: (0.3173) | Acc: (84.00%) (19528/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.7708) |  Loss2: (0.3172) | Acc: (84.00%) (20652/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.7719) |  Loss2: (0.3171) | Acc: (84.00%) (21725/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.7706) |  Loss2: (0.3171) | Acc: (84.00%) (22809/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.7687) |  Loss2: (0.3170) | Acc: (84.00%) (23915/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.7653) |  Loss2: (0.3169) | Acc: (84.00%) (25010/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.7653) |  Loss2: (0.3169) | Acc: (84.00%) (26095/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.7638) |  Loss2: (0.3168) | Acc: (84.00%) (27191/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.7622) |  Loss2: (0.3168) | Acc: (84.00%) (28293/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.7602) |  Loss2: (0.3167) | Acc: (84.00%) (29380/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.7592) |  Loss2: (0.3166) | Acc: (84.00%) (30473/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.7582) |  Loss2: (0.3166) | Acc: (84.00%) (31566/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.7568) |  Loss2: (0.3165) | Acc: (84.00%) (32667/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.7547) |  Loss2: (0.3165) | Acc: (84.00%) (33786/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.7547) |  Loss2: (0.3164) | Acc: (84.00%) (34880/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.7538) |  Loss2: (0.3163) | Acc: (84.00%) (35967/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.7519) |  Loss2: (0.3163) | Acc: (84.00%) (37077/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.7503) |  Loss2: (0.3162) | Acc: (84.00%) (38186/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.7497) |  Loss2: (0.3161) | Acc: (85.00%) (39278/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.7492) |  Loss2: (0.3161) | Acc: (85.00%) (40379/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.7471) |  Loss2: (0.3160) | Acc: (85.00%) (41501/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.7453) |  Loss2: (0.3160) | Acc: (85.00%) (42579/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_065.pth.tar'
# TEST : Loss: (0.5032) | Acc: (83.00%) (8332/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1512,  0.0562,  0.0939],
          [ 0.0962, -0.2767, -0.0983],
          [ 0.2397, -0.0968,  0.0159]],

         [[-0.0070, -0.1110,  0.2486],
          [ 0.0133, -0.0968,  0.0329],
          [ 0.0550, -0.0082, -0.0743]],

         [[ 0.0116, -0.0671,  0.1732],
          [ 0.1035, -0.1862,  0.1496],
          [-0.0958,  0.0352, -0.1862]]],


        [[[ 0.0935, -0.0840, -0.0065],
          [ 0.1349,  0.1705,  0.1817],
          [ 0.0086, -0.0651,  0.1178]],

         [[-0.1641, -0.1884,  0.1594],
          [ 0.0284, -0.0534,  0.1086],
          [ 0.1226, -0.2037,  0.1389]],

         [[ 0.0985, -0.0491, -0.0082],
          [ 0.0129,  0.0278, -0.1089],
          [-0.0335, -0.1539, -0.0072]]],


        [[[-0.0702,  0.0923, -0.0409],
          [-0.1233,  0.0672,  0.1203],
          [ 0.1154,  0.1685,  0.0036]],

         [[ 0.0998,  0.1331,  0.1144],
          [ 0.0707, -0.0405, -0.0367],
          [-0.0468, -0.0318,  0.1412]],

         [[-0.0474,  0.1455,  0.0749],
          [-0.0426, -0.0462, -0.0602],
          [-0.0399, -0.0086,  0.0444]]],


        ...,


        [[[ 0.2267, -0.1133, -0.1121],
          [-0.0418, -0.1039, -0.0970],
          [-0.0896,  0.0274,  0.0640]],

         [[ 0.1690,  0.1931, -0.1154],
          [ 0.2110, -0.1394, -0.1188],
          [-0.0620, -0.0792, -0.0347]],

         [[ 0.1896,  0.1068, -0.0938],
          [-0.0712, -0.1548, -0.0721],
          [ 0.0651,  0.1802, -0.1294]]],


        [[[-0.1093,  0.0881, -0.1528],
          [-0.1405, -0.1422,  0.1224],
          [-0.1588,  0.1528, -0.1313]],

         [[ 0.0922, -0.1230,  0.0785],
          [ 0.1533,  0.0061, -0.0858],
          [-0.1546,  0.1130, -0.1987]],

         [[ 0.1182, -0.0279, -0.0083],
          [ 0.1372, -0.1800,  0.0083],
          [-0.0375,  0.0104, -0.1264]]],


        [[[ 0.0783,  0.1387,  0.1038],
          [-0.0944, -0.1688,  0.2051],
          [-0.1643, -0.0144, -0.0578]],

         [[-0.0238, -0.0935,  0.1478],
          [-0.0114, -0.1986,  0.0441],
          [-0.0030,  0.0734,  0.0335]],

         [[-0.1510, -0.1973,  0.1391],
          [ 0.1389, -0.1792, -0.0285],
          [-0.0022, -0.0377,  0.2749]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5124e-05,  5.6209e-06,  9.3916e-06],
          [ 9.6227e-06, -2.7672e-05, -9.8275e-06],
          [ 2.3973e-05, -9.6759e-06,  1.5878e-06]],

         [[-7.0090e-07, -1.1100e-05,  2.4860e-05],
          [ 1.3269e-06, -9.6766e-06,  3.2934e-06],
          [ 5.5045e-06, -8.2172e-07, -7.4342e-06]],

         [[ 1.1566e-06, -6.7072e-06,  1.7325e-05],
          [ 1.0349e-05, -1.8621e-05,  1.4960e-05],
          [-9.5827e-06,  3.5245e-06, -1.8620e-05]]],


        [[[ 9.3536e-06, -8.4030e-06, -6.4518e-07],
          [ 1.3486e-05,  1.7052e-05,  1.8167e-05],
          [ 8.5807e-07, -6.5108e-06,  1.1779e-05]],

         [[-1.6409e-05, -1.8842e-05,  1.5943e-05],
          [ 2.8439e-06, -5.3449e-06,  1.0863e-05],
          [ 1.2258e-05, -2.0365e-05,  1.3889e-05]],

         [[ 9.8507e-06, -4.9134e-06, -8.2325e-07],
          [ 1.2886e-06,  2.7812e-06, -1.0890e-05],
          [-3.3530e-06, -1.5391e-05, -7.1689e-07]]],


        [[[-7.0191e-06,  9.2259e-06, -4.0865e-06],
          [-1.2333e-05,  6.7172e-06,  1.2031e-05],
          [ 1.1538e-05,  1.6852e-05,  3.6068e-07]],

         [[ 9.9815e-06,  1.3309e-05,  1.1436e-05],
          [ 7.0652e-06, -4.0530e-06, -3.6742e-06],
          [-4.6834e-06, -3.1845e-06,  1.4121e-05]],

         [[-4.7421e-06,  1.4551e-05,  7.4864e-06],
          [-4.2570e-06, -4.6228e-06, -6.0244e-06],
          [-3.9864e-06, -8.5668e-07,  4.4423e-06]]],


        ...,


        [[[ 2.2669e-05, -1.1329e-05, -1.1214e-05],
          [-4.1755e-06, -1.0391e-05, -9.7038e-06],
          [-8.9642e-06,  2.7439e-06,  6.4035e-06]],

         [[ 1.6903e-05,  1.9310e-05, -1.1540e-05],
          [ 2.1097e-05, -1.3942e-05, -1.1880e-05],
          [-6.2004e-06, -7.9247e-06, -3.4657e-06]],

         [[ 1.8959e-05,  1.0676e-05, -9.3812e-06],
          [-7.1208e-06, -1.5478e-05, -7.2083e-06],
          [ 6.5109e-06,  1.8023e-05, -1.2943e-05]]],


        [[[-1.0934e-05,  8.8129e-06, -1.5276e-05],
          [-1.4048e-05, -1.4217e-05,  1.2242e-05],
          [-1.5885e-05,  1.5277e-05, -1.3126e-05]],

         [[ 9.2176e-06, -1.2302e-05,  7.8538e-06],
          [ 1.5334e-05,  6.0996e-07, -8.5771e-06],
          [-1.5456e-05,  1.1298e-05, -1.9868e-05]],

         [[ 1.1818e-05, -2.7877e-06, -8.3449e-07],
          [ 1.3721e-05, -1.8004e-05,  8.2872e-07],
          [-3.7507e-06,  1.0446e-06, -1.2642e-05]]],


        [[[ 7.8255e-06,  1.3870e-05,  1.0380e-05],
          [-9.4363e-06, -1.6876e-05,  2.0514e-05],
          [-1.6428e-05, -1.4375e-06, -5.7803e-06]],

         [[-2.3813e-06, -9.3465e-06,  1.4775e-05],
          [-1.1447e-06, -1.9856e-05,  4.4112e-06],
          [-3.0465e-07,  7.3425e-06,  3.3517e-06]],

         [[-1.5104e-05, -1.9728e-05,  1.3912e-05],
          [ 1.3888e-05, -1.7916e-05, -2.8463e-06],
          [-2.1644e-07, -3.7713e-06,  2.7493e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4206]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0712]], device='cuda:0')

Epoch: 66 | Batch_idx: 0 |  Loss: (0.6736) |  Loss2: (0.3136) | Acc: (87.00%) (112/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.6922) |  Loss2: (0.3135) | Acc: (86.00%) (1220/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.7032) |  Loss2: (0.3134) | Acc: (86.00%) (2334/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.7165) |  Loss2: (0.3134) | Acc: (86.00%) (3420/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.7064) |  Loss2: (0.3133) | Acc: (86.00%) (4534/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.7025) |  Loss2: (0.3132) | Acc: (86.00%) (5653/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.7050) |  Loss2: (0.3132) | Acc: (86.00%) (6744/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.7015) |  Loss2: (0.3131) | Acc: (86.00%) (7869/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.6984) |  Loss2: (0.3130) | Acc: (86.00%) (8995/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.6957) |  Loss2: (0.3130) | Acc: (86.00%) (10113/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.6932) |  Loss2: (0.3129) | Acc: (86.00%) (11237/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.6922) |  Loss2: (0.3128) | Acc: (86.00%) (12356/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.6930) |  Loss2: (0.3128) | Acc: (86.00%) (13461/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.6941) |  Loss2: (0.3127) | Acc: (86.00%) (14560/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.6915) |  Loss2: (0.3126) | Acc: (86.00%) (15673/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.6909) |  Loss2: (0.3126) | Acc: (86.00%) (16791/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.6909) |  Loss2: (0.3125) | Acc: (86.00%) (17894/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.6876) |  Loss2: (0.3124) | Acc: (86.00%) (19035/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.6901) |  Loss2: (0.3124) | Acc: (86.00%) (20127/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.6889) |  Loss2: (0.3123) | Acc: (86.00%) (21250/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.6890) |  Loss2: (0.3123) | Acc: (86.00%) (22353/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.6884) |  Loss2: (0.3122) | Acc: (86.00%) (23477/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.6874) |  Loss2: (0.3122) | Acc: (86.00%) (24606/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.6848) |  Loss2: (0.3121) | Acc: (87.00%) (25757/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.6836) |  Loss2: (0.3120) | Acc: (87.00%) (26888/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.6826) |  Loss2: (0.3120) | Acc: (87.00%) (28016/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.6826) |  Loss2: (0.3119) | Acc: (87.00%) (29134/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.6829) |  Loss2: (0.3119) | Acc: (87.00%) (30252/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.6841) |  Loss2: (0.3118) | Acc: (87.00%) (31347/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.6839) |  Loss2: (0.3118) | Acc: (87.00%) (32460/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.6832) |  Loss2: (0.3117) | Acc: (87.00%) (33575/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.6836) |  Loss2: (0.3117) | Acc: (87.00%) (34684/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.6819) |  Loss2: (0.3116) | Acc: (87.00%) (35813/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.6823) |  Loss2: (0.3115) | Acc: (87.00%) (36919/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.6821) |  Loss2: (0.3115) | Acc: (87.00%) (38040/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.6823) |  Loss2: (0.3114) | Acc: (87.00%) (39146/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.6812) |  Loss2: (0.3114) | Acc: (87.00%) (40279/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.6820) |  Loss2: (0.3113) | Acc: (87.00%) (41370/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.6816) |  Loss2: (0.3113) | Acc: (87.00%) (42472/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.6810) |  Loss2: (0.3112) | Acc: (87.00%) (43553/50000)
# TEST : Loss: (0.4720) | Acc: (84.00%) (8420/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1512,  0.0562,  0.0939],
          [ 0.0962, -0.2766, -0.0982],
          [ 0.2396, -0.0967,  0.0159]],

         [[-0.0070, -0.1110,  0.2485],
          [ 0.0133, -0.0967,  0.0329],
          [ 0.0550, -0.0082, -0.0743]],

         [[ 0.0116, -0.0670,  0.1732],
          [ 0.1035, -0.1861,  0.1495],
          [-0.0958,  0.0352, -0.1861]]],


        [[[ 0.0935, -0.0840, -0.0064],
          [ 0.1348,  0.1705,  0.1816],
          [ 0.0086, -0.0651,  0.1177]],

         [[-0.1640, -0.1883,  0.1594],
          [ 0.0284, -0.0534,  0.1086],
          [ 0.1225, -0.2036,  0.1388]],

         [[ 0.0985, -0.0491, -0.0082],
          [ 0.0129,  0.0278, -0.1089],
          [-0.0335, -0.1539, -0.0072]]],


        [[[-0.0702,  0.0922, -0.0408],
          [-0.1233,  0.0671,  0.1203],
          [ 0.1153,  0.1685,  0.0036]],

         [[ 0.0998,  0.1330,  0.1143],
          [ 0.0706, -0.0405, -0.0367],
          [-0.0468, -0.0318,  0.1412]],

         [[-0.0474,  0.1454,  0.0748],
          [-0.0426, -0.0462, -0.0602],
          [-0.0398, -0.0086,  0.0444]]],


        ...,


        [[[ 0.2266, -0.1132, -0.1121],
          [-0.0417, -0.1039, -0.0970],
          [-0.0896,  0.0274,  0.0640]],

         [[ 0.1690,  0.1930, -0.1154],
          [ 0.2109, -0.1394, -0.1188],
          [-0.0620, -0.0792, -0.0346]],

         [[ 0.1895,  0.1067, -0.0938],
          [-0.0712, -0.1547, -0.0721],
          [ 0.0651,  0.1802, -0.1294]]],


        [[[-0.1093,  0.0881, -0.1527],
          [-0.1404, -0.1421,  0.1224],
          [-0.1588,  0.1527, -0.1312]],

         [[ 0.0921, -0.1230,  0.0785],
          [ 0.1533,  0.0061, -0.0857],
          [-0.1545,  0.1129, -0.1986]],

         [[ 0.1181, -0.0279, -0.0083],
          [ 0.1372, -0.1800,  0.0083],
          [-0.0375,  0.0104, -0.1264]]],


        [[[ 0.0782,  0.1386,  0.1038],
          [-0.0943, -0.1687,  0.2051],
          [-0.1642, -0.0144, -0.0578]],

         [[-0.0238, -0.0934,  0.1477],
          [-0.0114, -0.1985,  0.0441],
          [-0.0030,  0.0734,  0.0335]],

         [[-0.1510, -0.1972,  0.1391],
          [ 0.1388, -0.1791, -0.0285],
          [-0.0022, -0.0377,  0.2748]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5118e-05,  5.6187e-06,  9.3878e-06],
          [ 9.6189e-06, -2.7662e-05, -9.8238e-06],
          [ 2.3964e-05, -9.6721e-06,  1.5872e-06]],

         [[-7.0063e-07, -1.1096e-05,  2.4850e-05],
          [ 1.3264e-06, -9.6728e-06,  3.2921e-06],
          [ 5.5023e-06, -8.2139e-07, -7.4313e-06]],

         [[ 1.1561e-06, -6.7046e-06,  1.7318e-05],
          [ 1.0345e-05, -1.8614e-05,  1.4954e-05],
          [-9.5789e-06,  3.5232e-06, -1.8613e-05]]],


        [[[ 9.3498e-06, -8.3998e-06, -6.4492e-07],
          [ 1.3481e-05,  1.7045e-05,  1.8160e-05],
          [ 8.5775e-07, -6.5082e-06,  1.1774e-05]],

         [[-1.6403e-05, -1.8834e-05,  1.5937e-05],
          [ 2.8428e-06, -5.3428e-06,  1.0859e-05],
          [ 1.2253e-05, -2.0357e-05,  1.3884e-05]],

         [[ 9.8469e-06, -4.9115e-06, -8.2292e-07],
          [ 1.2881e-06,  2.7801e-06, -1.0886e-05],
          [-3.3516e-06, -1.5385e-05, -7.1662e-07]]],


        [[[-7.0165e-06,  9.2224e-06, -4.0849e-06],
          [-1.2328e-05,  6.7146e-06,  1.2027e-05],
          [ 1.1533e-05,  1.6846e-05,  3.6054e-07]],

         [[ 9.9777e-06,  1.3303e-05,  1.1431e-05],
          [ 7.0626e-06, -4.0514e-06, -3.6728e-06],
          [-4.6816e-06, -3.1832e-06,  1.4116e-05]],

         [[-4.7402e-06,  1.4545e-05,  7.4835e-06],
          [-4.2554e-06, -4.6211e-06, -6.0221e-06],
          [-3.9848e-06, -8.5635e-07,  4.4405e-06]]],


        ...,


        [[[ 2.2660e-05, -1.1325e-05, -1.1210e-05],
          [-4.1739e-06, -1.0387e-05, -9.7001e-06],
          [-8.9607e-06,  2.7428e-06,  6.4009e-06]],

         [[ 1.6897e-05,  1.9302e-05, -1.1535e-05],
          [ 2.1089e-05, -1.3937e-05, -1.1875e-05],
          [-6.1979e-06, -7.9214e-06, -3.4644e-06]],

         [[ 1.8951e-05,  1.0672e-05, -9.3775e-06],
          [-7.1178e-06, -1.5472e-05, -7.2054e-06],
          [ 6.5082e-06,  1.8016e-05, -1.2938e-05]]],


        [[[-1.0929e-05,  8.8094e-06, -1.5270e-05],
          [-1.4043e-05, -1.4211e-05,  1.2237e-05],
          [-1.5878e-05,  1.5271e-05, -1.3121e-05]],

         [[ 9.2141e-06, -1.2297e-05,  7.8506e-06],
          [ 1.5328e-05,  6.0972e-07, -8.5736e-06],
          [-1.5450e-05,  1.1293e-05, -1.9860e-05]],

         [[ 1.1813e-05, -2.7866e-06, -8.3416e-07],
          [ 1.3716e-05, -1.7997e-05,  8.2839e-07],
          [-3.7493e-06,  1.0442e-06, -1.2638e-05]]],


        [[[ 7.8224e-06,  1.3865e-05,  1.0376e-05],
          [-9.4325e-06, -1.6870e-05,  2.0506e-05],
          [-1.6422e-05, -1.4370e-06, -5.7780e-06]],

         [[-2.3804e-06, -9.3428e-06,  1.4769e-05],
          [-1.1443e-06, -1.9848e-05,  4.4095e-06],
          [-3.0453e-07,  7.3396e-06,  3.3504e-06]],

         [[-1.5099e-05, -1.9720e-05,  1.3906e-05],
          [ 1.3883e-05, -1.7909e-05, -2.8452e-06],
          [-2.1635e-07, -3.7699e-06,  2.7482e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4412]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0627]], device='cuda:0')

Epoch: 67 | Batch_idx: 0 |  Loss: (0.6375) |  Loss2: (0.3091) | Acc: (87.00%) (112/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.6535) |  Loss2: (0.3090) | Acc: (88.00%) (1240/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.6732) |  Loss2: (0.3089) | Acc: (87.00%) (2347/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.6631) |  Loss2: (0.3089) | Acc: (87.00%) (3471/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.6743) |  Loss2: (0.3088) | Acc: (86.00%) (4555/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.6757) |  Loss2: (0.3088) | Acc: (87.00%) (5691/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.6760) |  Loss2: (0.3087) | Acc: (87.00%) (6816/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.6726) |  Loss2: (0.3087) | Acc: (87.00%) (7949/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.6725) |  Loss2: (0.3086) | Acc: (87.00%) (9066/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.6763) |  Loss2: (0.3086) | Acc: (87.00%) (10179/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.6765) |  Loss2: (0.3085) | Acc: (87.00%) (11295/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.6760) |  Loss2: (0.3085) | Acc: (87.00%) (12422/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.6764) |  Loss2: (0.3085) | Acc: (87.00%) (13532/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.6748) |  Loss2: (0.3084) | Acc: (87.00%) (14657/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.6726) |  Loss2: (0.3083) | Acc: (87.00%) (15780/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.6711) |  Loss2: (0.3083) | Acc: (87.00%) (16920/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.6713) |  Loss2: (0.3082) | Acc: (87.00%) (18043/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.6714) |  Loss2: (0.3082) | Acc: (87.00%) (19177/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.6692) |  Loss2: (0.3081) | Acc: (87.00%) (20328/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.6670) |  Loss2: (0.3081) | Acc: (87.00%) (21462/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.6673) |  Loss2: (0.3080) | Acc: (87.00%) (22578/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.6655) |  Loss2: (0.3080) | Acc: (87.00%) (23724/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.6649) |  Loss2: (0.3079) | Acc: (87.00%) (24843/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.6636) |  Loss2: (0.3079) | Acc: (87.00%) (25982/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.6629) |  Loss2: (0.3078) | Acc: (87.00%) (27103/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.6633) |  Loss2: (0.3078) | Acc: (87.00%) (28229/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.6627) |  Loss2: (0.3077) | Acc: (87.00%) (29356/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.6622) |  Loss2: (0.3077) | Acc: (87.00%) (30480/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.6627) |  Loss2: (0.3076) | Acc: (87.00%) (31585/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.6621) |  Loss2: (0.3076) | Acc: (87.00%) (32711/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.6623) |  Loss2: (0.3075) | Acc: (87.00%) (33830/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.6622) |  Loss2: (0.3075) | Acc: (87.00%) (34958/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.6620) |  Loss2: (0.3074) | Acc: (87.00%) (36084/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.6625) |  Loss2: (0.3074) | Acc: (87.00%) (37200/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.6636) |  Loss2: (0.3073) | Acc: (87.00%) (38306/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.6640) |  Loss2: (0.3073) | Acc: (87.00%) (39430/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.6640) |  Loss2: (0.3072) | Acc: (87.00%) (40541/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.6634) |  Loss2: (0.3072) | Acc: (87.00%) (41676/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.6627) |  Loss2: (0.3071) | Acc: (87.00%) (42809/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.6628) |  Loss2: (0.3071) | Acc: (87.00%) (43887/50000)
# TEST : Loss: (0.4649) | Acc: (84.00%) (8456/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1511,  0.0562,  0.0938],
          [ 0.0962, -0.2765, -0.0982],
          [ 0.2395, -0.0967,  0.0159]],

         [[-0.0070, -0.1109,  0.2484],
          [ 0.0133, -0.0967,  0.0329],
          [ 0.0550, -0.0082, -0.0743]],

         [[ 0.0116, -0.0670,  0.1731],
          [ 0.1034, -0.1861,  0.1495],
          [-0.0958,  0.0352, -0.1861]]],


        [[[ 0.0935, -0.0840, -0.0064],
          [ 0.1348,  0.1704,  0.1815],
          [ 0.0086, -0.0651,  0.1177]],

         [[-0.1640, -0.1883,  0.1593],
          [ 0.0284, -0.0534,  0.1085],
          [ 0.1225, -0.2035,  0.1388]],

         [[ 0.0984, -0.0491, -0.0082],
          [ 0.0129,  0.0278, -0.1088],
          [-0.0335, -0.1538, -0.0072]]],


        [[[-0.0701,  0.0922, -0.0408],
          [-0.1232,  0.0671,  0.1202],
          [ 0.1153,  0.1684,  0.0036]],

         [[ 0.0997,  0.1330,  0.1143],
          [ 0.0706, -0.0405, -0.0367],
          [-0.0468, -0.0318,  0.1411]],

         [[-0.0474,  0.1454,  0.0748],
          [-0.0425, -0.0462, -0.0602],
          [-0.0398, -0.0086,  0.0444]]],


        ...,


        [[[ 0.2265, -0.1132, -0.1121],
          [-0.0417, -0.1038, -0.0970],
          [-0.0896,  0.0274,  0.0640]],

         [[ 0.1689,  0.1929, -0.1153],
          [ 0.2108, -0.1393, -0.1187],
          [-0.0620, -0.0792, -0.0346]],

         [[ 0.1894,  0.1067, -0.0937],
          [-0.0711, -0.1547, -0.0720],
          [ 0.0651,  0.1801, -0.1293]]],


        [[[-0.1092,  0.0881, -0.1526],
          [-0.1404, -0.1420,  0.1223],
          [-0.1587,  0.1527, -0.1312]],

         [[ 0.0921, -0.1229,  0.0785],
          [ 0.1532,  0.0061, -0.0857],
          [-0.1544,  0.1129, -0.1985]],

         [[ 0.1181, -0.0279, -0.0083],
          [ 0.1371, -0.1799,  0.0083],
          [-0.0375,  0.0104, -0.1263]]],


        [[[ 0.0782,  0.1386,  0.1037],
          [-0.0943, -0.1686,  0.2050],
          [-0.1642, -0.0144, -0.0578]],

         [[-0.0238, -0.0934,  0.1476],
          [-0.0114, -0.1984,  0.0441],
          [-0.0030,  0.0734,  0.0335]],

         [[-0.1509, -0.1971,  0.1390],
          [ 0.1388, -0.1790, -0.0284],
          [-0.0022, -0.0377,  0.2747]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5112e-05,  5.6165e-06,  9.3840e-06],
          [ 9.6151e-06, -2.7651e-05, -9.8200e-06],
          [ 2.3955e-05, -9.6684e-06,  1.5865e-06]],

         [[-7.0035e-07, -1.1092e-05,  2.4840e-05],
          [ 1.3258e-06, -9.6691e-06,  3.2908e-06],
          [ 5.5002e-06, -8.2106e-07, -7.4284e-06]],

         [[ 1.1557e-06, -6.7019e-06,  1.7311e-05],
          [ 1.0341e-05, -1.8607e-05,  1.4948e-05],
          [-9.5751e-06,  3.5219e-06, -1.8606e-05]]],


        [[[ 9.3460e-06, -8.3966e-06, -6.4467e-07],
          [ 1.3475e-05,  1.7039e-05,  1.8153e-05],
          [ 8.5742e-07, -6.5056e-06,  1.1769e-05]],

         [[-1.6396e-05, -1.8827e-05,  1.5930e-05],
          [ 2.8417e-06, -5.3408e-06,  1.0854e-05],
          [ 1.2248e-05, -2.0349e-05,  1.3879e-05]],

         [[ 9.8432e-06, -4.9096e-06, -8.2259e-07],
          [ 1.2875e-06,  2.7790e-06, -1.0882e-05],
          [-3.3503e-06, -1.5379e-05, -7.1634e-07]]],


        [[[-7.0139e-06,  9.2189e-06, -4.0833e-06],
          [-1.2323e-05,  6.7120e-06,  1.2022e-05],
          [ 1.1529e-05,  1.6840e-05,  3.6040e-07]],

         [[ 9.9740e-06,  1.3298e-05,  1.1427e-05],
          [ 7.0600e-06, -4.0498e-06, -3.6713e-06],
          [-4.6797e-06, -3.1818e-06,  1.4110e-05]],

         [[-4.7383e-06,  1.4539e-05,  7.4806e-06],
          [-4.2538e-06, -4.6193e-06, -6.0198e-06],
          [-3.9832e-06, -8.5602e-07,  4.4388e-06]]],


        ...,


        [[[ 2.2651e-05, -1.1321e-05, -1.1205e-05],
          [-4.1723e-06, -1.0383e-05, -9.6963e-06],
          [-8.9572e-06,  2.7417e-06,  6.3983e-06]],

         [[ 1.6891e-05,  1.9295e-05, -1.1531e-05],
          [ 2.1081e-05, -1.3932e-05, -1.1871e-05],
          [-6.1954e-06, -7.9182e-06, -3.4631e-06]],

         [[ 1.8943e-05,  1.0668e-05, -9.3737e-06],
          [-7.1149e-06, -1.5466e-05, -7.2025e-06],
          [ 6.5056e-06,  1.8009e-05, -1.2933e-05]]],


        [[[-1.0925e-05,  8.8059e-06, -1.5265e-05],
          [-1.4037e-05, -1.4205e-05,  1.2232e-05],
          [-1.5872e-05,  1.5265e-05, -1.3115e-05]],

         [[ 9.2106e-06, -1.2292e-05,  7.8474e-06],
          [ 1.5322e-05,  6.0948e-07, -8.5701e-06],
          [-1.5444e-05,  1.1289e-05, -1.9853e-05]],

         [[ 1.1808e-05, -2.7855e-06, -8.3383e-07],
          [ 1.3710e-05, -1.7990e-05,  8.2806e-07],
          [-3.7478e-06,  1.0438e-06, -1.2633e-05]]],


        [[[ 7.8194e-06,  1.3859e-05,  1.0372e-05],
          [-9.4288e-06, -1.6863e-05,  2.0498e-05],
          [-1.6415e-05, -1.4364e-06, -5.7757e-06]],

         [[-2.3794e-06, -9.3390e-06,  1.4763e-05],
          [-1.1439e-06, -1.9841e-05,  4.4077e-06],
          [-3.0441e-07,  7.3367e-06,  3.3490e-06]],

         [[-1.5093e-05, -1.9712e-05,  1.3901e-05],
          [ 1.3877e-05, -1.7902e-05, -2.8441e-06],
          [-2.1627e-07, -3.7684e-06,  2.7472e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4528]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0095]], device='cuda:0')

Epoch: 68 | Batch_idx: 0 |  Loss: (0.6923) |  Loss2: (0.3051) | Acc: (88.00%) (113/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.6733) |  Loss2: (0.3050) | Acc: (87.00%) (1232/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.6604) |  Loss2: (0.3050) | Acc: (88.00%) (2378/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.6608) |  Loss2: (0.3049) | Acc: (88.00%) (3501/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.6576) |  Loss2: (0.3048) | Acc: (88.00%) (4627/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.6579) |  Loss2: (0.3048) | Acc: (88.00%) (5748/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.6585) |  Loss2: (0.3047) | Acc: (87.00%) (6862/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.6624) |  Loss2: (0.3047) | Acc: (87.00%) (7988/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.6646) |  Loss2: (0.3046) | Acc: (87.00%) (9106/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.6619) |  Loss2: (0.3046) | Acc: (87.00%) (10227/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.6613) |  Loss2: (0.3046) | Acc: (87.00%) (11356/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.6618) |  Loss2: (0.3045) | Acc: (87.00%) (12477/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.6651) |  Loss2: (0.3045) | Acc: (87.00%) (13571/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.6643) |  Loss2: (0.3044) | Acc: (87.00%) (14696/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.6619) |  Loss2: (0.3044) | Acc: (87.00%) (15829/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.6602) |  Loss2: (0.3043) | Acc: (87.00%) (16963/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.6563) |  Loss2: (0.3043) | Acc: (87.00%) (18118/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.6546) |  Loss2: (0.3043) | Acc: (87.00%) (19260/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.6557) |  Loss2: (0.3042) | Acc: (87.00%) (20374/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.6560) |  Loss2: (0.3042) | Acc: (87.00%) (21491/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.6553) |  Loss2: (0.3041) | Acc: (87.00%) (22626/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.6560) |  Loss2: (0.3041) | Acc: (87.00%) (23753/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.6547) |  Loss2: (0.3040) | Acc: (87.00%) (24888/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.6525) |  Loss2: (0.3040) | Acc: (88.00%) (26032/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.6519) |  Loss2: (0.3039) | Acc: (88.00%) (27163/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.6506) |  Loss2: (0.3039) | Acc: (88.00%) (28314/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.6505) |  Loss2: (0.3039) | Acc: (88.00%) (29454/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.6502) |  Loss2: (0.3038) | Acc: (88.00%) (30575/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.6511) |  Loss2: (0.3038) | Acc: (88.00%) (31694/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.6513) |  Loss2: (0.3037) | Acc: (88.00%) (32803/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.6514) |  Loss2: (0.3037) | Acc: (88.00%) (33922/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.6516) |  Loss2: (0.3036) | Acc: (88.00%) (35044/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.6509) |  Loss2: (0.3036) | Acc: (88.00%) (36174/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.6501) |  Loss2: (0.3035) | Acc: (88.00%) (37317/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.6500) |  Loss2: (0.3035) | Acc: (88.00%) (38446/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.6494) |  Loss2: (0.3034) | Acc: (88.00%) (39586/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.6491) |  Loss2: (0.3034) | Acc: (88.00%) (40712/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.6496) |  Loss2: (0.3033) | Acc: (88.00%) (41837/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.6498) |  Loss2: (0.3033) | Acc: (88.00%) (42954/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.6494) |  Loss2: (0.3032) | Acc: (88.00%) (44044/50000)
# TEST : Loss: (0.4555) | Acc: (84.00%) (8474/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1511,  0.0561,  0.0938],
          [ 0.0961, -0.2764, -0.0982],
          [ 0.2395, -0.0966,  0.0159]],

         [[-0.0070, -0.1109,  0.2483],
          [ 0.0133, -0.0967,  0.0329],
          [ 0.0550, -0.0082, -0.0743]],

         [[ 0.0116, -0.0670,  0.1730],
          [ 0.1034, -0.1860,  0.1494],
          [-0.0957,  0.0352, -0.1860]]],


        [[[ 0.0934, -0.0839, -0.0064],
          [ 0.1347,  0.1703,  0.1815],
          [ 0.0086, -0.0650,  0.1176]],

         [[-0.1639, -0.1882,  0.1592],
          [ 0.0284, -0.0534,  0.1085],
          [ 0.1224, -0.2034,  0.1387]],

         [[ 0.0984, -0.0491, -0.0082],
          [ 0.0129,  0.0278, -0.1088],
          [-0.0335, -0.1537, -0.0072]]],


        [[[-0.0701,  0.0922, -0.0408],
          [-0.1232,  0.0671,  0.1202],
          [ 0.1152,  0.1683,  0.0036]],

         [[ 0.0997,  0.1329,  0.1142],
          [ 0.0706, -0.0405, -0.0367],
          [-0.0468, -0.0318,  0.1411]],

         [[-0.0474,  0.1453,  0.0748],
          [-0.0425, -0.0462, -0.0602],
          [-0.0398, -0.0086,  0.0444]]],


        ...,


        [[[ 0.2264, -0.1132, -0.1120],
          [-0.0417, -0.1038, -0.0969],
          [-0.0895,  0.0274,  0.0640]],

         [[ 0.1688,  0.1929, -0.1153],
          [ 0.2107, -0.1393, -0.1187],
          [-0.0619, -0.0792, -0.0346]],

         [[ 0.1894,  0.1066, -0.0937],
          [-0.0711, -0.1546, -0.0720],
          [ 0.0650,  0.1800, -0.1293]]],


        [[[-0.1092,  0.0880, -0.1526],
          [-0.1403, -0.1420,  0.1223],
          [-0.1587,  0.1526, -0.1311]],

         [[ 0.0921, -0.1229,  0.0784],
          [ 0.1532,  0.0061, -0.0857],
          [-0.1544,  0.1128, -0.1985]],

         [[ 0.1180, -0.0278, -0.0083],
          [ 0.1371, -0.1798,  0.0083],
          [-0.0375,  0.0104, -0.1263]]],


        [[[ 0.0782,  0.1385,  0.1037],
          [-0.0942, -0.1686,  0.2049],
          [-0.1641, -0.0144, -0.0577]],

         [[-0.0238, -0.0934,  0.1476],
          [-0.0114, -0.1983,  0.0441],
          [-0.0030,  0.0733,  0.0335]],

         [[-0.1509, -0.1970,  0.1390],
          [ 0.1387, -0.1789, -0.0284],
          [-0.0022, -0.0377,  0.2746]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5106e-05,  5.6143e-06,  9.3802e-06],
          [ 9.6113e-06, -2.7641e-05, -9.8162e-06],
          [ 2.3945e-05, -9.6646e-06,  1.5859e-06]],

         [[-7.0008e-07, -1.1087e-05,  2.4830e-05],
          [ 1.3253e-06, -9.6653e-06,  3.2895e-06],
          [ 5.4980e-06, -8.2073e-07, -7.4255e-06]],

         [[ 1.1553e-06, -6.6993e-06,  1.7304e-05],
          [ 1.0337e-05, -1.8600e-05,  1.4942e-05],
          [-9.5713e-06,  3.5205e-06, -1.8599e-05]]],


        [[[ 9.3422e-06, -8.3934e-06, -6.4441e-07],
          [ 1.3470e-05,  1.7033e-05,  1.8146e-05],
          [ 8.5709e-07, -6.5029e-06,  1.1765e-05]],

         [[-1.6390e-05, -1.8819e-05,  1.5924e-05],
          [ 2.8407e-06, -5.3388e-06,  1.0850e-05],
          [ 1.2244e-05, -2.0341e-05,  1.3874e-05]],

         [[ 9.8394e-06, -4.9077e-06, -8.2226e-07],
          [ 1.2870e-06,  2.7779e-06, -1.0877e-05],
          [-3.3490e-06, -1.5373e-05, -7.1607e-07]]],


        [[[-7.0113e-06,  9.2154e-06, -4.0817e-06],
          [-1.2318e-05,  6.7093e-06,  1.2017e-05],
          [ 1.1525e-05,  1.6833e-05,  3.6027e-07]],

         [[ 9.9702e-06,  1.3293e-05,  1.1422e-05],
          [ 7.0574e-06, -4.0482e-06, -3.6699e-06],
          [-4.6778e-06, -3.1805e-06,  1.4105e-05]],

         [[-4.7364e-06,  1.4533e-05,  7.4777e-06],
          [-4.2522e-06, -4.6176e-06, -6.0174e-06],
          [-3.9816e-06, -8.5569e-07,  4.4370e-06]]],


        ...,


        [[[ 2.2643e-05, -1.1316e-05, -1.1201e-05],
          [-4.1707e-06, -1.0379e-05, -9.6925e-06],
          [-8.9537e-06,  2.7406e-06,  6.3956e-06]],

         [[ 1.6884e-05,  1.9287e-05, -1.1527e-05],
          [ 2.1073e-05, -1.3926e-05, -1.1866e-05],
          [-6.1930e-06, -7.9150e-06, -3.4618e-06]],

         [[ 1.8936e-05,  1.0664e-05, -9.3699e-06],
          [-7.1120e-06, -1.5460e-05, -7.1996e-06],
          [ 6.5030e-06,  1.8002e-05, -1.2928e-05]]],


        [[[-1.0920e-05,  8.8024e-06, -1.5259e-05],
          [-1.4032e-05, -1.4199e-05,  1.2228e-05],
          [-1.5865e-05,  1.5260e-05, -1.3110e-05]],

         [[ 9.2071e-06, -1.2288e-05,  7.8442e-06],
          [ 1.5316e-05,  6.0925e-07, -8.5667e-06],
          [-1.5438e-05,  1.1285e-05, -1.9845e-05]],

         [[ 1.1804e-05, -2.7844e-06, -8.3351e-07],
          [ 1.3705e-05, -1.7983e-05,  8.2773e-07],
          [-3.7464e-06,  1.0434e-06, -1.2628e-05]]],


        [[[ 7.8165e-06,  1.3854e-05,  1.0368e-05],
          [-9.4250e-06, -1.6857e-05,  2.0489e-05],
          [-1.6409e-05, -1.4359e-06, -5.7734e-06]],

         [[-2.3785e-06, -9.3352e-06,  1.4758e-05],
          [-1.1434e-06, -1.9833e-05,  4.4060e-06],
          [-3.0429e-07,  7.3338e-06,  3.3477e-06]],

         [[-1.5087e-05, -1.9705e-05,  1.3896e-05],
          [ 1.3872e-05, -1.7895e-05, -2.8430e-06],
          [-2.1619e-07, -3.7670e-06,  2.7461e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4666]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0755]], device='cuda:0')

Epoch: 69 | Batch_idx: 0 |  Loss: (0.6633) |  Loss2: (0.3012) | Acc: (85.00%) (109/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.6417) |  Loss2: (0.3012) | Acc: (88.00%) (1241/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.6368) |  Loss2: (0.3011) | Acc: (88.00%) (2375/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.6365) |  Loss2: (0.3011) | Acc: (88.00%) (3506/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.6410) |  Loss2: (0.3011) | Acc: (87.00%) (4616/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.6430) |  Loss2: (0.3010) | Acc: (87.00%) (5741/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.6464) |  Loss2: (0.3010) | Acc: (87.00%) (6867/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.6541) |  Loss2: (0.3009) | Acc: (87.00%) (7978/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.6522) |  Loss2: (0.3009) | Acc: (87.00%) (9094/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.6483) |  Loss2: (0.3009) | Acc: (87.00%) (10241/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.6444) |  Loss2: (0.3008) | Acc: (88.00%) (11387/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.6429) |  Loss2: (0.3008) | Acc: (88.00%) (12528/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.6417) |  Loss2: (0.3007) | Acc: (88.00%) (13669/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.6406) |  Loss2: (0.3007) | Acc: (88.00%) (14816/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.6396) |  Loss2: (0.3007) | Acc: (88.00%) (15949/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.6397) |  Loss2: (0.3006) | Acc: (88.00%) (17089/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.6408) |  Loss2: (0.3006) | Acc: (88.00%) (18205/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.6395) |  Loss2: (0.3005) | Acc: (88.00%) (19341/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.6412) |  Loss2: (0.3005) | Acc: (88.00%) (20451/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.6414) |  Loss2: (0.3004) | Acc: (88.00%) (21573/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.6435) |  Loss2: (0.3004) | Acc: (88.00%) (22682/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.6444) |  Loss2: (0.3004) | Acc: (88.00%) (23812/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.6457) |  Loss2: (0.3003) | Acc: (88.00%) (24929/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.6456) |  Loss2: (0.3003) | Acc: (88.00%) (26065/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.6448) |  Loss2: (0.3002) | Acc: (88.00%) (27206/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.6444) |  Loss2: (0.3002) | Acc: (88.00%) (28336/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.6436) |  Loss2: (0.3001) | Acc: (88.00%) (29473/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.6444) |  Loss2: (0.3001) | Acc: (88.00%) (30589/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.6440) |  Loss2: (0.3000) | Acc: (88.00%) (31733/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.6443) |  Loss2: (0.3000) | Acc: (88.00%) (32845/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.6449) |  Loss2: (0.3000) | Acc: (88.00%) (33954/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.6446) |  Loss2: (0.2999) | Acc: (88.00%) (35071/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.6441) |  Loss2: (0.2999) | Acc: (88.00%) (36209/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.6443) |  Loss2: (0.2998) | Acc: (88.00%) (37338/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.6441) |  Loss2: (0.2998) | Acc: (88.00%) (38470/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.6446) |  Loss2: (0.2997) | Acc: (88.00%) (39591/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.6441) |  Loss2: (0.2997) | Acc: (88.00%) (40729/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.6431) |  Loss2: (0.2996) | Acc: (88.00%) (41885/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.6428) |  Loss2: (0.2996) | Acc: (88.00%) (43026/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.6424) |  Loss2: (0.2995) | Acc: (88.00%) (44124/50000)
# TEST : Loss: (0.4489) | Acc: (84.00%) (8498/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1510,  0.0561,  0.0938],
          [ 0.0961, -0.2763, -0.0981],
          [ 0.2394, -0.0966,  0.0159]],

         [[-0.0070, -0.1108,  0.2482],
          [ 0.0132, -0.0966,  0.0329],
          [ 0.0550, -0.0082, -0.0742]],

         [[ 0.0115, -0.0670,  0.1730],
          [ 0.1033, -0.1859,  0.1494],
          [-0.0957,  0.0352, -0.1859]]],


        [[[ 0.0934, -0.0839, -0.0064],
          [ 0.1346,  0.1703,  0.1814],
          [ 0.0086, -0.0650,  0.1176]],

         [[-0.1638, -0.1881,  0.1592],
          [ 0.0284, -0.0534,  0.1085],
          [ 0.1224, -0.2033,  0.1387]],

         [[ 0.0984, -0.0491, -0.0082],
          [ 0.0129,  0.0278, -0.1087],
          [-0.0335, -0.1537, -0.0072]]],


        [[[-0.0701,  0.0921, -0.0408],
          [-0.1231,  0.0671,  0.1201],
          [ 0.1152,  0.1683,  0.0036]],

         [[ 0.0997,  0.1329,  0.1142],
          [ 0.0705, -0.0405, -0.0367],
          [-0.0468, -0.0318,  0.1410]],

         [[-0.0473,  0.1453,  0.0747],
          [-0.0425, -0.0462, -0.0602],
          [-0.0398, -0.0086,  0.0444]]],


        ...,


        [[[ 0.2263, -0.1131, -0.1120],
          [-0.0417, -0.1037, -0.0969],
          [-0.0895,  0.0274,  0.0639]],

         [[ 0.1688,  0.1928, -0.1152],
          [ 0.2106, -0.1392, -0.1186],
          [-0.0619, -0.0791, -0.0346]],

         [[ 0.1893,  0.1066, -0.0937],
          [-0.0711, -0.1545, -0.0720],
          [ 0.0650,  0.1800, -0.1292]]],


        [[[-0.1092,  0.0880, -0.1525],
          [-0.1403, -0.1419,  0.1222],
          [-0.1586,  0.1525, -0.1310]],

         [[ 0.0920, -0.1228,  0.0784],
          [ 0.1531,  0.0061, -0.0856],
          [-0.1543,  0.1128, -0.1984]],

         [[ 0.1180, -0.0278, -0.0083],
          [ 0.1370, -0.1798,  0.0083],
          [-0.0374,  0.0104, -0.1262]]],


        [[[ 0.0781,  0.1385,  0.1036],
          [-0.0942, -0.1685,  0.2048],
          [-0.1640, -0.0144, -0.0577]],

         [[-0.0238, -0.0933,  0.1475],
          [-0.0114, -0.1983,  0.0440],
          [-0.0030,  0.0733,  0.0335]],

         [[-0.1508, -0.1970,  0.1389],
          [ 0.1387, -0.1789, -0.0284],
          [-0.0022, -0.0377,  0.2745]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.5101e-05,  5.6121e-06,  9.3764e-06],
          [ 9.6076e-06, -2.7630e-05, -9.8124e-06],
          [ 2.3936e-05, -9.6608e-06,  1.5852e-06]],

         [[-6.9981e-07, -1.1083e-05,  2.4821e-05],
          [ 1.3248e-06, -9.6615e-06,  3.2882e-06],
          [ 5.4958e-06, -8.2041e-07, -7.4225e-06]],

         [[ 1.1548e-06, -6.6967e-06,  1.7297e-05],
          [ 1.0333e-05, -1.8593e-05,  1.4936e-05],
          [-9.5675e-06,  3.5192e-06, -1.8592e-05]]],


        [[[ 9.3384e-06, -8.3902e-06, -6.4416e-07],
          [ 1.3465e-05,  1.7026e-05,  1.8139e-05],
          [ 8.5676e-07, -6.5003e-06,  1.1760e-05]],

         [[-1.6383e-05, -1.8811e-05,  1.5917e-05],
          [ 2.8396e-06, -5.3367e-06,  1.0845e-05],
          [ 1.2239e-05, -2.0333e-05,  1.3868e-05]],

         [[ 9.8356e-06, -4.9058e-06, -8.2194e-07],
          [ 1.2865e-06,  2.7768e-06, -1.0873e-05],
          [-3.3477e-06, -1.5368e-05, -7.1580e-07]]],


        [[[-7.0087e-06,  9.2120e-06, -4.0801e-06],
          [-1.2313e-05,  6.7067e-06,  1.2013e-05],
          [ 1.1520e-05,  1.6827e-05,  3.6013e-07]],

         [[ 9.9664e-06,  1.3288e-05,  1.1418e-05],
          [ 7.0548e-06, -4.0466e-06, -3.6684e-06],
          [-4.6759e-06, -3.1792e-06,  1.4100e-05]],

         [[-4.7345e-06,  1.4528e-05,  7.4747e-06],
          [-4.2506e-06, -4.6158e-06, -6.0151e-06],
          [-3.9800e-06, -8.5537e-07,  4.4353e-06]]],


        ...,


        [[[ 2.2634e-05, -1.1312e-05, -1.1196e-05],
          [-4.1690e-06, -1.0375e-05, -9.6887e-06],
          [-8.9502e-06,  2.7395e-06,  6.3930e-06]],

         [[ 1.6878e-05,  1.9280e-05, -1.1522e-05],
          [ 2.1065e-05, -1.3921e-05, -1.1861e-05],
          [-6.1905e-06, -7.9118e-06, -3.4605e-06]],

         [[ 1.8928e-05,  1.0660e-05, -9.3661e-06],
          [-7.1091e-06, -1.5454e-05, -7.1967e-06],
          [ 6.5004e-06,  1.7995e-05, -1.2922e-05]]],


        [[[-1.0916e-05,  8.7989e-06, -1.5253e-05],
          [-1.4027e-05, -1.4193e-05,  1.2223e-05],
          [-1.5859e-05,  1.5254e-05, -1.3105e-05]],

         [[ 9.2036e-06, -1.2283e-05,  7.8410e-06],
          [ 1.5311e-05,  6.0901e-07, -8.5635e-06],
          [-1.5432e-05,  1.1280e-05, -1.9837e-05]],

         [[ 1.1799e-05, -2.7833e-06, -8.3318e-07],
          [ 1.3700e-05, -1.7976e-05,  8.2741e-07],
          [-3.7449e-06,  1.0430e-06, -1.2624e-05]]],


        [[[ 7.8136e-06,  1.3849e-05,  1.0364e-05],
          [-9.4212e-06, -1.6850e-05,  2.0481e-05],
          [-1.6403e-05, -1.4353e-06, -5.7712e-06]],

         [[-2.3775e-06, -9.3314e-06,  1.4752e-05],
          [-1.1430e-06, -1.9826e-05,  4.4042e-06],
          [-3.0417e-07,  7.3309e-06,  3.3464e-06]],

         [[-1.5081e-05, -1.9697e-05,  1.3891e-05],
          [ 1.3867e-05, -1.7888e-05, -2.8419e-06],
          [-2.1611e-07, -3.7655e-06,  2.7451e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4742]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0373]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (87.00%) (1237/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (2349/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (87.00%) (3481/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (4601/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (87.00%) (5731/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (87.00%) (6866/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (8002/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (9151/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (10275/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (11402/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (12523/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (13652/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (14766/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (15902/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (17032/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (18197/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (19324/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (20450/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (21584/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (22716/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3386) |  Loss2: (0.0000) | Acc: (88.00%) (23857/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (24989/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (26137/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (27272/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (28425/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (29540/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (30678/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (31803/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (32943/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (34065/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (35207/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (36339/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (37475/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (38615/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (39736/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (40838/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (41969/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (43114/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (44206/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_070.pth.tar'
# TEST : Loss: (0.5174) | Acc: (82.00%) (8292/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1463,  0.0618,  0.0977],
          [ 0.1048, -0.2696, -0.0951],
          [ 0.2467, -0.0914,  0.0155]],

         [[-0.0028, -0.1045,  0.2535],
          [ 0.0214, -0.0893,  0.0376],
          [ 0.0606, -0.0038, -0.0733]],

         [[ 0.0152, -0.0617,  0.1759],
          [ 0.1089, -0.1813,  0.1511],
          [-0.0935,  0.0361, -0.1880]]],


        [[[ 0.0927, -0.0850, -0.0064],
          [ 0.1339,  0.1691,  0.1813],
          [ 0.0070, -0.0669,  0.1163]],

         [[-0.1637, -0.1879,  0.1603],
          [ 0.0288, -0.0532,  0.1095],
          [ 0.1217, -0.2041,  0.1384]],

         [[ 0.0979, -0.0494, -0.0067],
          [ 0.0126,  0.0272, -0.1077],
          [-0.0343, -0.1549, -0.0075]]],


        [[[-0.0706,  0.0918, -0.0412],
          [-0.1233,  0.0670,  0.1197],
          [ 0.1152,  0.1684,  0.0031]],

         [[ 0.0995,  0.1331,  0.1142],
          [ 0.0708, -0.0399, -0.0365],
          [-0.0463, -0.0311,  0.1411]],

         [[-0.0471,  0.1456,  0.0750],
          [-0.0420, -0.0455, -0.0598],
          [-0.0391, -0.0077,  0.0446]]],


        ...,


        [[[ 0.2282, -0.1109, -0.1100],
          [-0.0428, -0.1040, -0.0963],
          [-0.0906,  0.0271,  0.0637]],

         [[ 0.1721,  0.1974, -0.1112],
          [ 0.2108, -0.1371, -0.1159],
          [-0.0616, -0.0771, -0.0325]],

         [[ 0.1899,  0.1085, -0.0921],
          [-0.0725, -0.1544, -0.0712],
          [ 0.0629,  0.1792, -0.1296]]],


        [[[-0.1089,  0.0884, -0.1514],
          [-0.1402, -0.1419,  0.1223],
          [-0.1581,  0.1524, -0.1318]],

         [[ 0.0910, -0.1230,  0.0791],
          [ 0.1521,  0.0057, -0.0857],
          [-0.1546,  0.1124, -0.1994]],

         [[ 0.1169, -0.0283, -0.0083],
          [ 0.1360, -0.1803,  0.0077],
          [-0.0380,  0.0098, -0.1275]]],


        [[[ 0.0808,  0.1423,  0.1060],
          [-0.0908, -0.1672,  0.2061],
          [-0.1637, -0.0121, -0.0577]],

         [[-0.0230, -0.0915,  0.1482],
          [-0.0079, -0.1965,  0.0462],
          [-0.0005,  0.0781,  0.0355]],

         [[-0.1510, -0.1976,  0.1370],
          [ 0.1414, -0.1790, -0.0280],
          [ 0.0002, -0.0336,  0.2762]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0186,  0.0127,  0.0256],
          [ 0.0101,  0.0035,  0.0026],
          [ 0.0139,  0.0200,  0.0042]],

         [[ 0.0087,  0.0058,  0.0127],
          [ 0.0022, -0.0006, -0.0079],
          [ 0.0094,  0.0180, -0.0041]],

         [[ 0.0224,  0.0103,  0.0054],
          [ 0.0358,  0.0171, -0.0029],
          [ 0.0599,  0.0474,  0.0126]]],


        [[[ 0.0166,  0.0142, -0.0056],
          [ 0.0227,  0.0198, -0.0030],
          [ 0.0258,  0.0254,  0.0097]],

         [[-0.0055, -0.0078, -0.0214],
          [ 0.0044,  0.0017, -0.0144],
          [ 0.0099,  0.0100,  0.0006]],

         [[ 0.0044,  0.0014, -0.0118],
          [ 0.0128,  0.0113, -0.0028],
          [ 0.0179,  0.0190,  0.0125]]],


        [[[ 0.0020, -0.0005, -0.0026],
          [ 0.0021, -0.0014, -0.0051],
          [ 0.0036,  0.0002, -0.0013]],

         [[ 0.0069,  0.0029,  0.0010],
          [ 0.0082,  0.0024, -0.0006],
          [ 0.0092,  0.0038,  0.0028]],

         [[ 0.0022, -0.0008, -0.0027],
          [ 0.0064,  0.0018, -0.0011],
          [ 0.0101,  0.0058,  0.0045]]],


        ...,


        [[[ 0.0544,  0.0506,  0.0457],
          [ 0.0530,  0.0445,  0.0401],
          [ 0.0265,  0.0192,  0.0190]],

         [[ 0.0640,  0.0613,  0.0567],
          [ 0.0604,  0.0513,  0.0472],
          [ 0.0347,  0.0284,  0.0295]],

         [[ 0.0504,  0.0552,  0.0571],
          [ 0.0450,  0.0466,  0.0499],
          [ 0.0202,  0.0204,  0.0268]]],


        [[[ 0.0052,  0.0077,  0.0023],
          [ 0.0086,  0.0078,  0.0040],
          [ 0.0026, -0.0020, -0.0008]],

         [[ 0.0025,  0.0035,  0.0005],
          [ 0.0047,  0.0035,  0.0012],
          [-0.0054, -0.0089, -0.0067]],

         [[-0.0004,  0.0024,  0.0018],
          [ 0.0008, -0.0001, -0.0004],
          [-0.0064, -0.0102, -0.0074]]],


        [[[ 0.0682,  0.0555,  0.0651],
          [ 0.0814,  0.0739,  0.0837],
          [ 0.0377,  0.0319,  0.0600]],

         [[ 0.0038, -0.0142, -0.0032],
          [ 0.0158, -0.0011,  0.0090],
          [-0.0455, -0.0604, -0.0322]],

         [[ 0.0475,  0.0314,  0.0390],
          [ 0.0621,  0.0478,  0.0572],
          [ 0.0162, -0.0020,  0.0217]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4744]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 71 | Batch_idx: 0 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (1257/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (88.00%) (2387/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (3523/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (4682/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (5810/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (88.00%) (6948/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (89.00%) (8089/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (9242/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (88.00%) (10366/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (11507/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (89.00%) (12655/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (89.00%) (13793/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (14947/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (16067/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (17195/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (88.00%) (18332/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (19475/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (20599/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (21740/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (22883/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (24036/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (25165/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (89.00%) (26326/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (27478/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (28591/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (29741/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (30896/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (32047/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (33192/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (34326/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (35449/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (89.00%) (36583/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (37690/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (38812/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (88.00%) (39948/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (41085/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (42242/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (88.00%) (43377/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (44479/50000)
# TEST : Loss: (0.5176) | Acc: (82.00%) (8288/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1493,  0.0592,  0.0923],
          [ 0.1024, -0.2730, -0.1009],
          [ 0.2407, -0.0945,  0.0105]],

         [[-0.0030, -0.1018,  0.2536],
          [ 0.0208, -0.0889,  0.0363],
          [ 0.0555, -0.0051, -0.0758]],

         [[ 0.0130, -0.0612,  0.1742],
          [ 0.1050, -0.1844,  0.1475],
          [-0.1037,  0.0287, -0.1954]]],


        [[[ 0.0943, -0.0828, -0.0040],
          [ 0.1342,  0.1698,  0.1818],
          [ 0.0070, -0.0663,  0.1162]],

         [[-0.1633, -0.1870,  0.1615],
          [ 0.0285, -0.0526,  0.1099],
          [ 0.1213, -0.2038,  0.1384]],

         [[ 0.0977, -0.0494, -0.0066],
          [ 0.0123,  0.0271, -0.1079],
          [-0.0341, -0.1547, -0.0077]]],


        [[[-0.0703,  0.0920, -0.0410],
          [-0.1232,  0.0671,  0.1198],
          [ 0.1149,  0.1682,  0.0031]],

         [[ 0.0998,  0.1331,  0.1141],
          [ 0.0707, -0.0398, -0.0365],
          [-0.0466, -0.0311,  0.1409]],

         [[-0.0468,  0.1456,  0.0748],
          [-0.0420, -0.0456, -0.0600],
          [-0.0394, -0.0080,  0.0443]]],


        ...,


        [[[ 0.2254, -0.1142, -0.1133],
          [-0.0448, -0.1061, -0.0972],
          [-0.0915,  0.0270,  0.0644]],

         [[ 0.1711,  0.1958, -0.1130],
          [ 0.2101, -0.1380, -0.1160],
          [-0.0620, -0.0768, -0.0315]],

         [[ 0.1883,  0.1058, -0.0948],
          [-0.0730, -0.1555, -0.0718],
          [ 0.0629,  0.1798, -0.1285]]],


        [[[-0.1073,  0.0895, -0.1508],
          [-0.1396, -0.1416,  0.1218],
          [-0.1569,  0.1534, -0.1319]],

         [[ 0.0923, -0.1222,  0.0792],
          [ 0.1524,  0.0055, -0.0865],
          [-0.1538,  0.1130, -0.1996]],

         [[ 0.1184, -0.0274, -0.0079],
          [ 0.1369, -0.1799,  0.0075],
          [-0.0368,  0.0107, -0.1272]]],


        [[[ 0.0784,  0.1412,  0.1047],
          [-0.0928, -0.1717,  0.2031],
          [-0.1664, -0.0152, -0.0596]],

         [[-0.0250, -0.0910,  0.1494],
          [-0.0103, -0.2000,  0.0454],
          [-0.0043,  0.0752,  0.0348]],

         [[-0.1492, -0.1931,  0.1412],
          [ 0.1433, -0.1776, -0.0250],
          [ 0.0018, -0.0314,  0.2797]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1035, -0.0958, -0.1037],
          [-0.0755, -0.0725, -0.0750],
          [-0.0750, -0.0698, -0.0677]],

         [[-0.1032, -0.1005, -0.1069],
          [-0.0811, -0.0857, -0.0874],
          [-0.0848, -0.0935, -0.0906]],

         [[-0.0821, -0.0804, -0.0857],
          [-0.0631, -0.0673, -0.0673],
          [-0.0730, -0.0737, -0.0679]]],


        [[[ 0.0087,  0.0144,  0.0103],
          [ 0.0074,  0.0085,  0.0013],
          [ 0.0110,  0.0129,  0.0012]],

         [[-0.0094, -0.0106, -0.0141],
          [-0.0038, -0.0113, -0.0203],
          [ 0.0033, -0.0007, -0.0150]],

         [[-0.0101, -0.0085, -0.0104],
          [-0.0044, -0.0077, -0.0154],
          [ 0.0030,  0.0026, -0.0102]]],


        [[[-0.0017, -0.0004,  0.0016],
          [ 0.0001,  0.0007,  0.0029],
          [-0.0009, -0.0003,  0.0012]],

         [[-0.0034, -0.0022,  0.0008],
          [-0.0031, -0.0022,  0.0007],
          [-0.0046, -0.0036, -0.0011]],

         [[-0.0073, -0.0059, -0.0033],
          [-0.0080, -0.0065, -0.0038],
          [-0.0098, -0.0083, -0.0058]]],


        ...,


        [[[ 0.0415,  0.0364,  0.0337],
          [ 0.0476,  0.0408,  0.0356],
          [ 0.0473,  0.0408,  0.0301]],

         [[ 0.0573,  0.0491,  0.0439],
          [ 0.0624,  0.0527,  0.0424],
          [ 0.0578,  0.0484,  0.0321]],

         [[ 0.0481,  0.0420,  0.0401],
          [ 0.0528,  0.0463,  0.0398],
          [ 0.0513,  0.0443,  0.0324]]],


        [[[-0.0059,  0.0002,  0.0031],
          [ 0.0010,  0.0061,  0.0034],
          [ 0.0029,  0.0041,  0.0034]],

         [[-0.0075, -0.0055, -0.0022],
          [-0.0012,  0.0007, -0.0028],
          [-0.0001, -0.0007, -0.0029]],

         [[-0.0151, -0.0140, -0.0110],
          [-0.0089, -0.0075, -0.0103],
          [-0.0088, -0.0092, -0.0109]]],


        [[[-0.0553, -0.0519, -0.0732],
          [-0.0650, -0.0576, -0.0592],
          [-0.0527, -0.0406, -0.0411]],

         [[-0.0570, -0.0667, -0.0909],
          [-0.0684, -0.0669, -0.0701],
          [-0.0587, -0.0507, -0.0562]],

         [[-0.0531, -0.0577, -0.0713],
          [-0.0573, -0.0558, -0.0518],
          [-0.0374, -0.0325, -0.0317]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4742]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 72 | Batch_idx: 0 |  Loss: (0.3518) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (90.00%) (1269/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (2417/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (3568/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (4701/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (5836/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (6978/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (8123/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (9273/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (10410/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (11547/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (12685/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (13829/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (14961/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (16103/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (17249/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (18400/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (19541/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (20680/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (21836/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (22971/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (24113/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (25251/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (26407/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (27550/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (28702/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (29834/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (30975/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (32119/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (33266/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (34416/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (35553/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (36695/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (37835/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (38959/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (40103/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (41244/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (42373/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (43529/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (44617/50000)
# TEST : Loss: (0.4656) | Acc: (84.00%) (8467/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1465,  0.0600,  0.0902],
          [ 0.1041, -0.2729, -0.1019],
          [ 0.2452, -0.0892,  0.0108]],

         [[-0.0000, -0.0978,  0.2552],
          [ 0.0216, -0.0871,  0.0374],
          [ 0.0572, -0.0008, -0.0759]],

         [[ 0.0155, -0.0578,  0.1764],
          [ 0.1042, -0.1836,  0.1492],
          [-0.1053,  0.0301, -0.1954]]],


        [[[ 0.0955, -0.0822, -0.0046],
          [ 0.1360,  0.1708,  0.1813],
          [ 0.0088, -0.0658,  0.1164]],

         [[-0.1623, -0.1870,  0.1599],
          [ 0.0294, -0.0529,  0.1079],
          [ 0.1220, -0.2046,  0.1369]],

         [[ 0.1001, -0.0480, -0.0067],
          [ 0.0147,  0.0280, -0.1087],
          [-0.0316, -0.1541, -0.0078]]],


        [[[-0.0706,  0.0915, -0.0414],
          [-0.1236,  0.0667,  0.1195],
          [ 0.1147,  0.1683,  0.0032]],

         [[ 0.1000,  0.1330,  0.1140],
          [ 0.0709, -0.0398, -0.0364],
          [-0.0461, -0.0307,  0.1413]],

         [[-0.0465,  0.1454,  0.0747],
          [-0.0418, -0.0457, -0.0600],
          [-0.0389, -0.0077,  0.0445]]],


        ...,


        [[[ 0.2235, -0.1172, -0.1149],
          [-0.0468, -0.1089, -0.0978],
          [-0.0938,  0.0249,  0.0646]],

         [[ 0.1693,  0.1932, -0.1141],
          [ 0.2087, -0.1405, -0.1169],
          [-0.0631, -0.0783, -0.0314]],

         [[ 0.1875,  0.1042, -0.0954],
          [-0.0730, -0.1569, -0.0725],
          [ 0.0628,  0.1791, -0.1282]]],


        [[[-0.1083,  0.0888, -0.1507],
          [-0.1392, -0.1412,  0.1225],
          [-0.1571,  0.1531, -0.1325]],

         [[ 0.0908, -0.1232,  0.0791],
          [ 0.1523,  0.0055, -0.0859],
          [-0.1539,  0.1126, -0.2000]],

         [[ 0.1178, -0.0278, -0.0078],
          [ 0.1379, -0.1791,  0.0086],
          [-0.0358,  0.0114, -0.1267]]],


        [[[ 0.0777,  0.1421,  0.1067],
          [-0.0918, -0.1708,  0.2055],
          [-0.1669, -0.0151, -0.0600]],

         [[-0.0260, -0.0909,  0.1504],
          [-0.0107, -0.2004,  0.0466],
          [-0.0066,  0.0736,  0.0333]],

         [[-0.1531, -0.1972,  0.1377],
          [ 0.1414, -0.1805, -0.0267],
          [-0.0011, -0.0344,  0.2771]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0015,  0.0094, -0.0083],
          [ 0.0247,  0.0363,  0.0162],
          [ 0.0348,  0.0352,  0.0187]],

         [[ 0.0326,  0.0374,  0.0224],
          [ 0.0415,  0.0373,  0.0227],
          [ 0.0400,  0.0200,  0.0087]],

         [[ 0.0282,  0.0520,  0.0710],
          [ 0.0346,  0.0524,  0.0734],
          [ 0.0335,  0.0356,  0.0577]]],


        [[[ 0.0078,  0.0021, -0.0031],
          [ 0.0030, -0.0069, -0.0135],
          [ 0.0086,  0.0021, -0.0061]],

         [[-0.0065, -0.0111, -0.0175],
          [-0.0079, -0.0157, -0.0236],
          [-0.0006, -0.0051, -0.0147]],

         [[-0.0233, -0.0271, -0.0314],
          [-0.0236, -0.0308, -0.0357],
          [-0.0178, -0.0223, -0.0277]]],


        [[[-0.0032, -0.0016,  0.0013],
          [-0.0047, -0.0020,  0.0012],
          [-0.0069, -0.0032, -0.0009]],

         [[-0.0025, -0.0005,  0.0026],
          [-0.0045, -0.0015,  0.0015],
          [-0.0078, -0.0035, -0.0010]],

         [[-0.0025, -0.0005,  0.0022],
          [-0.0049, -0.0026,  0.0003],
          [-0.0077, -0.0048, -0.0022]]],


        ...,


        [[[-0.0998, -0.0885, -0.0774],
          [-0.0923, -0.0843, -0.0769],
          [-0.0885, -0.0844, -0.0744]],

         [[-0.0879, -0.0677, -0.0529],
          [-0.0782, -0.0603, -0.0475],
          [-0.0769, -0.0645, -0.0515]],

         [[-0.0766, -0.0604, -0.0481],
          [-0.0721, -0.0605, -0.0502],
          [-0.0744, -0.0679, -0.0583]]],


        [[[-0.0205, -0.0100, -0.0051],
          [-0.0065, -0.0014,  0.0023],
          [ 0.0011, -0.0018,  0.0043]],

         [[-0.0332, -0.0221, -0.0161],
          [-0.0221, -0.0155, -0.0094],
          [-0.0150, -0.0171, -0.0100]],

         [[-0.0325, -0.0257, -0.0233],
          [-0.0225, -0.0174, -0.0136],
          [-0.0205, -0.0217, -0.0152]]],


        [[[-0.1323, -0.1507, -0.1282],
          [-0.1288, -0.1386, -0.1133],
          [-0.1074, -0.1217, -0.1060]],

         [[-0.0473, -0.0684, -0.0573],
          [-0.0477, -0.0627, -0.0513],
          [-0.0314, -0.0540, -0.0499]],

         [[-0.0209, -0.0367, -0.0244],
          [-0.0333, -0.0481, -0.0382],
          [-0.0176, -0.0405, -0.0390]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4740]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 73 | Batch_idx: 0 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (87.00%) (1236/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (2397/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2926) |  Loss2: (0.0000) | Acc: (89.00%) (3559/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (4722/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (5869/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (7009/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2947) |  Loss2: (0.0000) | Acc: (89.00%) (8165/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (9311/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (10451/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (11588/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (12740/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (13873/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (15007/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (16156/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (17312/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (18441/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (19599/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (20744/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (21901/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (23054/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (24195/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (25351/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (26490/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3037) |  Loss2: (0.0000) | Acc: (89.00%) (27623/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (28783/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (29931/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (31055/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (32188/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (33317/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3049) |  Loss2: (0.0000) | Acc: (89.00%) (34449/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (35575/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (36703/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (37841/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (38982/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (40130/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (41273/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (42434/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3065) |  Loss2: (0.0000) | Acc: (89.00%) (43572/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (44675/50000)
# TEST : Loss: (0.4699) | Acc: (84.00%) (8470/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1470,  0.0603,  0.0911],
          [ 0.1034, -0.2759, -0.1003],
          [ 0.2438, -0.0907,  0.0131]],

         [[-0.0012, -0.0964,  0.2566],
          [ 0.0215, -0.0881,  0.0405],
          [ 0.0569, -0.0010, -0.0726]],

         [[ 0.0172, -0.0550,  0.1778],
          [ 0.1050, -0.1841,  0.1509],
          [-0.1046,  0.0304, -0.1929]]],


        [[[ 0.0959, -0.0824, -0.0054],
          [ 0.1358,  0.1706,  0.1804],
          [ 0.0086, -0.0661,  0.1155]],

         [[-0.1622, -0.1878,  0.1584],
          [ 0.0286, -0.0538,  0.1060],
          [ 0.1212, -0.2057,  0.1351]],

         [[ 0.0991, -0.0500, -0.0088],
          [ 0.0128,  0.0256, -0.1114],
          [-0.0329, -0.1562, -0.0104]]],


        [[[-0.0708,  0.0913, -0.0419],
          [-0.1237,  0.0667,  0.1191],
          [ 0.1146,  0.1684,  0.0031]],

         [[ 0.0999,  0.1330,  0.1138],
          [ 0.0711, -0.0393, -0.0362],
          [-0.0457, -0.0301,  0.1416]],

         [[-0.0465,  0.1455,  0.0747],
          [-0.0415, -0.0452, -0.0598],
          [-0.0386, -0.0073,  0.0447]]],


        ...,


        [[[ 0.2255, -0.1151, -0.1140],
          [-0.0453, -0.1076, -0.0973],
          [-0.0936,  0.0252,  0.0639]],

         [[ 0.1704,  0.1952, -0.1127],
          [ 0.2097, -0.1391, -0.1161],
          [-0.0630, -0.0780, -0.0314]],

         [[ 0.1869,  0.1044, -0.0959],
          [-0.0733, -0.1571, -0.0735],
          [ 0.0619,  0.1779, -0.1300]]],


        [[[-0.1070,  0.0900, -0.1494],
          [-0.1396, -0.1417,  0.1220],
          [-0.1557,  0.1540, -0.1322]],

         [[ 0.0908, -0.1231,  0.0792],
          [ 0.1506,  0.0038, -0.0875],
          [-0.1538,  0.1123, -0.2008]],

         [[ 0.1173, -0.0284, -0.0084],
          [ 0.1360, -0.1810,  0.0065],
          [-0.0361,  0.0106, -0.1280]]],


        [[[ 0.0785,  0.1429,  0.1053],
          [-0.0889, -0.1719,  0.2056],
          [-0.1629, -0.0142, -0.0584]],

         [[-0.0262, -0.0922,  0.1473],
          [-0.0100, -0.2029,  0.0458],
          [-0.0047,  0.0737,  0.0345]],

         [[-0.1567, -0.2010,  0.1331],
          [ 0.1395, -0.1844, -0.0270],
          [-0.0012, -0.0354,  0.2791]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0889, -0.1100, -0.1237],
          [-0.0788, -0.1045, -0.1179],
          [-0.0776, -0.1223, -0.1232]],

         [[-0.0816, -0.1079, -0.1295],
          [-0.0685, -0.0914, -0.1119],
          [-0.0692, -0.1070, -0.1123]],

         [[-0.0607, -0.0884, -0.1148],
          [-0.0453, -0.0626, -0.0883],
          [-0.0488, -0.0762, -0.0905]]],


        [[[ 0.0067,  0.0056,  0.0030],
          [ 0.0134,  0.0097,  0.0057],
          [ 0.0094,  0.0080,  0.0036]],

         [[ 0.0097,  0.0102,  0.0072],
          [ 0.0193,  0.0184,  0.0145],
          [ 0.0157,  0.0184,  0.0160]],

         [[ 0.0011,  0.0041,  0.0011],
          [ 0.0123,  0.0151,  0.0112],
          [ 0.0096,  0.0152,  0.0137]]],


        [[[-0.0010,  0.0002,  0.0020],
          [-0.0002,  0.0011,  0.0028],
          [ 0.0007,  0.0018,  0.0024]],

         [[-0.0031, -0.0023, -0.0003],
          [-0.0019, -0.0010,  0.0005],
          [-0.0011, -0.0003,  0.0005]],

         [[-0.0057, -0.0051, -0.0026],
          [-0.0040, -0.0028, -0.0005],
          [-0.0040, -0.0022, -0.0001]]],


        ...,


        [[[ 0.0215,  0.0243,  0.0253],
          [ 0.0085,  0.0114,  0.0122],
          [ 0.0135,  0.0207,  0.0301]],

         [[ 0.0200,  0.0246,  0.0226],
          [ 0.0093,  0.0129,  0.0122],
          [ 0.0164,  0.0215,  0.0257]],

         [[ 0.0211,  0.0244,  0.0220],
          [ 0.0120,  0.0143,  0.0158],
          [ 0.0183,  0.0206,  0.0251]]],


        [[[-0.0111, -0.0011, -0.0015],
          [-0.0123, -0.0049, -0.0025],
          [-0.0128, -0.0107, -0.0037]],

         [[ 0.0052,  0.0131,  0.0132],
          [ 0.0032,  0.0089,  0.0104],
          [ 0.0026,  0.0034,  0.0078]],

         [[ 0.0092,  0.0157,  0.0174],
          [ 0.0062,  0.0116,  0.0132],
          [ 0.0063,  0.0081,  0.0104]]],


        [[[-0.0322, -0.0101, -0.0242],
          [-0.0525, -0.0189, -0.0280],
          [-0.0954, -0.0567, -0.0449]],

         [[-0.0603, -0.0540, -0.0784],
          [-0.0709, -0.0540, -0.0725],
          [-0.1136, -0.0915, -0.0953]],

         [[-0.0712, -0.0632, -0.0892],
          [-0.0688, -0.0522, -0.0723],
          [-0.0972, -0.0808, -0.0908]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4738]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

Epoch: 74 | Batch_idx: 0 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (1262/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (2413/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (3564/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (4703/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (89.00%) (5872/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (89.00%) (7020/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (8187/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (9349/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (10496/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (11650/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (12812/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (13963/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (15112/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (90.00%) (16249/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (90.00%) (17409/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (90.00%) (18560/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (90.00%) (19713/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (20849/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (21972/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (23130/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (24281/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (25423/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (26581/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (27725/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (28860/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (30011/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (31157/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (32310/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (33461/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (34611/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (89.00%) (35755/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (36898/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (38051/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (39183/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (40321/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (41465/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (42634/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (43792/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (44910/50000)
# TEST : Loss: (0.4197) | Acc: (86.00%) (8655/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1482,  0.0593,  0.0918],
          [ 0.1027, -0.2781, -0.1012],
          [ 0.2424, -0.0915,  0.0132]],

         [[-0.0027, -0.0969,  0.2573],
          [ 0.0210, -0.0895,  0.0398],
          [ 0.0557, -0.0010, -0.0720]],

         [[ 0.0168, -0.0539,  0.1814],
          [ 0.1047, -0.1843,  0.1527],
          [-0.1042,  0.0316, -0.1893]]],


        [[[ 0.0955, -0.0819, -0.0050],
          [ 0.1360,  0.1714,  0.1811],
          [ 0.0094, -0.0651,  0.1164]],

         [[-0.1616, -0.1870,  0.1582],
          [ 0.0293, -0.0531,  0.1057],
          [ 0.1223, -0.2049,  0.1351]],

         [[ 0.0996, -0.0496, -0.0092],
          [ 0.0132,  0.0256, -0.1124],
          [-0.0317, -0.1558, -0.0111]]],


        [[[-0.0708,  0.0915, -0.0415],
          [-0.1243,  0.0663,  0.1189],
          [ 0.1137,  0.1678,  0.0027]],

         [[ 0.0999,  0.1333,  0.1140],
          [ 0.0705, -0.0398, -0.0364],
          [-0.0465, -0.0306,  0.1411]],

         [[-0.0465,  0.1455,  0.0745],
          [-0.0420, -0.0458, -0.0603],
          [-0.0393, -0.0078,  0.0441]]],


        ...,


        [[[ 0.2277, -0.1136, -0.1127],
          [-0.0440, -0.1078, -0.0965],
          [-0.0920,  0.0252,  0.0637]],

         [[ 0.1693,  0.1942, -0.1134],
          [ 0.2085, -0.1408, -0.1172],
          [-0.0630, -0.0792, -0.0332]],

         [[ 0.1862,  0.1037, -0.0965],
          [-0.0735, -0.1582, -0.0747],
          [ 0.0624,  0.1768, -0.1326]]],


        [[[-0.1071,  0.0899, -0.1488],
          [-0.1406, -0.1422,  0.1219],
          [-0.1572,  0.1532, -0.1324]],

         [[ 0.0905, -0.1228,  0.0800],
          [ 0.1497,  0.0035, -0.0870],
          [-0.1547,  0.1123, -0.2002]],

         [[ 0.1176, -0.0276, -0.0071],
          [ 0.1362, -0.1803,  0.0075],
          [-0.0358,  0.0114, -0.1270]]],


        [[[ 0.0733,  0.1432,  0.1055],
          [-0.0891, -0.1707,  0.2081],
          [-0.1644, -0.0134, -0.0572]],

         [[-0.0303, -0.0911,  0.1483],
          [-0.0103, -0.2022,  0.0478],
          [-0.0070,  0.0739,  0.0349]],

         [[-0.1595, -0.1988,  0.1356],
          [ 0.1402, -0.1820, -0.0225],
          [-0.0007, -0.0322,  0.2839]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0333,  0.0551,  0.0649],
          [ 0.0402,  0.0536,  0.0438],
          [-0.0015,  0.0165, -0.0096]],

         [[ 0.0468,  0.0622,  0.0678],
          [ 0.0595,  0.0675,  0.0559],
          [ 0.0284,  0.0416,  0.0150]],

         [[ 0.0650,  0.0778,  0.0793],
          [ 0.0818,  0.0880,  0.0734],
          [ 0.0587,  0.0690,  0.0377]]],


        [[[ 0.0126,  0.0045, -0.0137],
          [ 0.0204,  0.0156, -0.0032],
          [ 0.0214,  0.0185,  0.0014]],

         [[ 0.0151,  0.0073, -0.0059],
          [ 0.0207,  0.0169,  0.0030],
          [ 0.0197,  0.0194,  0.0065]],

         [[ 0.0082,  0.0021, -0.0106],
          [ 0.0104,  0.0075, -0.0065],
          [ 0.0083,  0.0076, -0.0063]]],


        [[[-0.0048, -0.0049, -0.0057],
          [-0.0014,  0.0007,  0.0001],
          [ 0.0024,  0.0045,  0.0045]],

         [[-0.0047, -0.0042, -0.0035],
          [-0.0017,  0.0012,  0.0021],
          [ 0.0019,  0.0051,  0.0066]],

         [[-0.0046, -0.0039, -0.0029],
          [-0.0024,  0.0003,  0.0015],
          [-0.0003,  0.0029,  0.0046]]],


        ...,


        [[[-0.0211, -0.0213, -0.0276],
          [-0.0233, -0.0199, -0.0237],
          [-0.0111, -0.0092, -0.0147]],

         [[-0.0355, -0.0353, -0.0385],
          [-0.0287, -0.0234, -0.0243],
          [-0.0135, -0.0087, -0.0120]],

         [[-0.0288, -0.0278, -0.0318],
          [-0.0195, -0.0162, -0.0183],
          [-0.0040, -0.0021, -0.0063]]],


        [[[ 0.0005,  0.0071,  0.0073],
          [-0.0073,  0.0014,  0.0014],
          [-0.0087, -0.0015, -0.0012]],

         [[ 0.0017,  0.0076,  0.0050],
          [-0.0059,  0.0027, -0.0001],
          [-0.0052,  0.0009, -0.0016]],

         [[ 0.0121,  0.0169,  0.0133],
          [ 0.0056,  0.0127,  0.0100],
          [ 0.0052,  0.0104,  0.0086]]],


        [[[-0.0160, -0.0322, -0.0425],
          [-0.0291, -0.0521, -0.0640],
          [-0.0224, -0.0561, -0.0614]],

         [[-0.0496, -0.0682, -0.0673],
          [-0.0630, -0.0846, -0.0846],
          [-0.0596, -0.0893, -0.0836]],

         [[-0.0919, -0.1143, -0.1139],
          [-0.1099, -0.1323, -0.1294],
          [-0.1088, -0.1383, -0.1291]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4736]], device='cuda:0')

percentage_weight_grad tensor([[0.0000]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.5916) |  Loss2: (0.2978) | Acc: (91.00%) (117/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.5955) |  Loss2: (0.2978) | Acc: (89.00%) (1267/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.6174) |  Loss2: (0.2978) | Acc: (89.00%) (2399/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.6242) |  Loss2: (0.2978) | Acc: (88.00%) (3522/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.6382) |  Loss2: (0.2977) | Acc: (88.00%) (4634/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.6491) |  Loss2: (0.2977) | Acc: (88.00%) (5747/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.6460) |  Loss2: (0.2976) | Acc: (87.00%) (6871/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.6449) |  Loss2: (0.2976) | Acc: (87.00%) (7990/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.6420) |  Loss2: (0.2975) | Acc: (88.00%) (9129/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.6454) |  Loss2: (0.2975) | Acc: (87.00%) (10237/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.6457) |  Loss2: (0.2975) | Acc: (87.00%) (11351/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.6475) |  Loss2: (0.2974) | Acc: (87.00%) (12474/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.6441) |  Loss2: (0.2974) | Acc: (87.00%) (13606/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.6429) |  Loss2: (0.2974) | Acc: (87.00%) (14724/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.6427) |  Loss2: (0.2973) | Acc: (87.00%) (15849/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.6419) |  Loss2: (0.2973) | Acc: (87.00%) (16974/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.6400) |  Loss2: (0.2973) | Acc: (87.00%) (18120/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.6386) |  Loss2: (0.2972) | Acc: (87.00%) (19238/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.6377) |  Loss2: (0.2972) | Acc: (87.00%) (20387/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.6363) |  Loss2: (0.2972) | Acc: (88.00%) (21531/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.6357) |  Loss2: (0.2971) | Acc: (88.00%) (22657/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.6368) |  Loss2: (0.2971) | Acc: (88.00%) (23790/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.6369) |  Loss2: (0.2970) | Acc: (88.00%) (24913/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.6360) |  Loss2: (0.2970) | Acc: (88.00%) (26039/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.6358) |  Loss2: (0.2970) | Acc: (88.00%) (27168/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.6345) |  Loss2: (0.2969) | Acc: (88.00%) (28317/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.6347) |  Loss2: (0.2969) | Acc: (88.00%) (29444/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.6342) |  Loss2: (0.2969) | Acc: (88.00%) (30585/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.6345) |  Loss2: (0.2968) | Acc: (88.00%) (31720/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.6332) |  Loss2: (0.2968) | Acc: (88.00%) (32867/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.6331) |  Loss2: (0.2967) | Acc: (88.00%) (33990/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.6333) |  Loss2: (0.2967) | Acc: (88.00%) (35098/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.6328) |  Loss2: (0.2967) | Acc: (88.00%) (36230/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.6323) |  Loss2: (0.2966) | Acc: (88.00%) (37361/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.6308) |  Loss2: (0.2966) | Acc: (88.00%) (38515/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.6304) |  Loss2: (0.2965) | Acc: (88.00%) (39648/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.6292) |  Loss2: (0.2965) | Acc: (88.00%) (40797/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.6280) |  Loss2: (0.2964) | Acc: (88.00%) (41947/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.6277) |  Loss2: (0.2964) | Acc: (88.00%) (43092/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.6269) |  Loss2: (0.2964) | Acc: (88.00%) (44192/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_075.pth.tar'
# TEST : Loss: (0.4499) | Acc: (85.00%) (8530/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1474,  0.0600,  0.0926],
          [ 0.1036, -0.2771, -0.0999],
          [ 0.2437, -0.0900,  0.0149]],

         [[-0.0023, -0.0963,  0.2579],
          [ 0.0215, -0.0890,  0.0407],
          [ 0.0565, -0.0001, -0.0708]],

         [[ 0.0168, -0.0539,  0.1816],
          [ 0.1045, -0.1844,  0.1529],
          [-0.1040,  0.0317, -0.1886]]],


        [[[ 0.0956, -0.0818, -0.0048],
          [ 0.1359,  0.1713,  0.1812],
          [ 0.0093, -0.0652,  0.1164]],

         [[-0.1614, -0.1868,  0.1584],
          [ 0.0293, -0.0531,  0.1058],
          [ 0.1222, -0.2049,  0.1351]],

         [[ 0.0997, -0.0495, -0.0090],
          [ 0.0132,  0.0256, -0.1122],
          [-0.0317, -0.1557, -0.0109]]],


        [[[-0.0708,  0.0915, -0.0415],
          [-0.1242,  0.0662,  0.1189],
          [ 0.1137,  0.1678,  0.0027]],

         [[ 0.0999,  0.1332,  0.1140],
          [ 0.0704, -0.0398, -0.0364],
          [-0.0466, -0.0306,  0.1410]],

         [[-0.0464,  0.1455,  0.0745],
          [-0.0420, -0.0458, -0.0603],
          [-0.0393, -0.0078,  0.0441]]],


        ...,


        [[[ 0.2277, -0.1136, -0.1126],
          [-0.0438, -0.1078, -0.0963],
          [-0.0921,  0.0251,  0.0636]],

         [[ 0.1693,  0.1940, -0.1133],
          [ 0.2084, -0.1409, -0.1171],
          [-0.0632, -0.0794, -0.0334]],

         [[ 0.1864,  0.1038, -0.0962],
          [-0.0732, -0.1580, -0.0744],
          [ 0.0624,  0.1767, -0.1325]]],


        [[[-0.1070,  0.0899, -0.1487],
          [-0.1405, -0.1421,  0.1219],
          [-0.1572,  0.1531, -0.1324]],

         [[ 0.0906, -0.1227,  0.0801],
          [ 0.1496,  0.0036, -0.0869],
          [-0.1546,  0.1122, -0.2001]],

         [[ 0.1176, -0.0275, -0.0070],
          [ 0.1361, -0.1803,  0.0075],
          [-0.0359,  0.0113, -0.1269]]],


        [[[ 0.0743,  0.1445,  0.1068],
          [-0.0877, -0.1690,  0.2096],
          [-0.1631, -0.0118, -0.0558]],

         [[-0.0291, -0.0896,  0.1497],
          [-0.0087, -0.2003,  0.0496],
          [-0.0054,  0.0757,  0.0365]],

         [[-0.1591, -0.1981,  0.1362],
          [ 0.1411, -0.1810, -0.0214],
          [ 0.0002, -0.0310,  0.2849]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4741e-05,  6.0003e-06,  9.2607e-06],
          [ 1.0356e-05, -2.7710e-05, -9.9901e-06],
          [ 2.4367e-05, -9.0023e-06,  1.4885e-06]],

         [[-2.3059e-07, -9.6343e-06,  2.5789e-05],
          [ 2.1523e-06, -8.8969e-06,  4.0668e-06],
          [ 5.6521e-06, -1.0783e-08, -7.0771e-06]],

         [[ 1.6765e-06, -5.3912e-06,  1.8162e-05],
          [ 1.0455e-05, -1.8439e-05,  1.5293e-05],
          [-1.0398e-05,  3.1686e-06, -1.8865e-05]]],


        [[[ 9.5594e-06, -8.1753e-06, -4.7656e-07],
          [ 1.3594e-05,  1.7126e-05,  1.8116e-05],
          [ 9.2944e-07, -6.5157e-06,  1.1642e-05]],

         [[-1.6143e-05, -1.8675e-05,  1.5839e-05],
          [ 2.9275e-06, -5.3091e-06,  1.0580e-05],
          [ 1.2216e-05, -2.0489e-05,  1.3507e-05]],

         [[ 9.9706e-06, -4.9456e-06, -8.9788e-07],
          [ 1.3224e-06,  2.5576e-06, -1.1216e-05],
          [-3.1664e-06, -1.5571e-05, -1.0890e-06]]],


        [[[-7.0795e-06,  9.1527e-06, -4.1471e-06],
          [-1.2425e-05,  6.6249e-06,  1.1886e-05],
          [ 1.1370e-05,  1.6776e-05,  2.6713e-07]],

         [[ 9.9880e-06,  1.3324e-05,  1.1402e-05],
          [ 7.0443e-06, -3.9789e-06, -3.6425e-06],
          [-4.6551e-06, -3.0643e-06,  1.4100e-05]],

         [[-4.6449e-06,  1.4545e-05,  7.4498e-06],
          [-4.1990e-06, -4.5834e-06, -6.0301e-06],
          [-3.9256e-06, -7.8358e-07,  4.4074e-06]]],


        ...,


        [[[ 2.2769e-05, -1.1363e-05, -1.1257e-05],
          [-4.3848e-06, -1.0779e-05, -9.6340e-06],
          [-9.2055e-06,  2.5072e-06,  6.3636e-06]],

         [[ 1.6929e-05,  1.9404e-05, -1.1328e-05],
          [ 2.0842e-05, -1.4086e-05, -1.1711e-05],
          [-6.3198e-06, -7.9389e-06, -3.3400e-06]],

         [[ 1.8638e-05,  1.0383e-05, -9.6158e-06],
          [-7.3193e-06, -1.5799e-05, -7.4449e-06],
          [ 6.2383e-06,  1.7669e-05, -1.3250e-05]]],


        [[[-1.0698e-05,  8.9928e-06, -1.4870e-05],
          [-1.4054e-05, -1.4214e-05,  1.2186e-05],
          [-1.5715e-05,  1.5308e-05, -1.3238e-05]],

         [[ 9.0563e-06, -1.2267e-05,  8.0096e-06],
          [ 1.4963e-05,  3.5541e-07, -8.6923e-06],
          [-1.5464e-05,  1.1216e-05, -2.0010e-05]],

         [[ 1.1757e-05, -2.7498e-06, -6.9899e-07],
          [ 1.3610e-05, -1.8026e-05,  7.5467e-07],
          [-3.5907e-06,  1.1277e-06, -1.2693e-05]]],


        [[[ 7.4308e-06,  1.4447e-05,  1.0675e-05],
          [-8.7728e-06, -1.6903e-05,  2.0961e-05],
          [-1.6306e-05, -1.1799e-06, -5.5828e-06]],

         [[-2.9090e-06, -8.9590e-06,  1.4966e-05],
          [-8.6681e-07, -2.0027e-05,  4.9607e-06],
          [-5.3857e-07,  7.5740e-06,  3.6493e-06]],

         [[-1.5908e-05, -1.9812e-05,  1.3620e-05],
          [ 1.4105e-05, -1.8099e-05, -2.1390e-06],
          [ 1.5433e-08, -3.1043e-06,  2.8488e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4848]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0667]], device='cuda:0')

Epoch: 76 | Batch_idx: 0 |  Loss: (0.5211) |  Loss2: (0.2947) | Acc: (93.00%) (120/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.5895) |  Loss2: (0.2947) | Acc: (89.00%) (1267/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.6034) |  Loss2: (0.2946) | Acc: (89.00%) (2412/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.6033) |  Loss2: (0.2946) | Acc: (89.00%) (3554/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.6145) |  Loss2: (0.2946) | Acc: (89.00%) (4678/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.6168) |  Loss2: (0.2945) | Acc: (89.00%) (5826/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.6130) |  Loss2: (0.2945) | Acc: (89.00%) (6970/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.6140) |  Loss2: (0.2944) | Acc: (89.00%) (8106/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.6141) |  Loss2: (0.2944) | Acc: (89.00%) (9249/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.6084) |  Loss2: (0.2944) | Acc: (89.00%) (10418/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.6067) |  Loss2: (0.2943) | Acc: (89.00%) (11565/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.6083) |  Loss2: (0.2943) | Acc: (89.00%) (12696/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.6078) |  Loss2: (0.2943) | Acc: (89.00%) (13843/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.6072) |  Loss2: (0.2942) | Acc: (89.00%) (14971/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.6055) |  Loss2: (0.2942) | Acc: (89.00%) (16124/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.6053) |  Loss2: (0.2941) | Acc: (89.00%) (17266/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.6061) |  Loss2: (0.2941) | Acc: (89.00%) (18408/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.6049) |  Loss2: (0.2940) | Acc: (89.00%) (19565/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.6050) |  Loss2: (0.2940) | Acc: (89.00%) (20705/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.6057) |  Loss2: (0.2939) | Acc: (89.00%) (21844/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.6037) |  Loss2: (0.2939) | Acc: (89.00%) (23011/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.6065) |  Loss2: (0.2939) | Acc: (89.00%) (24126/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.6046) |  Loss2: (0.2938) | Acc: (89.00%) (25293/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.6048) |  Loss2: (0.2938) | Acc: (89.00%) (26433/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.6037) |  Loss2: (0.2937) | Acc: (89.00%) (27594/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.6049) |  Loss2: (0.2937) | Acc: (89.00%) (28719/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.6038) |  Loss2: (0.2936) | Acc: (89.00%) (29869/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.6045) |  Loss2: (0.2936) | Acc: (89.00%) (31009/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.6050) |  Loss2: (0.2935) | Acc: (89.00%) (32140/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.6036) |  Loss2: (0.2935) | Acc: (89.00%) (33291/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.6025) |  Loss2: (0.2934) | Acc: (89.00%) (34450/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.6027) |  Loss2: (0.2934) | Acc: (89.00%) (35582/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.6028) |  Loss2: (0.2934) | Acc: (89.00%) (36718/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.6037) |  Loss2: (0.2933) | Acc: (89.00%) (37843/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.6026) |  Loss2: (0.2933) | Acc: (89.00%) (39006/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.6031) |  Loss2: (0.2932) | Acc: (89.00%) (40143/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.6024) |  Loss2: (0.2932) | Acc: (89.00%) (41293/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.6018) |  Loss2: (0.2931) | Acc: (89.00%) (42443/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.6013) |  Loss2: (0.2931) | Acc: (89.00%) (43580/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.6006) |  Loss2: (0.2930) | Acc: (89.00%) (44691/50000)
# TEST : Loss: (0.4302) | Acc: (86.00%) (8601/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1473,  0.0600,  0.0926],
          [ 0.1035, -0.2770, -0.0999],
          [ 0.2436, -0.0900,  0.0149]],

         [[-0.0023, -0.0963,  0.2578],
          [ 0.0215, -0.0889,  0.0407],
          [ 0.0565, -0.0001, -0.0707]],

         [[ 0.0168, -0.0539,  0.1816],
          [ 0.1045, -0.1843,  0.1529],
          [-0.1039,  0.0317, -0.1886]]],


        [[[ 0.0956, -0.0817, -0.0048],
          [ 0.1359,  0.1712,  0.1811],
          [ 0.0093, -0.0651,  0.1164]],

         [[-0.1614, -0.1867,  0.1583],
          [ 0.0293, -0.0531,  0.1058],
          [ 0.1221, -0.2048,  0.1350]],

         [[ 0.0997, -0.0494, -0.0090],
          [ 0.0132,  0.0256, -0.1121],
          [-0.0317, -0.1557, -0.0109]]],


        [[[-0.0708,  0.0915, -0.0415],
          [-0.1242,  0.0662,  0.1188],
          [ 0.1137,  0.1677,  0.0027]],

         [[ 0.0998,  0.1332,  0.1140],
          [ 0.0704, -0.0398, -0.0364],
          [-0.0465, -0.0306,  0.1409]],

         [[-0.0464,  0.1454,  0.0745],
          [-0.0420, -0.0458, -0.0603],
          [-0.0392, -0.0078,  0.0441]]],


        ...,


        [[[ 0.2276, -0.1136, -0.1125],
          [-0.0438, -0.1078, -0.0963],
          [-0.0920,  0.0251,  0.0636]],

         [[ 0.1692,  0.1940, -0.1132],
          [ 0.2083, -0.1408, -0.1171],
          [-0.0632, -0.0794, -0.0334]],

         [[ 0.1863,  0.1038, -0.0961],
          [-0.0732, -0.1579, -0.0744],
          [ 0.0624,  0.1766, -0.1324]]],


        [[[-0.1069,  0.0899, -0.1486],
          [-0.1405, -0.1421,  0.1218],
          [-0.1571,  0.1530, -0.1323]],

         [[ 0.0905, -0.1226,  0.0801],
          [ 0.1496,  0.0036, -0.0869],
          [-0.1546,  0.1121, -0.2000]],

         [[ 0.1175, -0.0275, -0.0070],
          [ 0.1360, -0.1802,  0.0075],
          [-0.0359,  0.0113, -0.1269]]],


        [[[ 0.0743,  0.1444,  0.1067],
          [-0.0877, -0.1690,  0.2095],
          [-0.1630, -0.0118, -0.0558]],

         [[-0.0291, -0.0896,  0.1496],
          [-0.0087, -0.2002,  0.0496],
          [-0.0054,  0.0757,  0.0365]],

         [[-0.1590, -0.1980,  0.1362],
          [ 0.1410, -0.1809, -0.0214],
          [ 0.0002, -0.0310,  0.2848]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4735e-05,  5.9980e-06,  9.2572e-06],
          [ 1.0352e-05, -2.7699e-05, -9.9863e-06],
          [ 2.4358e-05, -8.9988e-06,  1.4879e-06]],

         [[-2.3050e-07, -9.6305e-06,  2.5778e-05],
          [ 2.1514e-06, -8.8935e-06,  4.0652e-06],
          [ 5.6499e-06, -1.0778e-08, -7.0744e-06]],

         [[ 1.6758e-06, -5.3892e-06,  1.8155e-05],
          [ 1.0450e-05, -1.8432e-05,  1.5287e-05],
          [-1.0394e-05,  3.1673e-06, -1.8857e-05]]],


        [[[ 9.5556e-06, -8.1721e-06, -4.7638e-07],
          [ 1.3589e-05,  1.7120e-05,  1.8109e-05],
          [ 9.2907e-07, -6.5131e-06,  1.1637e-05]],

         [[-1.6137e-05, -1.8668e-05,  1.5832e-05],
          [ 2.9263e-06, -5.3071e-06,  1.0576e-05],
          [ 1.2211e-05, -2.0481e-05,  1.3502e-05]],

         [[ 9.9668e-06, -4.9437e-06, -8.9751e-07],
          [ 1.3218e-06,  2.5566e-06, -1.1211e-05],
          [-3.1652e-06, -1.5566e-05, -1.0886e-06]]],


        [[[-7.0767e-06,  9.1492e-06, -4.1455e-06],
          [-1.2420e-05,  6.6222e-06,  1.1881e-05],
          [ 1.1366e-05,  1.6770e-05,  2.6703e-07]],

         [[ 9.9842e-06,  1.3319e-05,  1.1397e-05],
          [ 7.0416e-06, -3.9772e-06, -3.6410e-06],
          [-4.6534e-06, -3.0631e-06,  1.4095e-05]],

         [[-4.6431e-06,  1.4540e-05,  7.4468e-06],
          [-4.1974e-06, -4.5816e-06, -6.0277e-06],
          [-3.9240e-06, -7.8329e-07,  4.4057e-06]]],


        ...,


        [[[ 2.2761e-05, -1.1358e-05, -1.1253e-05],
          [-4.3831e-06, -1.0775e-05, -9.6302e-06],
          [-9.2020e-06,  2.5063e-06,  6.3610e-06]],

         [[ 1.6923e-05,  1.9397e-05, -1.1324e-05],
          [ 2.0833e-05, -1.4081e-05, -1.1706e-05],
          [-6.3175e-06, -7.9357e-06, -3.3387e-06]],

         [[ 1.8630e-05,  1.0379e-05, -9.6120e-06],
          [-7.3163e-06, -1.5793e-05, -7.4420e-06],
          [ 6.2358e-06,  1.7662e-05, -1.3244e-05]]],


        [[[-1.0694e-05,  8.9893e-06, -1.4865e-05],
          [-1.4048e-05, -1.4208e-05,  1.2181e-05],
          [-1.5709e-05,  1.5302e-05, -1.3233e-05]],

         [[ 9.0528e-06, -1.2262e-05,  8.0064e-06],
          [ 1.4957e-05,  3.5527e-07, -8.6888e-06],
          [-1.5458e-05,  1.1212e-05, -2.0003e-05]],

         [[ 1.1753e-05, -2.7487e-06, -6.9871e-07],
          [ 1.3604e-05, -1.8019e-05,  7.5438e-07],
          [-3.5892e-06,  1.1273e-06, -1.2688e-05]]],


        [[[ 7.4279e-06,  1.4441e-05,  1.0671e-05],
          [-8.7693e-06, -1.6897e-05,  2.0953e-05],
          [-1.6299e-05, -1.1795e-06, -5.5806e-06]],

         [[-2.9079e-06, -8.9555e-06,  1.4960e-05],
          [-8.6648e-07, -2.0020e-05,  4.9588e-06],
          [-5.3835e-07,  7.5711e-06,  3.6479e-06]],

         [[-1.5901e-05, -1.9804e-05,  1.3615e-05],
          [ 1.4100e-05, -1.8092e-05, -2.1382e-06],
          [ 1.5426e-08, -3.1030e-06,  2.8476e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.4960]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.1010]], device='cuda:0')

Epoch: 77 | Batch_idx: 0 |  Loss: (0.5420) |  Loss2: (0.2913) | Acc: (90.00%) (116/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.6005) |  Loss2: (0.2912) | Acc: (88.00%) (1252/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.5867) |  Loss2: (0.2912) | Acc: (89.00%) (2411/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.5783) |  Loss2: (0.2911) | Acc: (90.00%) (3575/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.5875) |  Loss2: (0.2911) | Acc: (89.00%) (4708/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.5850) |  Loss2: (0.2910) | Acc: (89.00%) (5857/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.5863) |  Loss2: (0.2910) | Acc: (89.00%) (7005/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.5887) |  Loss2: (0.2909) | Acc: (89.00%) (8157/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.5895) |  Loss2: (0.2909) | Acc: (89.00%) (9285/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.5887) |  Loss2: (0.2909) | Acc: (89.00%) (10426/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.5907) |  Loss2: (0.2908) | Acc: (89.00%) (11565/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.5925) |  Loss2: (0.2908) | Acc: (89.00%) (12701/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.5915) |  Loss2: (0.2907) | Acc: (89.00%) (13861/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.5905) |  Loss2: (0.2907) | Acc: (89.00%) (15011/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.5885) |  Loss2: (0.2906) | Acc: (89.00%) (16174/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.5875) |  Loss2: (0.2906) | Acc: (89.00%) (17327/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.5880) |  Loss2: (0.2905) | Acc: (89.00%) (18468/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.5887) |  Loss2: (0.2905) | Acc: (89.00%) (19628/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.5890) |  Loss2: (0.2905) | Acc: (89.00%) (20785/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.5871) |  Loss2: (0.2904) | Acc: (89.00%) (21944/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.5874) |  Loss2: (0.2904) | Acc: (89.00%) (23090/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.5875) |  Loss2: (0.2903) | Acc: (89.00%) (24244/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.5886) |  Loss2: (0.2903) | Acc: (89.00%) (25388/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.5888) |  Loss2: (0.2903) | Acc: (89.00%) (26535/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.5880) |  Loss2: (0.2902) | Acc: (89.00%) (27688/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.5875) |  Loss2: (0.2902) | Acc: (89.00%) (28847/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.5874) |  Loss2: (0.2901) | Acc: (89.00%) (29990/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.5872) |  Loss2: (0.2901) | Acc: (89.00%) (31142/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.5864) |  Loss2: (0.2900) | Acc: (89.00%) (32304/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.5869) |  Loss2: (0.2900) | Acc: (89.00%) (33454/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.5870) |  Loss2: (0.2900) | Acc: (89.00%) (34594/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.5864) |  Loss2: (0.2899) | Acc: (89.00%) (35760/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.5858) |  Loss2: (0.2899) | Acc: (89.00%) (36913/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.5854) |  Loss2: (0.2898) | Acc: (89.00%) (38068/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.5850) |  Loss2: (0.2898) | Acc: (89.00%) (39221/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.5849) |  Loss2: (0.2898) | Acc: (89.00%) (40370/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.5851) |  Loss2: (0.2897) | Acc: (89.00%) (41514/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.5856) |  Loss2: (0.2897) | Acc: (89.00%) (42652/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.5859) |  Loss2: (0.2896) | Acc: (89.00%) (43799/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.5853) |  Loss2: (0.2896) | Acc: (89.00%) (44924/50000)
# TEST : Loss: (0.4208) | Acc: (85.00%) (8599/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1473,  0.0600,  0.0925],
          [ 0.1035, -0.2769, -0.0998],
          [ 0.2435, -0.0900,  0.0149]],

         [[-0.0023, -0.0963,  0.2577],
          [ 0.0215, -0.0889,  0.0406],
          [ 0.0565, -0.0001, -0.0707]],

         [[ 0.0168, -0.0539,  0.1815],
          [ 0.1045, -0.1842,  0.1528],
          [-0.1039,  0.0317, -0.1885]]],


        [[[ 0.0955, -0.0817, -0.0048],
          [ 0.1358,  0.1711,  0.1810],
          [ 0.0093, -0.0651,  0.1163]],

         [[-0.1613, -0.1866,  0.1583],
          [ 0.0293, -0.0531,  0.1057],
          [ 0.1221, -0.2047,  0.1350]],

         [[ 0.0996, -0.0494, -0.0090],
          [ 0.0132,  0.0256, -0.1121],
          [-0.0316, -0.1556, -0.0109]]],


        [[[-0.0707,  0.0915, -0.0414],
          [-0.1242,  0.0662,  0.1188],
          [ 0.1136,  0.1676,  0.0027]],

         [[ 0.0998,  0.1331,  0.1139],
          [ 0.0704, -0.0398, -0.0364],
          [-0.0465, -0.0306,  0.1409]],

         [[-0.0464,  0.1453,  0.0744],
          [-0.0420, -0.0458, -0.0603],
          [-0.0392, -0.0078,  0.0440]]],


        ...,


        [[[ 0.2275, -0.1135, -0.1125],
          [-0.0438, -0.1077, -0.0963],
          [-0.0920,  0.0251,  0.0636]],

         [[ 0.1692,  0.1939, -0.1132],
          [ 0.2083, -0.1408, -0.1170],
          [-0.0632, -0.0793, -0.0334]],

         [[ 0.1862,  0.1037, -0.0961],
          [-0.0731, -0.1579, -0.0744],
          [ 0.0623,  0.1766, -0.1324]]],


        [[[-0.1069,  0.0899, -0.1486],
          [-0.1404, -0.1420,  0.1218],
          [-0.1570,  0.1530, -0.1323]],

         [[ 0.0905, -0.1226,  0.0800],
          [ 0.1495,  0.0036, -0.0869],
          [-0.1545,  0.1121, -0.1999]],

         [[ 0.1175, -0.0275, -0.0070],
          [ 0.1360, -0.1801,  0.0075],
          [-0.0359,  0.0113, -0.1268]]],


        [[[ 0.0742,  0.1444,  0.1067],
          [-0.0877, -0.1689,  0.2094],
          [-0.1629, -0.0118, -0.0558]],

         [[-0.0291, -0.0895,  0.1495],
          [-0.0087, -0.2001,  0.0496],
          [-0.0054,  0.0757,  0.0365]],

         [[-0.1589, -0.1980,  0.1361],
          [ 0.1409, -0.1808, -0.0214],
          [ 0.0002, -0.0310,  0.2846]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4729e-05,  5.9957e-06,  9.2537e-06],
          [ 1.0348e-05, -2.7689e-05, -9.9825e-06],
          [ 2.4348e-05, -8.9953e-06,  1.4873e-06]],

         [[-2.3041e-07, -9.6268e-06,  2.5768e-05],
          [ 2.1505e-06, -8.8900e-06,  4.0636e-06],
          [ 5.6477e-06, -1.0774e-08, -7.0718e-06]],

         [[ 1.6751e-06, -5.3871e-06,  1.8148e-05],
          [ 1.0446e-05, -1.8425e-05,  1.5281e-05],
          [-1.0390e-05,  3.1660e-06, -1.8850e-05]]],


        [[[ 9.5518e-06, -8.1689e-06, -4.7619e-07],
          [ 1.3584e-05,  1.7113e-05,  1.8102e-05],
          [ 9.2871e-07, -6.5105e-06,  1.1632e-05]],

         [[-1.6130e-05, -1.8660e-05,  1.5826e-05],
          [ 2.9252e-06, -5.3050e-06,  1.0572e-05],
          [ 1.2207e-05, -2.0473e-05,  1.3497e-05]],

         [[ 9.9630e-06, -4.9418e-06, -8.9715e-07],
          [ 1.3213e-06,  2.5556e-06, -1.1207e-05],
          [-3.1641e-06, -1.5560e-05, -1.0881e-06]]],


        [[[-7.0741e-06,  9.1457e-06, -4.1439e-06],
          [-1.2415e-05,  6.6196e-06,  1.1877e-05],
          [ 1.1362e-05,  1.6763e-05,  2.6693e-07]],

         [[ 9.9804e-06,  1.3314e-05,  1.1393e-05],
          [ 7.0390e-06, -3.9756e-06, -3.6396e-06],
          [-4.6517e-06, -3.0619e-06,  1.4089e-05]],

         [[-4.6414e-06,  1.4534e-05,  7.4439e-06],
          [-4.1958e-06, -4.5799e-06, -6.0254e-06],
          [-3.9224e-06, -7.8300e-07,  4.4040e-06]]],


        ...,


        [[[ 2.2752e-05, -1.1354e-05, -1.1248e-05],
          [-4.3813e-06, -1.0771e-05, -9.6264e-06],
          [-9.1985e-06,  2.5054e-06,  6.3584e-06]],

         [[ 1.6916e-05,  1.9389e-05, -1.1320e-05],
          [ 2.0825e-05, -1.4075e-05, -1.1702e-05],
          [-6.3152e-06, -7.9325e-06, -3.3373e-06]],

         [[ 1.8623e-05,  1.0375e-05, -9.6082e-06],
          [-7.3134e-06, -1.5787e-05, -7.4390e-06],
          [ 6.2334e-06,  1.7655e-05, -1.3239e-05]]],


        [[[-1.0690e-05,  8.9858e-06, -1.4859e-05],
          [-1.4043e-05, -1.4202e-05,  1.2177e-05],
          [-1.5702e-05,  1.5296e-05, -1.3228e-05]],

         [[ 9.0493e-06, -1.2258e-05,  8.0032e-06],
          [ 1.4951e-05,  3.5513e-07, -8.6853e-06],
          [-1.5452e-05,  1.1207e-05, -1.9995e-05]],

         [[ 1.1748e-05, -2.7476e-06, -6.9844e-07],
          [ 1.3599e-05, -1.8012e-05,  7.5409e-07],
          [-3.5878e-06,  1.1269e-06, -1.2683e-05]]],


        [[[ 7.4250e-06,  1.4436e-05,  1.0667e-05],
          [-8.7658e-06, -1.6890e-05,  2.0945e-05],
          [-1.6293e-05, -1.1790e-06, -5.5784e-06]],

         [[-2.9067e-06, -8.9520e-06,  1.4954e-05],
          [-8.6615e-07, -2.0012e-05,  4.9569e-06],
          [-5.3813e-07,  7.5682e-06,  3.6464e-06]],

         [[-1.5895e-05, -1.9797e-05,  1.3610e-05],
          [ 1.4095e-05, -1.8085e-05, -2.1374e-06],
          [ 1.5420e-08, -3.1018e-06,  2.8464e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5017]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0278]], device='cuda:0')

Epoch: 78 | Batch_idx: 0 |  Loss: (0.5647) |  Loss2: (0.2880) | Acc: (89.00%) (114/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.5934) |  Loss2: (0.2879) | Acc: (89.00%) (1262/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.5768) |  Loss2: (0.2878) | Acc: (90.00%) (2421/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.5727) |  Loss2: (0.2878) | Acc: (90.00%) (3589/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.5690) |  Loss2: (0.2878) | Acc: (90.00%) (4749/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.5669) |  Loss2: (0.2877) | Acc: (90.00%) (5911/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.5680) |  Loss2: (0.2877) | Acc: (90.00%) (7063/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.5702) |  Loss2: (0.2876) | Acc: (90.00%) (8213/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.5707) |  Loss2: (0.2876) | Acc: (90.00%) (9372/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.5700) |  Loss2: (0.2875) | Acc: (90.00%) (10529/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.5705) |  Loss2: (0.2875) | Acc: (90.00%) (11680/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.5708) |  Loss2: (0.2874) | Acc: (90.00%) (12836/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.5706) |  Loss2: (0.2874) | Acc: (90.00%) (13995/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.5696) |  Loss2: (0.2874) | Acc: (90.00%) (15157/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.5693) |  Loss2: (0.2873) | Acc: (90.00%) (16301/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.5685) |  Loss2: (0.2873) | Acc: (90.00%) (17466/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.5685) |  Loss2: (0.2872) | Acc: (90.00%) (18625/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.5692) |  Loss2: (0.2872) | Acc: (90.00%) (19776/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.5696) |  Loss2: (0.2872) | Acc: (90.00%) (20929/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.5701) |  Loss2: (0.2871) | Acc: (90.00%) (22068/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.5690) |  Loss2: (0.2871) | Acc: (90.00%) (23243/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.5708) |  Loss2: (0.2870) | Acc: (90.00%) (24380/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.5713) |  Loss2: (0.2870) | Acc: (90.00%) (25526/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.5719) |  Loss2: (0.2870) | Acc: (90.00%) (26674/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.5715) |  Loss2: (0.2869) | Acc: (90.00%) (27832/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.5711) |  Loss2: (0.2869) | Acc: (90.00%) (28991/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.5712) |  Loss2: (0.2868) | Acc: (90.00%) (30136/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.5718) |  Loss2: (0.2868) | Acc: (90.00%) (31287/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.5721) |  Loss2: (0.2868) | Acc: (90.00%) (32438/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.5715) |  Loss2: (0.2867) | Acc: (90.00%) (33596/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.5717) |  Loss2: (0.2867) | Acc: (90.00%) (34755/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.5719) |  Loss2: (0.2866) | Acc: (90.00%) (35894/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.5726) |  Loss2: (0.2866) | Acc: (90.00%) (37042/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.5732) |  Loss2: (0.2865) | Acc: (90.00%) (38193/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.5724) |  Loss2: (0.2865) | Acc: (90.00%) (39353/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.5729) |  Loss2: (0.2865) | Acc: (90.00%) (40499/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.5737) |  Loss2: (0.2864) | Acc: (90.00%) (41638/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.5738) |  Loss2: (0.2864) | Acc: (90.00%) (42796/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.5734) |  Loss2: (0.2863) | Acc: (90.00%) (43952/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.5732) |  Loss2: (0.2863) | Acc: (90.00%) (45062/50000)
# TEST : Loss: (0.4186) | Acc: (86.00%) (8615/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1472,  0.0599,  0.0925],
          [ 0.1034, -0.2768, -0.0998],
          [ 0.2434, -0.0899,  0.0149]],

         [[-0.0023, -0.0962,  0.2576],
          [ 0.0215, -0.0889,  0.0406],
          [ 0.0565, -0.0001, -0.0707]],

         [[ 0.0167, -0.0539,  0.1814],
          [ 0.1044, -0.1842,  0.1528],
          [-0.1039,  0.0316, -0.1884]]],


        [[[ 0.0955, -0.0817, -0.0048],
          [ 0.1358,  0.1711,  0.1810],
          [ 0.0093, -0.0651,  0.1163]],

         [[-0.1612, -0.1865,  0.1582],
          [ 0.0292, -0.0530,  0.1057],
          [ 0.1220, -0.2046,  0.1349]],

         [[ 0.0996, -0.0494, -0.0090],
          [ 0.0132,  0.0255, -0.1120],
          [-0.0316, -0.1555, -0.0109]]],


        [[[-0.0707,  0.0914, -0.0414],
          [-0.1241,  0.0662,  0.1187],
          [ 0.1136,  0.1676,  0.0027]],

         [[ 0.0998,  0.1331,  0.1139],
          [ 0.0704, -0.0397, -0.0364],
          [-0.0465, -0.0306,  0.1408]],

         [[-0.0464,  0.1453,  0.0744],
          [-0.0419, -0.0458, -0.0602],
          [-0.0392, -0.0078,  0.0440]]],


        ...,


        [[[ 0.2274, -0.1135, -0.1124],
          [-0.0438, -0.1077, -0.0962],
          [-0.0919,  0.0250,  0.0636]],

         [[ 0.1691,  0.1938, -0.1132],
          [ 0.2082, -0.1407, -0.1170],
          [-0.0631, -0.0793, -0.0334]],

         [[ 0.1862,  0.1037, -0.0960],
          [-0.0731, -0.1578, -0.0744],
          [ 0.0623,  0.1765, -0.1323]]],


        [[[-0.1069,  0.0898, -0.1485],
          [-0.1404, -0.1420,  0.1217],
          [-0.1570,  0.1529, -0.1322]],

         [[ 0.0905, -0.1225,  0.0800],
          [ 0.1495,  0.0035, -0.0868],
          [-0.1545,  0.1120, -0.1999]],

         [[ 0.1174, -0.0275, -0.0070],
          [ 0.1359, -0.1801,  0.0075],
          [-0.0359,  0.0113, -0.1268]]],


        [[[ 0.0742,  0.1443,  0.1066],
          [-0.0876, -0.1688,  0.2094],
          [-0.1629, -0.0118, -0.0558]],

         [[-0.0291, -0.0895,  0.1495],
          [-0.0087, -0.2000,  0.0495],
          [-0.0054,  0.0757,  0.0364]],

         [[-0.1589, -0.1979,  0.1360],
          [ 0.1409, -0.1808, -0.0214],
          [ 0.0002, -0.0310,  0.2845]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4723e-05,  5.9934e-06,  9.2502e-06],
          [ 1.0344e-05, -2.7678e-05, -9.9788e-06],
          [ 2.4339e-05, -8.9918e-06,  1.4867e-06]],

         [[-2.3032e-07, -9.6230e-06,  2.5757e-05],
          [ 2.1496e-06, -8.8865e-06,  4.0620e-06],
          [ 5.6455e-06, -1.0770e-08, -7.0692e-06]],

         [[ 1.6745e-06, -5.3851e-06,  1.8141e-05],
          [ 1.0442e-05, -1.8418e-05,  1.5275e-05],
          [-1.0386e-05,  3.1649e-06, -1.8842e-05]]],


        [[[ 9.5480e-06, -8.1657e-06, -4.7601e-07],
          [ 1.3578e-05,  1.7107e-05,  1.8095e-05],
          [ 9.2835e-07, -6.5079e-06,  1.1628e-05]],

         [[-1.6124e-05, -1.8653e-05,  1.5820e-05],
          [ 2.9240e-06, -5.3030e-06,  1.0568e-05],
          [ 1.2202e-05, -2.0465e-05,  1.3491e-05]],

         [[ 9.9593e-06, -4.9399e-06, -8.9678e-07],
          [ 1.3208e-06,  2.5546e-06, -1.1202e-05],
          [-3.1629e-06, -1.5554e-05, -1.0877e-06]]],


        [[[-7.0715e-06,  9.1422e-06, -4.1423e-06],
          [-1.2410e-05,  6.6170e-06,  1.1872e-05],
          [ 1.1357e-05,  1.6757e-05,  2.6683e-07]],

         [[ 9.9767e-06,  1.3309e-05,  1.1389e-05],
          [ 7.0364e-06, -3.9740e-06, -3.6381e-06],
          [-4.6499e-06, -3.0608e-06,  1.4084e-05]],

         [[-4.6396e-06,  1.4528e-05,  7.4410e-06],
          [-4.1942e-06, -4.5781e-06, -6.0231e-06],
          [-3.9208e-06, -7.8271e-07,  4.4022e-06]]],


        ...,


        [[[ 2.2743e-05, -1.1349e-05, -1.1244e-05],
          [-4.3796e-06, -1.0767e-05, -9.6226e-06],
          [-9.1950e-06,  2.5044e-06,  6.3558e-06]],

         [[ 1.6910e-05,  1.9382e-05, -1.1315e-05],
          [ 2.0817e-05, -1.4070e-05, -1.1697e-05],
          [-6.3128e-06, -7.9293e-06, -3.3360e-06]],

         [[ 1.8616e-05,  1.0371e-05, -9.6044e-06],
          [-7.3105e-06, -1.5780e-05, -7.4361e-06],
          [ 6.2309e-06,  1.7648e-05, -1.3234e-05]]],


        [[[-1.0686e-05,  8.9823e-06, -1.4853e-05],
          [-1.4038e-05, -1.4196e-05,  1.2172e-05],
          [-1.5696e-05,  1.5290e-05, -1.3223e-05]],

         [[ 9.0458e-06, -1.2253e-05,  8.0000e-06],
          [ 1.4946e-05,  3.5500e-07, -8.6818e-06],
          [-1.5446e-05,  1.1203e-05, -1.9987e-05]],

         [[ 1.1743e-05, -2.7465e-06, -6.9817e-07],
          [ 1.3594e-05, -1.8005e-05,  7.5380e-07],
          [-3.5863e-06,  1.1264e-06, -1.2678e-05]]],


        [[[ 7.4221e-06,  1.4430e-05,  1.0663e-05],
          [-8.7623e-06, -1.6884e-05,  2.0937e-05],
          [-1.6286e-05, -1.1785e-06, -5.5762e-06]],

         [[-2.9055e-06, -8.9485e-06,  1.4948e-05],
          [-8.6582e-07, -2.0004e-05,  4.9550e-06],
          [-5.3792e-07,  7.5653e-06,  3.6449e-06]],

         [[-1.5888e-05, -1.9789e-05,  1.3605e-05],
          [ 1.4089e-05, -1.8078e-05, -2.1366e-06],
          [ 1.5414e-08, -3.1006e-06,  2.8453e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5189]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0208]], device='cuda:0')

Epoch: 79 | Batch_idx: 0 |  Loss: (0.5933) |  Loss2: (0.2847) | Acc: (90.00%) (116/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.5872) |  Loss2: (0.2846) | Acc: (89.00%) (1266/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.5875) |  Loss2: (0.2846) | Acc: (89.00%) (2413/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.5798) |  Loss2: (0.2845) | Acc: (90.00%) (3574/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.5839) |  Loss2: (0.2845) | Acc: (89.00%) (4713/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.5768) |  Loss2: (0.2844) | Acc: (89.00%) (5873/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.5740) |  Loss2: (0.2844) | Acc: (89.00%) (7026/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.5733) |  Loss2: (0.2844) | Acc: (89.00%) (8179/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.5761) |  Loss2: (0.2843) | Acc: (89.00%) (9318/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.5740) |  Loss2: (0.2843) | Acc: (89.00%) (10477/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.5695) |  Loss2: (0.2842) | Acc: (90.00%) (11649/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.5674) |  Loss2: (0.2842) | Acc: (90.00%) (12817/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.5720) |  Loss2: (0.2842) | Acc: (89.00%) (13936/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.5734) |  Loss2: (0.2841) | Acc: (89.00%) (15088/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.5733) |  Loss2: (0.2841) | Acc: (89.00%) (16240/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.5740) |  Loss2: (0.2840) | Acc: (89.00%) (17370/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.5724) |  Loss2: (0.2840) | Acc: (89.00%) (18531/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.5739) |  Loss2: (0.2840) | Acc: (89.00%) (19672/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.5745) |  Loss2: (0.2839) | Acc: (89.00%) (20820/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.5753) |  Loss2: (0.2839) | Acc: (89.00%) (21959/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.5756) |  Loss2: (0.2838) | Acc: (89.00%) (23103/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.5746) |  Loss2: (0.2838) | Acc: (89.00%) (24265/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.5751) |  Loss2: (0.2837) | Acc: (89.00%) (25414/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.5749) |  Loss2: (0.2837) | Acc: (89.00%) (26575/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.5741) |  Loss2: (0.2837) | Acc: (89.00%) (27735/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.5726) |  Loss2: (0.2836) | Acc: (89.00%) (28905/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.5716) |  Loss2: (0.2836) | Acc: (90.00%) (30080/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.5712) |  Loss2: (0.2835) | Acc: (90.00%) (31239/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.5726) |  Loss2: (0.2835) | Acc: (89.00%) (32370/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.5721) |  Loss2: (0.2835) | Acc: (90.00%) (33530/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.5727) |  Loss2: (0.2834) | Acc: (90.00%) (34676/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.5726) |  Loss2: (0.2834) | Acc: (90.00%) (35830/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.5726) |  Loss2: (0.2834) | Acc: (89.00%) (36973/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.5726) |  Loss2: (0.2833) | Acc: (89.00%) (38124/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.5719) |  Loss2: (0.2833) | Acc: (90.00%) (39285/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.5714) |  Loss2: (0.2832) | Acc: (89.00%) (40435/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.5711) |  Loss2: (0.2832) | Acc: (90.00%) (41599/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.5709) |  Loss2: (0.2832) | Acc: (90.00%) (42762/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.5709) |  Loss2: (0.2831) | Acc: (90.00%) (43909/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.5711) |  Loss2: (0.2831) | Acc: (90.00%) (45016/50000)
# TEST : Loss: (0.4160) | Acc: (86.00%) (8642/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1472,  0.0599,  0.0925],
          [ 0.1034, -0.2767, -0.0997],
          [ 0.2433, -0.0899,  0.0149]],

         [[-0.0023, -0.0962,  0.2575],
          [ 0.0215, -0.0888,  0.0406],
          [ 0.0564, -0.0001, -0.0707]],

         [[ 0.0167, -0.0538,  0.1813],
          [ 0.1044, -0.1841,  0.1527],
          [-0.1038,  0.0316, -0.1883]]],


        [[[ 0.0954, -0.0816, -0.0048],
          [ 0.1357,  0.1710,  0.1809],
          [ 0.0093, -0.0651,  0.1162]],

         [[-0.1612, -0.1865,  0.1581],
          [ 0.0292, -0.0530,  0.1056],
          [ 0.1220, -0.2046,  0.1349]],

         [[ 0.0996, -0.0494, -0.0090],
          [ 0.0132,  0.0255, -0.1120],
          [-0.0316, -0.1555, -0.0109]]],


        [[[-0.0707,  0.0914, -0.0414],
          [-0.1241,  0.0661,  0.1187],
          [ 0.1135,  0.1675,  0.0027]],

         [[ 0.0997,  0.1330,  0.1138],
          [ 0.0703, -0.0397, -0.0364],
          [-0.0465, -0.0306,  0.1408]],

         [[-0.0464,  0.1452,  0.0744],
          [-0.0419, -0.0458, -0.0602],
          [-0.0392, -0.0078,  0.0440]]],


        ...,


        [[[ 0.2273, -0.1135, -0.1124],
          [-0.0438, -0.1076, -0.0962],
          [-0.0919,  0.0250,  0.0635]],

         [[ 0.1690,  0.1937, -0.1131],
          [ 0.2081, -0.1406, -0.1169],
          [-0.0631, -0.0793, -0.0333]],

         [[ 0.1861,  0.1037, -0.0960],
          [-0.0731, -0.1577, -0.0743],
          [ 0.0623,  0.1764, -0.1323]]],


        [[[-0.1068,  0.0898, -0.1485],
          [-0.1403, -0.1419,  0.1217],
          [-0.1569,  0.1528, -0.1322]],

         [[ 0.0904, -0.1225,  0.0800],
          [ 0.1494,  0.0035, -0.0868],
          [-0.1544,  0.1120, -0.1998]],

         [[ 0.1174, -0.0275, -0.0070],
          [ 0.1359, -0.1800,  0.0075],
          [-0.0358,  0.0113, -0.1267]]],


        [[[ 0.0742,  0.1442,  0.1066],
          [-0.0876, -0.1688,  0.2093],
          [-0.1628, -0.0118, -0.0557]],

         [[-0.0290, -0.0894,  0.1494],
          [-0.0087, -0.2000,  0.0495],
          [-0.0054,  0.0756,  0.0364]],

         [[-0.1588, -0.1978,  0.1360],
          [ 0.1408, -0.1807, -0.0214],
          [ 0.0002, -0.0310,  0.2844]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.4717e-05,  5.9910e-06,  9.2468e-06],
          [ 1.0340e-05, -2.7668e-05, -9.9750e-06],
          [ 2.4330e-05, -8.9883e-06,  1.4861e-06]],

         [[-2.3023e-07, -9.6192e-06,  2.5747e-05],
          [ 2.1488e-06, -8.8830e-06,  4.0604e-06],
          [ 5.6433e-06, -1.0766e-08, -7.0666e-06]],

         [[ 1.6738e-06, -5.3830e-06,  1.8134e-05],
          [ 1.0438e-05, -1.8411e-05,  1.5270e-05],
          [-1.0382e-05,  3.1637e-06, -1.8834e-05]]],


        [[[ 9.5442e-06, -8.1625e-06, -4.7583e-07],
          [ 1.3573e-05,  1.7100e-05,  1.8088e-05],
          [ 9.2798e-07, -6.5052e-06,  1.1623e-05]],

         [[-1.6117e-05, -1.8645e-05,  1.5813e-05],
          [ 2.9228e-06, -5.3009e-06,  1.0564e-05],
          [ 1.2197e-05, -2.0456e-05,  1.3486e-05]],

         [[ 9.9555e-06, -4.9380e-06, -8.9642e-07],
          [ 1.3203e-06,  2.5535e-06, -1.1198e-05],
          [-3.1617e-06, -1.5548e-05, -1.0873e-06]]],


        [[[-7.0689e-06,  9.1387e-06, -4.1407e-06],
          [-1.2405e-05,  6.6144e-06,  1.1867e-05],
          [ 1.1353e-05,  1.6750e-05,  2.6673e-07]],

         [[ 9.9729e-06,  1.3303e-05,  1.1384e-05],
          [ 7.0338e-06, -3.9724e-06, -3.6366e-06],
          [-4.6482e-06, -3.0596e-06,  1.4079e-05]],

         [[-4.6379e-06,  1.4522e-05,  7.4381e-06],
          [-4.1926e-06, -4.5764e-06, -6.0207e-06],
          [-3.9192e-06, -7.8242e-07,  4.4005e-06]]],


        ...,


        [[[ 2.2735e-05, -1.1345e-05, -1.1240e-05],
          [-4.3778e-06, -1.0763e-05, -9.6188e-06],
          [-9.1915e-06,  2.5035e-06,  6.3531e-06]],

         [[ 1.6903e-05,  1.9374e-05, -1.1311e-05],
          [ 2.0809e-05, -1.4065e-05, -1.1692e-05],
          [-6.3105e-06, -7.9261e-06, -3.3347e-06]],

         [[ 1.8609e-05,  1.0366e-05, -9.6007e-06],
          [-7.3076e-06, -1.5774e-05, -7.4332e-06],
          [ 6.2284e-06,  1.7641e-05, -1.3229e-05]]],


        [[[-1.0682e-05,  8.9788e-06, -1.4847e-05],
          [-1.4033e-05, -1.4190e-05,  1.2167e-05],
          [-1.5689e-05,  1.5284e-05, -1.3217e-05]],

         [[ 9.0424e-06, -1.2248e-05,  7.9968e-06],
          [ 1.4940e-05,  3.5486e-07, -8.6783e-06],
          [-1.5441e-05,  1.1198e-05, -1.9980e-05]],

         [[ 1.1739e-05, -2.7454e-06, -6.9789e-07],
          [ 1.3589e-05, -1.7998e-05,  7.5351e-07],
          [-3.5849e-06,  1.1260e-06, -1.2672e-05]]],


        [[[ 7.4192e-06,  1.4424e-05,  1.0659e-05],
          [-8.7588e-06, -1.6878e-05,  2.0929e-05],
          [-1.6280e-05, -1.1781e-06, -5.5740e-06]],

         [[-2.9044e-06, -8.9450e-06,  1.4943e-05],
          [-8.6550e-07, -1.9997e-05,  4.9531e-06],
          [-5.3770e-07,  7.5624e-06,  3.6435e-06]],

         [[-1.5882e-05, -1.9781e-05,  1.3599e-05],
          [ 1.4084e-05, -1.8071e-05, -2.1358e-06],
          [ 1.5408e-08, -3.0993e-06,  2.8441e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5167]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0458]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (1275/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (2412/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (3559/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (4699/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (5851/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (6992/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (8146/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (9308/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (10445/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (11593/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (12742/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2950) |  Loss2: (0.0000) | Acc: (89.00%) (13887/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (15035/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (16190/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (17341/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (18488/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (19637/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (20804/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (21959/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (23097/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (24246/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (25398/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (26548/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (27706/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (28838/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (29992/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (31142/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (32277/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (33425/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (34566/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (35722/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (36864/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (38021/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (89.00%) (39163/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (40316/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (41455/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (42601/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (43732/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (44828/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_080.pth.tar'
# TEST : Loss: (0.4520) | Acc: (85.00%) (8533/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1414,  0.0622,  0.0944],
          [ 0.1049, -0.2788, -0.0992],
          [ 0.2413, -0.0910,  0.0136]],

         [[-0.0002, -0.0956,  0.2588],
          [ 0.0211, -0.0920,  0.0406],
          [ 0.0544, -0.0016, -0.0722]],

         [[ 0.0187, -0.0520,  0.1849],
          [ 0.1048, -0.1855,  0.1553],
          [-0.1033,  0.0321, -0.1868]]],


        [[[ 0.0946, -0.0826, -0.0053],
          [ 0.1350,  0.1700,  0.1801],
          [ 0.0078, -0.0667,  0.1146]],

         [[-0.1614, -0.1872,  0.1573],
          [ 0.0293, -0.0537,  0.1047],
          [ 0.1213, -0.2057,  0.1332]],

         [[ 0.0990, -0.0509, -0.0104],
          [ 0.0128,  0.0239, -0.1135],
          [-0.0326, -0.1576, -0.0133]]],


        [[[-0.0706,  0.0913, -0.0414],
          [-0.1239,  0.0662,  0.1187],
          [ 0.1132,  0.1671,  0.0025]],

         [[ 0.1000,  0.1332,  0.1141],
          [ 0.0706, -0.0394, -0.0361],
          [-0.0463, -0.0306,  0.1409]],

         [[-0.0463,  0.1452,  0.0744],
          [-0.0417, -0.0456, -0.0601],
          [-0.0391, -0.0079,  0.0440]]],


        ...,


        [[[ 0.2266, -0.1131, -0.1126],
          [-0.0447, -0.1085, -0.0962],
          [-0.0923,  0.0242,  0.0636]],

         [[ 0.1691,  0.1950, -0.1126],
          [ 0.2085, -0.1401, -0.1163],
          [-0.0624, -0.0789, -0.0331]],

         [[ 0.1830,  0.1017, -0.0985],
          [-0.0752, -0.1600, -0.0767],
          [ 0.0611,  0.1748, -0.1342]]],


        [[[-0.1055,  0.0904, -0.1482],
          [-0.1397, -0.1420,  0.1210],
          [-0.1556,  0.1532, -0.1330]],

         [[ 0.0918, -0.1216,  0.0806],
          [ 0.1499,  0.0035, -0.0869],
          [-0.1527,  0.1127, -0.2001]],

         [[ 0.1182, -0.0272, -0.0068],
          [ 0.1361, -0.1802,  0.0072],
          [-0.0347,  0.0117, -0.1269]]],


        [[[ 0.0740,  0.1424,  0.1040],
          [-0.0890, -0.1757,  0.2032],
          [-0.1634, -0.0163, -0.0595]],

         [[-0.0286, -0.0907,  0.1487],
          [-0.0072, -0.2043,  0.0467],
          [-0.0030,  0.0734,  0.0349]],

         [[-0.1594, -0.1984,  0.1365],
          [ 0.1407, -0.1849, -0.0229],
          [ 0.0009, -0.0336,  0.2836]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0019, -0.0077, -0.0142],
          [-0.0028, -0.0136, -0.0146],
          [ 0.0004, -0.0136, -0.0183]],

         [[-0.0153, -0.0283, -0.0446],
          [-0.0240, -0.0383, -0.0455],
          [-0.0248, -0.0427, -0.0505]],

         [[ 0.0086,  0.0021, -0.0136],
          [-0.0013, -0.0126, -0.0187],
          [-0.0007, -0.0198, -0.0275]]],


        [[[ 0.0165,  0.0136,  0.0066],
          [ 0.0117,  0.0134,  0.0096],
          [ 0.0053,  0.0070,  0.0062]],

         [[ 0.0172,  0.0137,  0.0094],
          [ 0.0128,  0.0136,  0.0128],
          [ 0.0047,  0.0055,  0.0074]],

         [[ 0.0267,  0.0234,  0.0202],
          [ 0.0218,  0.0237,  0.0238],
          [ 0.0131,  0.0159,  0.0187]]],


        [[[ 0.0016,  0.0026,  0.0050],
          [ 0.0004,  0.0008,  0.0036],
          [-0.0017, -0.0011,  0.0003]],

         [[ 0.0000, -0.0002,  0.0015],
          [-0.0005, -0.0018,  0.0003],
          [-0.0024, -0.0035, -0.0028]],

         [[ 0.0006,  0.0002,  0.0011],
          [ 0.0003, -0.0010,  0.0003],
          [-0.0014, -0.0027, -0.0028]]],


        ...,


        [[[-0.0078,  0.0002,  0.0032],
          [-0.0229, -0.0108, -0.0007],
          [-0.0262, -0.0146, -0.0073]],

         [[-0.0091, -0.0062, -0.0038],
          [-0.0237, -0.0149, -0.0059],
          [-0.0279, -0.0176, -0.0116]],

         [[-0.0099, -0.0072, -0.0034],
          [-0.0243, -0.0164, -0.0074],
          [-0.0302, -0.0202, -0.0141]]],


        [[[ 0.0257,  0.0195,  0.0200],
          [ 0.0116,  0.0081,  0.0117],
          [ 0.0041,  0.0038,  0.0080]],

         [[ 0.0117,  0.0041,  0.0024],
          [-0.0019, -0.0067, -0.0052],
          [-0.0105, -0.0120, -0.0095]],

         [[ 0.0178,  0.0104,  0.0091],
          [ 0.0026, -0.0023, -0.0007],
          [-0.0058, -0.0074, -0.0055]]],


        [[[-0.0364, -0.0519, -0.0596],
          [-0.0612, -0.0686, -0.0756],
          [-0.0722, -0.0727, -0.0778]],

         [[-0.0222, -0.0389, -0.0450],
          [-0.0438, -0.0495, -0.0551],
          [-0.0525, -0.0500, -0.0544]],

         [[-0.0221, -0.0325, -0.0295],
          [-0.0521, -0.0495, -0.0442],
          [-0.0673, -0.0573, -0.0497]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5170]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 81 | Batch_idx: 0 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (1265/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (3591/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (4747/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (5893/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (7045/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (90.00%) (8197/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (9375/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (10522/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (11674/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (12832/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (13966/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (15134/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (16305/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (17444/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (18609/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (19764/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (20908/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (22047/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (23199/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (24356/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (25517/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (26687/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (27823/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (28989/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (30136/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (31289/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (32434/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (90.00%) (33572/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (34725/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (90.00%) (35876/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (37034/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (38207/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (90.00%) (39359/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (40519/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (41669/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (42841/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2883) |  Loss2: (0.0000) | Acc: (90.00%) (43983/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (45100/50000)
# TEST : Loss: (0.4558) | Acc: (84.00%) (8476/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1425,  0.0625,  0.0921],
          [ 0.1065, -0.2775, -0.1009],
          [ 0.2442, -0.0900,  0.0076]],

         [[-0.0019, -0.0961,  0.2556],
          [ 0.0215, -0.0911,  0.0388],
          [ 0.0548, -0.0019, -0.0789]],

         [[ 0.0157, -0.0525,  0.1817],
          [ 0.1013, -0.1859,  0.1526],
          [-0.1099,  0.0278, -0.1956]]],


        [[[ 0.0934, -0.0846, -0.0076],
          [ 0.1350,  0.1684,  0.1786],
          [ 0.0071, -0.0691,  0.1127]],

         [[-0.1607, -0.1871,  0.1571],
          [ 0.0307, -0.0536,  0.1049],
          [ 0.1217, -0.2069,  0.1327]],

         [[ 0.0986, -0.0517, -0.0110],
          [ 0.0131,  0.0228, -0.1139],
          [-0.0329, -0.1597, -0.0142]]],


        [[[-0.0710,  0.0911, -0.0415],
          [-0.1242,  0.0660,  0.1186],
          [ 0.1135,  0.1673,  0.0028]],

         [[ 0.0995,  0.1328,  0.1138],
          [ 0.0701, -0.0398, -0.0363],
          [-0.0463, -0.0305,  0.1409]],

         [[-0.0462,  0.1453,  0.0746],
          [-0.0417, -0.0456, -0.0600],
          [-0.0388, -0.0077,  0.0441]]],


        ...,


        [[[ 0.2276, -0.1124, -0.1118],
          [-0.0449, -0.1079, -0.0948],
          [-0.0914,  0.0263,  0.0665]],

         [[ 0.1712,  0.1970, -0.1103],
          [ 0.2099, -0.1381, -0.1137],
          [-0.0594, -0.0756, -0.0291]],

         [[ 0.1857,  0.1033, -0.0973],
          [-0.0719, -0.1576, -0.0751],
          [ 0.0660,  0.1785, -0.1314]]],


        [[[-0.1067,  0.0903, -0.1475],
          [-0.1402, -0.1422,  0.1206],
          [-0.1541,  0.1543, -0.1334]],

         [[ 0.0891, -0.1228,  0.0803],
          [ 0.1480,  0.0020, -0.0882],
          [-0.1527,  0.1124, -0.2013]],

         [[ 0.1157, -0.0284, -0.0074],
          [ 0.1345, -0.1816,  0.0060],
          [-0.0350,  0.0112, -0.1282]]],


        [[[ 0.0760,  0.1446,  0.1060],
          [-0.0870, -0.1764,  0.2039],
          [-0.1624, -0.0160, -0.0619]],

         [[-0.0266, -0.0900,  0.1489],
          [-0.0047, -0.2051,  0.0467],
          [-0.0015,  0.0751,  0.0331]],

         [[-0.1580, -0.1988,  0.1354],
          [ 0.1435, -0.1857, -0.0229],
          [ 0.0040, -0.0307,  0.2838]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0270,  0.0270,  0.0440],
          [ 0.0130,  0.0305,  0.0455],
          [ 0.0160,  0.0477,  0.0751]],

         [[ 0.0099,  0.0125,  0.0339],
          [-0.0078,  0.0084,  0.0286],
          [-0.0098,  0.0179,  0.0476]],

         [[-0.0060,  0.0016,  0.0230],
          [-0.0311, -0.0131,  0.0079],
          [-0.0413, -0.0185,  0.0112]]],


        [[[ 0.0149,  0.0077,  0.0080],
          [ 0.0017, -0.0076, -0.0108],
          [-0.0014, -0.0042, -0.0061]],

         [[ 0.0006, -0.0046, -0.0024],
          [-0.0116, -0.0200, -0.0215],
          [-0.0120, -0.0140, -0.0144]],

         [[-0.0005, -0.0041, -0.0019],
          [-0.0108, -0.0180, -0.0196],
          [-0.0112, -0.0124, -0.0113]]],


        [[[-0.0057, -0.0016, -0.0016],
          [-0.0078, -0.0049, -0.0047],
          [-0.0049, -0.0023, -0.0013]],

         [[-0.0036,  0.0002, -0.0002],
          [-0.0054, -0.0028, -0.0029],
          [-0.0030, -0.0006,  0.0004]],

         [[-0.0040, -0.0001,  0.0003],
          [-0.0058, -0.0028, -0.0021],
          [-0.0041, -0.0013,  0.0006]]],


        ...,


        [[[-0.0360, -0.0344, -0.0266],
          [-0.0354, -0.0330, -0.0282],
          [-0.0290, -0.0259, -0.0254]],

         [[-0.0275, -0.0259, -0.0198],
          [-0.0284, -0.0254, -0.0212],
          [-0.0228, -0.0193, -0.0182]],

         [[-0.0163, -0.0111, -0.0031],
          [-0.0137, -0.0073, -0.0023],
          [-0.0078, -0.0021, -0.0004]]],


        [[[ 0.0119,  0.0028,  0.0045],
          [ 0.0069, -0.0028, -0.0024],
          [ 0.0055, -0.0034, -0.0047]],

         [[ 0.0137,  0.0045,  0.0051],
          [ 0.0091,  0.0004, -0.0007],
          [ 0.0068, -0.0003, -0.0022]],

         [[ 0.0134,  0.0065,  0.0086],
          [ 0.0092,  0.0030,  0.0035],
          [ 0.0078,  0.0031,  0.0026]]],


        [[[-0.0796, -0.0873, -0.0782],
          [-0.0694, -0.0745, -0.0759],
          [-0.0564, -0.0489, -0.0452]],

         [[-0.0841, -0.0870, -0.0823],
          [-0.0730, -0.0758, -0.0815],
          [-0.0591, -0.0531, -0.0521]],

         [[-0.0606, -0.0716, -0.0810],
          [-0.0437, -0.0545, -0.0753],
          [-0.0342, -0.0373, -0.0468]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5168]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 82 | Batch_idx: 0 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (89.00%) (1264/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (90.00%) (2421/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (3574/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (4739/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (5891/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (7070/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (8225/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (9408/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (10590/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (11743/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (12916/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (14080/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (15230/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (16388/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (17545/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (18690/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (19846/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (21004/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (22174/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (23328/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (24508/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (25676/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (26852/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (28009/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (29174/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (30337/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (31490/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (32646/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (33785/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (34925/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (36072/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (37223/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (38373/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (39510/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (40670/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (41823/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (42972/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (44128/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (45241/50000)
# TEST : Loss: (0.4773) | Acc: (85.00%) (8506/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1395,  0.0622,  0.0931],
          [ 0.1093, -0.2785, -0.0974],
          [ 0.2450, -0.0899,  0.0136]],

         [[ 0.0026, -0.0937,  0.2600],
          [ 0.0272, -0.0882,  0.0459],
          [ 0.0591,  0.0018, -0.0705]],

         [[ 0.0189, -0.0519,  0.1855],
          [ 0.1047, -0.1860,  0.1571],
          [-0.1085,  0.0274, -0.1911]]],


        [[[ 0.0937, -0.0833, -0.0065],
          [ 0.1347,  0.1691,  0.1795],
          [ 0.0075, -0.0679,  0.1142]],

         [[-0.1609, -0.1867,  0.1573],
          [ 0.0297, -0.0539,  0.1049],
          [ 0.1213, -0.2064,  0.1333]],

         [[ 0.0977, -0.0517, -0.0108],
          [ 0.0116,  0.0223, -0.1140],
          [-0.0333, -0.1592, -0.0138]]],


        [[[-0.0710,  0.0909, -0.0418],
          [-0.1243,  0.0659,  0.1182],
          [ 0.1132,  0.1671,  0.0023]],

         [[ 0.0998,  0.1331,  0.1140],
          [ 0.0701, -0.0395, -0.0362],
          [-0.0466, -0.0307,  0.1405]],

         [[-0.0460,  0.1454,  0.0747],
          [-0.0416, -0.0454, -0.0600],
          [-0.0388, -0.0076,  0.0439]]],


        ...,


        [[[ 0.2282, -0.1108, -0.1111],
          [-0.0450, -0.1082, -0.0953],
          [-0.0926,  0.0258,  0.0654]],

         [[ 0.1702,  0.1972, -0.1102],
          [ 0.2084, -0.1395, -0.1150],
          [-0.0621, -0.0778, -0.0313]],

         [[ 0.1842,  0.1029, -0.0980],
          [-0.0737, -0.1597, -0.0769],
          [ 0.0623,  0.1753, -0.1340]]],


        [[[-0.1058,  0.0909, -0.1464],
          [-0.1400, -0.1420,  0.1212],
          [-0.1549,  0.1540, -0.1335]],

         [[ 0.0899, -0.1217,  0.0820],
          [ 0.1481,  0.0025, -0.0871],
          [-0.1531,  0.1125, -0.2009]],

         [[ 0.1171, -0.0269, -0.0053],
          [ 0.1350, -0.1808,  0.0074],
          [-0.0352,  0.0114, -0.1277]]],


        [[[ 0.0755,  0.1411,  0.1014],
          [-0.0878, -0.1811,  0.2032],
          [-0.1632, -0.0175, -0.0622]],

         [[-0.0259, -0.0929,  0.1447],
          [-0.0036, -0.2076,  0.0465],
          [ 0.0015,  0.0777,  0.0346]],

         [[-0.1593, -0.2014,  0.1317],
          [ 0.1431, -0.1879, -0.0225],
          [ 0.0067, -0.0274,  0.2860]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1696,  0.1849,  0.1747],
          [ 0.1816,  0.1896,  0.1871],
          [ 0.2156,  0.2103,  0.1977]],

         [[ 0.1975,  0.2029,  0.1796],
          [ 0.2132,  0.2066,  0.1918],
          [ 0.2392,  0.2200,  0.1964]],

         [[ 0.2374,  0.2412,  0.2170],
          [ 0.2477,  0.2404,  0.2275],
          [ 0.2710,  0.2510,  0.2293]]],


        [[[ 0.0099,  0.0122,  0.0111],
          [ 0.0017,  0.0036,  0.0027],
          [-0.0200, -0.0186, -0.0212]],

         [[ 0.0107,  0.0148,  0.0135],
          [ 0.0040,  0.0066,  0.0068],
          [-0.0171, -0.0154, -0.0160]],

         [[ 0.0112,  0.0124,  0.0101],
          [ 0.0051,  0.0050,  0.0048],
          [-0.0145, -0.0152, -0.0155]]],


        [[[-0.0017, -0.0020, -0.0049],
          [ 0.0007,  0.0015,  0.0007],
          [ 0.0026,  0.0031,  0.0028]],

         [[-0.0011, -0.0012, -0.0038],
          [ 0.0027,  0.0034,  0.0028],
          [ 0.0062,  0.0067,  0.0063]],

         [[ 0.0041,  0.0038,  0.0015],
          [ 0.0077,  0.0078,  0.0073],
          [ 0.0110,  0.0109,  0.0108]]],


        ...,


        [[[-0.0489, -0.0408, -0.0364],
          [-0.0412, -0.0313, -0.0275],
          [-0.0240, -0.0170, -0.0152]],

         [[-0.0508, -0.0450, -0.0437],
          [-0.0474, -0.0421, -0.0414],
          [-0.0312, -0.0283, -0.0282]],

         [[-0.0645, -0.0610, -0.0581],
          [-0.0591, -0.0550, -0.0545],
          [-0.0420, -0.0397, -0.0408]]],


        [[[ 0.0211,  0.0152,  0.0195],
          [ 0.0081,  0.0037,  0.0062],
          [-0.0056, -0.0110, -0.0051]],

         [[ 0.0280,  0.0204,  0.0233],
          [ 0.0209,  0.0148,  0.0150],
          [ 0.0084,  0.0016,  0.0054]],

         [[ 0.0073,  0.0007,  0.0042],
          [ 0.0018, -0.0037, -0.0029],
          [-0.0084, -0.0145, -0.0106]]],


        [[[ 0.0334,  0.0426,  0.0263],
          [ 0.0368,  0.0331,  0.0280],
          [ 0.0286,  0.0262,  0.0432]],

         [[ 0.0085,  0.0188,  0.0220],
          [ 0.0057,  0.0029,  0.0143],
          [-0.0085, -0.0082,  0.0221]],

         [[ 0.0223,  0.0407,  0.0475],
          [ 0.0130,  0.0134,  0.0261],
          [-0.0151, -0.0133,  0.0186]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5165]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 83 | Batch_idx: 0 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (3649/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (4804/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (5972/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (7139/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (9438/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2594) |  Loss2: (0.0000) | Acc: (91.00%) (10607/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (90.00%) (11763/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (12934/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (14080/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (15254/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (16420/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (17579/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (18741/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (19895/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (21041/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (22223/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (23386/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (24544/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (25702/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (26848/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (28019/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (29186/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (30328/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (31475/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (32650/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (33804/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (34964/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (36149/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (37310/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (38466/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (39635/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (40791/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (41943/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (43113/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (44276/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (45386/50000)
# TEST : Loss: (0.4710) | Acc: (85.00%) (8511/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1397,  0.0586,  0.0901],
          [ 0.1057, -0.2843, -0.1015],
          [ 0.2430, -0.0932,  0.0102]],

         [[-0.0012, -0.1000,  0.2542],
          [ 0.0204, -0.0970,  0.0387],
          [ 0.0533, -0.0055, -0.0775]],

         [[ 0.0161, -0.0564,  0.1822],
          [ 0.0979, -0.1925,  0.1534],
          [-0.1140,  0.0219, -0.1942]]],


        [[[ 0.0919, -0.0853, -0.0075],
          [ 0.1336,  0.1676,  0.1788],
          [ 0.0064, -0.0698,  0.1132]],

         [[-0.1612, -0.1874,  0.1573],
          [ 0.0296, -0.0545,  0.1048],
          [ 0.1207, -0.2079,  0.1327]],

         [[ 0.0976, -0.0520, -0.0102],
          [ 0.0116,  0.0216, -0.1139],
          [-0.0334, -0.1604, -0.0141]]],


        [[[-0.0714,  0.0907, -0.0421],
          [-0.1244,  0.0659,  0.1180],
          [ 0.1133,  0.1673,  0.0023]],

         [[ 0.0995,  0.1330,  0.1137],
          [ 0.0700, -0.0394, -0.0362],
          [-0.0462, -0.0302,  0.1406]],

         [[-0.0461,  0.1453,  0.0744],
          [-0.0416, -0.0453, -0.0601],
          [-0.0385, -0.0073,  0.0439]]],


        ...,


        [[[ 0.2261, -0.1128, -0.1134],
          [-0.0470, -0.1101, -0.0969],
          [-0.0961,  0.0223,  0.0625]],

         [[ 0.1690,  0.1966, -0.1109],
          [ 0.2075, -0.1399, -0.1150],
          [-0.0648, -0.0802, -0.0331]],

         [[ 0.1844,  0.1034, -0.0978],
          [-0.0725, -0.1584, -0.0755],
          [ 0.0615,  0.1744, -0.1341]]],


        [[[-0.1060,  0.0914, -0.1453],
          [-0.1404, -0.1418,  0.1213],
          [-0.1546,  0.1549, -0.1335]],

         [[ 0.0890, -0.1218,  0.0824],
          [ 0.1472,  0.0020, -0.0875],
          [-0.1531,  0.1129, -0.2013]],

         [[ 0.1162, -0.0272, -0.0051],
          [ 0.1348, -0.1806,  0.0073],
          [-0.0348,  0.0121, -0.1278]]],


        [[[ 0.0793,  0.1446,  0.1016],
          [-0.0860, -0.1815,  0.2033],
          [-0.1637, -0.0177, -0.0631]],

         [[-0.0209, -0.0889,  0.1461],
          [-0.0004, -0.2065,  0.0483],
          [ 0.0028,  0.0799,  0.0359]],

         [[-0.1584, -0.1998,  0.1324],
          [ 0.1435, -0.1879, -0.0206],
          [ 0.0063, -0.0252,  0.2887]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0765,  0.1210,  0.1591],
          [ 0.0831,  0.1213,  0.1409],
          [ 0.0642,  0.1073,  0.1102]],

         [[ 0.0691,  0.1228,  0.1660],
          [ 0.0851,  0.1281,  0.1462],
          [ 0.0699,  0.1148,  0.1125]],

         [[ 0.0447,  0.1012,  0.1480],
          [ 0.0705,  0.1150,  0.1339],
          [ 0.0619,  0.1027,  0.1005]]],


        [[[-0.0182, -0.0249, -0.0200],
          [ 0.0009,  0.0008,  0.0074],
          [ 0.0054,  0.0049,  0.0115]],

         [[-0.0148, -0.0171, -0.0131],
          [ 0.0010,  0.0051,  0.0130],
          [ 0.0025,  0.0076,  0.0175]],

         [[ 0.0139,  0.0109,  0.0110],
          [ 0.0258,  0.0295,  0.0332],
          [ 0.0280,  0.0326,  0.0382]]],


        [[[-0.0077, -0.0080, -0.0077],
          [-0.0084, -0.0100, -0.0086],
          [-0.0068, -0.0097, -0.0083]],

         [[ 0.0040,  0.0062,  0.0068],
          [ 0.0035,  0.0047,  0.0063],
          [ 0.0052,  0.0051,  0.0069]],

         [[ 0.0107,  0.0136,  0.0141],
          [ 0.0119,  0.0145,  0.0161],
          [ 0.0147,  0.0166,  0.0180]]],


        ...,


        [[[-0.0238, -0.0223, -0.0097],
          [-0.0312, -0.0303, -0.0154],
          [-0.0324, -0.0343, -0.0234]],

         [[-0.0376, -0.0377, -0.0266],
          [-0.0399, -0.0394, -0.0239],
          [-0.0392, -0.0379, -0.0243]],

         [[-0.0244, -0.0268, -0.0194],
          [-0.0275, -0.0269, -0.0169],
          [-0.0215, -0.0193, -0.0121]]],


        [[[-0.0008, -0.0033,  0.0002],
          [-0.0088, -0.0027,  0.0020],
          [-0.0095, -0.0031, -0.0016]],

         [[ 0.0086,  0.0108,  0.0157],
          [-0.0018,  0.0086,  0.0151],
          [-0.0054,  0.0067,  0.0109]],

         [[ 0.0124,  0.0152,  0.0202],
          [ 0.0038,  0.0111,  0.0162],
          [-0.0028,  0.0058,  0.0082]]],


        [[[ 0.0586,  0.0487,  0.0434],
          [ 0.0402,  0.0563,  0.0732],
          [ 0.0564,  0.0797,  0.0989]],

         [[ 0.0848,  0.0664,  0.0464],
          [ 0.0617,  0.0627,  0.0622],
          [ 0.0677,  0.0767,  0.0836]],

         [[-0.0145, -0.0227, -0.0332],
          [-0.0379, -0.0307, -0.0291],
          [-0.0266, -0.0159, -0.0091]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5163]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 84 | Batch_idx: 0 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (1289/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (2458/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (4794/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (5975/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (7140/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (8308/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (9465/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (10629/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (11777/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (12956/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (14133/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (15298/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (16456/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (17615/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (18785/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2587) |  Loss2: (0.0000) | Acc: (91.00%) (19948/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (21102/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (91.00%) (22276/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (23444/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (24594/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (25748/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (26912/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (28043/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (29223/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (30392/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (31549/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (32718/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (33873/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (35040/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (90.00%) (36197/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (37365/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (38521/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (39689/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (40831/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (41990/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (43164/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (90.00%) (44323/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (45443/50000)
# TEST : Loss: (0.4682) | Acc: (85.00%) (8568/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1391,  0.0620,  0.0920],
          [ 0.1080, -0.2831, -0.0999],
          [ 0.2484, -0.0886,  0.0121]],

         [[-0.0030, -0.0972,  0.2565],
          [ 0.0205, -0.0961,  0.0414],
          [ 0.0572, -0.0011, -0.0746]],

         [[ 0.0118, -0.0570,  0.1808],
          [ 0.0959, -0.1940,  0.1531],
          [-0.1128,  0.0231, -0.1944]]],


        [[[ 0.0921, -0.0851, -0.0076],
          [ 0.1341,  0.1678,  0.1791],
          [ 0.0064, -0.0702,  0.1129]],

         [[-0.1610, -0.1877,  0.1568],
          [ 0.0302, -0.0545,  0.1049],
          [ 0.1212, -0.2079,  0.1325]],

         [[ 0.0981, -0.0519, -0.0098],
          [ 0.0127,  0.0221, -0.1129],
          [-0.0320, -0.1598, -0.0136]]],


        [[[-0.0717,  0.0905, -0.0421],
          [-0.1247,  0.0660,  0.1181],
          [ 0.1129,  0.1672,  0.0024]],

         [[ 0.0993,  0.1328,  0.1138],
          [ 0.0699, -0.0393, -0.0360],
          [-0.0462, -0.0301,  0.1408]],

         [[-0.0462,  0.1450,  0.0743],
          [-0.0418, -0.0455, -0.0602],
          [-0.0385, -0.0074,  0.0439]]],


        ...,


        [[[ 0.2253, -0.1140, -0.1160],
          [-0.0462, -0.1098, -0.0971],
          [-0.0938,  0.0243,  0.0644]],

         [[ 0.1681,  0.1957, -0.1132],
          [ 0.2080, -0.1394, -0.1153],
          [-0.0632, -0.0787, -0.0319]],

         [[ 0.1848,  0.1033, -0.0995],
          [-0.0706, -0.1574, -0.0757],
          [ 0.0639,  0.1760, -0.1330]]],


        [[[-0.1056,  0.0915, -0.1451],
          [-0.1402, -0.1420,  0.1209],
          [-0.1540,  0.1547, -0.1341]],

         [[ 0.0902, -0.1208,  0.0835],
          [ 0.1480,  0.0024, -0.0869],
          [-0.1518,  0.1133, -0.2012]],

         [[ 0.1171, -0.0265, -0.0041],
          [ 0.1357, -0.1801,  0.0078],
          [-0.0334,  0.0125, -0.1277]]],


        [[[ 0.0772,  0.1427,  0.1009],
          [-0.0881, -0.1849,  0.2012],
          [-0.1675, -0.0230, -0.0680]],

         [[-0.0244, -0.0909,  0.1467],
          [-0.0046, -0.2094,  0.0471],
          [-0.0030,  0.0754,  0.0316]],

         [[-0.1605, -0.2003,  0.1334],
          [ 0.1410, -0.1885, -0.0201],
          [ 0.0032, -0.0255,  0.2876]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0183,  0.0259,  0.0166],
          [ 0.0377,  0.0222, -0.0045],
          [ 0.0106,  0.0180, -0.0213]],

         [[-0.0094,  0.0056, -0.0032],
          [ 0.0114,  0.0031, -0.0228],
          [-0.0140, -0.0020, -0.0379]],

         [[ 0.0211,  0.0306,  0.0277],
          [ 0.0366,  0.0274,  0.0140],
          [ 0.0192,  0.0301,  0.0086]]],


        [[[-0.0020, -0.0027,  0.0020],
          [-0.0027, -0.0038, -0.0016],
          [-0.0006, -0.0022,  0.0003]],

         [[ 0.0016, -0.0010,  0.0021],
          [ 0.0024,  0.0013,  0.0019],
          [ 0.0063,  0.0043,  0.0034]],

         [[ 0.0006, -0.0022, -0.0003],
          [-0.0000, -0.0016, -0.0013],
          [ 0.0020, -0.0004, -0.0005]]],


        [[[ 0.0011,  0.0002, -0.0025],
          [ 0.0010,  0.0009, -0.0021],
          [ 0.0012,  0.0016, -0.0012]],

         [[ 0.0013,  0.0011, -0.0011],
          [ 0.0010,  0.0017, -0.0007],
          [ 0.0008,  0.0023,  0.0003]],

         [[-0.0034, -0.0039, -0.0054],
          [-0.0035, -0.0031, -0.0049],
          [-0.0032, -0.0021, -0.0034]]],


        ...,


        [[[-0.0210, -0.0271, -0.0209],
          [-0.0018, -0.0145, -0.0158],
          [ 0.0171,  0.0060, -0.0004]],

         [[-0.0262, -0.0339, -0.0289],
          [-0.0101, -0.0235, -0.0207],
          [ 0.0044, -0.0052, -0.0039]],

         [[-0.0134, -0.0213, -0.0183],
          [ 0.0007, -0.0116, -0.0126],
          [ 0.0149,  0.0049,  0.0015]]],


        [[[-0.0021,  0.0017, -0.0002],
          [ 0.0007,  0.0054,  0.0035],
          [-0.0039,  0.0002, -0.0010]],

         [[-0.0038,  0.0021,  0.0024],
          [-0.0012,  0.0048,  0.0038],
          [-0.0059, -0.0004, -0.0013]],

         [[ 0.0018,  0.0054,  0.0047],
          [ 0.0038,  0.0079,  0.0060],
          [-0.0010,  0.0026,  0.0014]]],


        [[[-0.0269, -0.0094, -0.0115],
          [-0.0429, -0.0340, -0.0438],
          [-0.0574, -0.0559, -0.0697]],

         [[ 0.0003,  0.0134,  0.0076],
          [-0.0215, -0.0176, -0.0266],
          [-0.0459, -0.0446, -0.0525]],

         [[-0.0204, -0.0097, -0.0111],
          [-0.0270, -0.0277, -0.0346],
          [-0.0479, -0.0548, -0.0583]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.5161]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.5270) |  Loss2: (0.2818) | Acc: (92.00%) (118/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.5649) |  Loss2: (0.2818) | Acc: (90.00%) (1269/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.6071) |  Loss2: (0.2818) | Acc: (88.00%) (2381/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.6182) |  Loss2: (0.2817) | Acc: (88.00%) (3494/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.6178) |  Loss2: (0.2816) | Acc: (88.00%) (4623/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.6111) |  Loss2: (0.2816) | Acc: (88.00%) (5763/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.6169) |  Loss2: (0.2815) | Acc: (88.00%) (6883/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.6188) |  Loss2: (0.2815) | Acc: (88.00%) (8006/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.6188) |  Loss2: (0.2814) | Acc: (87.00%) (9118/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.6186) |  Loss2: (0.2814) | Acc: (88.00%) (10251/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.6186) |  Loss2: (0.2813) | Acc: (88.00%) (11380/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.6178) |  Loss2: (0.2813) | Acc: (88.00%) (12517/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.6176) |  Loss2: (0.2812) | Acc: (88.00%) (13655/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.6168) |  Loss2: (0.2812) | Acc: (88.00%) (14787/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.6166) |  Loss2: (0.2811) | Acc: (88.00%) (15916/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.6191) |  Loss2: (0.2811) | Acc: (88.00%) (17027/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.6158) |  Loss2: (0.2810) | Acc: (88.00%) (18190/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.6134) |  Loss2: (0.2810) | Acc: (88.00%) (19338/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.6123) |  Loss2: (0.2809) | Acc: (88.00%) (20479/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.6129) |  Loss2: (0.2809) | Acc: (88.00%) (21605/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.6120) |  Loss2: (0.2808) | Acc: (88.00%) (22743/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.6101) |  Loss2: (0.2808) | Acc: (88.00%) (23892/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.6077) |  Loss2: (0.2807) | Acc: (88.00%) (25042/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.6066) |  Loss2: (0.2807) | Acc: (88.00%) (26190/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.6071) |  Loss2: (0.2806) | Acc: (88.00%) (27322/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.6051) |  Loss2: (0.2806) | Acc: (88.00%) (28485/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.6055) |  Loss2: (0.2805) | Acc: (88.00%) (29604/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.6066) |  Loss2: (0.2805) | Acc: (88.00%) (30723/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.6059) |  Loss2: (0.2804) | Acc: (88.00%) (31857/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.6059) |  Loss2: (0.2804) | Acc: (88.00%) (33001/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.6049) |  Loss2: (0.2803) | Acc: (88.00%) (34154/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.6031) |  Loss2: (0.2803) | Acc: (88.00%) (35309/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.6024) |  Loss2: (0.2802) | Acc: (88.00%) (36457/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.6022) |  Loss2: (0.2802) | Acc: (88.00%) (37593/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.6011) |  Loss2: (0.2801) | Acc: (88.00%) (38744/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.6012) |  Loss2: (0.2801) | Acc: (88.00%) (39870/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.6010) |  Loss2: (0.2801) | Acc: (88.00%) (41005/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.6006) |  Loss2: (0.2800) | Acc: (88.00%) (42153/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.5993) |  Loss2: (0.2800) | Acc: (88.00%) (43308/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.5984) |  Loss2: (0.2799) | Acc: (88.00%) (44418/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_085.pth.tar'
# TEST : Loss: (0.4634) | Acc: (85.00%) (8545/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1387,  0.0621,  0.0921],
          [ 0.1076, -0.2834, -0.0997],
          [ 0.2479, -0.0889,  0.0125]],

         [[-0.0016, -0.0960,  0.2575],
          [ 0.0212, -0.0954,  0.0424],
          [ 0.0580, -0.0004, -0.0733]],

         [[ 0.0127, -0.0563,  0.1811],
          [ 0.0963, -0.1936,  0.1533],
          [-0.1121,  0.0234, -0.1938]]],


        [[[ 0.0921, -0.0850, -0.0077],
          [ 0.1340,  0.1677,  0.1789],
          [ 0.0062, -0.0704,  0.1126]],

         [[-0.1611, -0.1877,  0.1565],
          [ 0.0300, -0.0547,  0.1046],
          [ 0.1208, -0.2083,  0.1320]],

         [[ 0.0979, -0.0520, -0.0100],
          [ 0.0125,  0.0218, -0.1132],
          [-0.0323, -0.1602, -0.0141]]],


        [[[-0.0717,  0.0905, -0.0420],
          [-0.1247,  0.0659,  0.1181],
          [ 0.1128,  0.1671,  0.0024]],

         [[ 0.0992,  0.1327,  0.1137],
          [ 0.0698, -0.0393, -0.0360],
          [-0.0463, -0.0302,  0.1407]],

         [[-0.0462,  0.1450,  0.0743],
          [-0.0418, -0.0455, -0.0602],
          [-0.0386, -0.0075,  0.0438]]],


        ...,


        [[[ 0.2253, -0.1136, -0.1157],
          [-0.0464, -0.1097, -0.0970],
          [-0.0941,  0.0242,  0.0643]],

         [[ 0.1681,  0.1960, -0.1129],
          [ 0.2077, -0.1393, -0.1153],
          [-0.0635, -0.0786, -0.0320]],

         [[ 0.1850,  0.1037, -0.0991],
          [-0.0707, -0.1572, -0.0756],
          [ 0.0636,  0.1760, -0.1331]]],


        [[[-0.1056,  0.0914, -0.1450],
          [-0.1402, -0.1420,  0.1207],
          [-0.1540,  0.1546, -0.1341]],

         [[ 0.0903, -0.1206,  0.0836],
          [ 0.1480,  0.0025, -0.0869],
          [-0.1516,  0.1133, -0.2010]],

         [[ 0.1173, -0.0264, -0.0040],
          [ 0.1358, -0.1800,  0.0079],
          [-0.0333,  0.0126, -0.1275]]],


        [[[ 0.0758,  0.1412,  0.0995],
          [-0.0898, -0.1866,  0.1995],
          [-0.1689, -0.0246, -0.0695]],

         [[-0.0259, -0.0922,  0.1455],
          [-0.0064, -0.2111,  0.0456],
          [-0.0047,  0.0736,  0.0297]],

         [[-0.1610, -0.2007,  0.1330],
          [ 0.1399, -0.1893, -0.0208],
          [ 0.0022, -0.0265,  0.2864]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3871e-05,  6.2088e-06,  9.2078e-06],
          [ 1.0759e-05, -2.8340e-05, -9.9685e-06],
          [ 2.4794e-05, -8.8949e-06,  1.2482e-06]],

         [[-1.6283e-07, -9.6014e-06,  2.5755e-05],
          [ 2.1216e-06, -9.5423e-06,  4.2449e-06],
          [ 5.7982e-06, -4.4681e-08, -7.3332e-06]],

         [[ 1.2743e-06, -5.6267e-06,  1.8110e-05],
          [ 9.6309e-06, -1.9360e-05,  1.5334e-05],
          [-1.1209e-05,  2.3421e-06, -1.9385e-05]]],


        [[[ 9.2109e-06, -8.4957e-06, -7.6619e-07],
          [ 1.3401e-05,  1.6768e-05,  1.7891e-05],
          [ 6.2321e-07, -7.0394e-06,  1.1255e-05]],

         [[-1.6108e-05, -1.8766e-05,  1.5654e-05],
          [ 3.0018e-06, -5.4691e-06,  1.0455e-05],
          [ 1.2078e-05, -2.0826e-05,  1.3197e-05]],

         [[ 9.7928e-06, -5.2006e-06, -9.9548e-07],
          [ 1.2499e-06,  2.1834e-06, -1.1319e-05],
          [-3.2338e-06, -1.6018e-05, -1.4075e-06]]],


        [[[-7.1690e-06,  9.0458e-06, -4.2022e-06],
          [-1.2466e-05,  6.5949e-06,  1.1813e-05],
          [ 1.1280e-05,  1.6715e-05,  2.4170e-07]],

         [[ 9.9237e-06,  1.3270e-05,  1.1373e-05],
          [ 6.9841e-06, -3.9323e-06, -3.5970e-06],
          [-4.6283e-06, -3.0165e-06,  1.4071e-05]],

         [[-4.6240e-06,  1.4495e-05,  7.4291e-06],
          [-4.1815e-06, -4.5480e-06, -6.0156e-06],
          [-3.8578e-06, -7.4868e-07,  4.3806e-06]]],


        ...,


        [[[ 2.2532e-05, -1.1362e-05, -1.1568e-05],
          [-4.6353e-06, -1.0965e-05, -9.6982e-06],
          [-9.4141e-06,  2.4200e-06,  6.4300e-06]],

         [[ 1.6808e-05,  1.9604e-05, -1.1287e-05],
          [ 2.0775e-05, -1.3927e-05, -1.1526e-05],
          [-6.3539e-06, -7.8641e-06, -3.2021e-06]],

         [[ 1.8498e-05,  1.0373e-05, -9.9098e-06],
          [-7.0681e-06, -1.5722e-05, -7.5563e-06],
          [ 6.3583e-06,  1.7598e-05, -1.3305e-05]]],


        [[[-1.0558e-05,  9.1427e-06, -1.4505e-05],
          [-1.4019e-05, -1.4201e-05,  1.2074e-05],
          [-1.5398e-05,  1.5459e-05, -1.3406e-05]],

         [[ 9.0314e-06, -1.2064e-05,  8.3630e-06],
          [ 1.4802e-05,  2.4659e-07, -8.6853e-06],
          [-1.5164e-05,  1.1331e-05, -2.0100e-05]],

         [[ 1.1726e-05, -2.6354e-06, -3.9542e-07],
          [ 1.3577e-05, -1.7996e-05,  7.8583e-07],
          [-3.3346e-06,  1.2594e-06, -1.2750e-05]]],


        [[[ 7.5812e-06,  1.4122e-05,  9.9453e-06],
          [-8.9783e-06, -1.8657e-05,  1.9954e-05],
          [-1.6889e-05, -2.4595e-06, -6.9498e-06]],

         [[-2.5852e-06, -9.2240e-06,  1.4547e-05],
          [-6.3949e-07, -2.1106e-05,  4.5555e-06],
          [-4.7233e-07,  7.3553e-06,  2.9724e-06]],

         [[-1.6095e-05, -2.0071e-05,  1.3295e-05],
          [ 1.3990e-05, -1.8929e-05, -2.0812e-06],
          [ 2.2270e-07, -2.6529e-06,  2.8641e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6127]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0583]], device='cuda:0')

Epoch: 86 | Batch_idx: 0 |  Loss: (0.5884) |  Loss2: (0.2782) | Acc: (87.00%) (112/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.5806) |  Loss2: (0.2781) | Acc: (89.00%) (1254/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.5741) |  Loss2: (0.2781) | Acc: (89.00%) (2411/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.5651) |  Loss2: (0.2780) | Acc: (90.00%) (3574/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.5675) |  Loss2: (0.2780) | Acc: (90.00%) (4727/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.5711) |  Loss2: (0.2780) | Acc: (89.00%) (5873/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.5722) |  Loss2: (0.2779) | Acc: (89.00%) (7020/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.5717) |  Loss2: (0.2779) | Acc: (89.00%) (8167/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.5708) |  Loss2: (0.2778) | Acc: (89.00%) (9317/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.5719) |  Loss2: (0.2778) | Acc: (89.00%) (10453/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.5715) |  Loss2: (0.2778) | Acc: (89.00%) (11595/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.5728) |  Loss2: (0.2777) | Acc: (89.00%) (12733/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.5716) |  Loss2: (0.2777) | Acc: (89.00%) (13886/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.5702) |  Loss2: (0.2776) | Acc: (89.00%) (15043/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.5679) |  Loss2: (0.2776) | Acc: (89.00%) (16217/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.5688) |  Loss2: (0.2776) | Acc: (89.00%) (17354/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.5673) |  Loss2: (0.2775) | Acc: (89.00%) (18521/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.5679) |  Loss2: (0.2775) | Acc: (89.00%) (19671/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.5682) |  Loss2: (0.2774) | Acc: (89.00%) (20814/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.5684) |  Loss2: (0.2774) | Acc: (89.00%) (21959/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.5675) |  Loss2: (0.2774) | Acc: (89.00%) (23120/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.5658) |  Loss2: (0.2773) | Acc: (89.00%) (24291/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.5659) |  Loss2: (0.2773) | Acc: (89.00%) (25445/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.5652) |  Loss2: (0.2772) | Acc: (89.00%) (26596/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.5654) |  Loss2: (0.2772) | Acc: (89.00%) (27735/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.5642) |  Loss2: (0.2772) | Acc: (89.00%) (28896/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.5637) |  Loss2: (0.2771) | Acc: (89.00%) (30043/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.5642) |  Loss2: (0.2771) | Acc: (89.00%) (31191/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.5647) |  Loss2: (0.2770) | Acc: (89.00%) (32335/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.5647) |  Loss2: (0.2770) | Acc: (89.00%) (33489/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.5647) |  Loss2: (0.2769) | Acc: (89.00%) (34628/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.5651) |  Loss2: (0.2769) | Acc: (89.00%) (35778/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.5651) |  Loss2: (0.2768) | Acc: (89.00%) (36926/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.5643) |  Loss2: (0.2768) | Acc: (89.00%) (38086/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.5646) |  Loss2: (0.2768) | Acc: (89.00%) (39231/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.5645) |  Loss2: (0.2767) | Acc: (89.00%) (40394/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.5643) |  Loss2: (0.2767) | Acc: (89.00%) (41559/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.5644) |  Loss2: (0.2766) | Acc: (89.00%) (42703/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.5638) |  Loss2: (0.2766) | Acc: (89.00%) (43862/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.5637) |  Loss2: (0.2765) | Acc: (89.00%) (44969/50000)
# TEST : Loss: (0.4376) | Acc: (86.00%) (8602/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1387,  0.0621,  0.0920],
          [ 0.1075, -0.2833, -0.0996],
          [ 0.2478, -0.0889,  0.0125]],

         [[-0.0016, -0.0960,  0.2574],
          [ 0.0212, -0.0954,  0.0424],
          [ 0.0580, -0.0004, -0.0733]],

         [[ 0.0127, -0.0562,  0.1810],
          [ 0.0963, -0.1935,  0.1533],
          [-0.1120,  0.0234, -0.1938]]],


        [[[ 0.0921, -0.0849, -0.0077],
          [ 0.1340,  0.1676,  0.1788],
          [ 0.0062, -0.0704,  0.1125]],

         [[-0.1610, -0.1876,  0.1565],
          [ 0.0300, -0.0547,  0.1045],
          [ 0.1207, -0.2082,  0.1319]],

         [[ 0.0979, -0.0520, -0.0100],
          [ 0.0125,  0.0218, -0.1131],
          [-0.0323, -0.1601, -0.0141]]],


        [[[-0.0717,  0.0904, -0.0420],
          [-0.1246,  0.0659,  0.1181],
          [ 0.1128,  0.1671,  0.0024]],

         [[ 0.0992,  0.1326,  0.1137],
          [ 0.0698, -0.0393, -0.0360],
          [-0.0463, -0.0302,  0.1407]],

         [[-0.0462,  0.1449,  0.0743],
          [-0.0418, -0.0455, -0.0601],
          [-0.0386, -0.0075,  0.0438]]],


        ...,


        [[[ 0.2252, -0.1136, -0.1156],
          [-0.0463, -0.1096, -0.0969],
          [-0.0941,  0.0242,  0.0643]],

         [[ 0.1680,  0.1960, -0.1128],
          [ 0.2077, -0.1392, -0.1152],
          [-0.0635, -0.0786, -0.0320]],

         [[ 0.1849,  0.1037, -0.0991],
          [-0.0707, -0.1572, -0.0755],
          [ 0.0636,  0.1759, -0.1330]]],


        [[[-0.1055,  0.0914, -0.1450],
          [-0.1401, -0.1420,  0.1207],
          [-0.1539,  0.1545, -0.1340]],

         [[ 0.0903, -0.1206,  0.0836],
          [ 0.1480,  0.0025, -0.0868],
          [-0.1516,  0.1133, -0.2009]],

         [[ 0.1172, -0.0263, -0.0040],
          [ 0.1357, -0.1799,  0.0079],
          [-0.0333,  0.0126, -0.1274]]],


        [[[ 0.0758,  0.1412,  0.0994],
          [-0.0897, -0.1865,  0.1995],
          [-0.1688, -0.0246, -0.0695]],

         [[-0.0258, -0.0922,  0.1454],
          [-0.0064, -0.2110,  0.0455],
          [-0.0047,  0.0735,  0.0297]],

         [[-0.1609, -0.2006,  0.1329],
          [ 0.1399, -0.1892, -0.0208],
          [ 0.0022, -0.0265,  0.2863]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3866e-05,  6.2063e-06,  9.2043e-06],
          [ 1.0754e-05, -2.8328e-05, -9.9648e-06],
          [ 2.4784e-05, -8.8914e-06,  1.2477e-06]],

         [[-1.6276e-07, -9.5976e-06,  2.5744e-05],
          [ 2.1208e-06, -9.5385e-06,  4.2433e-06],
          [ 5.7959e-06, -4.4664e-08, -7.3303e-06]],

         [[ 1.2738e-06, -5.6245e-06,  1.8103e-05],
          [ 9.6272e-06, -1.9353e-05,  1.5328e-05],
          [-1.1205e-05,  2.3412e-06, -1.9377e-05]]],


        [[[ 9.2074e-06, -8.4925e-06, -7.6590e-07],
          [ 1.3395e-05,  1.6762e-05,  1.7884e-05],
          [ 6.2298e-07, -7.0368e-06,  1.1251e-05]],

         [[-1.6101e-05, -1.8758e-05,  1.5648e-05],
          [ 3.0006e-06, -5.4669e-06,  1.0451e-05],
          [ 1.2073e-05, -2.0818e-05,  1.3192e-05]],

         [[ 9.7890e-06, -5.1986e-06, -9.9508e-07],
          [ 1.2495e-06,  2.1826e-06, -1.1314e-05],
          [-3.2325e-06, -1.6011e-05, -1.4070e-06]]],


        [[[-7.1660e-06,  9.0423e-06, -4.2006e-06],
          [-1.2461e-05,  6.5923e-06,  1.1809e-05],
          [ 1.1275e-05,  1.6709e-05,  2.4161e-07]],

         [[ 9.9199e-06,  1.3265e-05,  1.1368e-05],
          [ 6.9815e-06, -3.9307e-06, -3.5956e-06],
          [-4.6266e-06, -3.0153e-06,  1.4066e-05]],

         [[-4.6223e-06,  1.4489e-05,  7.4262e-06],
          [-4.1799e-06, -4.5463e-06, -6.0132e-06],
          [-3.8564e-06, -7.4839e-07,  4.3789e-06]]],


        ...,


        [[[ 2.2523e-05, -1.1357e-05, -1.1563e-05],
          [-4.6336e-06, -1.0961e-05, -9.6945e-06],
          [-9.4103e-06,  2.4190e-06,  6.4273e-06]],

         [[ 1.6801e-05,  1.9596e-05, -1.1283e-05],
          [ 2.0767e-05, -1.3921e-05, -1.1522e-05],
          [-6.3513e-06, -7.8608e-06, -3.2008e-06]],

         [[ 1.8491e-05,  1.0369e-05, -9.9060e-06],
          [-7.0655e-06, -1.5716e-05, -7.5534e-06],
          [ 6.3556e-06,  1.7591e-05, -1.3300e-05]]],


        [[[-1.0554e-05,  9.1392e-06, -1.4499e-05],
          [-1.4013e-05, -1.4195e-05,  1.2069e-05],
          [-1.5393e-05,  1.5453e-05, -1.3400e-05]],

         [[ 9.0279e-06, -1.2059e-05,  8.3598e-06],
          [ 1.4796e-05,  2.4649e-07, -8.6818e-06],
          [-1.5158e-05,  1.1327e-05, -2.0092e-05]],

         [[ 1.1721e-05, -2.6344e-06, -3.9527e-07],
          [ 1.3572e-05, -1.7989e-05,  7.8553e-07],
          [-3.3333e-06,  1.2589e-06, -1.2745e-05]]],


        [[[ 7.5783e-06,  1.4117e-05,  9.9415e-06],
          [-8.9748e-06, -1.8650e-05,  1.9946e-05],
          [-1.6882e-05, -2.4586e-06, -6.9471e-06]],

         [[-2.5842e-06, -9.2205e-06,  1.4542e-05],
          [-6.3923e-07, -2.1098e-05,  4.5538e-06],
          [-4.7215e-07,  7.3524e-06,  2.9713e-06]],

         [[-1.6089e-05, -2.0064e-05,  1.3290e-05],
          [ 1.3985e-05, -1.8922e-05, -2.0804e-06],
          [ 2.2261e-07, -2.6519e-06,  2.8630e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6512]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0060]], device='cuda:0')

Epoch: 87 | Batch_idx: 0 |  Loss: (0.5940) |  Loss2: (0.2747) | Acc: (88.00%) (113/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.5428) |  Loss2: (0.2747) | Acc: (89.00%) (1265/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.5413) |  Loss2: (0.2747) | Acc: (90.00%) (2429/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.5344) |  Loss2: (0.2746) | Acc: (90.00%) (3594/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.5318) |  Loss2: (0.2746) | Acc: (90.00%) (4760/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.5339) |  Loss2: (0.2745) | Acc: (90.00%) (5932/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.5361) |  Loss2: (0.2745) | Acc: (90.00%) (7091/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.5380) |  Loss2: (0.2745) | Acc: (90.00%) (8258/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.5380) |  Loss2: (0.2744) | Acc: (90.00%) (9426/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.5389) |  Loss2: (0.2744) | Acc: (90.00%) (10586/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.5406) |  Loss2: (0.2743) | Acc: (90.00%) (11735/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.5432) |  Loss2: (0.2743) | Acc: (90.00%) (12883/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.5448) |  Loss2: (0.2742) | Acc: (90.00%) (14035/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.5447) |  Loss2: (0.2742) | Acc: (90.00%) (15201/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.5466) |  Loss2: (0.2742) | Acc: (90.00%) (16351/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.5460) |  Loss2: (0.2741) | Acc: (90.00%) (17516/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.5469) |  Loss2: (0.2741) | Acc: (90.00%) (18665/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.5461) |  Loss2: (0.2740) | Acc: (90.00%) (19827/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.5453) |  Loss2: (0.2740) | Acc: (90.00%) (20990/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.5437) |  Loss2: (0.2739) | Acc: (90.00%) (22168/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.5453) |  Loss2: (0.2739) | Acc: (90.00%) (23301/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.5461) |  Loss2: (0.2739) | Acc: (90.00%) (24447/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.5480) |  Loss2: (0.2738) | Acc: (90.00%) (25588/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.5470) |  Loss2: (0.2738) | Acc: (90.00%) (26755/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.5480) |  Loss2: (0.2737) | Acc: (90.00%) (27916/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.5476) |  Loss2: (0.2737) | Acc: (90.00%) (29077/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.5475) |  Loss2: (0.2736) | Acc: (90.00%) (30232/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.5474) |  Loss2: (0.2736) | Acc: (90.00%) (31398/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.5476) |  Loss2: (0.2736) | Acc: (90.00%) (32549/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.5473) |  Loss2: (0.2735) | Acc: (90.00%) (33714/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.5467) |  Loss2: (0.2735) | Acc: (90.00%) (34893/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.5461) |  Loss2: (0.2734) | Acc: (90.00%) (36058/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.5463) |  Loss2: (0.2734) | Acc: (90.00%) (37208/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.5469) |  Loss2: (0.2734) | Acc: (90.00%) (38351/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.5461) |  Loss2: (0.2733) | Acc: (90.00%) (39530/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.5476) |  Loss2: (0.2733) | Acc: (90.00%) (40660/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.5476) |  Loss2: (0.2732) | Acc: (90.00%) (41811/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.5484) |  Loss2: (0.2732) | Acc: (90.00%) (42953/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.5496) |  Loss2: (0.2732) | Acc: (90.00%) (44085/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.5493) |  Loss2: (0.2731) | Acc: (90.00%) (45196/50000)
# TEST : Loss: (0.4312) | Acc: (86.00%) (8600/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1386,  0.0620,  0.0920],
          [ 0.1075, -0.2832, -0.0996],
          [ 0.2477, -0.0889,  0.0125]],

         [[-0.0016, -0.0959,  0.2573],
          [ 0.0212, -0.0953,  0.0424],
          [ 0.0579, -0.0004, -0.0733]],

         [[ 0.0127, -0.0562,  0.1810],
          [ 0.0962, -0.1935,  0.1532],
          [-0.1120,  0.0234, -0.1937]]],


        [[[ 0.0920, -0.0849, -0.0077],
          [ 0.1339,  0.1676,  0.1788],
          [ 0.0062, -0.0703,  0.1125]],

         [[-0.1609, -0.1875,  0.1564],
          [ 0.0300, -0.0546,  0.1045],
          [ 0.1207, -0.2081,  0.1319]],

         [[ 0.0979, -0.0520, -0.0099],
          [ 0.0125,  0.0218, -0.1131],
          [-0.0323, -0.1600, -0.0141]]],


        [[[-0.0716,  0.0904, -0.0420],
          [-0.1246,  0.0659,  0.1180],
          [ 0.1127,  0.1670,  0.0024]],

         [[ 0.0992,  0.1326,  0.1136],
          [ 0.0698, -0.0393, -0.0359],
          [-0.0462, -0.0301,  0.1406]],

         [[-0.0462,  0.1448,  0.0742],
          [-0.0418, -0.0454, -0.0601],
          [-0.0385, -0.0075,  0.0438]]],


        ...,


        [[[ 0.2251, -0.1135, -0.1156],
          [-0.0463, -0.1096, -0.0969],
          [-0.0941,  0.0242,  0.0642]],

         [[ 0.1679,  0.1959, -0.1128],
          [ 0.2076, -0.1392, -0.1152],
          [-0.0635, -0.0786, -0.0320]],

         [[ 0.1848,  0.1036, -0.0990],
          [-0.0706, -0.1571, -0.0755],
          [ 0.0635,  0.1758, -0.1329]]],


        [[[-0.1055,  0.0914, -0.1449],
          [-0.1401, -0.1419,  0.1206],
          [-0.1539,  0.1545, -0.1340]],

         [[ 0.0902, -0.1205,  0.0836],
          [ 0.1479,  0.0025, -0.0868],
          [-0.1515,  0.1132, -0.2008]],

         [[ 0.1172, -0.0263, -0.0040],
          [ 0.1357, -0.1798,  0.0079],
          [-0.0333,  0.0126, -0.1274]]],


        [[[ 0.0758,  0.1411,  0.0994],
          [-0.0897, -0.1864,  0.1994],
          [-0.1688, -0.0246, -0.0694]],

         [[-0.0258, -0.0922,  0.1454],
          [-0.0064, -0.2109,  0.0455],
          [-0.0047,  0.0735,  0.0297]],

         [[-0.1608, -0.2006,  0.1328],
          [ 0.1398, -0.1891, -0.0208],
          [ 0.0022, -0.0265,  0.2862]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3861e-05,  6.2038e-06,  9.2008e-06],
          [ 1.0750e-05, -2.8316e-05, -9.9610e-06],
          [ 2.4774e-05, -8.8879e-06,  1.2472e-06]],

         [[-1.6270e-07, -9.5939e-06,  2.5734e-05],
          [ 2.1200e-06, -9.5347e-06,  4.2417e-06],
          [ 5.7936e-06, -4.4647e-08, -7.3274e-06]],

         [[ 1.2733e-06, -5.6223e-06,  1.8096e-05],
          [ 9.6234e-06, -1.9345e-05,  1.5322e-05],
          [-1.1200e-05,  2.3402e-06, -1.9370e-05]]],


        [[[ 9.2039e-06, -8.4893e-06, -7.6561e-07],
          [ 1.3390e-05,  1.6756e-05,  1.7877e-05],
          [ 6.2274e-07, -7.0342e-06,  1.1247e-05]],

         [[-1.6095e-05, -1.8751e-05,  1.5642e-05],
          [ 2.9995e-06, -5.4647e-06,  1.0447e-05],
          [ 1.2069e-05, -2.0810e-05,  1.3186e-05]],

         [[ 9.7852e-06, -5.1965e-06, -9.9468e-07],
          [ 1.2490e-06,  2.1817e-06, -1.1310e-05],
          [-3.2312e-06, -1.6005e-05, -1.4064e-06]]],


        [[[-7.1631e-06,  9.0388e-06, -4.1990e-06],
          [-1.2456e-05,  6.5896e-06,  1.1804e-05],
          [ 1.1271e-05,  1.6702e-05,  2.4152e-07]],

         [[ 9.9161e-06,  1.3260e-05,  1.1364e-05],
          [ 6.9789e-06, -3.9291e-06, -3.5941e-06],
          [-4.6248e-06, -3.0141e-06,  1.4060e-05]],

         [[-4.6205e-06,  1.4484e-05,  7.4233e-06],
          [-4.1783e-06, -4.5445e-06, -6.0109e-06],
          [-3.8549e-06, -7.4810e-07,  4.3771e-06]]],


        ...,


        [[[ 2.2514e-05, -1.1353e-05, -1.1558e-05],
          [-4.6319e-06, -1.0957e-05, -9.6907e-06],
          [-9.4065e-06,  2.4181e-06,  6.4247e-06]],

         [[ 1.6795e-05,  1.9589e-05, -1.1278e-05],
          [ 2.0758e-05, -1.3916e-05, -1.1518e-05],
          [-6.3487e-06, -7.8576e-06, -3.1995e-06]],

         [[ 1.8484e-05,  1.0365e-05, -9.9022e-06],
          [-7.0629e-06, -1.5709e-05, -7.5505e-06],
          [ 6.3530e-06,  1.7584e-05, -1.3295e-05]]],


        [[[-1.0550e-05,  9.1357e-06, -1.4493e-05],
          [-1.4008e-05, -1.4189e-05,  1.2064e-05],
          [-1.5387e-05,  1.5447e-05, -1.3395e-05]],

         [[ 9.0244e-06, -1.2054e-05,  8.3566e-06],
          [ 1.4790e-05,  2.4639e-07, -8.6783e-06],
          [-1.5153e-05,  1.1323e-05, -2.0084e-05]],

         [[ 1.1717e-05, -2.6334e-06, -3.9513e-07],
          [ 1.3567e-05, -1.7982e-05,  7.8524e-07],
          [-3.3320e-06,  1.2584e-06, -1.2740e-05]]],


        [[[ 7.5754e-06,  1.4112e-05,  9.9378e-06],
          [-8.9713e-06, -1.8642e-05,  1.9939e-05],
          [-1.6876e-05, -2.4576e-06, -6.9445e-06]],

         [[-2.5832e-06, -9.2170e-06,  1.4536e-05],
          [-6.3898e-07, -2.1089e-05,  4.5520e-06],
          [-4.7197e-07,  7.3495e-06,  2.9701e-06]],

         [[-1.6082e-05, -2.0056e-05,  1.3285e-05],
          [ 1.3980e-05, -1.8914e-05, -2.0796e-06],
          [ 2.2252e-07, -2.6508e-06,  2.8618e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6698]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0088]], device='cuda:0')

Epoch: 88 | Batch_idx: 0 |  Loss: (0.5464) |  Loss2: (0.2716) | Acc: (91.00%) (117/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.5516) |  Loss2: (0.2715) | Acc: (90.00%) (1272/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.5478) |  Loss2: (0.2715) | Acc: (90.00%) (2437/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.5557) |  Loss2: (0.2715) | Acc: (89.00%) (3569/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.5548) |  Loss2: (0.2714) | Acc: (89.00%) (4719/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.5527) |  Loss2: (0.2714) | Acc: (89.00%) (5874/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.5501) |  Loss2: (0.2714) | Acc: (90.00%) (7041/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.5528) |  Loss2: (0.2713) | Acc: (90.00%) (8183/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.5538) |  Loss2: (0.2713) | Acc: (89.00%) (9326/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.5533) |  Loss2: (0.2712) | Acc: (89.00%) (10479/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.5540) |  Loss2: (0.2712) | Acc: (89.00%) (11632/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.5514) |  Loss2: (0.2712) | Acc: (90.00%) (12799/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.5503) |  Loss2: (0.2711) | Acc: (90.00%) (13959/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.5486) |  Loss2: (0.2711) | Acc: (90.00%) (15121/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.5502) |  Loss2: (0.2710) | Acc: (90.00%) (16270/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.5490) |  Loss2: (0.2710) | Acc: (90.00%) (17438/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.5478) |  Loss2: (0.2710) | Acc: (90.00%) (18597/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.5479) |  Loss2: (0.2709) | Acc: (90.00%) (19754/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.5465) |  Loss2: (0.2709) | Acc: (90.00%) (20924/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.5453) |  Loss2: (0.2708) | Acc: (90.00%) (22085/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.5451) |  Loss2: (0.2708) | Acc: (90.00%) (23241/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.5438) |  Loss2: (0.2708) | Acc: (90.00%) (24401/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.5437) |  Loss2: (0.2707) | Acc: (90.00%) (25571/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.5442) |  Loss2: (0.2707) | Acc: (90.00%) (26722/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.5438) |  Loss2: (0.2706) | Acc: (90.00%) (27893/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.5428) |  Loss2: (0.2706) | Acc: (90.00%) (29060/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.5427) |  Loss2: (0.2706) | Acc: (90.00%) (30221/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.5412) |  Loss2: (0.2705) | Acc: (90.00%) (31396/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.5397) |  Loss2: (0.2705) | Acc: (90.00%) (32586/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.5407) |  Loss2: (0.2704) | Acc: (90.00%) (33718/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.5398) |  Loss2: (0.2704) | Acc: (90.00%) (34890/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.5399) |  Loss2: (0.2704) | Acc: (90.00%) (36053/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.5401) |  Loss2: (0.2703) | Acc: (90.00%) (37216/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.5397) |  Loss2: (0.2703) | Acc: (90.00%) (38390/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.5398) |  Loss2: (0.2702) | Acc: (90.00%) (39544/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.5389) |  Loss2: (0.2702) | Acc: (90.00%) (40719/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.5386) |  Loss2: (0.2702) | Acc: (90.00%) (41886/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.5377) |  Loss2: (0.2701) | Acc: (90.00%) (43052/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.5376) |  Loss2: (0.2701) | Acc: (90.00%) (44215/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.5378) |  Loss2: (0.2700) | Acc: (90.00%) (45325/50000)
# TEST : Loss: (0.4237) | Acc: (86.00%) (8636/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1386,  0.0620,  0.0920],
          [ 0.1075, -0.2831, -0.0996],
          [ 0.2476, -0.0888,  0.0125]],

         [[-0.0016, -0.0959,  0.2572],
          [ 0.0212, -0.0953,  0.0424],
          [ 0.0579, -0.0004, -0.0732]],

         [[ 0.0127, -0.0562,  0.1809],
          [ 0.0962, -0.1934,  0.1532],
          [-0.1120,  0.0234, -0.1936]]],


        [[[ 0.0920, -0.0849, -0.0077],
          [ 0.1338,  0.1675,  0.1787],
          [ 0.0062, -0.0703,  0.1124]],

         [[-0.1609, -0.1874,  0.1564],
          [ 0.0300, -0.0546,  0.1044],
          [ 0.1206, -0.2080,  0.1318]],

         [[ 0.0978, -0.0519, -0.0099],
          [ 0.0125,  0.0218, -0.1131],
          [-0.0323, -0.1600, -0.0141]]],


        [[[-0.0716,  0.0904, -0.0420],
          [-0.1245,  0.0659,  0.1180],
          [ 0.1127,  0.1670,  0.0024]],

         [[ 0.0991,  0.1325,  0.1136],
          [ 0.0698, -0.0393, -0.0359],
          [-0.0462, -0.0301,  0.1406]],

         [[-0.0462,  0.1448,  0.0742],
          [-0.0418, -0.0454, -0.0601],
          [-0.0385, -0.0075,  0.0438]]],


        ...,


        [[[ 0.2251, -0.1135, -0.1155],
          [-0.0463, -0.1095, -0.0969],
          [-0.0940,  0.0242,  0.0642]],

         [[ 0.1679,  0.1958, -0.1127],
          [ 0.2075, -0.1391, -0.1151],
          [-0.0635, -0.0785, -0.0320]],

         [[ 0.1848,  0.1036, -0.0990],
          [-0.0706, -0.1570, -0.0755],
          [ 0.0635,  0.1758, -0.1329]]],


        [[[-0.1055,  0.0913, -0.1449],
          [-0.1400, -0.1418,  0.1206],
          [-0.1538,  0.1544, -0.1339]],

         [[ 0.0902, -0.1205,  0.0835],
          [ 0.1478,  0.0025, -0.0867],
          [-0.1515,  0.1132, -0.2008]],

         [[ 0.1171, -0.0263, -0.0039],
          [ 0.1356, -0.1797,  0.0078],
          [-0.0333,  0.0126, -0.1273]]],


        [[[ 0.0757,  0.1411,  0.0993],
          [-0.0897, -0.1863,  0.1993],
          [-0.1687, -0.0246, -0.0694]],

         [[-0.0258, -0.0921,  0.1453],
          [-0.0064, -0.2108,  0.0455],
          [-0.0047,  0.0735,  0.0297]],

         [[-0.1608, -0.2005,  0.1328],
          [ 0.1397, -0.1891, -0.0208],
          [ 0.0022, -0.0265,  0.2861]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3856e-05,  6.2013e-06,  9.1973e-06],
          [ 1.0746e-05, -2.8306e-05, -9.9572e-06],
          [ 2.4764e-05, -8.8844e-06,  1.2468e-06]],

         [[-1.6263e-07, -9.5901e-06,  2.5724e-05],
          [ 2.1192e-06, -9.5309e-06,  4.2401e-06],
          [ 5.7912e-06, -4.4630e-08, -7.3245e-06]],

         [[ 1.2728e-06, -5.6201e-06,  1.8089e-05],
          [ 9.6196e-06, -1.9338e-05,  1.5317e-05],
          [-1.1196e-05,  2.3393e-06, -1.9362e-05]]],


        [[[ 9.2004e-06, -8.4861e-06, -7.6532e-07],
          [ 1.3385e-05,  1.6749e-05,  1.7870e-05],
          [ 6.2250e-07, -7.0315e-06,  1.1242e-05]],

         [[-1.6088e-05, -1.8743e-05,  1.5636e-05],
          [ 2.9983e-06, -5.4625e-06,  1.0443e-05],
          [ 1.2064e-05, -2.0802e-05,  1.3181e-05]],

         [[ 9.7814e-06, -5.1945e-06, -9.9428e-07],
          [ 1.2485e-06,  2.1808e-06, -1.1305e-05],
          [-3.2298e-06, -1.5999e-05, -1.4059e-06]]],


        [[[-7.1602e-06,  9.0353e-06, -4.1974e-06],
          [-1.2451e-05,  6.5870e-06,  1.1799e-05],
          [ 1.1267e-05,  1.6696e-05,  2.4143e-07]],

         [[ 9.9123e-06,  1.3254e-05,  1.1360e-05],
          [ 6.9762e-06, -3.9275e-06, -3.5926e-06],
          [-4.6231e-06, -3.0130e-06,  1.4055e-05]],

         [[-4.6188e-06,  1.4478e-05,  7.4204e-06],
          [-4.1767e-06, -4.5428e-06, -6.0086e-06],
          [-3.8535e-06, -7.4781e-07,  4.3754e-06]]],


        ...,


        [[[ 2.2506e-05, -1.1348e-05, -1.1554e-05],
          [-4.6301e-06, -1.0952e-05, -9.6869e-06],
          [-9.4027e-06,  2.4171e-06,  6.4221e-06]],

         [[ 1.6788e-05,  1.9581e-05, -1.1274e-05],
          [ 2.0750e-05, -1.3911e-05, -1.1513e-05],
          [-6.3461e-06, -7.8544e-06, -3.1982e-06]],

         [[ 1.8477e-05,  1.0361e-05, -9.8984e-06],
          [-7.0603e-06, -1.5703e-05, -7.5476e-06],
          [ 6.3504e-06,  1.7577e-05, -1.3289e-05]]],


        [[[-1.0546e-05,  9.1322e-06, -1.4487e-05],
          [-1.4003e-05, -1.4183e-05,  1.2060e-05],
          [-1.5381e-05,  1.5441e-05, -1.3390e-05]],

         [[ 9.0209e-06, -1.2050e-05,  8.3534e-06],
          [ 1.4784e-05,  2.4629e-07, -8.6748e-06],
          [-1.5147e-05,  1.1318e-05, -2.0077e-05]],

         [[ 1.1712e-05, -2.6323e-06, -3.9498e-07],
          [ 1.3561e-05, -1.7975e-05,  7.8495e-07],
          [-3.3307e-06,  1.2579e-06, -1.2734e-05]]],


        [[[ 7.5725e-06,  1.4107e-05,  9.9340e-06],
          [-8.9678e-06, -1.8634e-05,  1.9931e-05],
          [-1.6869e-05, -2.4567e-06, -6.9419e-06]],

         [[-2.5821e-06, -9.2135e-06,  1.4530e-05],
          [-6.3872e-07, -2.1081e-05,  4.5503e-06],
          [-4.7178e-07,  7.3465e-06,  2.9689e-06]],

         [[-1.6076e-05, -2.0048e-05,  1.3280e-05],
          [ 1.3975e-05, -1.8906e-05, -2.0788e-06],
          [ 2.2243e-07, -2.6498e-06,  2.8606e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6885]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0717]], device='cuda:0')

Epoch: 89 | Batch_idx: 0 |  Loss: (0.4798) |  Loss2: (0.2685) | Acc: (92.00%) (118/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.5177) |  Loss2: (0.2685) | Acc: (91.00%) (1284/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.5254) |  Loss2: (0.2684) | Acc: (90.00%) (2445/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.5297) |  Loss2: (0.2684) | Acc: (90.00%) (3600/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.5273) |  Loss2: (0.2683) | Acc: (90.00%) (4772/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.5285) |  Loss2: (0.2683) | Acc: (90.00%) (5928/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.5338) |  Loss2: (0.2683) | Acc: (90.00%) (7084/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.5341) |  Loss2: (0.2683) | Acc: (90.00%) (8244/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.5357) |  Loss2: (0.2682) | Acc: (90.00%) (9405/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.5335) |  Loss2: (0.2682) | Acc: (90.00%) (10573/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.5335) |  Loss2: (0.2682) | Acc: (90.00%) (11730/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.5324) |  Loss2: (0.2681) | Acc: (90.00%) (12893/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.5315) |  Loss2: (0.2681) | Acc: (90.00%) (14062/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.5331) |  Loss2: (0.2681) | Acc: (90.00%) (15222/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.5320) |  Loss2: (0.2680) | Acc: (90.00%) (16396/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.5325) |  Loss2: (0.2680) | Acc: (90.00%) (17549/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.5317) |  Loss2: (0.2680) | Acc: (90.00%) (18723/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.5330) |  Loss2: (0.2679) | Acc: (90.00%) (19878/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.5321) |  Loss2: (0.2679) | Acc: (90.00%) (21049/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.5318) |  Loss2: (0.2679) | Acc: (90.00%) (22218/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.5332) |  Loss2: (0.2678) | Acc: (90.00%) (23355/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.5348) |  Loss2: (0.2678) | Acc: (90.00%) (24496/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.5338) |  Loss2: (0.2678) | Acc: (90.00%) (25669/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.5357) |  Loss2: (0.2677) | Acc: (90.00%) (26810/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.5355) |  Loss2: (0.2677) | Acc: (90.00%) (27974/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.5370) |  Loss2: (0.2677) | Acc: (90.00%) (29107/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.5368) |  Loss2: (0.2676) | Acc: (90.00%) (30272/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.5364) |  Loss2: (0.2676) | Acc: (90.00%) (31432/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.5364) |  Loss2: (0.2675) | Acc: (90.00%) (32593/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.5355) |  Loss2: (0.2675) | Acc: (90.00%) (33767/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.5350) |  Loss2: (0.2675) | Acc: (90.00%) (34928/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.5349) |  Loss2: (0.2674) | Acc: (90.00%) (36084/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.5340) |  Loss2: (0.2674) | Acc: (90.00%) (37252/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.5335) |  Loss2: (0.2674) | Acc: (90.00%) (38418/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.5331) |  Loss2: (0.2673) | Acc: (90.00%) (39589/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.5332) |  Loss2: (0.2673) | Acc: (90.00%) (40751/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.5335) |  Loss2: (0.2673) | Acc: (90.00%) (41908/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.5336) |  Loss2: (0.2672) | Acc: (90.00%) (43073/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.5336) |  Loss2: (0.2672) | Acc: (90.00%) (44237/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.5336) |  Loss2: (0.2671) | Acc: (90.00%) (45352/50000)
# TEST : Loss: (0.4150) | Acc: (86.00%) (8654/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1385,  0.0620,  0.0919],
          [ 0.1074, -0.2829, -0.0995],
          [ 0.2475, -0.0888,  0.0125]],

         [[-0.0016, -0.0959,  0.2571],
          [ 0.0212, -0.0953,  0.0424],
          [ 0.0579, -0.0004, -0.0732]],

         [[ 0.0127, -0.0562,  0.1808],
          [ 0.0962, -0.1933,  0.1531],
          [-0.1119,  0.0234, -0.1935]]],


        [[[ 0.0920, -0.0848, -0.0077],
          [ 0.1338,  0.1674,  0.1786],
          [ 0.0062, -0.0703,  0.1124]],

         [[-0.1608, -0.1874,  0.1563],
          [ 0.0300, -0.0546,  0.1044],
          [ 0.1206, -0.2079,  0.1318]],

         [[ 0.0978, -0.0519, -0.0099],
          [ 0.0125,  0.0218, -0.1130],
          [-0.0323, -0.1599, -0.0141]]],


        [[[-0.0716,  0.0903, -0.0420],
          [-0.1245,  0.0658,  0.1179],
          [ 0.1126,  0.1669,  0.0024]],

         [[ 0.0991,  0.1325,  0.1136],
          [ 0.0697, -0.0393, -0.0359],
          [-0.0462, -0.0301,  0.1405]],

         [[-0.0462,  0.1447,  0.0742],
          [-0.0418, -0.0454, -0.0601],
          [-0.0385, -0.0075,  0.0437]]],


        ...,


        [[[ 0.2250, -0.1134, -0.1155],
          [-0.0463, -0.1095, -0.0968],
          [-0.0940,  0.0242,  0.0642]],

         [[ 0.1678,  0.1957, -0.1127],
          [ 0.2074, -0.1391, -0.1151],
          [-0.0634, -0.0785, -0.0320]],

         [[ 0.1847,  0.1036, -0.0989],
          [-0.0706, -0.1570, -0.0754],
          [ 0.0635,  0.1757, -0.1328]]],


        [[[-0.1054,  0.0913, -0.1448],
          [-0.1400, -0.1418,  0.1205],
          [-0.1538,  0.1544, -0.1338]],

         [[ 0.0902, -0.1205,  0.0835],
          [ 0.1478,  0.0025, -0.0867],
          [-0.1514,  0.1131, -0.2007]],

         [[ 0.1171, -0.0263, -0.0039],
          [ 0.1356, -0.1797,  0.0078],
          [-0.0333,  0.0126, -0.1273]]],


        [[[ 0.0757,  0.1410,  0.0993],
          [-0.0896, -0.1863,  0.1992],
          [-0.1686, -0.0246, -0.0694]],

         [[-0.0258, -0.0921,  0.1452],
          [-0.0064, -0.2107,  0.0455],
          [-0.0047,  0.0734,  0.0297]],

         [[-0.1607, -0.2004,  0.1327],
          [ 0.1397, -0.1890, -0.0208],
          [ 0.0022, -0.0265,  0.2859]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3850e-05,  6.1989e-06,  9.1938e-06],
          [ 1.0742e-05, -2.8295e-05, -9.9534e-06],
          [ 2.4754e-05, -8.8809e-06,  1.2463e-06]],

         [[-1.6257e-07, -9.5863e-06,  2.5713e-05],
          [ 2.1184e-06, -9.5272e-06,  4.2385e-06],
          [ 5.7889e-06, -4.4613e-08, -7.3216e-06]],

         [[ 1.2723e-06, -5.6179e-06,  1.8082e-05],
          [ 9.6158e-06, -1.9330e-05,  1.5311e-05],
          [-1.1192e-05,  2.3383e-06, -1.9355e-05]]],


        [[[ 9.1969e-06, -8.4829e-06, -7.6503e-07],
          [ 1.3380e-05,  1.6743e-05,  1.7863e-05],
          [ 6.2227e-07, -7.0289e-06,  1.1238e-05]],

         [[-1.6082e-05, -1.8736e-05,  1.5630e-05],
          [ 2.9971e-06, -5.4603e-06,  1.0439e-05],
          [ 1.2059e-05, -2.0794e-05,  1.3176e-05]],

         [[ 9.7776e-06, -5.1924e-06, -9.9388e-07],
          [ 1.2480e-06,  2.1799e-06, -1.1301e-05],
          [-3.2285e-06, -1.5992e-05, -1.4053e-06]]],


        [[[-7.1573e-06,  9.0318e-06, -4.1958e-06],
          [-1.2446e-05,  6.5844e-06,  1.1795e-05],
          [ 1.1262e-05,  1.6689e-05,  2.4134e-07]],

         [[ 9.9085e-06,  1.3249e-05,  1.1355e-05],
          [ 6.9736e-06, -3.9259e-06, -3.5912e-06],
          [-4.6213e-06, -3.0118e-06,  1.4050e-05]],

         [[-4.6170e-06,  1.4472e-05,  7.4175e-06],
          [-4.1751e-06, -4.5410e-06, -6.0062e-06],
          [-3.8520e-06, -7.4752e-07,  4.3736e-06]]],


        ...,


        [[[ 2.2497e-05, -1.1344e-05, -1.1549e-05],
          [-4.6284e-06, -1.0948e-05, -9.6831e-06],
          [-9.3989e-06,  2.4162e-06,  6.4195e-06]],

         [[ 1.6782e-05,  1.9574e-05, -1.1270e-05],
          [ 2.0742e-05, -1.3906e-05, -1.1509e-05],
          [-6.3434e-06, -7.8512e-06, -3.1969e-06]],

         [[ 1.8470e-05,  1.0357e-05, -9.8946e-06],
          [-7.0577e-06, -1.5697e-05, -7.5446e-06],
          [ 6.3478e-06,  1.7570e-05, -1.3284e-05]]],


        [[[-1.0542e-05,  9.1287e-06, -1.4481e-05],
          [-1.3998e-05, -1.4178e-05,  1.2055e-05],
          [-1.5375e-05,  1.5435e-05, -1.3385e-05]],

         [[ 9.0174e-06, -1.2045e-05,  8.3502e-06],
          [ 1.4778e-05,  2.4619e-07, -8.6713e-06],
          [-1.5141e-05,  1.1314e-05, -2.0069e-05]],

         [[ 1.1707e-05, -2.6313e-06, -3.9483e-07],
          [ 1.3556e-05, -1.7968e-05,  7.8466e-07],
          [-3.3294e-06,  1.2574e-06, -1.2729e-05]]],


        [[[ 7.5696e-06,  1.4101e-05,  9.9302e-06],
          [-8.9643e-06, -1.8627e-05,  1.9923e-05],
          [-1.6863e-05, -2.4557e-06, -6.9393e-06]],

         [[-2.5811e-06, -9.2100e-06,  1.4524e-05],
          [-6.3847e-07, -2.1073e-05,  4.5485e-06],
          [-4.7160e-07,  7.3436e-06,  2.9678e-06]],

         [[-1.6070e-05, -2.0041e-05,  1.3275e-05],
          [ 1.3969e-05, -1.8899e-05, -2.0780e-06],
          [ 2.2234e-07, -2.6488e-06,  2.8595e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6991]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0273]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (2423/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (89.00%) (3566/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (4727/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (5882/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (7050/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (8197/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (9358/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (10533/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (11685/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (12860/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (14029/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (15184/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (16340/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (17495/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (18663/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (19807/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (20959/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (22113/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (23264/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (24442/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (25587/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (26746/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (90.00%) (27896/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (29045/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (30211/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (31376/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (32540/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (33706/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (34875/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (36039/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (37186/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (38350/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2686) |  Loss2: (0.0000) | Acc: (90.00%) (39521/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (40683/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (41846/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (43006/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (44167/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (45296/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_090.pth.tar'
# TEST : Loss: (0.4285) | Acc: (85.00%) (8599/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1392,  0.0582,  0.0889],
          [ 0.1071, -0.2874, -0.1018],
          [ 0.2471, -0.0907,  0.0108]],

         [[-0.0019, -0.0979,  0.2559],
          [ 0.0217, -0.0979,  0.0422],
          [ 0.0582, -0.0014, -0.0738]],

         [[ 0.0152, -0.0542,  0.1839],
          [ 0.0990, -0.1923,  0.1567],
          [-0.1098,  0.0248, -0.1909]]],


        [[[ 0.0956, -0.0806, -0.0051],
          [ 0.1359,  0.1700,  0.1796],
          [ 0.0082, -0.0681,  0.1131]],

         [[-0.1579, -0.1842,  0.1581],
          [ 0.0312, -0.0530,  0.1046],
          [ 0.1220, -0.2064,  0.1321]],

         [[ 0.0992, -0.0504, -0.0095],
          [ 0.0127,  0.0221, -0.1137],
          [-0.0317, -0.1594, -0.0145]]],


        [[[-0.0712,  0.0901, -0.0423],
          [-0.1241,  0.0657,  0.1177],
          [ 0.1130,  0.1669,  0.0022]],

         [[ 0.0999,  0.1327,  0.1136],
          [ 0.0703, -0.0390, -0.0357],
          [-0.0456, -0.0300,  0.1404]],

         [[-0.0454,  0.1448,  0.0741],
          [-0.0411, -0.0453, -0.0600],
          [-0.0380, -0.0074,  0.0435]]],


        ...,


        [[[ 0.2228, -0.1165, -0.1179],
          [-0.0486, -0.1115, -0.0979],
          [-0.0954,  0.0230,  0.0643]],

         [[ 0.1682,  0.1954, -0.1128],
          [ 0.2078, -0.1385, -0.1143],
          [-0.0618, -0.0768, -0.0298]],

         [[ 0.1850,  0.1039, -0.0984],
          [-0.0695, -0.1556, -0.0744],
          [ 0.0655,  0.1779, -0.1305]]],


        [[[-0.1051,  0.0903, -0.1468],
          [-0.1394, -0.1423,  0.1190],
          [-0.1526,  0.1547, -0.1342]],

         [[ 0.0903, -0.1210,  0.0820],
          [ 0.1480,  0.0019, -0.0879],
          [-0.1502,  0.1136, -0.2007]],

         [[ 0.1175, -0.0261, -0.0043],
          [ 0.1361, -0.1795,  0.0075],
          [-0.0320,  0.0136, -0.1267]]],


        [[[ 0.0797,  0.1478,  0.1065],
          [-0.0829, -0.1792,  0.2084],
          [-0.1649, -0.0193, -0.0630]],

         [[-0.0238, -0.0882,  0.1494],
          [-0.0007, -0.2059,  0.0515],
          [-0.0011,  0.0769,  0.0328]],

         [[-0.1619, -0.2014,  0.1315],
          [ 0.1424, -0.1874, -0.0177],
          [ 0.0048, -0.0237,  0.2883]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.2098, -0.2051, -0.2244],
          [-0.1779, -0.1880, -0.2239],
          [-0.1150, -0.1458, -0.2054]],

         [[-0.1134, -0.1177, -0.1464],
          [-0.0712, -0.0891, -0.1357],
          [-0.0008, -0.0370, -0.1036]],

         [[ 0.0306,  0.0279,  0.0108],
          [ 0.0726,  0.0583,  0.0193],
          [ 0.1367,  0.1072,  0.0484]]],


        [[[ 0.0052,  0.0119,  0.0126],
          [ 0.0066,  0.0108,  0.0090],
          [-0.0049,  0.0009,  0.0042]],

         [[ 0.0143,  0.0200,  0.0196],
          [ 0.0157,  0.0186,  0.0156],
          [ 0.0032,  0.0080,  0.0100]],

         [[ 0.0140,  0.0182,  0.0164],
          [ 0.0151,  0.0177,  0.0142],
          [ 0.0037,  0.0093,  0.0113]]],


        [[[-0.0025, -0.0051,  0.0001],
          [-0.0012, -0.0027,  0.0025],
          [ 0.0010, -0.0012,  0.0045]],

         [[-0.0013, -0.0041,  0.0011],
          [-0.0003, -0.0018,  0.0034],
          [ 0.0009, -0.0016,  0.0042]],

         [[ 0.0033,  0.0019,  0.0079],
          [ 0.0031,  0.0027,  0.0082],
          [ 0.0036,  0.0016,  0.0073]]],


        ...,


        [[[ 0.0489,  0.0364,  0.0245],
          [ 0.0460,  0.0303,  0.0279],
          [ 0.0487,  0.0314,  0.0322]],

         [[ 0.0172,  0.0116,  0.0066],
          [ 0.0143,  0.0051,  0.0106],
          [ 0.0196,  0.0073,  0.0148]],

         [[-0.0168, -0.0292, -0.0353],
          [-0.0200, -0.0351, -0.0310],
          [-0.0166, -0.0331, -0.0280]]],


        [[[-0.0216, -0.0247, -0.0205],
          [-0.0117, -0.0159, -0.0165],
          [ 0.0066,  0.0020, -0.0074]],

         [[-0.0113, -0.0140, -0.0088],
          [-0.0006, -0.0043, -0.0042],
          [ 0.0188,  0.0148,  0.0059]],

         [[ 0.0126,  0.0099,  0.0154],
          [ 0.0214,  0.0176,  0.0185],
          [ 0.0380,  0.0342,  0.0272]]],


        [[[ 0.1064,  0.0970,  0.1038],
          [ 0.1096,  0.0940,  0.0897],
          [ 0.1024,  0.0849,  0.0839]],

         [[ 0.0768,  0.0745,  0.0789],
          [ 0.0755,  0.0695,  0.0634],
          [ 0.0649,  0.0566,  0.0551]],

         [[ 0.0313,  0.0251,  0.0331],
          [ 0.0290,  0.0182,  0.0185],
          [ 0.0224,  0.0062,  0.0125]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6987]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 91 | Batch_idx: 0 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (2437/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (3598/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (90.00%) (4773/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (5957/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (7148/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (8307/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (9474/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (10642/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (11805/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (12974/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (14151/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (15322/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (16504/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (17658/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (18824/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (19981/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (21156/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (22327/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2519) |  Loss2: (0.0000) | Acc: (91.00%) (23482/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (24651/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (91.00%) (25815/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (26984/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (28157/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (29339/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (30502/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2517) |  Loss2: (0.0000) | Acc: (91.00%) (31653/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (32786/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (33951/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (35101/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (36257/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (37407/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (38564/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (90.00%) (39714/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (40887/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (90.00%) (42041/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (90.00%) (43198/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (90.00%) (44373/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (90.00%) (45482/50000)
# TEST : Loss: (0.4393) | Acc: (86.00%) (8601/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1357,  0.0636,  0.0936],
          [ 0.1075, -0.2882, -0.1013],
          [ 0.2492, -0.0879,  0.0118]],

         [[-0.0005, -0.0936,  0.2595],
          [ 0.0225, -0.0973,  0.0440],
          [ 0.0616,  0.0030, -0.0706]],

         [[ 0.0164, -0.0497,  0.1896],
          [ 0.0989, -0.1920,  0.1602],
          [-0.1076,  0.0276, -0.1867]]],


        [[[ 0.0965, -0.0795, -0.0045],
          [ 0.1355,  0.1702,  0.1801],
          [ 0.0073, -0.0688,  0.1127]],

         [[-0.1560, -0.1822,  0.1597],
          [ 0.0315, -0.0520,  0.1061],
          [ 0.1219, -0.2061,  0.1327]],

         [[ 0.1008, -0.0492, -0.0084],
          [ 0.0124,  0.0218, -0.1135],
          [-0.0324, -0.1603, -0.0155]]],


        [[[-0.0716,  0.0901, -0.0423],
          [-0.1244,  0.0657,  0.1176],
          [ 0.1126,  0.1669,  0.0021]],

         [[ 0.0994,  0.1326,  0.1135],
          [ 0.0701, -0.0389, -0.0356],
          [-0.0458, -0.0298,  0.1404]],

         [[-0.0454,  0.1451,  0.0743],
          [-0.0410, -0.0449, -0.0598],
          [-0.0378, -0.0070,  0.0437]]],


        ...,


        [[[ 0.2243, -0.1145, -0.1158],
          [-0.0490, -0.1114, -0.0964],
          [-0.0950,  0.0238,  0.0655]],

         [[ 0.1694,  0.1969, -0.1115],
          [ 0.2077, -0.1385, -0.1137],
          [-0.0614, -0.0765, -0.0298]],

         [[ 0.1847,  0.1042, -0.0985],
          [-0.0701, -0.1561, -0.0748],
          [ 0.0655,  0.1779, -0.1313]]],


        [[[-0.1045,  0.0904, -0.1463],
          [-0.1407, -0.1432,  0.1188],
          [-0.1536,  0.1539, -0.1344]],

         [[ 0.0922, -0.1196,  0.0834],
          [ 0.1481,  0.0023, -0.0869],
          [-0.1496,  0.1144, -0.1994]],

         [[ 0.1212, -0.0231, -0.0014],
          [ 0.1382, -0.1773,  0.0102],
          [-0.0300,  0.0156, -0.1242]]],


        [[[ 0.0811,  0.1515,  0.1071],
          [-0.0826, -0.1788,  0.2096],
          [-0.1687, -0.0209, -0.0651]],

         [[-0.0220, -0.0850,  0.1502],
          [ 0.0005, -0.2050,  0.0529],
          [-0.0032,  0.0772,  0.0321]],

         [[-0.1624, -0.1995,  0.1322],
          [ 0.1437, -0.1861, -0.0157],
          [ 0.0046, -0.0224,  0.2887]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0577,  0.0271,  0.0537],
          [ 0.0467,  0.0402,  0.0708],
          [ 0.0400,  0.0476,  0.0585]],

         [[ 0.0301, -0.0091,  0.0166],
          [ 0.0111,  0.0004,  0.0354],
          [-0.0025,  0.0002,  0.0212]],

         [[-0.0001, -0.0438, -0.0290],
          [-0.0134, -0.0231,  0.0051],
          [-0.0306, -0.0192,  0.0016]]],


        [[[-0.0220, -0.0176, -0.0200],
          [-0.0144, -0.0110, -0.0136],
          [-0.0200, -0.0181, -0.0162]],

         [[-0.0299, -0.0264, -0.0285],
          [-0.0219, -0.0194, -0.0227],
          [-0.0272, -0.0256, -0.0243]],

         [[-0.0247, -0.0222, -0.0242],
          [-0.0200, -0.0190, -0.0212],
          [-0.0266, -0.0266, -0.0247]]],


        [[[-0.0058, -0.0033, -0.0034],
          [-0.0008, -0.0004, -0.0005],
          [ 0.0015,  0.0002, -0.0001]],

         [[-0.0029, -0.0006, -0.0007],
          [ 0.0014,  0.0019,  0.0021],
          [ 0.0030,  0.0021,  0.0026]],

         [[-0.0005,  0.0013,  0.0010],
          [ 0.0023,  0.0026,  0.0030],
          [ 0.0039,  0.0029,  0.0039]]],


        ...,


        [[[-0.0590, -0.0639, -0.0509],
          [-0.0542, -0.0569, -0.0405],
          [-0.0431, -0.0407, -0.0261]],

         [[-0.0205, -0.0237, -0.0092],
          [-0.0121, -0.0123,  0.0067],
          [ 0.0001,  0.0034,  0.0206]],

         [[ 0.0108,  0.0091,  0.0236],
          [ 0.0169,  0.0178,  0.0363],
          [ 0.0276,  0.0313,  0.0460]]],


        [[[-0.0104, -0.0063,  0.0018],
          [-0.0130, -0.0101, -0.0068],
          [-0.0138, -0.0093, -0.0050]],

         [[ 0.0019,  0.0062,  0.0136],
          [ 0.0005,  0.0050,  0.0081],
          [ 0.0000,  0.0060,  0.0104]],

         [[ 0.0070,  0.0109,  0.0143],
          [ 0.0072,  0.0119,  0.0126],
          [ 0.0086,  0.0145,  0.0174]]],


        [[[ 0.0969,  0.0486,  0.0378],
          [ 0.0371,  0.0121,  0.0126],
          [ 0.0142,  0.0011, -0.0044]],

         [[ 0.0098, -0.0316, -0.0437],
          [-0.0378, -0.0584, -0.0623],
          [-0.0527, -0.0621, -0.0708]],

         [[-0.0519, -0.0865, -0.0964],
          [-0.0910, -0.1046, -0.1118],
          [-0.1056, -0.1097, -0.1226]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6985]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 92 | Batch_idx: 0 |  Loss: (0.3397) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (1280/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (3645/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (4824/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (6001/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (92.00%) (7184/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (8349/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (9528/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (10697/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (11883/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (13061/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (14235/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (15394/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (16580/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (17757/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (18932/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (20088/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (21261/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (22442/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (23614/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (24779/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (25949/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (27114/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (28270/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (29441/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (30594/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (31757/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (32912/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (34078/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (35251/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (36423/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (37583/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (38737/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (39913/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (41089/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (42257/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (43432/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (44604/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (45729/50000)
# TEST : Loss: (0.4737) | Acc: (85.00%) (8529/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1327,  0.0650,  0.0940],
          [ 0.1092, -0.2893, -0.1021],
          [ 0.2526, -0.0868,  0.0132]],

         [[-0.0005, -0.0933,  0.2588],
          [ 0.0212, -0.1002,  0.0423],
          [ 0.0613,  0.0009, -0.0711]],

         [[ 0.0149, -0.0491,  0.1888],
          [ 0.0958, -0.1951,  0.1579],
          [-0.1106,  0.0236, -0.1892]]],


        [[[ 0.0962, -0.0798, -0.0056],
          [ 0.1353,  0.1700,  0.1795],
          [ 0.0069, -0.0691,  0.1120]],

         [[-0.1564, -0.1829,  0.1581],
          [ 0.0311, -0.0528,  0.1048],
          [ 0.1213, -0.2071,  0.1312]],

         [[ 0.1001, -0.0503, -0.0103],
          [ 0.0116,  0.0205, -0.1152],
          [-0.0328, -0.1615, -0.0172]]],


        [[[-0.0712,  0.0901, -0.0422],
          [-0.1242,  0.0658,  0.1175],
          [ 0.1128,  0.1670,  0.0021]],

         [[ 0.0998,  0.1327,  0.1134],
          [ 0.0703, -0.0389, -0.0359],
          [-0.0456, -0.0298,  0.1400]],

         [[-0.0453,  0.1448,  0.0739],
          [-0.0411, -0.0453, -0.0605],
          [-0.0378, -0.0073,  0.0431]]],


        ...,


        [[[ 0.2245, -0.1147, -0.1170],
          [-0.0486, -0.1115, -0.0961],
          [-0.0930,  0.0258,  0.0677]],

         [[ 0.1680,  0.1952, -0.1140],
          [ 0.2068, -0.1397, -0.1148],
          [-0.0601, -0.0753, -0.0292]],

         [[ 0.1842,  0.1036, -0.0999],
          [-0.0694, -0.1558, -0.0748],
          [ 0.0680,  0.1801, -0.1298]]],


        [[[-0.1020,  0.0914, -0.1456],
          [-0.1394, -0.1427,  0.1195],
          [-0.1528,  0.1543, -0.1335]],

         [[ 0.0942, -0.1191,  0.0835],
          [ 0.1489,  0.0019, -0.0870],
          [-0.1491,  0.1141, -0.1992]],

         [[ 0.1235, -0.0222, -0.0011],
          [ 0.1394, -0.1774,  0.0101],
          [-0.0292,  0.0155, -0.1238]]],


        [[[ 0.0768,  0.1484,  0.1033],
          [-0.0841, -0.1831,  0.2068],
          [-0.1679, -0.0214, -0.0647]],

         [[-0.0266, -0.0894,  0.1447],
          [-0.0015, -0.2102,  0.0490],
          [-0.0036,  0.0759,  0.0318]],

         [[-0.1660, -0.2019,  0.1283],
          [ 0.1436, -0.1879, -0.0166],
          [ 0.0060, -0.0208,  0.2910]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0428,  0.0818,  0.0997],
          [ 0.0427,  0.0830,  0.0913],
          [ 0.0479,  0.0777,  0.0819]],

         [[ 0.0179,  0.0508,  0.0610],
          [ 0.0177,  0.0534,  0.0581],
          [ 0.0220,  0.0501,  0.0499]],

         [[ 0.0167,  0.0415,  0.0500],
          [-0.0029,  0.0257,  0.0319],
          [-0.0116,  0.0090,  0.0097]]],


        [[[-0.0029, -0.0038, -0.0121],
          [ 0.0082,  0.0064, -0.0008],
          [ 0.0028,  0.0026, -0.0067]],

         [[-0.0045, -0.0056, -0.0128],
          [ 0.0076,  0.0060, -0.0013],
          [ 0.0034,  0.0035, -0.0062]],

         [[-0.0037, -0.0048, -0.0126],
          [ 0.0074,  0.0047, -0.0038],
          [ 0.0031,  0.0014, -0.0087]]],


        [[[ 0.0069,  0.0042,  0.0046],
          [ 0.0039,  0.0029,  0.0045],
          [ 0.0003, -0.0005,  0.0009]],

         [[ 0.0025,  0.0003,  0.0011],
          [-0.0002, -0.0005,  0.0016],
          [-0.0036, -0.0042, -0.0023]],

         [[-0.0029, -0.0046, -0.0036],
          [-0.0045, -0.0044, -0.0025],
          [-0.0071, -0.0075, -0.0059]]],


        ...,


        [[[ 0.0492,  0.0327,  0.0248],
          [ 0.0590,  0.0452,  0.0365],
          [ 0.0685,  0.0507,  0.0410]],

         [[ 0.0480,  0.0310,  0.0232],
          [ 0.0546,  0.0397,  0.0307],
          [ 0.0645,  0.0458,  0.0354]],

         [[ 0.0345,  0.0220,  0.0169],
          [ 0.0399,  0.0304,  0.0241],
          [ 0.0505,  0.0381,  0.0310]]],


        [[[ 0.0035,  0.0029,  0.0040],
          [-0.0022, -0.0028, -0.0002],
          [-0.0060, -0.0055, -0.0028]],

         [[ 0.0063,  0.0069,  0.0081],
          [-0.0000,  0.0016,  0.0046],
          [-0.0062, -0.0036,  0.0009]],

         [[ 0.0097,  0.0130,  0.0149],
          [ 0.0042,  0.0085,  0.0120],
          [-0.0009,  0.0036,  0.0077]]],


        [[[-0.0940, -0.1032, -0.1268],
          [-0.1162, -0.1127, -0.1173],
          [-0.1042, -0.1021, -0.1080]],

         [[-0.0978, -0.1050, -0.1264],
          [-0.1167, -0.1187, -0.1219],
          [-0.1002, -0.1034, -0.1066]],

         [[-0.0776, -0.0887, -0.1081],
          [-0.0951, -0.1035, -0.1043],
          [-0.0762, -0.0863, -0.0875]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6982]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 93 | Batch_idx: 0 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (2479/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (6018/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (7190/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (8370/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (9532/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (92.00%) (10731/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (11893/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (13058/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (14221/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (15395/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (16560/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (17743/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (18921/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (20104/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (21284/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (22455/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (23602/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (24772/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (25944/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (27119/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (28294/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (29461/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (30623/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (31798/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (32969/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (34152/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (35344/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (36534/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (37708/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (38860/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (40027/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (41200/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (42369/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (43544/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (44724/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (45859/50000)
# TEST : Loss: (0.4509) | Acc: (86.00%) (8618/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1310,  0.0649,  0.0932],
          [ 0.1092, -0.2917, -0.1033],
          [ 0.2557, -0.0853,  0.0139]],

         [[ 0.0003, -0.0940,  0.2582],
          [ 0.0215, -0.1025,  0.0420],
          [ 0.0644,  0.0020, -0.0698]],

         [[ 0.0151, -0.0497,  0.1892],
          [ 0.0951, -0.1967,  0.1587],
          [-0.1088,  0.0244, -0.1876]]],


        [[[ 0.0983, -0.0783, -0.0048],
          [ 0.1373,  0.1716,  0.1800],
          [ 0.0089, -0.0681,  0.1119]],

         [[-0.1542, -0.1814,  0.1580],
          [ 0.0328, -0.0513,  0.1047],
          [ 0.1224, -0.2066,  0.1304]],

         [[ 0.1018, -0.0494, -0.0110],
          [ 0.0130,  0.0216, -0.1156],
          [-0.0314, -0.1609, -0.0179]]],


        [[[-0.0715,  0.0898, -0.0423],
          [-0.1244,  0.0656,  0.1175],
          [ 0.1123,  0.1668,  0.0020]],

         [[ 0.0998,  0.1327,  0.1136],
          [ 0.0703, -0.0387, -0.0355],
          [-0.0458, -0.0298,  0.1401]],

         [[-0.0451,  0.1448,  0.0741],
          [-0.0410, -0.0451, -0.0600],
          [-0.0379, -0.0073,  0.0432]]],


        ...,


        [[[ 0.2254, -0.1147, -0.1172],
          [-0.0482, -0.1117, -0.0958],
          [-0.0936,  0.0259,  0.0683]],

         [[ 0.1688,  0.1956, -0.1135],
          [ 0.2081, -0.1387, -0.1133],
          [-0.0594, -0.0738, -0.0271]],

         [[ 0.1841,  0.1025, -0.1012],
          [-0.0682, -0.1554, -0.0744],
          [ 0.0692,  0.1814, -0.1284]]],


        [[[-0.1027,  0.0909, -0.1461],
          [-0.1393, -0.1432,  0.1189],
          [-0.1520,  0.1541, -0.1343]],

         [[ 0.0950, -0.1179,  0.0845],
          [ 0.1501,  0.0028, -0.0860],
          [-0.1472,  0.1152, -0.1987]],

         [[ 0.1247, -0.0206,  0.0005],
          [ 0.1410, -0.1760,  0.0114],
          [-0.0272,  0.0169, -0.1231]]],


        [[[ 0.0763,  0.1498,  0.1055],
          [-0.0843, -0.1828,  0.2091],
          [-0.1693, -0.0196, -0.0645]],

         [[-0.0256, -0.0878,  0.1459],
          [-0.0001, -0.2089,  0.0516],
          [-0.0037,  0.0786,  0.0326]],

         [[-0.1671, -0.2033,  0.1260],
          [ 0.1432, -0.1896, -0.0177],
          [ 0.0050, -0.0207,  0.2895]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0119,  0.0118,  0.0045],
          [ 0.0424,  0.0408,  0.0317],
          [ 0.0751,  0.0376,  0.0235]],

         [[-0.0368, -0.0313, -0.0316],
          [-0.0144, -0.0148, -0.0160],
          [ 0.0183, -0.0211, -0.0294]],

         [[-0.0467, -0.0363, -0.0368],
          [-0.0231, -0.0161, -0.0144],
          [ 0.0064, -0.0203, -0.0239]]],


        [[[ 0.0009, -0.0002, -0.0070],
          [-0.0017,  0.0010,  0.0019],
          [-0.0047, -0.0009,  0.0043]],

         [[-0.0004, -0.0018, -0.0088],
          [-0.0029, -0.0007, -0.0003],
          [-0.0051, -0.0013,  0.0029]],

         [[-0.0003, -0.0031, -0.0115],
          [-0.0003,  0.0003, -0.0015],
          [-0.0013,  0.0015,  0.0031]]],


        [[[-0.0030, -0.0008,  0.0004],
          [ 0.0002,  0.0017,  0.0016],
          [ 0.0051,  0.0050,  0.0039]],

         [[-0.0066, -0.0044, -0.0025],
          [-0.0045, -0.0031, -0.0026],
          [-0.0003, -0.0003, -0.0009]],

         [[-0.0076, -0.0056, -0.0031],
          [-0.0059, -0.0040, -0.0028],
          [-0.0028, -0.0020, -0.0017]]],


        ...,


        [[[ 0.0338,  0.0272,  0.0425],
          [ 0.0101,  0.0073,  0.0264],
          [-0.0064, -0.0046,  0.0177]],

         [[ 0.0420,  0.0353,  0.0512],
          [ 0.0207,  0.0183,  0.0375],
          [ 0.0031,  0.0065,  0.0285]],

         [[ 0.0462,  0.0402,  0.0530],
          [ 0.0279,  0.0242,  0.0394],
          [ 0.0158,  0.0177,  0.0359]]],


        [[[ 0.0101,  0.0025, -0.0018],
          [-0.0003, -0.0006, -0.0018],
          [-0.0075, -0.0042, -0.0002]],

         [[ 0.0057, -0.0014, -0.0035],
          [-0.0052, -0.0047, -0.0029],
          [-0.0127, -0.0077, -0.0016]],

         [[ 0.0127,  0.0094,  0.0093],
          [ 0.0060,  0.0096,  0.0116],
          [ 0.0013,  0.0084,  0.0147]]],


        [[[-0.0415, -0.0672, -0.0810],
          [-0.0240, -0.0441, -0.0441],
          [-0.0365, -0.0440, -0.0249]],

         [[-0.0380, -0.0610, -0.0700],
          [-0.0199, -0.0367, -0.0312],
          [-0.0273, -0.0318, -0.0102]],

         [[-0.0430, -0.0647, -0.0718],
          [-0.0281, -0.0464, -0.0390],
          [-0.0306, -0.0370, -0.0152]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6979]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 94 | Batch_idx: 0 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (2467/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (3651/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (4841/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (5990/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (91.00%) (7174/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (91.00%) (8358/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (91.00%) (9536/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (91.00%) (10711/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (91.00%) (11885/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (13072/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (14254/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (15436/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (16619/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (17785/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (18963/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (20147/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (21326/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (22510/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (23682/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (24852/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (26029/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (91.00%) (27200/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (91.00%) (28363/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (91.00%) (29553/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (30720/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (31902/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (33078/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (34246/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (91.00%) (35433/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (36607/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (37773/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (38947/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (40108/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (41270/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (42449/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (43633/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (44804/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (45936/50000)
# TEST : Loss: (0.4851) | Acc: (85.00%) (8505/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1347,  0.0622,  0.0898],
          [ 0.1045, -0.2976, -0.1095],
          [ 0.2526, -0.0880,  0.0092]],

         [[-0.0043, -0.0967,  0.2561],
          [ 0.0171, -0.1061,  0.0396],
          [ 0.0616,  0.0013, -0.0707]],

         [[ 0.0124, -0.0502,  0.1889],
          [ 0.0924, -0.1980,  0.1579],
          [-0.1115,  0.0234, -0.1885]]],


        [[[ 0.0976, -0.0789, -0.0056],
          [ 0.1368,  0.1719,  0.1800],
          [ 0.0083, -0.0680,  0.1121]],

         [[-0.1551, -0.1824,  0.1566],
          [ 0.0321, -0.0512,  0.1042],
          [ 0.1218, -0.2065,  0.1305]],

         [[ 0.1004, -0.0505, -0.0120],
          [ 0.0116,  0.0212, -0.1161],
          [-0.0326, -0.1612, -0.0182]]],


        [[[-0.0716,  0.0897, -0.0423],
          [-0.1248,  0.0653,  0.1173],
          [ 0.1119,  0.1664,  0.0017]],

         [[ 0.0999,  0.1328,  0.1137],
          [ 0.0702, -0.0387, -0.0355],
          [-0.0460, -0.0299,  0.1399]],

         [[-0.0449,  0.1450,  0.0743],
          [-0.0409, -0.0450, -0.0599],
          [-0.0379, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2275, -0.1140, -0.1169],
          [-0.0465, -0.1108, -0.0950],
          [-0.0931,  0.0255,  0.0680]],

         [[ 0.1699,  0.1957, -0.1134],
          [ 0.2089, -0.1383, -0.1132],
          [-0.0594, -0.0747, -0.0283]],

         [[ 0.1846,  0.1027, -0.1004],
          [-0.0675, -0.1547, -0.0735],
          [ 0.0695,  0.1810, -0.1286]]],


        [[[-0.1039,  0.0897, -0.1468],
          [-0.1402, -0.1435,  0.1185],
          [-0.1520,  0.1546, -0.1344]],

         [[ 0.0943, -0.1183,  0.0844],
          [ 0.1500,  0.0032, -0.0856],
          [-0.1462,  0.1165, -0.1979]],

         [[ 0.1242, -0.0210,  0.0004],
          [ 0.1411, -0.1754,  0.0119],
          [-0.0264,  0.0180, -0.1226]]],


        [[[ 0.0778,  0.1526,  0.1084],
          [-0.0822, -0.1804,  0.2140],
          [-0.1695, -0.0214, -0.0644]],

         [[-0.0246, -0.0857,  0.1485],
          [ 0.0015, -0.2072,  0.0553],
          [-0.0039,  0.0769,  0.0318]],

         [[-0.1664, -0.2027,  0.1269],
          [ 0.1449, -0.1873, -0.0130],
          [ 0.0053, -0.0203,  0.2917]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0323, -0.0342, -0.0563],
          [-0.0519, -0.0479, -0.0685],
          [-0.0877, -0.0860, -0.1034]],

         [[-0.0055, -0.0116, -0.0301],
          [-0.0234, -0.0232, -0.0421],
          [-0.0575, -0.0570, -0.0721]],

         [[-0.0092, -0.0122, -0.0179],
          [-0.0143, -0.0159, -0.0267],
          [-0.0352, -0.0373, -0.0470]]],


        [[[-0.0012,  0.0083,  0.0046],
          [-0.0032,  0.0059,  0.0027],
          [ 0.0061,  0.0108,  0.0065]],

         [[-0.0048,  0.0041, -0.0007],
          [-0.0068,  0.0016, -0.0017],
          [ 0.0025,  0.0057,  0.0015]],

         [[-0.0167, -0.0085, -0.0131],
          [-0.0181, -0.0105, -0.0128],
          [-0.0069, -0.0041, -0.0068]]],


        [[[ 0.0002,  0.0020,  0.0033],
          [-0.0017,  0.0010,  0.0030],
          [-0.0037, -0.0023,  0.0000]],

         [[-0.0010,  0.0004,  0.0012],
          [-0.0023, -0.0000,  0.0013],
          [-0.0036, -0.0028, -0.0011]],

         [[-0.0042, -0.0028, -0.0023],
          [-0.0050, -0.0029, -0.0021],
          [-0.0063, -0.0054, -0.0041]]],


        ...,


        [[[ 0.0629,  0.0545,  0.0510],
          [ 0.0493,  0.0404,  0.0427],
          [ 0.0505,  0.0418,  0.0444]],

         [[ 0.0487,  0.0418,  0.0391],
          [ 0.0372,  0.0294,  0.0304],
          [ 0.0414,  0.0333,  0.0345]],

         [[ 0.0218,  0.0194,  0.0189],
          [ 0.0144,  0.0113,  0.0127],
          [ 0.0213,  0.0186,  0.0191]]],


        [[[ 0.0055,  0.0092,  0.0170],
          [-0.0025,  0.0001,  0.0070],
          [-0.0101, -0.0089, -0.0050]],

         [[ 0.0039,  0.0082,  0.0155],
          [-0.0040, -0.0001,  0.0073],
          [-0.0101, -0.0068, -0.0018]],

         [[ 0.0036,  0.0087,  0.0152],
          [-0.0034,  0.0016,  0.0081],
          [-0.0093, -0.0044,  0.0001]]],


        [[[ 0.1403,  0.1459,  0.1292],
          [ 0.1534,  0.1567,  0.1544],
          [ 0.1671,  0.1747,  0.1922]],

         [[ 0.0947,  0.0954,  0.0832],
          [ 0.0976,  0.0957,  0.0920],
          [ 0.1076,  0.1106,  0.1230]],

         [[ 0.0808,  0.0791,  0.0709],
          [ 0.0865,  0.0848,  0.0799],
          [ 0.1013,  0.1046,  0.1089]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.6976]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.4190) |  Loss2: (0.2660) | Acc: (93.00%) (120/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.4972) |  Loss2: (0.2659) | Acc: (91.00%) (1286/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.5186) |  Loss2: (0.2659) | Acc: (90.00%) (2442/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.5183) |  Loss2: (0.2659) | Acc: (90.00%) (3609/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.5288) |  Loss2: (0.2658) | Acc: (90.00%) (4765/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.5322) |  Loss2: (0.2658) | Acc: (90.00%) (5924/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.5342) |  Loss2: (0.2657) | Acc: (90.00%) (7077/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.5368) |  Loss2: (0.2657) | Acc: (90.00%) (8218/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.5380) |  Loss2: (0.2656) | Acc: (90.00%) (9370/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.5370) |  Loss2: (0.2656) | Acc: (90.00%) (10531/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.5354) |  Loss2: (0.2655) | Acc: (90.00%) (11697/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.5372) |  Loss2: (0.2655) | Acc: (90.00%) (12845/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.5359) |  Loss2: (0.2654) | Acc: (90.00%) (14008/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.5360) |  Loss2: (0.2654) | Acc: (90.00%) (15151/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.5364) |  Loss2: (0.2653) | Acc: (90.00%) (16310/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.5349) |  Loss2: (0.2653) | Acc: (90.00%) (17478/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.5343) |  Loss2: (0.2652) | Acc: (90.00%) (18635/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.5343) |  Loss2: (0.2652) | Acc: (90.00%) (19791/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.5335) |  Loss2: (0.2651) | Acc: (90.00%) (20955/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.5322) |  Loss2: (0.2651) | Acc: (90.00%) (22131/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.5305) |  Loss2: (0.2650) | Acc: (90.00%) (23317/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.5309) |  Loss2: (0.2649) | Acc: (90.00%) (24475/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.5303) |  Loss2: (0.2649) | Acc: (90.00%) (25637/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.5301) |  Loss2: (0.2648) | Acc: (90.00%) (26800/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.5295) |  Loss2: (0.2648) | Acc: (90.00%) (27979/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.5289) |  Loss2: (0.2647) | Acc: (90.00%) (29150/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.5281) |  Loss2: (0.2647) | Acc: (90.00%) (30317/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.5281) |  Loss2: (0.2646) | Acc: (90.00%) (31472/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.5293) |  Loss2: (0.2646) | Acc: (90.00%) (32610/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.5286) |  Loss2: (0.2646) | Acc: (90.00%) (33778/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.5282) |  Loss2: (0.2645) | Acc: (90.00%) (34936/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.5272) |  Loss2: (0.2645) | Acc: (90.00%) (36117/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.5270) |  Loss2: (0.2644) | Acc: (90.00%) (37280/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.5260) |  Loss2: (0.2644) | Acc: (90.00%) (38464/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.5260) |  Loss2: (0.2643) | Acc: (90.00%) (39626/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.5255) |  Loss2: (0.2643) | Acc: (90.00%) (40805/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.5256) |  Loss2: (0.2642) | Acc: (90.00%) (41974/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.5254) |  Loss2: (0.2642) | Acc: (90.00%) (43136/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.5250) |  Loss2: (0.2641) | Acc: (90.00%) (44306/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.5247) |  Loss2: (0.2641) | Acc: (90.00%) (45433/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_095.pth.tar'
# TEST : Loss: (0.4332) | Acc: (85.00%) (8594/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1341,  0.0625,  0.0902],
          [ 0.1050, -0.2972, -0.1091],
          [ 0.2538, -0.0869,  0.0104]],

         [[-0.0041, -0.0967,  0.2561],
          [ 0.0171, -0.1061,  0.0398],
          [ 0.0624,  0.0021, -0.0698]],

         [[ 0.0125, -0.0502,  0.1888],
          [ 0.0922, -0.1981,  0.1577],
          [-0.1110,  0.0239, -0.1880]]],


        [[[ 0.0974, -0.0790, -0.0057],
          [ 0.1365,  0.1717,  0.1799],
          [ 0.0081, -0.0683,  0.1119]],

         [[-0.1551, -0.1824,  0.1565],
          [ 0.0319, -0.0513,  0.1041],
          [ 0.1216, -0.2066,  0.1302]],

         [[ 0.1004, -0.0505, -0.0120],
          [ 0.0116,  0.0211, -0.1161],
          [-0.0328, -0.1614, -0.0184]]],


        [[[-0.0715,  0.0897, -0.0422],
          [-0.1247,  0.0653,  0.1173],
          [ 0.1118,  0.1664,  0.0017]],

         [[ 0.0999,  0.1328,  0.1137],
          [ 0.0701, -0.0387, -0.0355],
          [-0.0460, -0.0299,  0.1398]],

         [[-0.0448,  0.1451,  0.0743],
          [-0.0408, -0.0449, -0.0598],
          [-0.0379, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2262, -0.1149, -0.1177],
          [-0.0475, -0.1115, -0.0956],
          [-0.0941,  0.0246,  0.0673]],

         [[ 0.1688,  0.1947, -0.1142],
          [ 0.2079, -0.1389, -0.1138],
          [-0.0603, -0.0755, -0.0290]],

         [[ 0.1843,  0.1026, -0.1005],
          [-0.0676, -0.1547, -0.0736],
          [ 0.0692,  0.1806, -0.1287]]],


        [[[-0.1036,  0.0899, -0.1467],
          [-0.1400, -0.1435,  0.1184],
          [-0.1518,  0.1546, -0.1343]],

         [[ 0.0945, -0.1180,  0.0845],
          [ 0.1501,  0.0033, -0.0856],
          [-0.1460,  0.1166, -0.1977]],

         [[ 0.1243, -0.0209,  0.0003],
          [ 0.1411, -0.1754,  0.0117],
          [-0.0264,  0.0179, -0.1226]]],


        [[[ 0.0771,  0.1519,  0.1078],
          [-0.0832, -0.1812,  0.2131],
          [-0.1707, -0.0225, -0.0658]],

         [[-0.0250, -0.0859,  0.1483],
          [ 0.0009, -0.2075,  0.0551],
          [-0.0046,  0.0765,  0.0312]],

         [[-0.1665, -0.2026,  0.1269],
          [ 0.1445, -0.1874, -0.0130],
          [ 0.0047, -0.0206,  0.2912]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3411e-05,  6.2453e-06,  9.0210e-06],
          [ 1.0499e-05, -2.9721e-05, -1.0909e-05],
          [ 2.5377e-05, -8.6868e-06,  1.0375e-06]],

         [[-4.1218e-07, -9.6687e-06,  2.5611e-05],
          [ 1.7147e-06, -1.0613e-05,  3.9791e-06],
          [ 6.2375e-06,  2.0712e-07, -6.9792e-06]],

         [[ 1.2485e-06, -5.0239e-06,  1.8876e-05],
          [ 9.2239e-06, -1.9812e-05,  1.5774e-05],
          [-1.1099e-05,  2.3868e-06, -1.8797e-05]]],


        [[[ 9.7373e-06, -7.9019e-06, -5.6508e-07],
          [ 1.3654e-05,  1.7167e-05,  1.7989e-05],
          [ 8.0635e-07, -6.8252e-06,  1.1186e-05]],

         [[-1.5512e-05, -1.8244e-05,  1.5650e-05],
          [ 3.1932e-06, -5.1316e-06,  1.0412e-05],
          [ 1.2155e-05, -2.0662e-05,  1.3022e-05]],

         [[ 1.0038e-05, -5.0501e-06, -1.1994e-06],
          [ 1.1572e-06,  2.1093e-06, -1.1606e-05],
          [-3.2758e-06, -1.6136e-05, -1.8424e-06]]],


        [[[-7.1514e-06,  8.9729e-06, -4.2228e-06],
          [-1.2471e-05,  6.5332e-06,  1.1725e-05],
          [ 1.1179e-05,  1.6636e-05,  1.7161e-07]],

         [[ 9.9915e-06,  1.3278e-05,  1.1368e-05],
          [ 7.0124e-06, -3.8673e-06, -3.5464e-06],
          [-4.6024e-06, -2.9939e-06,  1.3976e-05]],

         [[-4.4777e-06,  1.4506e-05,  7.4312e-06],
          [-4.0836e-06, -4.4868e-06, -5.9815e-06],
          [-3.7883e-06, -7.1858e-07,  4.3147e-06]]],


        ...,


        [[[ 2.2619e-05, -1.1492e-05, -1.1769e-05],
          [-4.7473e-06, -1.1154e-05, -9.5588e-06],
          [-9.4135e-06,  2.4556e-06,  6.7307e-06]],

         [[ 1.6881e-05,  1.9474e-05, -1.1424e-05],
          [ 2.0794e-05, -1.3890e-05, -1.1380e-05],
          [-6.0284e-06, -7.5508e-06, -2.9022e-06]],

         [[ 1.8433e-05,  1.0258e-05, -1.0048e-05],
          [-6.7580e-06, -1.5472e-05, -7.3562e-06],
          [ 6.9228e-06,  1.8061e-05, -1.2873e-05]]],


        [[[-1.0361e-05,  8.9851e-06, -1.4666e-05],
          [-1.4004e-05, -1.4345e-05,  1.1840e-05],
          [-1.5183e-05,  1.5460e-05, -1.3433e-05]],

         [[ 9.4544e-06, -1.1805e-05,  8.4466e-06],
          [ 1.5014e-05,  3.2580e-07, -8.5567e-06],
          [-1.4598e-05,  1.1658e-05, -1.9773e-05]],

         [[ 1.2429e-05, -2.0895e-06,  2.9297e-08],
          [ 1.4112e-05, -1.7540e-05,  1.1707e-06],
          [-2.6424e-06,  1.7924e-06, -1.2263e-05]]],


        [[[ 7.7087e-06,  1.5189e-05,  1.0778e-05],
          [-8.3159e-06, -1.8123e-05,  2.1305e-05],
          [-1.7067e-05, -2.2495e-06, -6.5775e-06]],

         [[-2.5023e-06, -8.5945e-06,  1.4828e-05],
          [ 8.9353e-08, -2.0749e-05,  5.5084e-06],
          [-4.6133e-07,  7.6450e-06,  3.1178e-06]],

         [[-1.6653e-05, -2.0260e-05,  1.2692e-05],
          [ 1.4448e-05, -1.8735e-05, -1.3012e-06],
          [ 4.7358e-07, -2.0623e-06,  2.9118e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7220]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0423]], device='cuda:0')

Epoch: 96 | Batch_idx: 0 |  Loss: (0.6309) |  Loss2: (0.2622) | Acc: (86.00%) (111/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.4965) |  Loss2: (0.2622) | Acc: (91.00%) (1292/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.5045) |  Loss2: (0.2622) | Acc: (91.00%) (2466/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.5043) |  Loss2: (0.2621) | Acc: (91.00%) (3637/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.4954) |  Loss2: (0.2621) | Acc: (92.00%) (4832/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.5008) |  Loss2: (0.2620) | Acc: (91.00%) (6002/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.4993) |  Loss2: (0.2620) | Acc: (91.00%) (7174/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.4996) |  Loss2: (0.2620) | Acc: (91.00%) (8347/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.5007) |  Loss2: (0.2619) | Acc: (91.00%) (9524/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.5029) |  Loss2: (0.2619) | Acc: (91.00%) (10688/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.5025) |  Loss2: (0.2619) | Acc: (91.00%) (11857/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.5047) |  Loss2: (0.2618) | Acc: (91.00%) (13019/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.5032) |  Loss2: (0.2618) | Acc: (91.00%) (14207/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.5038) |  Loss2: (0.2617) | Acc: (91.00%) (15382/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.5043) |  Loss2: (0.2617) | Acc: (91.00%) (16550/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.5025) |  Loss2: (0.2617) | Acc: (91.00%) (17725/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.5041) |  Loss2: (0.2616) | Acc: (91.00%) (18881/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.5035) |  Loss2: (0.2616) | Acc: (91.00%) (20069/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.5035) |  Loss2: (0.2616) | Acc: (91.00%) (21236/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.5022) |  Loss2: (0.2615) | Acc: (91.00%) (22422/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.5025) |  Loss2: (0.2615) | Acc: (91.00%) (23593/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.5026) |  Loss2: (0.2614) | Acc: (91.00%) (24768/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.5022) |  Loss2: (0.2614) | Acc: (91.00%) (25955/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.5011) |  Loss2: (0.2614) | Acc: (91.00%) (27149/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.5019) |  Loss2: (0.2613) | Acc: (91.00%) (28307/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.5015) |  Loss2: (0.2613) | Acc: (91.00%) (29484/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.5019) |  Loss2: (0.2612) | Acc: (91.00%) (30649/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.5010) |  Loss2: (0.2612) | Acc: (91.00%) (31839/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.5004) |  Loss2: (0.2612) | Acc: (91.00%) (33022/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.4991) |  Loss2: (0.2611) | Acc: (91.00%) (34209/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.4986) |  Loss2: (0.2611) | Acc: (91.00%) (35388/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.4986) |  Loss2: (0.2610) | Acc: (91.00%) (36549/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.4983) |  Loss2: (0.2610) | Acc: (91.00%) (37714/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.4992) |  Loss2: (0.2609) | Acc: (91.00%) (38878/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.4990) |  Loss2: (0.2609) | Acc: (91.00%) (40063/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.4989) |  Loss2: (0.2609) | Acc: (91.00%) (41240/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.4988) |  Loss2: (0.2608) | Acc: (91.00%) (42426/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.4988) |  Loss2: (0.2608) | Acc: (91.00%) (43607/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.4986) |  Loss2: (0.2607) | Acc: (91.00%) (44780/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.4997) |  Loss2: (0.2607) | Acc: (91.00%) (45885/50000)
# TEST : Loss: (0.4166) | Acc: (86.00%) (8649/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1341,  0.0624,  0.0902],
          [ 0.1049, -0.2971, -0.1090],
          [ 0.2537, -0.0868,  0.0104]],

         [[-0.0041, -0.0966,  0.2560],
          [ 0.0171, -0.1061,  0.0398],
          [ 0.0624,  0.0021, -0.0698]],

         [[ 0.0125, -0.0502,  0.1887],
          [ 0.0922, -0.1980,  0.1577],
          [-0.1109,  0.0239, -0.1879]]],


        [[[ 0.0973, -0.0790, -0.0056],
          [ 0.1365,  0.1716,  0.1798],
          [ 0.0081, -0.0682,  0.1118]],

         [[-0.1551, -0.1824,  0.1564],
          [ 0.0319, -0.0513,  0.1041],
          [ 0.1215, -0.2065,  0.1302]],

         [[ 0.1003, -0.0505, -0.0120],
          [ 0.0116,  0.0211, -0.1160],
          [-0.0327, -0.1613, -0.0184]]],


        [[[-0.0715,  0.0897, -0.0422],
          [-0.1247,  0.0653,  0.1172],
          [ 0.1117,  0.1663,  0.0017]],

         [[ 0.0999,  0.1327,  0.1136],
          [ 0.0701, -0.0387, -0.0354],
          [-0.0460, -0.0299,  0.1397]],

         [[-0.0448,  0.1450,  0.0743],
          [-0.0408, -0.0449, -0.0598],
          [-0.0379, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2261, -0.1149, -0.1176],
          [-0.0475, -0.1115, -0.0955],
          [-0.0941,  0.0245,  0.0673]],

         [[ 0.1687,  0.1947, -0.1142],
          [ 0.2079, -0.1388, -0.1138],
          [-0.0603, -0.0755, -0.0290]],

         [[ 0.1843,  0.1025, -0.1004],
          [-0.0676, -0.1547, -0.0735],
          [ 0.0692,  0.1805, -0.1287]]],


        [[[-0.1036,  0.0898, -0.1466],
          [-0.1400, -0.1434,  0.1184],
          [-0.1518,  0.1545, -0.1343]],

         [[ 0.0945, -0.1180,  0.0844],
          [ 0.1501,  0.0033, -0.0855],
          [-0.1459,  0.1165, -0.1977]],

         [[ 0.1242, -0.0209,  0.0003],
          [ 0.1411, -0.1753,  0.0117],
          [-0.0264,  0.0179, -0.1226]]],


        [[[ 0.0771,  0.1518,  0.1077],
          [-0.0831, -0.1812,  0.2130],
          [-0.1706, -0.0225, -0.0657]],

         [[-0.0250, -0.0859,  0.1482],
          [ 0.0009, -0.2074,  0.0551],
          [-0.0046,  0.0764,  0.0312]],

         [[-0.1665, -0.2025,  0.1269],
          [ 0.1444, -0.1873, -0.0130],
          [ 0.0047, -0.0206,  0.2911]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3406e-05,  6.2428e-06,  9.0176e-06],
          [ 1.0495e-05, -2.9709e-05, -1.0904e-05],
          [ 2.5367e-05, -8.6833e-06,  1.0371e-06]],

         [[-4.1202e-07, -9.6649e-06,  2.5600e-05],
          [ 1.7140e-06, -1.0609e-05,  3.9774e-06],
          [ 6.2351e-06,  2.0703e-07, -6.9766e-06]],

         [[ 1.2480e-06, -5.0220e-06,  1.8869e-05],
          [ 9.2204e-06, -1.9805e-05,  1.5768e-05],
          [-1.1094e-05,  2.3859e-06, -1.8790e-05]]],


        [[[ 9.7335e-06, -7.8987e-06, -5.6486e-07],
          [ 1.3649e-05,  1.7160e-05,  1.7982e-05],
          [ 8.0602e-07, -6.8225e-06,  1.1181e-05]],

         [[-1.5506e-05, -1.8237e-05,  1.5644e-05],
          [ 3.1919e-06, -5.1296e-06,  1.0408e-05],
          [ 1.2150e-05, -2.0654e-05,  1.3016e-05]],

         [[ 1.0034e-05, -5.0480e-06, -1.1989e-06],
          [ 1.1568e-06,  2.1084e-06, -1.1602e-05],
          [-3.2745e-06, -1.6129e-05, -1.8417e-06]]],


        [[[-7.1484e-06,  8.9694e-06, -4.2212e-06],
          [-1.2466e-05,  6.5306e-06,  1.1720e-05],
          [ 1.1174e-05,  1.6630e-05,  1.7154e-07]],

         [[ 9.9877e-06,  1.3272e-05,  1.1364e-05],
          [ 7.0097e-06, -3.8658e-06, -3.5450e-06],
          [-4.6007e-06, -2.9927e-06,  1.3971e-05]],

         [[-4.4759e-06,  1.4500e-05,  7.4283e-06],
          [-4.0820e-06, -4.4850e-06, -5.9792e-06],
          [-3.7868e-06, -7.1831e-07,  4.3129e-06]]],


        ...,


        [[[ 2.2611e-05, -1.1488e-05, -1.1765e-05],
          [-4.7454e-06, -1.1150e-05, -9.5550e-06],
          [-9.4097e-06,  2.4547e-06,  6.7281e-06]],

         [[ 1.6875e-05,  1.9466e-05, -1.1419e-05],
          [ 2.0786e-05, -1.3885e-05, -1.1375e-05],
          [-6.0261e-06, -7.5479e-06, -2.9011e-06]],

         [[ 1.8426e-05,  1.0254e-05, -1.0044e-05],
          [-6.7553e-06, -1.5467e-05, -7.3533e-06],
          [ 6.9202e-06,  1.8054e-05, -1.2867e-05]]],


        [[[-1.0357e-05,  8.9816e-06, -1.4660e-05],
          [-1.3999e-05, -1.4340e-05,  1.1836e-05],
          [-1.5177e-05,  1.5454e-05, -1.3428e-05]],

         [[ 9.4506e-06, -1.1800e-05,  8.4434e-06],
          [ 1.5008e-05,  3.2568e-07, -8.5535e-06],
          [-1.4592e-05,  1.1653e-05, -1.9766e-05]],

         [[ 1.2424e-05, -2.0887e-06,  2.9286e-08],
          [ 1.4107e-05, -1.7533e-05,  1.1702e-06],
          [-2.6414e-06,  1.7917e-06, -1.2259e-05]]],


        [[[ 7.7058e-06,  1.5183e-05,  1.0774e-05],
          [-8.3127e-06, -1.8116e-05,  2.1297e-05],
          [-1.7060e-05, -2.2486e-06, -6.5748e-06]],

         [[-2.5014e-06, -8.5910e-06,  1.4822e-05],
          [ 8.9319e-08, -2.0740e-05,  5.5062e-06],
          [-4.6115e-07,  7.6421e-06,  3.1166e-06]],

         [[-1.6646e-05, -2.0252e-05,  1.2687e-05],
          [ 1.4442e-05, -1.8728e-05, -1.3007e-06],
          [ 4.7340e-07, -2.0615e-06,  2.9106e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7351]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0040]], device='cuda:0')

Epoch: 97 | Batch_idx: 0 |  Loss: (0.4641) |  Loss2: (0.2590) | Acc: (92.00%) (118/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.4842) |  Loss2: (0.2589) | Acc: (91.00%) (1295/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.4918) |  Loss2: (0.2589) | Acc: (92.00%) (2477/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.4975) |  Loss2: (0.2589) | Acc: (91.00%) (3646/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.5001) |  Loss2: (0.2588) | Acc: (91.00%) (4817/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.4982) |  Loss2: (0.2588) | Acc: (92.00%) (6009/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.4953) |  Loss2: (0.2588) | Acc: (92.00%) (7186/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.4971) |  Loss2: (0.2587) | Acc: (91.00%) (8355/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.4963) |  Loss2: (0.2587) | Acc: (92.00%) (9541/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.4970) |  Loss2: (0.2587) | Acc: (91.00%) (10713/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.4991) |  Loss2: (0.2586) | Acc: (91.00%) (11880/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.4996) |  Loss2: (0.2586) | Acc: (91.00%) (13050/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.4982) |  Loss2: (0.2585) | Acc: (91.00%) (14234/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.4983) |  Loss2: (0.2585) | Acc: (91.00%) (15411/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.4979) |  Loss2: (0.2585) | Acc: (91.00%) (16584/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.4964) |  Loss2: (0.2584) | Acc: (91.00%) (17770/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.4953) |  Loss2: (0.2584) | Acc: (91.00%) (18947/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.4939) |  Loss2: (0.2584) | Acc: (91.00%) (20134/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.4932) |  Loss2: (0.2583) | Acc: (92.00%) (21321/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.4929) |  Loss2: (0.2583) | Acc: (91.00%) (22487/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.4935) |  Loss2: (0.2582) | Acc: (91.00%) (23658/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.4940) |  Loss2: (0.2582) | Acc: (91.00%) (24828/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.4943) |  Loss2: (0.2582) | Acc: (91.00%) (26001/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.4929) |  Loss2: (0.2581) | Acc: (91.00%) (27198/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.4926) |  Loss2: (0.2581) | Acc: (92.00%) (28383/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.4919) |  Loss2: (0.2581) | Acc: (92.00%) (29564/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.4916) |  Loss2: (0.2580) | Acc: (91.00%) (30732/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.4916) |  Loss2: (0.2580) | Acc: (91.00%) (31906/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.4910) |  Loss2: (0.2580) | Acc: (92.00%) (33095/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.4905) |  Loss2: (0.2580) | Acc: (92.00%) (34273/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.4891) |  Loss2: (0.2579) | Acc: (92.00%) (35474/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.4887) |  Loss2: (0.2579) | Acc: (92.00%) (36652/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.4888) |  Loss2: (0.2579) | Acc: (92.00%) (37824/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.4898) |  Loss2: (0.2578) | Acc: (92.00%) (38990/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.4891) |  Loss2: (0.2578) | Acc: (92.00%) (40174/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.4895) |  Loss2: (0.2577) | Acc: (92.00%) (41339/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.4900) |  Loss2: (0.2577) | Acc: (91.00%) (42506/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.4903) |  Loss2: (0.2577) | Acc: (92.00%) (43690/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.4900) |  Loss2: (0.2576) | Acc: (92.00%) (44872/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.4899) |  Loss2: (0.2576) | Acc: (92.00%) (46003/50000)
# TEST : Loss: (0.4038) | Acc: (86.00%) (8691/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1340,  0.0624,  0.0901],
          [ 0.1049, -0.2970, -0.1090],
          [ 0.2536, -0.0868,  0.0104]],

         [[-0.0041, -0.0966,  0.2559],
          [ 0.0171, -0.1060,  0.0398],
          [ 0.0623,  0.0021, -0.0697]],

         [[ 0.0125, -0.0502,  0.1886],
          [ 0.0922, -0.1980,  0.1576],
          [-0.1109,  0.0238, -0.1878]]],


        [[[ 0.0973, -0.0790, -0.0056],
          [ 0.1364,  0.1715,  0.1797],
          [ 0.0081, -0.0682,  0.1118]],

         [[-0.1550, -0.1823,  0.1564],
          [ 0.0319, -0.0513,  0.1040],
          [ 0.1215, -0.2065,  0.1301]],

         [[ 0.1003, -0.0505, -0.0120],
          [ 0.0116,  0.0211, -0.1160],
          [-0.0327, -0.1612, -0.0184]]],


        [[[-0.0715,  0.0897, -0.0422],
          [-0.1246,  0.0653,  0.1172],
          [ 0.1117,  0.1662,  0.0017]],

         [[ 0.0998,  0.1327,  0.1136],
          [ 0.0701, -0.0386, -0.0354],
          [-0.0460, -0.0299,  0.1397]],

         [[-0.0447,  0.1449,  0.0743],
          [-0.0408, -0.0448, -0.0598],
          [-0.0379, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2260, -0.1148, -0.1176],
          [-0.0474, -0.1115, -0.0955],
          [-0.0941,  0.0245,  0.0673]],

         [[ 0.1687,  0.1946, -0.1141],
          [ 0.2078, -0.1388, -0.1137],
          [-0.0602, -0.0755, -0.0290]],

         [[ 0.1842,  0.1025, -0.1004],
          [-0.0675, -0.1546, -0.0735],
          [ 0.0692,  0.1805, -0.1286]]],


        [[[-0.1035,  0.0898, -0.1465],
          [-0.1399, -0.1433,  0.1183],
          [-0.1517,  0.1545, -0.1342]],

         [[ 0.0945, -0.1180,  0.0844],
          [ 0.1500,  0.0033, -0.0855],
          [-0.1459,  0.1165, -0.1976]],

         [[ 0.1242, -0.0209,  0.0003],
          [ 0.1410, -0.1753,  0.0117],
          [-0.0264,  0.0179, -0.1225]]],


        [[[ 0.0770,  0.1518,  0.1077],
          [-0.0831, -0.1811,  0.2129],
          [-0.1705, -0.0225, -0.0657]],

         [[-0.0250, -0.0859,  0.1482],
          [ 0.0009, -0.2073,  0.0550],
          [-0.0046,  0.0764,  0.0312]],

         [[-0.1664, -0.2024,  0.1268],
          [ 0.1444, -0.1872, -0.0130],
          [ 0.0047, -0.0206,  0.2909]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3400e-05,  6.2404e-06,  9.0141e-06],
          [ 1.0490e-05, -2.9697e-05, -1.0900e-05],
          [ 2.5356e-05, -8.6798e-06,  1.0367e-06]],

         [[-4.1186e-07, -9.6612e-06,  2.5590e-05],
          [ 1.7134e-06, -1.0605e-05,  3.9758e-06],
          [ 6.2326e-06,  2.0695e-07, -6.9740e-06]],

         [[ 1.2475e-06, -5.0201e-06,  1.8861e-05],
          [ 9.2169e-06, -1.9797e-05,  1.5761e-05],
          [-1.1090e-05,  2.3849e-06, -1.8782e-05]]],


        [[[ 9.7297e-06, -7.8955e-06, -5.6464e-07],
          [ 1.3644e-05,  1.7153e-05,  1.7975e-05],
          [ 8.0569e-07, -6.8199e-06,  1.1177e-05]],

         [[-1.5501e-05, -1.8230e-05,  1.5638e-05],
          [ 3.1905e-06, -5.1275e-06,  1.0404e-05],
          [ 1.2146e-05, -2.0646e-05,  1.3011e-05]],

         [[ 1.0030e-05, -5.0460e-06, -1.1984e-06],
          [ 1.1563e-06,  2.1076e-06, -1.1597e-05],
          [-3.2732e-06, -1.6123e-05, -1.8410e-06]]],


        [[[-7.1455e-06,  8.9659e-06, -4.2196e-06],
          [-1.2461e-05,  6.5280e-06,  1.1716e-05],
          [ 1.1170e-05,  1.6624e-05,  1.7148e-07]],

         [[ 9.9839e-06,  1.3267e-05,  1.1360e-05],
          [ 7.0071e-06, -3.8644e-06, -3.5435e-06],
          [-4.5989e-06, -2.9915e-06,  1.3966e-05]],

         [[-4.4742e-06,  1.4494e-05,  7.4254e-06],
          [-4.0804e-06, -4.4833e-06, -5.9768e-06],
          [-3.7853e-06, -7.1803e-07,  4.3112e-06]]],


        ...,


        [[[ 2.2602e-05, -1.1483e-05, -1.1760e-05],
          [-4.7435e-06, -1.1145e-05, -9.5512e-06],
          [-9.4059e-06,  2.4538e-06,  6.7254e-06]],

         [[ 1.6868e-05,  1.9459e-05, -1.1415e-05],
          [ 2.0778e-05, -1.3879e-05, -1.1371e-05],
          [-6.0238e-06, -7.5450e-06, -2.8999e-06]],

         [[ 1.8419e-05,  1.0250e-05, -1.0041e-05],
          [-6.7527e-06, -1.5461e-05, -7.3503e-06],
          [ 6.9175e-06,  1.8047e-05, -1.2862e-05]]],


        [[[-1.0352e-05,  8.9781e-06, -1.4654e-05],
          [-1.3993e-05, -1.4334e-05,  1.1831e-05],
          [-1.5171e-05,  1.5448e-05, -1.3423e-05]],

         [[ 9.4468e-06, -1.1795e-05,  8.4402e-06],
          [ 1.5002e-05,  3.2555e-07, -8.5502e-06],
          [-1.4586e-05,  1.1648e-05, -1.9758e-05]],

         [[ 1.2419e-05, -2.0879e-06,  2.9275e-08],
          [ 1.4101e-05, -1.7526e-05,  1.1697e-06],
          [-2.6403e-06,  1.7909e-06, -1.2254e-05]]],


        [[[ 7.7029e-06,  1.5177e-05,  1.0770e-05],
          [-8.3095e-06, -1.8109e-05,  2.1289e-05],
          [-1.7054e-05, -2.2477e-06, -6.5722e-06]],

         [[-2.5004e-06, -8.5875e-06,  1.4817e-05],
          [ 8.9284e-08, -2.0732e-05,  5.5040e-06],
          [-4.6097e-07,  7.6392e-06,  3.1154e-06]],

         [[-1.6640e-05, -2.0244e-05,  1.2682e-05],
          [ 1.4436e-05, -1.8720e-05, -1.3002e-06],
          [ 4.7321e-07, -2.0607e-06,  2.9094e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7375]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0075]], device='cuda:0')

Epoch: 98 | Batch_idx: 0 |  Loss: (0.5138) |  Loss2: (0.2563) | Acc: (90.00%) (116/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.4878) |  Loss2: (0.2562) | Acc: (91.00%) (1292/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.4940) |  Loss2: (0.2562) | Acc: (91.00%) (2465/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.4963) |  Loss2: (0.2561) | Acc: (91.00%) (3633/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.4918) |  Loss2: (0.2561) | Acc: (91.00%) (4818/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.4892) |  Loss2: (0.2560) | Acc: (91.00%) (5994/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.4901) |  Loss2: (0.2560) | Acc: (91.00%) (7167/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.4855) |  Loss2: (0.2559) | Acc: (91.00%) (8357/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.4867) |  Loss2: (0.2559) | Acc: (91.00%) (9523/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.4903) |  Loss2: (0.2558) | Acc: (91.00%) (10689/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.4917) |  Loss2: (0.2558) | Acc: (91.00%) (11854/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.4909) |  Loss2: (0.2558) | Acc: (91.00%) (13041/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.4920) |  Loss2: (0.2557) | Acc: (91.00%) (14207/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.4906) |  Loss2: (0.2557) | Acc: (91.00%) (15385/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.4872) |  Loss2: (0.2557) | Acc: (91.00%) (16585/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.4861) |  Loss2: (0.2556) | Acc: (91.00%) (17762/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.4865) |  Loss2: (0.2556) | Acc: (91.00%) (18929/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.4862) |  Loss2: (0.2555) | Acc: (91.00%) (20100/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.4855) |  Loss2: (0.2555) | Acc: (91.00%) (21284/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.4837) |  Loss2: (0.2555) | Acc: (91.00%) (22481/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.4841) |  Loss2: (0.2554) | Acc: (91.00%) (23654/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.4844) |  Loss2: (0.2554) | Acc: (91.00%) (24826/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.4835) |  Loss2: (0.2554) | Acc: (91.00%) (26017/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.4826) |  Loss2: (0.2553) | Acc: (91.00%) (27201/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.4807) |  Loss2: (0.2553) | Acc: (92.00%) (28408/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.4806) |  Loss2: (0.2553) | Acc: (92.00%) (29592/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.4816) |  Loss2: (0.2552) | Acc: (92.00%) (30759/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.4810) |  Loss2: (0.2552) | Acc: (92.00%) (31950/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.4814) |  Loss2: (0.2551) | Acc: (92.00%) (33126/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.4818) |  Loss2: (0.2551) | Acc: (92.00%) (34288/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.4814) |  Loss2: (0.2551) | Acc: (92.00%) (35475/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.4823) |  Loss2: (0.2550) | Acc: (92.00%) (36639/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.4819) |  Loss2: (0.2550) | Acc: (92.00%) (37822/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.4818) |  Loss2: (0.2550) | Acc: (92.00%) (39006/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.4814) |  Loss2: (0.2549) | Acc: (92.00%) (40185/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.4813) |  Loss2: (0.2549) | Acc: (92.00%) (41367/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.4815) |  Loss2: (0.2549) | Acc: (92.00%) (42537/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.4817) |  Loss2: (0.2548) | Acc: (92.00%) (43709/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.4810) |  Loss2: (0.2548) | Acc: (92.00%) (44906/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.4803) |  Loss2: (0.2548) | Acc: (92.00%) (46051/50000)
# TEST : Loss: (0.3957) | Acc: (87.00%) (8704/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1340,  0.0624,  0.0901],
          [ 0.1049, -0.2969, -0.1090],
          [ 0.2535, -0.0868,  0.0104]],

         [[-0.0041, -0.0966,  0.2558],
          [ 0.0171, -0.1060,  0.0397],
          [ 0.0623,  0.0021, -0.0697]],

         [[ 0.0125, -0.0502,  0.1885],
          [ 0.0921, -0.1979,  0.1575],
          [-0.1109,  0.0238, -0.1877]]],


        [[[ 0.0973, -0.0789, -0.0056],
          [ 0.1364,  0.1715,  0.1797],
          [ 0.0081, -0.0682,  0.1117]],

         [[-0.1549, -0.1822,  0.1563],
          [ 0.0319, -0.0513,  0.1040],
          [ 0.1214, -0.2064,  0.1301]],

         [[ 0.1003, -0.0504, -0.0120],
          [ 0.0116,  0.0211, -0.1159],
          [-0.0327, -0.1612, -0.0184]]],


        [[[-0.0714,  0.0896, -0.0422],
          [-0.1246,  0.0653,  0.1171],
          [ 0.1117,  0.1662,  0.0017]],

         [[ 0.0998,  0.1326,  0.1136],
          [ 0.0700, -0.0386, -0.0354],
          [-0.0460, -0.0299,  0.1396]],

         [[-0.0447,  0.1449,  0.0742],
          [-0.0408, -0.0448, -0.0597],
          [-0.0378, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2259, -0.1148, -0.1176],
          [-0.0474, -0.1114, -0.0955],
          [-0.0940,  0.0245,  0.0672]],

         [[ 0.1686,  0.1945, -0.1141],
          [ 0.2077, -0.1387, -0.1137],
          [-0.0602, -0.0754, -0.0290]],

         [[ 0.1841,  0.1025, -0.1004],
          [-0.0675, -0.1545, -0.0735],
          [ 0.0691,  0.1804, -0.1286]]],


        [[[-0.1035,  0.0897, -0.1465],
          [-0.1399, -0.1433,  0.1183],
          [-0.1517,  0.1544, -0.1342]],

         [[ 0.0944, -0.1179,  0.0844],
          [ 0.1500,  0.0033, -0.0855],
          [-0.1458,  0.1164, -0.1975]],

         [[ 0.1241, -0.0209,  0.0003],
          [ 0.1410, -0.1752,  0.0117],
          [-0.0264,  0.0179, -0.1225]]],


        [[[ 0.0770,  0.1517,  0.1077],
          [-0.0831, -0.1810,  0.2128],
          [-0.1705, -0.0225, -0.0657]],

         [[-0.0250, -0.0858,  0.1481],
          [ 0.0009, -0.2072,  0.0550],
          [-0.0046,  0.0764,  0.0311]],

         [[-0.1663, -0.2024,  0.1268],
          [ 0.1443, -0.1871, -0.0130],
          [ 0.0047, -0.0206,  0.2908]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3395e-05,  6.2379e-06,  9.0106e-06],
          [ 1.0486e-05, -2.9686e-05, -1.0896e-05],
          [ 2.5346e-05, -8.6763e-06,  1.0363e-06]],

         [[-4.1169e-07, -9.6574e-06,  2.5579e-05],
          [ 1.7127e-06, -1.0601e-05,  3.9742e-06],
          [ 6.2301e-06,  2.0687e-07, -6.9714e-06]],

         [[ 1.2471e-06, -5.0182e-06,  1.8854e-05],
          [ 9.2134e-06, -1.9790e-05,  1.5755e-05],
          [-1.1086e-05,  2.3840e-06, -1.8774e-05]]],


        [[[ 9.7259e-06, -7.8923e-06, -5.6442e-07],
          [ 1.3639e-05,  1.7146e-05,  1.7968e-05],
          [ 8.0536e-07, -6.8173e-06,  1.1173e-05]],

         [[-1.5495e-05, -1.8223e-05,  1.5632e-05],
          [ 3.1892e-06, -5.1255e-06,  1.0400e-05],
          [ 1.2141e-05, -2.0637e-05,  1.3006e-05]],

         [[ 1.0026e-05, -5.0440e-06, -1.1979e-06],
          [ 1.1559e-06,  2.1068e-06, -1.1592e-05],
          [-3.2719e-06, -1.6117e-05, -1.8402e-06]]],


        [[[-7.1426e-06,  8.9624e-06, -4.2180e-06],
          [-1.2456e-05,  6.5253e-06,  1.1711e-05],
          [ 1.1165e-05,  1.6617e-05,  1.7141e-07]],

         [[ 9.9801e-06,  1.3262e-05,  1.1355e-05],
          [ 7.0045e-06, -3.8629e-06, -3.5420e-06],
          [-4.5972e-06, -2.9904e-06,  1.3961e-05]],

         [[-4.4724e-06,  1.4488e-05,  7.4225e-06],
          [-4.0788e-06, -4.4816e-06, -5.9745e-06],
          [-3.7839e-06, -7.1776e-07,  4.3095e-06]]],


        ...,


        [[[ 2.2593e-05, -1.1479e-05, -1.1755e-05],
          [-4.7416e-06, -1.1141e-05, -9.5474e-06],
          [-9.4022e-06,  2.4528e-06,  6.7228e-06]],

         [[ 1.6862e-05,  1.9451e-05, -1.1410e-05],
          [ 2.0770e-05, -1.3874e-05, -1.1367e-05],
          [-6.0214e-06, -7.5421e-06, -2.8987e-06]],

         [[ 1.8412e-05,  1.0246e-05, -1.0037e-05],
          [-6.7501e-06, -1.5455e-05, -7.3474e-06],
          [ 6.9149e-06,  1.8040e-05, -1.2857e-05]]],


        [[[-1.0348e-05,  8.9746e-06, -1.4648e-05],
          [-1.3988e-05, -1.4328e-05,  1.1826e-05],
          [-1.5166e-05,  1.5443e-05, -1.3418e-05]],

         [[ 9.4430e-06, -1.1791e-05,  8.4370e-06],
          [ 1.4996e-05,  3.2542e-07, -8.5470e-06],
          [-1.4580e-05,  1.1644e-05, -1.9751e-05]],

         [[ 1.2414e-05, -2.0871e-06,  2.9263e-08],
          [ 1.4096e-05, -1.7519e-05,  1.1692e-06],
          [-2.6393e-06,  1.7902e-06, -1.2249e-05]]],


        [[[ 7.7000e-06,  1.5172e-05,  1.0766e-05],
          [-8.3063e-06, -1.8102e-05,  2.1281e-05],
          [-1.7048e-05, -2.2468e-06, -6.5696e-06]],

         [[-2.4995e-06, -8.5840e-06,  1.4811e-05],
          [ 8.9250e-08, -2.0724e-05,  5.5018e-06],
          [-4.6079e-07,  7.6363e-06,  3.1141e-06]],

         [[-1.6633e-05, -2.0236e-05,  1.2676e-05],
          [ 1.4430e-05, -1.8712e-05, -1.2997e-06],
          [ 4.7303e-07, -2.0599e-06,  2.9083e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7398]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0447]], device='cuda:0')

Epoch: 99 | Batch_idx: 0 |  Loss: (0.4760) |  Loss2: (0.2536) | Acc: (92.00%) (119/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.4893) |  Loss2: (0.2535) | Acc: (92.00%) (1303/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.4741) |  Loss2: (0.2535) | Acc: (92.00%) (2496/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.4770) |  Loss2: (0.2535) | Acc: (92.00%) (3675/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.4802) |  Loss2: (0.2534) | Acc: (92.00%) (4858/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.4768) |  Loss2: (0.2534) | Acc: (92.00%) (6058/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.4756) |  Loss2: (0.2533) | Acc: (92.00%) (7237/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.4760) |  Loss2: (0.2533) | Acc: (92.00%) (8424/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.4768) |  Loss2: (0.2532) | Acc: (92.00%) (9606/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.4746) |  Loss2: (0.2532) | Acc: (92.00%) (10796/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.4788) |  Loss2: (0.2532) | Acc: (92.00%) (11954/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.4795) |  Loss2: (0.2531) | Acc: (92.00%) (13122/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.4787) |  Loss2: (0.2531) | Acc: (92.00%) (14307/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.4779) |  Loss2: (0.2530) | Acc: (92.00%) (15494/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.4782) |  Loss2: (0.2530) | Acc: (92.00%) (16681/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.4776) |  Loss2: (0.2530) | Acc: (92.00%) (17862/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.4775) |  Loss2: (0.2529) | Acc: (92.00%) (19042/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.4761) |  Loss2: (0.2529) | Acc: (92.00%) (20236/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.4768) |  Loss2: (0.2529) | Acc: (92.00%) (21416/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.4771) |  Loss2: (0.2528) | Acc: (92.00%) (22600/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.4775) |  Loss2: (0.2528) | Acc: (92.00%) (23778/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.4781) |  Loss2: (0.2528) | Acc: (92.00%) (24949/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.4767) |  Loss2: (0.2527) | Acc: (92.00%) (26140/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.4777) |  Loss2: (0.2527) | Acc: (92.00%) (27311/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.4777) |  Loss2: (0.2527) | Acc: (92.00%) (28489/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.4783) |  Loss2: (0.2527) | Acc: (92.00%) (29662/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.4780) |  Loss2: (0.2526) | Acc: (92.00%) (30848/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.4783) |  Loss2: (0.2526) | Acc: (92.00%) (32025/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.4783) |  Loss2: (0.2526) | Acc: (92.00%) (33208/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.4776) |  Loss2: (0.2525) | Acc: (92.00%) (34395/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.4779) |  Loss2: (0.2525) | Acc: (92.00%) (35568/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.4777) |  Loss2: (0.2525) | Acc: (92.00%) (36750/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.4774) |  Loss2: (0.2524) | Acc: (92.00%) (37940/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.4773) |  Loss2: (0.2524) | Acc: (92.00%) (39128/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.4773) |  Loss2: (0.2524) | Acc: (92.00%) (40304/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.4775) |  Loss2: (0.2523) | Acc: (92.00%) (41478/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.4766) |  Loss2: (0.2523) | Acc: (92.00%) (42675/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.4765) |  Loss2: (0.2523) | Acc: (92.00%) (43869/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.4758) |  Loss2: (0.2523) | Acc: (92.00%) (45062/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.4754) |  Loss2: (0.2522) | Acc: (92.00%) (46206/50000)
# TEST : Loss: (0.3922) | Acc: (87.00%) (8724/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1339,  0.0624,  0.0901],
          [ 0.1048, -0.2967, -0.1089],
          [ 0.2534, -0.0867,  0.0104]],

         [[-0.0041, -0.0965,  0.2557],
          [ 0.0171, -0.1060,  0.0397],
          [ 0.0623,  0.0021, -0.0697]],

         [[ 0.0125, -0.0502,  0.1885],
          [ 0.0921, -0.1978,  0.1575],
          [-0.1108,  0.0238, -0.1877]]],


        [[[ 0.0972, -0.0789, -0.0056],
          [ 0.1363,  0.1714,  0.1796],
          [ 0.0081, -0.0681,  0.1117]],

         [[-0.1549, -0.1822,  0.1563],
          [ 0.0319, -0.0512,  0.1040],
          [ 0.1214, -0.2063,  0.1300]],

         [[ 0.1002, -0.0504, -0.0120],
          [ 0.0116,  0.0211, -0.1159],
          [-0.0327, -0.1611, -0.0184]]],


        [[[-0.0714,  0.0896, -0.0422],
          [-0.1245,  0.0652,  0.1171],
          [ 0.1116,  0.1661,  0.0017]],

         [[ 0.0998,  0.1326,  0.1135],
          [ 0.0700, -0.0386, -0.0354],
          [-0.0460, -0.0299,  0.1396]],

         [[-0.0447,  0.1448,  0.0742],
          [-0.0408, -0.0448, -0.0597],
          [-0.0378, -0.0072,  0.0431]]],


        ...,


        [[[ 0.2258, -0.1147, -0.1175],
          [-0.0474, -0.1114, -0.0954],
          [-0.0940,  0.0245,  0.0672]],

         [[ 0.1686,  0.1944, -0.1141],
          [ 0.2076, -0.1387, -0.1136],
          [-0.0602, -0.0754, -0.0290]],

         [[ 0.1840,  0.1024, -0.1003],
          [-0.0675, -0.1545, -0.0734],
          [ 0.0691,  0.1803, -0.1285]]],


        [[[-0.1034,  0.0897, -0.1464],
          [-0.1398, -0.1432,  0.1182],
          [-0.1516,  0.1544, -0.1341]],

         [[ 0.0944, -0.1179,  0.0843],
          [ 0.1499,  0.0033, -0.0854],
          [-0.1457,  0.1164, -0.1974]],

         [[ 0.1241, -0.0209,  0.0003],
          [ 0.1409, -0.1751,  0.0117],
          [-0.0264,  0.0179, -0.1224]]],


        [[[ 0.0770,  0.1517,  0.1076],
          [-0.0830, -0.1809,  0.2127],
          [-0.1704, -0.0225, -0.0657]],

         [[-0.0250, -0.0858,  0.1480],
          [ 0.0009, -0.2072,  0.0550],
          [-0.0046,  0.0763,  0.0311]],

         [[-0.1663, -0.2023,  0.1267],
          [ 0.1442, -0.1870, -0.0130],
          [ 0.0047, -0.0206,  0.2907]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3390e-05,  6.2354e-06,  9.0071e-06],
          [ 1.0482e-05, -2.9674e-05, -1.0891e-05],
          [ 2.5335e-05, -8.6728e-06,  1.0359e-06]],

         [[-4.1153e-07, -9.6536e-06,  2.5569e-05],
          [ 1.7121e-06, -1.0597e-05,  3.9726e-06],
          [ 6.2276e-06,  2.0679e-07, -6.9688e-06]],

         [[ 1.2466e-06, -5.0163e-06,  1.8846e-05],
          [ 9.2099e-06, -1.9782e-05,  1.5748e-05],
          [-1.1081e-05,  2.3831e-06, -1.8767e-05]]],


        [[[ 9.7222e-06, -7.8891e-06, -5.6420e-07],
          [ 1.3633e-05,  1.7139e-05,  1.7961e-05],
          [ 8.0503e-07, -6.8147e-06,  1.1168e-05]],

         [[-1.5489e-05, -1.8216e-05,  1.5626e-05],
          [ 3.1879e-06, -5.1234e-06,  1.0396e-05],
          [ 1.2136e-05, -2.0629e-05,  1.3001e-05]],

         [[ 1.0022e-05, -5.0419e-06, -1.1975e-06],
          [ 1.1555e-06,  2.1060e-06, -1.1588e-05],
          [-3.2706e-06, -1.6110e-05, -1.8395e-06]]],


        [[[-7.1397e-06,  8.9589e-06, -4.2164e-06],
          [-1.2451e-05,  6.5227e-06,  1.1707e-05],
          [ 1.1161e-05,  1.6611e-05,  1.7134e-07]],

         [[ 9.9763e-06,  1.3257e-05,  1.1351e-05],
          [ 7.0019e-06, -3.8615e-06, -3.5406e-06],
          [-4.5954e-06, -2.9892e-06,  1.3955e-05]],

         [[-4.4707e-06,  1.4482e-05,  7.4196e-06],
          [-4.0772e-06, -4.4798e-06, -5.9722e-06],
          [-3.7824e-06, -7.1749e-07,  4.3077e-06]]],


        ...,


        [[[ 2.2584e-05, -1.1474e-05, -1.1751e-05],
          [-4.7397e-06, -1.1137e-05, -9.5436e-06],
          [-9.3984e-06,  2.4519e-06,  6.7202e-06]],

         [[ 1.6856e-05,  1.9444e-05, -1.1406e-05],
          [ 2.0762e-05, -1.3869e-05, -1.1362e-05],
          [-6.0191e-06, -7.5392e-06, -2.8976e-06]],

         [[ 1.8405e-05,  1.0242e-05, -1.0033e-05],
          [-6.7475e-06, -1.5449e-05, -7.3445e-06],
          [ 6.9123e-06,  1.8033e-05, -1.2852e-05]]],


        [[[-1.0344e-05,  8.9711e-06, -1.4643e-05],
          [-1.3983e-05, -1.4322e-05,  1.1822e-05],
          [-1.5160e-05,  1.5437e-05, -1.3412e-05]],

         [[ 9.4392e-06, -1.1786e-05,  8.4338e-06],
          [ 1.4990e-05,  3.2529e-07, -8.5438e-06],
          [-1.4575e-05,  1.1639e-05, -1.9743e-05]],

         [[ 1.2409e-05, -2.0863e-06,  2.9252e-08],
          [ 1.4091e-05, -1.7512e-05,  1.1688e-06],
          [-2.6383e-06,  1.7895e-06, -1.2245e-05]]],


        [[[ 7.6971e-06,  1.5166e-05,  1.0762e-05],
          [-8.3031e-06, -1.8095e-05,  2.1273e-05],
          [-1.7041e-05, -2.2460e-06, -6.5670e-06]],

         [[-2.4985e-06, -8.5805e-06,  1.4805e-05],
          [ 8.9216e-08, -2.0716e-05,  5.4997e-06],
          [-4.6060e-07,  7.6334e-06,  3.1129e-06]],

         [[-1.6627e-05, -2.0228e-05,  1.2671e-05],
          [ 1.4424e-05, -1.8705e-05, -1.2992e-06],
          [ 4.7285e-07, -2.0591e-06,  2.9071e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7486]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0137]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (3667/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (4841/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (6024/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (7192/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (8365/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (91.00%) (9528/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (10696/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (11862/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (13039/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (14231/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (91.00%) (15425/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (92.00%) (16626/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (17802/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (18982/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (20158/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (21324/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (22495/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (23677/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (24855/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (26037/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (27220/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (28411/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (29606/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (30789/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (31974/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (33116/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (34301/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (35483/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (36657/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (37833/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (39002/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (40185/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (41362/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (92.00%) (42541/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (43709/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (44886/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (46019/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_100.pth.tar'
# TEST : Loss: (0.4209) | Acc: (86.00%) (8697/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1323,  0.0646,  0.0918],
          [ 0.1076, -0.2954, -0.1062],
          [ 0.2528, -0.0863,  0.0123]],

         [[-0.0011, -0.0907,  0.2613],
          [ 0.0202, -0.1029,  0.0452],
          [ 0.0618,  0.0026, -0.0674]],

         [[ 0.0124, -0.0471,  0.1927],
          [ 0.0919, -0.1976,  0.1611],
          [-0.1130,  0.0223, -0.1866]]],


        [[[ 0.0962, -0.0787, -0.0056],
          [ 0.1358,  0.1717,  0.1800],
          [ 0.0084, -0.0674,  0.1123]],

         [[-0.1559, -0.1822,  0.1553],
          [ 0.0319, -0.0506,  0.1039],
          [ 0.1220, -0.2054,  0.1303]],

         [[ 0.0990, -0.0513, -0.0136],
          [ 0.0112,  0.0205, -0.1171],
          [-0.0324, -0.1614, -0.0193]]],


        [[[-0.0718,  0.0894, -0.0424],
          [-0.1248,  0.0653,  0.1171],
          [ 0.1112,  0.1660,  0.0018]],

         [[ 0.0994,  0.1324,  0.1133],
          [ 0.0698, -0.0385, -0.0353],
          [-0.0463, -0.0299,  0.1396]],

         [[-0.0450,  0.1445,  0.0739],
          [-0.0412, -0.0450, -0.0598],
          [-0.0384, -0.0074,  0.0430]]],


        ...,


        [[[ 0.2235, -0.1158, -0.1190],
          [-0.0505, -0.1133, -0.0966],
          [-0.0956,  0.0251,  0.0688]],

         [[ 0.1675,  0.1950, -0.1137],
          [ 0.2059, -0.1391, -0.1134],
          [-0.0602, -0.0734, -0.0261]],

         [[ 0.1842,  0.1033, -0.1005],
          [-0.0679, -0.1549, -0.0741],
          [ 0.0696,  0.1813, -0.1275]]],


        [[[-0.1032,  0.0905, -0.1453],
          [-0.1408, -0.1435,  0.1183],
          [-0.1521,  0.1543, -0.1339]],

         [[ 0.0950, -0.1164,  0.0857],
          [ 0.1492,  0.0034, -0.0851],
          [-0.1462,  0.1166, -0.1971]],

         [[ 0.1232, -0.0210,  0.0004],
          [ 0.1391, -0.1761,  0.0111],
          [-0.0282,  0.0170, -0.1229]]],


        [[[ 0.0743,  0.1532,  0.1093],
          [-0.0852, -0.1845,  0.2107],
          [-0.1679, -0.0209, -0.0649]],

         [[-0.0250, -0.0834,  0.1499],
          [ 0.0016, -0.2095,  0.0535],
          [ 0.0006,  0.0792,  0.0324]],

         [[-0.1681, -0.2026,  0.1256],
          [ 0.1427, -0.1926, -0.0174],
          [ 0.0073, -0.0216,  0.2883]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1012, -0.1294, -0.1518],
          [-0.0899, -0.1230, -0.1504],
          [-0.1254, -0.1680, -0.1621]],

         [[-0.0789, -0.0914, -0.1034],
          [-0.0559, -0.0788, -0.1019],
          [-0.0755, -0.1143, -0.1124]],

         [[-0.0253, -0.0375, -0.0432],
          [ 0.0034, -0.0188, -0.0385],
          [-0.0027, -0.0390, -0.0463]]],


        [[[-0.0085, -0.0043, -0.0080],
          [-0.0029,  0.0005,  0.0011],
          [ 0.0060,  0.0076,  0.0070]],

         [[-0.0140, -0.0087, -0.0111],
          [-0.0071, -0.0010, -0.0007],
          [ 0.0039,  0.0087,  0.0057]],

         [[-0.0182, -0.0143, -0.0145],
          [-0.0130, -0.0062, -0.0039],
          [-0.0038,  0.0023,  0.0016]]],


        [[[ 0.0014, -0.0027, -0.0018],
          [ 0.0035, -0.0004,  0.0001],
          [ 0.0041,  0.0011,  0.0010]],

         [[ 0.0033, -0.0007,  0.0003],
          [ 0.0047,  0.0014,  0.0021],
          [ 0.0062,  0.0038,  0.0037]],

         [[ 0.0027, -0.0000,  0.0006],
          [ 0.0037,  0.0017,  0.0019],
          [ 0.0058,  0.0047,  0.0044]]],


        ...,


        [[[-0.0063, -0.0239, -0.0150],
          [ 0.0052, -0.0131, -0.0058],
          [ 0.0246,  0.0068,  0.0026]],

         [[-0.0039, -0.0231, -0.0164],
          [ 0.0119, -0.0099, -0.0023],
          [ 0.0334,  0.0135,  0.0115]],

         [[-0.0521, -0.0664, -0.0557],
          [-0.0293, -0.0463, -0.0350],
          [-0.0030, -0.0188, -0.0181]]],


        [[[ 0.0100,  0.0052,  0.0009],
          [ 0.0141,  0.0073,  0.0039],
          [ 0.0094,  0.0056,  0.0042]],

         [[ 0.0023, -0.0044, -0.0115],
          [ 0.0059, -0.0023, -0.0080],
          [ 0.0006, -0.0041, -0.0080]],

         [[-0.0232, -0.0260, -0.0296],
          [-0.0172, -0.0219, -0.0254],
          [-0.0191, -0.0217, -0.0250]]],


        [[[-0.0793, -0.0840, -0.0707],
          [-0.0637, -0.0636, -0.0603],
          [-0.0754, -0.0730, -0.0682]],

         [[-0.0829, -0.0781, -0.0561],
          [-0.0590, -0.0580, -0.0510],
          [-0.0663, -0.0743, -0.0661]],

         [[-0.0395, -0.0419, -0.0359],
          [-0.0175, -0.0236, -0.0290],
          [-0.0219, -0.0397, -0.0409]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7489]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 101 | Batch_idx: 0 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (3627/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (4814/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (5985/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (7174/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (8341/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (91.00%) (9517/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (91.00%) (10706/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (91.00%) (11866/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (13042/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (91.00%) (14227/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (91.00%) (15405/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (91.00%) (16573/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (91.00%) (17746/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (91.00%) (18927/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (91.00%) (20111/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (91.00%) (21289/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (91.00%) (22469/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (91.00%) (23642/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (91.00%) (24833/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (91.00%) (26006/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (91.00%) (27187/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (28376/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (29554/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (91.00%) (30723/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (91.00%) (31900/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (91.00%) (33078/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (91.00%) (34256/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (91.00%) (35420/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (91.00%) (36607/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (91.00%) (37787/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (91.00%) (38959/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (91.00%) (40126/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (41304/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (91.00%) (42475/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (91.00%) (43667/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (91.00%) (44837/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (91.00%) (45962/50000)
# TEST : Loss: (0.4241) | Acc: (86.00%) (8645/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1309,  0.0656,  0.0915],
          [ 0.1074, -0.2967, -0.1097],
          [ 0.2531, -0.0840,  0.0100]],

         [[-0.0027, -0.0919,  0.2597],
          [ 0.0168, -0.1061,  0.0413],
          [ 0.0586,  0.0019, -0.0704]],

         [[ 0.0147, -0.0436,  0.1967],
          [ 0.0908, -0.1980,  0.1609],
          [-0.1148,  0.0230, -0.1873]]],


        [[[ 0.0973, -0.0777, -0.0053],
          [ 0.1361,  0.1722,  0.1801],
          [ 0.0089, -0.0670,  0.1126]],

         [[-0.1550, -0.1817,  0.1548],
          [ 0.0320, -0.0505,  0.1034],
          [ 0.1223, -0.2052,  0.1302]],

         [[ 0.0998, -0.0510, -0.0141],
          [ 0.0113,  0.0203, -0.1177],
          [-0.0318, -0.1615, -0.0196]]],


        [[[-0.0712,  0.0897, -0.0424],
          [-0.1245,  0.0654,  0.1169],
          [ 0.1112,  0.1660,  0.0017]],

         [[ 0.0998,  0.1326,  0.1131],
          [ 0.0697, -0.0385, -0.0356],
          [-0.0465, -0.0301,  0.1391]],

         [[-0.0449,  0.1444,  0.0737],
          [-0.0415, -0.0453, -0.0603],
          [-0.0388, -0.0079,  0.0424]]],


        ...,


        [[[ 0.2241, -0.1164, -0.1213],
          [-0.0490, -0.1124, -0.0965],
          [-0.0954,  0.0248,  0.0673]],

         [[ 0.1662,  0.1926, -0.1175],
          [ 0.2060, -0.1395, -0.1145],
          [-0.0609, -0.0747, -0.0282]],

         [[ 0.1827,  0.1016, -0.1033],
          [-0.0678, -0.1545, -0.0743],
          [ 0.0689,  0.1808, -0.1287]]],


        [[[-0.1054,  0.0873, -0.1481],
          [-0.1419, -0.1452,  0.1173],
          [-0.1517,  0.1541, -0.1333]],

         [[ 0.0943, -0.1183,  0.0835],
          [ 0.1495,  0.0031, -0.0852],
          [-0.1444,  0.1178, -0.1953]],

         [[ 0.1231, -0.0221, -0.0010],
          [ 0.1394, -0.1761,  0.0110],
          [-0.0267,  0.0182, -0.1213]]],


        [[[ 0.0700,  0.1531,  0.1091],
          [-0.0858, -0.1851,  0.2133],
          [-0.1713, -0.0256, -0.0679]],

         [[-0.0257, -0.0815,  0.1514],
          [ 0.0035, -0.2080,  0.0576],
          [-0.0003,  0.0775,  0.0314]],

         [[-0.1655, -0.1982,  0.1289],
          [ 0.1487, -0.1876, -0.0106],
          [ 0.0115, -0.0188,  0.2921]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0413,  0.1061,  0.1300],
          [ 0.0455,  0.1019,  0.1121],
          [ 0.0521,  0.0904,  0.0991]],

         [[ 0.0410,  0.1116,  0.1175],
          [ 0.0378,  0.0980,  0.0967],
          [ 0.0380,  0.0776,  0.0785]],

         [[ 0.0443,  0.0981,  0.1154],
          [ 0.0428,  0.0829,  0.0898],
          [ 0.0384,  0.0653,  0.0648]]],


        [[[-0.0247, -0.0149, -0.0150],
          [-0.0016,  0.0027,  0.0016],
          [ 0.0205,  0.0219,  0.0240]],

         [[-0.0222, -0.0092, -0.0037],
          [ 0.0039,  0.0119,  0.0148],
          [ 0.0288,  0.0328,  0.0379]],

         [[-0.0103,  0.0028,  0.0050],
          [ 0.0072,  0.0143,  0.0149],
          [ 0.0233,  0.0260,  0.0280]]],


        [[[-0.0071, -0.0020, -0.0000],
          [-0.0080, -0.0035, -0.0006],
          [-0.0083, -0.0041, -0.0022]],

         [[-0.0038,  0.0000,  0.0013],
          [-0.0055, -0.0021,  0.0004],
          [-0.0061, -0.0029, -0.0013]],

         [[-0.0031,  0.0002,  0.0011],
          [-0.0064, -0.0035, -0.0017],
          [-0.0087, -0.0062, -0.0054]]],


        ...,


        [[[ 0.0046,  0.0077,  0.0037],
          [-0.0033,  0.0023, -0.0110],
          [ 0.0019,  0.0094, -0.0028]],

         [[-0.0037, -0.0007, -0.0025],
          [-0.0107, -0.0024, -0.0129],
          [ 0.0009,  0.0120,  0.0017]],

         [[-0.0264, -0.0197, -0.0212],
          [-0.0315, -0.0191, -0.0236],
          [-0.0194, -0.0012, -0.0055]]],


        [[[ 0.0184,  0.0081,  0.0039],
          [ 0.0066,  0.0030, -0.0016],
          [-0.0054, -0.0019,  0.0007]],

         [[ 0.0185,  0.0082,  0.0037],
          [ 0.0072,  0.0044, -0.0002],
          [-0.0042,  0.0006,  0.0035]],

         [[ 0.0347,  0.0261,  0.0254],
          [ 0.0253,  0.0217,  0.0181],
          [ 0.0231,  0.0239,  0.0218]]],


        [[[-0.0066, -0.0041, -0.0210],
          [-0.0196, -0.0174, -0.0325],
          [-0.0149, -0.0185, -0.0344]],

         [[-0.0385, -0.0292, -0.0353],
          [-0.0532, -0.0517, -0.0512],
          [-0.0481, -0.0523, -0.0544]],

         [[ 0.0118,  0.0178,  0.0223],
          [-0.0045, -0.0079,  0.0014],
          [ 0.0067,  0.0007,  0.0066]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7486]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 102 | Batch_idx: 0 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (3661/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (4850/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (6038/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (7206/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (8408/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (9587/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (10769/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (11926/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (13124/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (14303/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (15484/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (16645/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (17821/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (19004/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (20196/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (21385/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (22574/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (23757/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (24944/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (26129/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (27310/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (28486/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (29690/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (30872/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (32067/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (33245/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (34420/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (35597/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (36790/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (37966/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (39141/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (40307/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (41489/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (42664/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (43869/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (45056/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (46202/50000)
# TEST : Loss: (0.4399) | Acc: (86.00%) (8621/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1295,  0.0670,  0.0938],
          [ 0.1111, -0.2950, -0.1077],
          [ 0.2580, -0.0798,  0.0117]],

         [[-0.0023, -0.0913,  0.2613],
          [ 0.0202, -0.1042,  0.0433],
          [ 0.0640,  0.0067, -0.0685]],

         [[ 0.0138, -0.0447,  0.1970],
          [ 0.0911, -0.1992,  0.1597],
          [-0.1133,  0.0233, -0.1892]]],


        [[[ 0.0977, -0.0767, -0.0040],
          [ 0.1367,  0.1732,  0.1819],
          [ 0.0088, -0.0659,  0.1140]],

         [[-0.1540, -0.1804,  0.1561],
          [ 0.0330, -0.0489,  0.1057],
          [ 0.1229, -0.2029,  0.1327]],

         [[ 0.1007, -0.0502, -0.0135],
          [ 0.0124,  0.0217, -0.1156],
          [-0.0310, -0.1594, -0.0171]]],


        [[[-0.0716,  0.0892, -0.0425],
          [-0.1247,  0.0651,  0.1166],
          [ 0.1110,  0.1658,  0.0016]],

         [[ 0.0996,  0.1325,  0.1132],
          [ 0.0697, -0.0384, -0.0355],
          [-0.0466, -0.0300,  0.1392]],

         [[-0.0448,  0.1447,  0.0741],
          [-0.0413, -0.0449, -0.0598],
          [-0.0387, -0.0075,  0.0429]]],


        ...,


        [[[ 0.2239, -0.1167, -0.1209],
          [-0.0498, -0.1138, -0.0976],
          [-0.0944,  0.0241,  0.0663]],

         [[ 0.1663,  0.1929, -0.1165],
          [ 0.2061, -0.1399, -0.1147],
          [-0.0590, -0.0747, -0.0286]],

         [[ 0.1827,  0.1020, -0.1023],
          [-0.0672, -0.1545, -0.0745],
          [ 0.0707,  0.1806, -0.1295]]],


        [[[-0.1045,  0.0882, -0.1471],
          [-0.1405, -0.1447,  0.1171],
          [-0.1491,  0.1554, -0.1338]],

         [[ 0.0937, -0.1186,  0.0832],
          [ 0.1490,  0.0019, -0.0867],
          [-0.1437,  0.1174, -0.1973]],

         [[ 0.1236, -0.0215, -0.0006],
          [ 0.1397, -0.1765,  0.0102],
          [-0.0254,  0.0183, -0.1226]]],


        [[[ 0.0698,  0.1561,  0.1113],
          [-0.0881, -0.1862,  0.2118],
          [-0.1742, -0.0254, -0.0695]],

         [[-0.0265, -0.0806,  0.1524],
          [-0.0004, -0.2113,  0.0553],
          [-0.0047,  0.0759,  0.0284]],

         [[-0.1667, -0.1985,  0.1288],
          [ 0.1456, -0.1912, -0.0139],
          [ 0.0068, -0.0216,  0.2877]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1004,  0.1009,  0.1056],
          [ 0.1225,  0.1063,  0.0933],
          [ 0.1019,  0.0840,  0.0706]],

         [[ 0.0831,  0.0790,  0.0759],
          [ 0.1133,  0.0895,  0.0670],
          [ 0.0977,  0.0706,  0.0482]],

         [[ 0.0890,  0.0849,  0.0737],
          [ 0.1278,  0.1038,  0.0703],
          [ 0.1223,  0.0927,  0.0587]]],


        [[[-0.0447, -0.0350, -0.0131],
          [-0.0423, -0.0349, -0.0184],
          [-0.0364, -0.0353, -0.0269]],

         [[-0.0447, -0.0358, -0.0128],
          [-0.0450, -0.0382, -0.0202],
          [-0.0415, -0.0402, -0.0303]],

         [[-0.0517, -0.0468, -0.0247],
          [-0.0519, -0.0483, -0.0313],
          [-0.0477, -0.0480, -0.0391]]],


        [[[-0.0033, -0.0009,  0.0018],
          [-0.0042, -0.0019,  0.0002],
          [-0.0062, -0.0039, -0.0015]],

         [[-0.0006,  0.0009,  0.0038],
          [-0.0010,  0.0001,  0.0019],
          [-0.0030, -0.0017,  0.0005]],

         [[ 0.0024,  0.0031,  0.0058],
          [ 0.0030,  0.0036,  0.0054],
          [ 0.0022,  0.0032,  0.0050]]],


        ...,


        [[[-0.0620, -0.0543, -0.0472],
          [-0.0677, -0.0570, -0.0526],
          [-0.0690, -0.0599, -0.0550]],

         [[-0.0709, -0.0553, -0.0393],
          [-0.0791, -0.0614, -0.0494],
          [-0.0811, -0.0661, -0.0549]],

         [[-0.0789, -0.0654, -0.0503],
          [-0.0841, -0.0689, -0.0580],
          [-0.0831, -0.0714, -0.0619]]],


        [[[ 0.0064,  0.0047,  0.0058],
          [ 0.0053,  0.0027,  0.0038],
          [ 0.0022,  0.0000,  0.0021]],

         [[-0.0071, -0.0100, -0.0088],
          [-0.0067, -0.0102, -0.0088],
          [-0.0093, -0.0116, -0.0090]],

         [[-0.0078, -0.0106, -0.0096],
          [-0.0067, -0.0100, -0.0090],
          [-0.0086, -0.0110, -0.0092]]],


        [[[ 0.1548,  0.1687,  0.1730],
          [ 0.1596,  0.1814,  0.1877],
          [ 0.1497,  0.1578,  0.1682]],

         [[ 0.1577,  0.1696,  0.1654],
          [ 0.1590,  0.1806,  0.1830],
          [ 0.1479,  0.1573,  0.1688]],

         [[ 0.0944,  0.1086,  0.1155],
          [ 0.0900,  0.1147,  0.1280],
          [ 0.0720,  0.0869,  0.1091]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7483]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 103 | Batch_idx: 0 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (93.00%) (2506/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (6110/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (7301/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (8484/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (93.00%) (9660/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (93.00%) (10851/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (93.00%) (12039/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (93.00%) (13235/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (93.00%) (14426/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (93.00%) (15618/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (93.00%) (16796/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (93.00%) (17980/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (19162/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (20354/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (21543/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (22731/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (23917/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (25106/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (26292/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (27486/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (28657/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (29830/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (31018/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (32204/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (33400/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (34574/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (35758/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (36928/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (38121/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (39295/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (40488/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (41676/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (42866/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (44032/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (45198/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (46333/50000)
# TEST : Loss: (0.4404) | Acc: (86.00%) (8615/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1350,  0.0628,  0.0885],
          [ 0.1088, -0.3004, -0.1136],
          [ 0.2596, -0.0777,  0.0121]],

         [[-0.0081, -0.0948,  0.2581],
          [ 0.0178, -0.1087,  0.0401],
          [ 0.0639,  0.0079, -0.0670]],

         [[ 0.0117, -0.0445,  0.1964],
          [ 0.0906, -0.2010,  0.1584],
          [-0.1129,  0.0250, -0.1869]]],


        [[[ 0.0973, -0.0769, -0.0040],
          [ 0.1369,  0.1734,  0.1818],
          [ 0.0086, -0.0668,  0.1134]],

         [[-0.1534, -0.1803,  0.1559],
          [ 0.0341, -0.0486,  0.1053],
          [ 0.1235, -0.2035,  0.1320]],

         [[ 0.1018, -0.0495, -0.0128],
          [ 0.0137,  0.0224, -0.1152],
          [-0.0299, -0.1593, -0.0171]]],


        [[[-0.0713,  0.0894, -0.0422],
          [-0.1244,  0.0654,  0.1172],
          [ 0.1113,  0.1661,  0.0020]],

         [[ 0.0995,  0.1322,  0.1130],
          [ 0.0697, -0.0385, -0.0354],
          [-0.0465, -0.0301,  0.1391]],

         [[-0.0453,  0.1438,  0.0733],
          [-0.0418, -0.0456, -0.0604],
          [-0.0391, -0.0082,  0.0422]]],


        ...,


        [[[ 0.2235, -0.1176, -0.1223],
          [-0.0504, -0.1147, -0.0989],
          [-0.0931,  0.0256,  0.0671]],

         [[ 0.1649,  0.1916, -0.1183],
          [ 0.2046, -0.1410, -0.1162],
          [-0.0586, -0.0734, -0.0280]],

         [[ 0.1818,  0.1021, -0.1025],
          [-0.0679, -0.1542, -0.0745],
          [ 0.0714,  0.1826, -0.1282]]],


        [[[-0.1042,  0.0875, -0.1479],
          [-0.1408, -0.1452,  0.1166],
          [-0.1494,  0.1543, -0.1343]],

         [[ 0.0948, -0.1181,  0.0833],
          [ 0.1496,  0.0025, -0.0861],
          [-0.1430,  0.1174, -0.1966]],

         [[ 0.1257, -0.0198,  0.0010],
          [ 0.1415, -0.1746,  0.0119],
          [-0.0238,  0.0195, -0.1211]]],


        [[[ 0.0727,  0.1580,  0.1087],
          [-0.0884, -0.1886,  0.2099],
          [-0.1759, -0.0266, -0.0700]],

         [[-0.0225, -0.0788,  0.1497],
          [ 0.0002, -0.2134,  0.0538],
          [-0.0048,  0.0764,  0.0293]],

         [[-0.1631, -0.1962,  0.1267],
          [ 0.1494, -0.1895, -0.0125],
          [ 0.0129, -0.0151,  0.2939]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1055, -0.0784, -0.0837],
          [-0.1125, -0.0806, -0.0973],
          [-0.1104, -0.0998, -0.1087]],

         [[-0.1131, -0.0986, -0.1001],
          [-0.1101, -0.0995, -0.1195],
          [-0.1049, -0.1193, -0.1357]],

         [[-0.1007, -0.0860, -0.0827],
          [-0.0944, -0.0823, -0.0936],
          [-0.0872, -0.0959, -0.1042]]],


        [[[ 0.0007,  0.0045,  0.0028],
          [-0.0111, -0.0047, -0.0055],
          [-0.0109, -0.0084, -0.0095]],

         [[-0.0085, -0.0055, -0.0086],
          [-0.0211, -0.0158, -0.0161],
          [-0.0212, -0.0182, -0.0165]],

         [[-0.0202, -0.0172, -0.0192],
          [-0.0305, -0.0250, -0.0244],
          [-0.0277, -0.0246, -0.0226]]],


        [[[ 0.0039,  0.0059,  0.0029],
          [-0.0029,  0.0013, -0.0009],
          [-0.0086, -0.0052, -0.0066]],

         [[ 0.0021,  0.0044,  0.0025],
          [-0.0031,  0.0015, -0.0003],
          [-0.0078, -0.0039, -0.0052]],

         [[-0.0020, -0.0003, -0.0017],
          [-0.0074, -0.0036, -0.0047],
          [-0.0114, -0.0083, -0.0089]]],


        ...,


        [[[ 0.0353,  0.0202,  0.0179],
          [ 0.0423,  0.0282,  0.0277],
          [ 0.0467,  0.0305,  0.0314]],

         [[ 0.0256,  0.0060,  0.0024],
          [ 0.0263,  0.0104,  0.0136],
          [ 0.0231,  0.0088,  0.0184]],

         [[ 0.0224,  0.0031,  0.0017],
          [ 0.0224,  0.0066,  0.0110],
          [ 0.0202,  0.0046,  0.0142]]],


        [[[-0.0092, -0.0051, -0.0135],
          [-0.0153, -0.0052, -0.0086],
          [-0.0187, -0.0087, -0.0032]],

         [[-0.0115, -0.0030, -0.0080],
          [-0.0132, -0.0010, -0.0033],
          [-0.0122, -0.0017,  0.0051]],

         [[-0.0192, -0.0109, -0.0138],
          [-0.0188, -0.0066, -0.0068],
          [-0.0176, -0.0064,  0.0017]]],


        [[[-0.1395, -0.1089, -0.1015],
          [-0.1138, -0.0837, -0.0805],
          [-0.1630, -0.1471, -0.1371]],

         [[-0.2307, -0.2072, -0.2118],
          [-0.2145, -0.1877, -0.1886],
          [-0.2638, -0.2406, -0.2323]],

         [[-0.1821, -0.1644, -0.1703],
          [-0.1682, -0.1431, -0.1481],
          [-0.2123, -0.1890, -0.1852]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7480]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 104 | Batch_idx: 0 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (2525/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (3706/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (4913/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (7299/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (8495/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (9684/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (10871/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (12048/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2038) |  Loss2: (0.0000) | Acc: (93.00%) (13241/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (93.00%) (14425/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (93.00%) (15603/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (93.00%) (16785/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (93.00%) (17985/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (19162/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (20355/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (21551/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (22722/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (23911/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (25114/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (26303/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (27483/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2058) |  Loss2: (0.0000) | Acc: (93.00%) (28689/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (93.00%) (29880/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (93.00%) (31082/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (93.00%) (32260/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (33446/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (34636/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (35804/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (36972/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (38159/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (39331/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (40524/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (41710/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (42902/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (44090/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (45271/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (46424/50000)
# TEST : Loss: (0.4076) | Acc: (86.00%) (8671/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1348,  0.0657,  0.0924],
          [ 0.1073, -0.3018, -0.1128],
          [ 0.2562, -0.0804,  0.0089]],

         [[-0.0086, -0.0925,  0.2614],
          [ 0.0178, -0.1083,  0.0427],
          [ 0.0620,  0.0071, -0.0674]],

         [[ 0.0136, -0.0405,  0.1998],
          [ 0.0925, -0.1988,  0.1608],
          [-0.1135,  0.0247, -0.1876]]],


        [[[ 0.0979, -0.0768, -0.0038],
          [ 0.1366,  0.1724,  0.1812],
          [ 0.0090, -0.0673,  0.1131]],

         [[-0.1533, -0.1806,  0.1559],
          [ 0.0336, -0.0497,  0.1047],
          [ 0.1234, -0.2044,  0.1312]],

         [[ 0.1014, -0.0501, -0.0131],
          [ 0.0131,  0.0212, -0.1161],
          [-0.0295, -0.1600, -0.0180]]],


        [[[-0.0714,  0.0892, -0.0424],
          [-0.1246,  0.0650,  0.1165],
          [ 0.1111,  0.1660,  0.0018]],

         [[ 0.0997,  0.1323,  0.1130],
          [ 0.0696, -0.0385, -0.0357],
          [-0.0465, -0.0300,  0.1390]],

         [[-0.0450,  0.1440,  0.0734],
          [-0.0417, -0.0456, -0.0606],
          [-0.0390, -0.0081,  0.0422]]],


        ...,


        [[[ 0.2263, -0.1156, -0.1199],
          [-0.0502, -0.1143, -0.0966],
          [-0.0947,  0.0240,  0.0674]],

         [[ 0.1675,  0.1938, -0.1157],
          [ 0.2054, -0.1399, -0.1135],
          [-0.0594, -0.0742, -0.0271]],

         [[ 0.1835,  0.1030, -0.1016],
          [-0.0675, -0.1543, -0.0738],
          [ 0.0701,  0.1808, -0.1285]]],


        [[[-0.1039,  0.0879, -0.1473],
          [-0.1406, -0.1451,  0.1166],
          [-0.1485,  0.1549, -0.1339]],

         [[ 0.0937, -0.1190,  0.0829],
          [ 0.1486,  0.0013, -0.0870],
          [-0.1431,  0.1169, -0.1972]],

         [[ 0.1244, -0.0209,  0.0001],
          [ 0.1399, -0.1765,  0.0102],
          [-0.0252,  0.0177, -0.1227]]],


        [[[ 0.0712,  0.1588,  0.1092],
          [-0.0872, -0.1874,  0.2132],
          [-0.1751, -0.0224, -0.0659]],

         [[-0.0244, -0.0783,  0.1493],
          [ 0.0006, -0.2132,  0.0554],
          [-0.0067,  0.0771,  0.0304]],

         [[-0.1696, -0.1994,  0.1225],
          [ 0.1461, -0.1913, -0.0126],
          [ 0.0096, -0.0144,  0.2949]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0653,  0.0467,  0.0460],
          [ 0.0627,  0.0456,  0.0484],
          [ 0.0205,  0.0286,  0.0326]],

         [[ 0.0381,  0.0191,  0.0235],
          [ 0.0342,  0.0170,  0.0242],
          [-0.0079,  0.0016,  0.0105]],

         [[ 0.0136,  0.0048,  0.0107],
          [ 0.0115,  0.0006,  0.0095],
          [-0.0289, -0.0162, -0.0034]]],


        [[[ 0.0026,  0.0104,  0.0150],
          [ 0.0086,  0.0156,  0.0174],
          [ 0.0043,  0.0066,  0.0061]],

         [[ 0.0128,  0.0183,  0.0223],
          [ 0.0179,  0.0227,  0.0249],
          [ 0.0119,  0.0127,  0.0137]],

         [[ 0.0246,  0.0281,  0.0327],
          [ 0.0298,  0.0338,  0.0372],
          [ 0.0235,  0.0246,  0.0263]]],


        [[[-0.0036, -0.0008,  0.0024],
          [-0.0044, -0.0010,  0.0023],
          [-0.0063, -0.0024,  0.0015]],

         [[-0.0045, -0.0017,  0.0016],
          [-0.0049, -0.0016,  0.0019],
          [-0.0061, -0.0023,  0.0020]],

         [[-0.0041, -0.0019,  0.0008],
          [-0.0046, -0.0022,  0.0004],
          [-0.0059, -0.0031,  0.0005]]],


        ...,


        [[[-0.0212, -0.0151, -0.0126],
          [-0.0200, -0.0164, -0.0134],
          [-0.0283, -0.0252, -0.0221]],

         [[-0.0313, -0.0244, -0.0244],
          [-0.0301, -0.0258, -0.0245],
          [-0.0370, -0.0342, -0.0317]],

         [[-0.0216, -0.0150, -0.0169],
          [-0.0242, -0.0207, -0.0200],
          [-0.0334, -0.0312, -0.0280]]],


        [[[-0.0033, -0.0029,  0.0050],
          [-0.0014,  0.0013,  0.0098],
          [-0.0075, -0.0038,  0.0062]],

         [[-0.0103, -0.0078, -0.0012],
          [-0.0110, -0.0070,  0.0013],
          [-0.0189, -0.0142, -0.0036]],

         [[-0.0060, -0.0049,  0.0004],
          [-0.0078, -0.0058,  0.0012],
          [-0.0174, -0.0140, -0.0042]]],


        [[[ 0.0344,  0.0277,  0.0285],
          [ 0.0551,  0.0457,  0.0455],
          [ 0.0472,  0.0474,  0.0564]],

         [[ 0.0500,  0.0426,  0.0426],
          [ 0.0651,  0.0525,  0.0539],
          [ 0.0549,  0.0526,  0.0651]],

         [[ 0.0199,  0.0154,  0.0211],
          [ 0.0295,  0.0209,  0.0258],
          [ 0.0204,  0.0260,  0.0389]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7477]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.4456) |  Loss2: (0.2512) | Acc: (92.00%) (119/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.4587) |  Loss2: (0.2512) | Acc: (92.00%) (1299/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.4699) |  Loss2: (0.2511) | Acc: (92.00%) (2480/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.4618) |  Loss2: (0.2511) | Acc: (92.00%) (3673/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.4698) |  Loss2: (0.2510) | Acc: (92.00%) (4835/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.4727) |  Loss2: (0.2510) | Acc: (92.00%) (6006/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.4791) |  Loss2: (0.2510) | Acc: (91.00%) (7164/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.4791) |  Loss2: (0.2509) | Acc: (91.00%) (8341/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.4800) |  Loss2: (0.2509) | Acc: (91.00%) (9518/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.4810) |  Loss2: (0.2509) | Acc: (91.00%) (10683/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.4792) |  Loss2: (0.2508) | Acc: (91.00%) (11868/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.4813) |  Loss2: (0.2508) | Acc: (91.00%) (13030/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.4823) |  Loss2: (0.2507) | Acc: (91.00%) (14200/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.4815) |  Loss2: (0.2507) | Acc: (91.00%) (15377/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.4806) |  Loss2: (0.2507) | Acc: (91.00%) (16559/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.4788) |  Loss2: (0.2506) | Acc: (91.00%) (17742/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.4775) |  Loss2: (0.2506) | Acc: (91.00%) (18929/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.4772) |  Loss2: (0.2506) | Acc: (91.00%) (20111/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.4764) |  Loss2: (0.2505) | Acc: (91.00%) (21285/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.4761) |  Loss2: (0.2505) | Acc: (91.00%) (22462/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.4764) |  Loss2: (0.2504) | Acc: (91.00%) (23634/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.4759) |  Loss2: (0.2504) | Acc: (91.00%) (24822/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.4776) |  Loss2: (0.2504) | Acc: (91.00%) (25974/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.4767) |  Loss2: (0.2503) | Acc: (91.00%) (27153/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.4763) |  Loss2: (0.2503) | Acc: (91.00%) (28340/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.4747) |  Loss2: (0.2502) | Acc: (91.00%) (29536/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.4735) |  Loss2: (0.2502) | Acc: (91.00%) (30726/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.4730) |  Loss2: (0.2501) | Acc: (91.00%) (31905/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.4732) |  Loss2: (0.2501) | Acc: (91.00%) (33080/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.4722) |  Loss2: (0.2500) | Acc: (92.00%) (34270/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.4714) |  Loss2: (0.2500) | Acc: (92.00%) (35453/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.4709) |  Loss2: (0.2499) | Acc: (92.00%) (36642/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.4713) |  Loss2: (0.2499) | Acc: (92.00%) (37813/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.4707) |  Loss2: (0.2498) | Acc: (92.00%) (39009/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.4702) |  Loss2: (0.2498) | Acc: (92.00%) (40193/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.4700) |  Loss2: (0.2497) | Acc: (92.00%) (41370/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.4693) |  Loss2: (0.2497) | Acc: (92.00%) (42565/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.4695) |  Loss2: (0.2496) | Acc: (92.00%) (43747/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.4696) |  Loss2: (0.2496) | Acc: (92.00%) (44926/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.4693) |  Loss2: (0.2495) | Acc: (92.00%) (46072/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_105.pth.tar'
# TEST : Loss: (0.4052) | Acc: (86.00%) (8688/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1350,  0.0657,  0.0919],
          [ 0.1067, -0.3021, -0.1135],
          [ 0.2555, -0.0810,  0.0082]],

         [[-0.0082, -0.0920,  0.2614],
          [ 0.0177, -0.1082,  0.0423],
          [ 0.0617,  0.0068, -0.0679]],

         [[ 0.0148, -0.0394,  0.2005],
          [ 0.0930, -0.1981,  0.1611],
          [-0.1132,  0.0248, -0.1873]]],


        [[[ 0.0975, -0.0774, -0.0042],
          [ 0.1363,  0.1719,  0.1808],
          [ 0.0088, -0.0677,  0.1127]],

         [[-0.1537, -0.1812,  0.1553],
          [ 0.0333, -0.0502,  0.1042],
          [ 0.1232, -0.2048,  0.1308]],

         [[ 0.1008, -0.0507, -0.0137],
          [ 0.0126,  0.0206, -0.1166],
          [-0.0299, -0.1605, -0.0184]]],


        [[[-0.0713,  0.0891, -0.0424],
          [-0.1246,  0.0650,  0.1164],
          [ 0.1111,  0.1659,  0.0017]],

         [[ 0.0997,  0.1323,  0.1130],
          [ 0.0696, -0.0385, -0.0357],
          [-0.0464, -0.0300,  0.1389]],

         [[-0.0450,  0.1439,  0.0733],
          [-0.0418, -0.0457, -0.0606],
          [-0.0390, -0.0082,  0.0421]]],


        ...,


        [[[ 0.2261, -0.1156, -0.1198],
          [-0.0503, -0.1143, -0.0966],
          [-0.0946,  0.0241,  0.0674]],

         [[ 0.1675,  0.1938, -0.1155],
          [ 0.2053, -0.1398, -0.1135],
          [-0.0593, -0.0741, -0.0271]],

         [[ 0.1835,  0.1031, -0.1015],
          [-0.0674, -0.1541, -0.0737],
          [ 0.0703,  0.1810, -0.1282]]],


        [[[-0.1038,  0.0880, -0.1472],
          [-0.1405, -0.1450,  0.1165],
          [-0.1484,  0.1549, -0.1339]],

         [[ 0.0938, -0.1189,  0.0829],
          [ 0.1486,  0.0013, -0.0869],
          [-0.1429,  0.1169, -0.1971]],

         [[ 0.1246, -0.0207,  0.0003],
          [ 0.1400, -0.1762,  0.0103],
          [-0.0251,  0.0178, -0.1226]]],


        [[[ 0.0713,  0.1593,  0.1095],
          [-0.0874, -0.1873,  0.2130],
          [-0.1754, -0.0226, -0.0664]],

         [[-0.0244, -0.0781,  0.1494],
          [ 0.0001, -0.2134,  0.0550],
          [-0.0073,  0.0766,  0.0296]],

         [[-0.1697, -0.1994,  0.1223],
          [ 0.1455, -0.1917, -0.0132],
          [ 0.0089, -0.0151,  0.2939]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3495e-05,  6.5655e-06,  9.1904e-06],
          [ 1.0671e-05, -3.0205e-05, -1.1351e-05],
          [ 2.5547e-05, -8.0977e-06,  8.1706e-07]],

         [[-8.1523e-07, -9.2021e-06,  2.6136e-05],
          [ 1.7692e-06, -1.0820e-05,  4.2335e-06],
          [ 6.1714e-06,  6.7518e-07, -6.7880e-06]],

         [[ 1.4765e-06, -3.9395e-06,  2.0051e-05],
          [ 9.3028e-06, -1.9811e-05,  1.6107e-05],
          [-1.1321e-05,  2.4817e-06, -1.8732e-05]]],


        [[[ 9.7504e-06, -7.7356e-06, -4.2459e-07],
          [ 1.3631e-05,  1.7189e-05,  1.8076e-05],
          [ 8.7791e-07, -6.7698e-06,  1.1271e-05]],

         [[-1.5368e-05, -1.8116e-05,  1.5530e-05],
          [ 3.3269e-06, -5.0171e-06,  1.0419e-05],
          [ 1.2317e-05, -2.0475e-05,  1.3077e-05]],

         [[ 1.0075e-05, -5.0731e-06, -1.3684e-06],
          [ 1.2578e-06,  2.0571e-06, -1.1656e-05],
          [-2.9920e-06, -1.6047e-05, -1.8368e-06]]],


        [[[-7.1334e-06,  8.9142e-06, -4.2433e-06],
          [-1.2455e-05,  6.4973e-06,  1.1644e-05],
          [ 1.1111e-05,  1.6589e-05,  1.7146e-07]],

         [[ 9.9706e-06,  1.3230e-05,  1.1297e-05],
          [ 6.9609e-06, -3.8507e-06, -3.5699e-06],
          [-4.6449e-06, -3.0000e-06,  1.3891e-05]],

         [[-4.4973e-06,  1.4387e-05,  7.3347e-06],
          [-4.1779e-06, -4.5690e-06, -6.0633e-06],
          [-3.8998e-06, -8.1554e-07,  4.2084e-06]]],


        ...,


        [[[ 2.2606e-05, -1.1561e-05, -1.1984e-05],
          [-5.0307e-06, -1.1429e-05, -9.6553e-06],
          [-9.4582e-06,  2.4135e-06,  6.7385e-06]],

         [[ 1.6747e-05,  1.9379e-05, -1.1554e-05],
          [ 2.0527e-05, -1.3980e-05, -1.1346e-05],
          [-5.9307e-06, -7.4066e-06, -2.7077e-06]],

         [[ 1.8354e-05,  1.0309e-05, -1.0148e-05],
          [-6.7378e-06, -1.5411e-05, -7.3671e-06],
          [ 7.0329e-06,  1.8098e-05, -1.2825e-05]]],


        [[[-1.0382e-05,  8.7960e-06, -1.4718e-05],
          [-1.4053e-05, -1.4503e-05,  1.1652e-05],
          [-1.4845e-05,  1.5488e-05, -1.3393e-05]],

         [[ 9.3762e-06, -1.1890e-05,  8.2931e-06],
          [ 1.4861e-05,  1.3369e-07, -8.6932e-06],
          [-1.4289e-05,  1.1693e-05, -1.9708e-05]],

         [[ 1.2457e-05, -2.0714e-06,  3.3054e-08],
          [ 1.3999e-05, -1.7625e-05,  1.0278e-06],
          [-2.5070e-06,  1.7817e-06, -1.2257e-05]]],


        [[[ 7.1327e-06,  1.5926e-05,  1.0949e-05],
          [-8.7443e-06, -1.8729e-05,  2.1298e-05],
          [-1.7540e-05, -2.2590e-06, -6.6373e-06]],

         [[-2.4437e-06, -7.8108e-06,  1.4943e-05],
          [ 1.2123e-08, -2.1343e-05,  5.5045e-06],
          [-7.3074e-07,  7.6562e-06,  2.9638e-06]],

         [[-1.6967e-05, -1.9938e-05,  1.2228e-05],
          [ 1.4552e-05, -1.9171e-05, -1.3184e-06],
          [ 8.9074e-07, -1.5086e-06,  2.9390e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.7871]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0191]], device='cuda:0')

Epoch: 106 | Batch_idx: 0 |  Loss: (0.4663) |  Loss2: (0.2476) | Acc: (89.00%) (114/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.4413) |  Loss2: (0.2476) | Acc: (93.00%) (1313/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.4458) |  Loss2: (0.2475) | Acc: (93.00%) (2508/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.4461) |  Loss2: (0.2475) | Acc: (93.00%) (3695/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.4514) |  Loss2: (0.2475) | Acc: (92.00%) (4870/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.4504) |  Loss2: (0.2474) | Acc: (92.00%) (6064/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.4497) |  Loss2: (0.2474) | Acc: (92.00%) (7254/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.4517) |  Loss2: (0.2473) | Acc: (92.00%) (8435/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.4520) |  Loss2: (0.2473) | Acc: (92.00%) (9620/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.4495) |  Loss2: (0.2472) | Acc: (92.00%) (10825/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.4481) |  Loss2: (0.2472) | Acc: (93.00%) (12027/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.4502) |  Loss2: (0.2472) | Acc: (92.00%) (13205/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.4512) |  Loss2: (0.2471) | Acc: (92.00%) (14380/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.4520) |  Loss2: (0.2471) | Acc: (92.00%) (15557/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.4512) |  Loss2: (0.2470) | Acc: (92.00%) (16754/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.4505) |  Loss2: (0.2470) | Acc: (92.00%) (17947/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.4523) |  Loss2: (0.2469) | Acc: (92.00%) (19125/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.4519) |  Loss2: (0.2469) | Acc: (92.00%) (20318/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.4529) |  Loss2: (0.2468) | Acc: (92.00%) (21504/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.4519) |  Loss2: (0.2468) | Acc: (92.00%) (22709/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.4506) |  Loss2: (0.2467) | Acc: (92.00%) (23911/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.4516) |  Loss2: (0.2467) | Acc: (92.00%) (25087/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.4509) |  Loss2: (0.2466) | Acc: (92.00%) (26298/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.4522) |  Loss2: (0.2466) | Acc: (92.00%) (27467/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.4528) |  Loss2: (0.2466) | Acc: (92.00%) (28647/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.4512) |  Loss2: (0.2465) | Acc: (92.00%) (29861/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.4522) |  Loss2: (0.2465) | Acc: (92.00%) (31032/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.4511) |  Loss2: (0.2464) | Acc: (92.00%) (32237/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.4502) |  Loss2: (0.2464) | Acc: (92.00%) (33441/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.4504) |  Loss2: (0.2463) | Acc: (92.00%) (34631/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.4497) |  Loss2: (0.2463) | Acc: (92.00%) (35830/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.4497) |  Loss2: (0.2462) | Acc: (93.00%) (37024/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.4505) |  Loss2: (0.2462) | Acc: (92.00%) (38185/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.4509) |  Loss2: (0.2461) | Acc: (92.00%) (39357/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.4509) |  Loss2: (0.2461) | Acc: (92.00%) (40553/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.4514) |  Loss2: (0.2460) | Acc: (92.00%) (41725/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.4516) |  Loss2: (0.2460) | Acc: (92.00%) (42903/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.4507) |  Loss2: (0.2459) | Acc: (92.00%) (44113/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.4511) |  Loss2: (0.2459) | Acc: (92.00%) (45293/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.4512) |  Loss2: (0.2458) | Acc: (92.00%) (46435/50000)
# TEST : Loss: (0.3954) | Acc: (87.00%) (8716/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1349,  0.0656,  0.0919],
          [ 0.1067, -0.3019, -0.1135],
          [ 0.2554, -0.0809,  0.0082]],

         [[-0.0081, -0.0920,  0.2613],
          [ 0.0177, -0.1082,  0.0423],
          [ 0.0617,  0.0067, -0.0679]],

         [[ 0.0148, -0.0394,  0.2004],
          [ 0.0930, -0.1980,  0.1610],
          [-0.1132,  0.0248, -0.1872]]],


        [[[ 0.0975, -0.0773, -0.0042],
          [ 0.1363,  0.1718,  0.1807],
          [ 0.0088, -0.0677,  0.1127]],

         [[-0.1536, -0.1811,  0.1552],
          [ 0.0333, -0.0502,  0.1041],
          [ 0.1231, -0.2047,  0.1307]],

         [[ 0.1007, -0.0507, -0.0137],
          [ 0.0126,  0.0206, -0.1165],
          [-0.0299, -0.1604, -0.0184]]],


        [[[-0.0713,  0.0891, -0.0424],
          [-0.1245,  0.0649,  0.1164],
          [ 0.1111,  0.1658,  0.0017]],

         [[ 0.0997,  0.1322,  0.1129],
          [ 0.0696, -0.0385, -0.0357],
          [-0.0464, -0.0300,  0.1389]],

         [[-0.0450,  0.1438,  0.0733],
          [-0.0418, -0.0457, -0.0606],
          [-0.0390, -0.0082,  0.0421]]],


        ...,


        [[[ 0.2260, -0.1156, -0.1198],
          [-0.0503, -0.1142, -0.0965],
          [-0.0945,  0.0241,  0.0674]],

         [[ 0.1674,  0.1937, -0.1155],
          [ 0.2052, -0.1397, -0.1134],
          [-0.0593, -0.0740, -0.0271]],

         [[ 0.1835,  0.1031, -0.1014],
          [-0.0674, -0.1541, -0.0736],
          [ 0.0703,  0.1809, -0.1282]]],


        [[[-0.1038,  0.0879, -0.1471],
          [-0.1405, -0.1450,  0.1165],
          [-0.1484,  0.1548, -0.1339]],

         [[ 0.0937, -0.1189,  0.0829],
          [ 0.1486,  0.0013, -0.0869],
          [-0.1428,  0.1169, -0.1970]],

         [[ 0.1245, -0.0207,  0.0003],
          [ 0.1399, -0.1762,  0.0103],
          [-0.0251,  0.0178, -0.1225]]],


        [[[ 0.0713,  0.1592,  0.1094],
          [-0.0874, -0.1872,  0.2129],
          [-0.1753, -0.0226, -0.0663]],

         [[-0.0244, -0.0781,  0.1494],
          [ 0.0001, -0.2133,  0.0550],
          [-0.0073,  0.0765,  0.0296]],

         [[-0.1696, -0.1993,  0.1222],
          [ 0.1455, -0.1916, -0.0132],
          [ 0.0089, -0.0151,  0.2938]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3490e-05,  6.5629e-06,  9.1869e-06],
          [ 1.0667e-05, -3.0193e-05, -1.1346e-05],
          [ 2.5536e-05, -8.0945e-06,  8.1673e-07]],

         [[-8.1490e-07, -9.1986e-06,  2.6125e-05],
          [ 1.7685e-06, -1.0816e-05,  4.2319e-06],
          [ 6.1690e-06,  6.7492e-07, -6.7854e-06]],

         [[ 1.4759e-06, -3.9379e-06,  2.0044e-05],
          [ 9.2993e-06, -1.9803e-05,  1.6100e-05],
          [-1.1317e-05,  2.4808e-06, -1.8724e-05]]],


        [[[ 9.7466e-06, -7.7327e-06, -4.2443e-07],
          [ 1.3626e-05,  1.7182e-05,  1.8069e-05],
          [ 8.7758e-07, -6.7672e-06,  1.1267e-05]],

         [[-1.5362e-05, -1.8109e-05,  1.5524e-05],
          [ 3.3256e-06, -5.0152e-06,  1.0415e-05],
          [ 1.2312e-05, -2.0467e-05,  1.3072e-05]],

         [[ 1.0071e-05, -5.0711e-06, -1.3679e-06],
          [ 1.2573e-06,  2.0563e-06, -1.1651e-05],
          [-2.9908e-06, -1.6040e-05, -1.8361e-06]]],


        [[[-7.1305e-06,  8.9107e-06, -4.2417e-06],
          [-1.2450e-05,  6.4946e-06,  1.1639e-05],
          [ 1.1106e-05,  1.6583e-05,  1.7139e-07]],

         [[ 9.9668e-06,  1.3224e-05,  1.1293e-05],
          [ 6.9582e-06, -3.8492e-06, -3.5685e-06],
          [-4.6432e-06, -2.9988e-06,  1.3885e-05]],

         [[-4.4955e-06,  1.4381e-05,  7.3318e-06],
          [-4.1763e-06, -4.5672e-06, -6.0610e-06],
          [-3.8983e-06, -8.1521e-07,  4.2068e-06]]],


        ...,


        [[[ 2.2597e-05, -1.1556e-05, -1.1980e-05],
          [-5.0287e-06, -1.1425e-05, -9.6515e-06],
          [-9.4544e-06,  2.4126e-06,  6.7359e-06]],

         [[ 1.6741e-05,  1.9371e-05, -1.1549e-05],
          [ 2.0519e-05, -1.3974e-05, -1.1342e-05],
          [-5.9284e-06, -7.4037e-06, -2.7066e-06]],

         [[ 1.8347e-05,  1.0305e-05, -1.0144e-05],
          [-6.7352e-06, -1.5405e-05, -7.3642e-06],
          [ 7.0302e-06,  1.8091e-05, -1.2819e-05]]],


        [[[-1.0378e-05,  8.7925e-06, -1.4712e-05],
          [-1.4047e-05, -1.4498e-05,  1.1648e-05],
          [-1.4839e-05,  1.5482e-05, -1.3388e-05]],

         [[ 9.3725e-06, -1.1885e-05,  8.2899e-06],
          [ 1.4855e-05,  1.3364e-07, -8.6897e-06],
          [-1.4283e-05,  1.1689e-05, -1.9701e-05]],

         [[ 1.2452e-05, -2.0706e-06,  3.3042e-08],
          [ 1.3993e-05, -1.7618e-05,  1.0274e-06],
          [-2.5061e-06,  1.7810e-06, -1.2252e-05]]],


        [[[ 7.1298e-06,  1.5919e-05,  1.0944e-05],
          [-8.7408e-06, -1.8722e-05,  2.1290e-05],
          [-1.7533e-05, -2.2581e-06, -6.6347e-06]],

         [[-2.4428e-06, -7.8079e-06,  1.4937e-05],
          [ 1.2118e-08, -2.1334e-05,  5.5023e-06],
          [-7.3044e-07,  7.6533e-06,  2.9626e-06]],

         [[-1.6960e-05, -1.9931e-05,  1.2223e-05],
          [ 1.4547e-05, -1.9164e-05, -1.3179e-06],
          [ 8.9038e-07, -1.5080e-06,  2.9378e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8133]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0600]], device='cuda:0')

Epoch: 107 | Batch_idx: 0 |  Loss: (0.4386) |  Loss2: (0.2441) | Acc: (94.00%) (121/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.4440) |  Loss2: (0.2441) | Acc: (92.00%) (1304/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.4482) |  Loss2: (0.2440) | Acc: (92.00%) (2491/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.4404) |  Loss2: (0.2440) | Acc: (93.00%) (3698/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.4414) |  Loss2: (0.2440) | Acc: (93.00%) (4884/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.4390) |  Loss2: (0.2439) | Acc: (93.00%) (6092/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.4404) |  Loss2: (0.2439) | Acc: (93.00%) (7286/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.4408) |  Loss2: (0.2438) | Acc: (93.00%) (8476/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.4437) |  Loss2: (0.2438) | Acc: (93.00%) (9653/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.4423) |  Loss2: (0.2438) | Acc: (93.00%) (10851/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.4440) |  Loss2: (0.2437) | Acc: (93.00%) (12040/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.4440) |  Loss2: (0.2437) | Acc: (93.00%) (13241/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.4460) |  Loss2: (0.2436) | Acc: (93.00%) (14424/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.4465) |  Loss2: (0.2436) | Acc: (93.00%) (15617/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.4466) |  Loss2: (0.2435) | Acc: (93.00%) (16808/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.4463) |  Loss2: (0.2435) | Acc: (93.00%) (18000/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.4469) |  Loss2: (0.2435) | Acc: (93.00%) (19187/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.4462) |  Loss2: (0.2434) | Acc: (93.00%) (20380/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.4464) |  Loss2: (0.2434) | Acc: (93.00%) (21573/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.4470) |  Loss2: (0.2433) | Acc: (93.00%) (22765/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.4461) |  Loss2: (0.2433) | Acc: (93.00%) (23957/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.4470) |  Loss2: (0.2433) | Acc: (93.00%) (25130/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.4468) |  Loss2: (0.2432) | Acc: (93.00%) (26323/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.4459) |  Loss2: (0.2432) | Acc: (93.00%) (27523/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.4460) |  Loss2: (0.2431) | Acc: (93.00%) (28716/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.4451) |  Loss2: (0.2431) | Acc: (93.00%) (29918/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.4461) |  Loss2: (0.2430) | Acc: (93.00%) (31088/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.4454) |  Loss2: (0.2430) | Acc: (93.00%) (32291/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.4446) |  Loss2: (0.2429) | Acc: (93.00%) (33492/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.4447) |  Loss2: (0.2429) | Acc: (93.00%) (34681/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.4446) |  Loss2: (0.2429) | Acc: (93.00%) (35867/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.4448) |  Loss2: (0.2428) | Acc: (93.00%) (37048/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.4444) |  Loss2: (0.2428) | Acc: (93.00%) (38242/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.4446) |  Loss2: (0.2427) | Acc: (93.00%) (39433/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.4433) |  Loss2: (0.2427) | Acc: (93.00%) (40640/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.4435) |  Loss2: (0.2426) | Acc: (93.00%) (41830/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.4428) |  Loss2: (0.2426) | Acc: (93.00%) (43027/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.4426) |  Loss2: (0.2426) | Acc: (93.00%) (44228/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.4422) |  Loss2: (0.2425) | Acc: (93.00%) (45426/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.4426) |  Loss2: (0.2425) | Acc: (93.00%) (46566/50000)
# TEST : Loss: (0.3903) | Acc: (87.00%) (8743/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1348,  0.0656,  0.0918],
          [ 0.1066, -0.3018, -0.1134],
          [ 0.2553, -0.0809,  0.0082]],

         [[-0.0081, -0.0920,  0.2611],
          [ 0.0177, -0.1081,  0.0423],
          [ 0.0617,  0.0067, -0.0678]],

         [[ 0.0148, -0.0394,  0.2004],
          [ 0.0930, -0.1980,  0.1609],
          [-0.1131,  0.0248, -0.1872]]],


        [[[ 0.0974, -0.0773, -0.0042],
          [ 0.1362,  0.1717,  0.1806],
          [ 0.0088, -0.0676,  0.1126]],

         [[-0.1536, -0.1810,  0.1552],
          [ 0.0332, -0.0501,  0.1041],
          [ 0.1231, -0.2046,  0.1307]],

         [[ 0.1007, -0.0507, -0.0137],
          [ 0.0126,  0.0206, -0.1165],
          [-0.0299, -0.1603, -0.0184]]],


        [[[-0.0713,  0.0891, -0.0424],
          [-0.1245,  0.0649,  0.1163],
          [ 0.1110,  0.1658,  0.0017]],

         [[ 0.0996,  0.1322,  0.1129],
          [ 0.0696, -0.0385, -0.0357],
          [-0.0464, -0.0300,  0.1388]],

         [[-0.0449,  0.1438,  0.0733],
          [-0.0417, -0.0457, -0.0606],
          [-0.0390, -0.0081,  0.0421]]],


        ...,


        [[[ 0.2259, -0.1155, -0.1197],
          [-0.0503, -0.1142, -0.0965],
          [-0.0945,  0.0241,  0.0673]],

         [[ 0.1673,  0.1936, -0.1154],
          [ 0.2051, -0.1397, -0.1134],
          [-0.0593, -0.0740, -0.0271]],

         [[ 0.1834,  0.1030, -0.1014],
          [-0.0673, -0.1540, -0.0736],
          [ 0.0703,  0.1808, -0.1281]]],


        [[[-0.1037,  0.0879, -0.1471],
          [-0.1404, -0.1449,  0.1164],
          [-0.1483,  0.1548, -0.1338]],

         [[ 0.0937, -0.1188,  0.0829],
          [ 0.1485,  0.0013, -0.0869],
          [-0.1428,  0.1168, -0.1969]],

         [[ 0.1245, -0.0207,  0.0003],
          [ 0.1399, -0.1761,  0.0103],
          [-0.0251,  0.0178, -0.1225]]],


        [[[ 0.0713,  0.1591,  0.1094],
          [-0.0874, -0.1871,  0.2128],
          [-0.1753, -0.0226, -0.0663]],

         [[-0.0244, -0.0780,  0.1493],
          [ 0.0001, -0.2133,  0.0550],
          [-0.0073,  0.0765,  0.0296]],

         [[-0.1695, -0.1992,  0.1222],
          [ 0.1454, -0.1916, -0.0132],
          [ 0.0089, -0.0151,  0.2937]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3485e-05,  6.5603e-06,  9.1834e-06],
          [ 1.0663e-05, -3.0182e-05, -1.1342e-05],
          [ 2.5526e-05, -8.0913e-06,  8.1640e-07]],

         [[-8.1457e-07, -9.1951e-06,  2.6115e-05],
          [ 1.7678e-06, -1.0811e-05,  4.2303e-06],
          [ 6.1665e-06,  6.7467e-07, -6.7828e-06]],

         [[ 1.4753e-06, -3.9363e-06,  2.0036e-05],
          [ 9.2958e-06, -1.9796e-05,  1.6094e-05],
          [-1.1312e-05,  2.4798e-06, -1.8717e-05]]],


        [[[ 9.7428e-06, -7.7298e-06, -4.2427e-07],
          [ 1.3621e-05,  1.7175e-05,  1.8062e-05],
          [ 8.7725e-07, -6.7646e-06,  1.1262e-05]],

         [[-1.5356e-05, -1.8102e-05,  1.5518e-05],
          [ 3.3243e-06, -5.0133e-06,  1.0411e-05],
          [ 1.2307e-05, -2.0459e-05,  1.3067e-05]],

         [[ 1.0067e-05, -5.0691e-06, -1.3673e-06],
          [ 1.2568e-06,  2.0555e-06, -1.1646e-05],
          [-2.9897e-06, -1.6034e-05, -1.8353e-06]]],


        [[[-7.1275e-06,  8.9072e-06, -4.2401e-06],
          [-1.2445e-05,  6.4920e-06,  1.1635e-05],
          [ 1.1102e-05,  1.6576e-05,  1.7133e-07]],

         [[ 9.9630e-06,  1.3219e-05,  1.1288e-05],
          [ 6.9556e-06, -3.8478e-06, -3.5670e-06],
          [-4.6414e-06, -2.9977e-06,  1.3880e-05]],

         [[-4.4938e-06,  1.4375e-05,  7.3289e-06],
          [-4.1747e-06, -4.5655e-06, -6.0586e-06],
          [-3.8968e-06, -8.1488e-07,  4.2052e-06]]],


        ...,


        [[[ 2.2588e-05, -1.1551e-05, -1.1975e-05],
          [-5.0268e-06, -1.1420e-05, -9.6477e-06],
          [-9.4506e-06,  2.4116e-06,  6.7333e-06]],

         [[ 1.6734e-05,  1.9363e-05, -1.1545e-05],
          [ 2.0511e-05, -1.3969e-05, -1.1338e-05],
          [-5.9260e-06, -7.4007e-06, -2.7055e-06]],

         [[ 1.8340e-05,  1.0301e-05, -1.0139e-05],
          [-6.7326e-06, -1.5399e-05, -7.3613e-06],
          [ 7.0276e-06,  1.8084e-05, -1.2814e-05]]],


        [[[-1.0374e-05,  8.7890e-06, -1.4707e-05],
          [-1.4042e-05, -1.4492e-05,  1.1643e-05],
          [-1.4833e-05,  1.5476e-05, -1.3383e-05]],

         [[ 9.3687e-06, -1.1880e-05,  8.2867e-06],
          [ 1.4850e-05,  1.3359e-07, -8.6862e-06],
          [-1.4277e-05,  1.1684e-05, -1.9693e-05]],

         [[ 1.2447e-05, -2.0698e-06,  3.3029e-08],
          [ 1.3988e-05, -1.7611e-05,  1.0270e-06],
          [-2.5051e-06,  1.7802e-06, -1.2247e-05]]],


        [[[ 7.1269e-06,  1.5913e-05,  1.0940e-05],
          [-8.7373e-06, -1.8714e-05,  2.1282e-05],
          [-1.7526e-05, -2.2572e-06, -6.6321e-06]],

         [[-2.4418e-06, -7.8050e-06,  1.4931e-05],
          [ 1.2113e-08, -2.1326e-05,  5.5001e-06],
          [-7.3015e-07,  7.6504e-06,  2.9615e-06]],

         [[-1.6954e-05, -1.9923e-05,  1.2219e-05],
          [ 1.4541e-05, -1.9156e-05, -1.3174e-06],
          [ 8.9001e-07, -1.5075e-06,  2.9367e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8315]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0232]], device='cuda:0')

Epoch: 108 | Batch_idx: 0 |  Loss: (0.4042) |  Loss2: (0.2408) | Acc: (95.00%) (122/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.4186) |  Loss2: (0.2407) | Acc: (93.00%) (1315/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.4099) |  Loss2: (0.2407) | Acc: (93.00%) (2526/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.4204) |  Loss2: (0.2406) | Acc: (93.00%) (3716/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.4222) |  Loss2: (0.2406) | Acc: (93.00%) (4902/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.4227) |  Loss2: (0.2405) | Acc: (93.00%) (6100/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.4253) |  Loss2: (0.2405) | Acc: (93.00%) (7300/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.4254) |  Loss2: (0.2404) | Acc: (93.00%) (8498/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.4297) |  Loss2: (0.2404) | Acc: (93.00%) (9676/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.4297) |  Loss2: (0.2404) | Acc: (93.00%) (10877/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.4321) |  Loss2: (0.2403) | Acc: (93.00%) (12061/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.4334) |  Loss2: (0.2403) | Acc: (93.00%) (13242/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.4353) |  Loss2: (0.2402) | Acc: (93.00%) (14421/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.4371) |  Loss2: (0.2402) | Acc: (93.00%) (15597/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.4384) |  Loss2: (0.2402) | Acc: (92.00%) (16779/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.4398) |  Loss2: (0.2401) | Acc: (92.00%) (17958/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.4407) |  Loss2: (0.2401) | Acc: (92.00%) (19137/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.4385) |  Loss2: (0.2400) | Acc: (92.00%) (20347/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.4393) |  Loss2: (0.2400) | Acc: (92.00%) (21528/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.4393) |  Loss2: (0.2400) | Acc: (92.00%) (22713/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.4405) |  Loss2: (0.2399) | Acc: (92.00%) (23907/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.4412) |  Loss2: (0.2399) | Acc: (92.00%) (25087/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.4416) |  Loss2: (0.2398) | Acc: (92.00%) (26283/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.4418) |  Loss2: (0.2398) | Acc: (92.00%) (27466/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.4418) |  Loss2: (0.2397) | Acc: (92.00%) (28658/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.4402) |  Loss2: (0.2397) | Acc: (92.00%) (29868/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.4392) |  Loss2: (0.2397) | Acc: (92.00%) (31068/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.4391) |  Loss2: (0.2396) | Acc: (92.00%) (32255/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.4388) |  Loss2: (0.2396) | Acc: (92.00%) (33449/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.4385) |  Loss2: (0.2395) | Acc: (93.00%) (34650/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.4379) |  Loss2: (0.2395) | Acc: (93.00%) (35853/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.4376) |  Loss2: (0.2395) | Acc: (93.00%) (37039/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.4374) |  Loss2: (0.2394) | Acc: (93.00%) (38227/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.4368) |  Loss2: (0.2394) | Acc: (93.00%) (39435/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.4362) |  Loss2: (0.2393) | Acc: (93.00%) (40639/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.4364) |  Loss2: (0.2393) | Acc: (93.00%) (41827/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.4368) |  Loss2: (0.2393) | Acc: (93.00%) (43014/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.4365) |  Loss2: (0.2392) | Acc: (93.00%) (44215/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.4361) |  Loss2: (0.2392) | Acc: (93.00%) (45425/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.4364) |  Loss2: (0.2391) | Acc: (93.00%) (46569/50000)
# TEST : Loss: (0.3873) | Acc: (87.00%) (8750/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1348,  0.0656,  0.0918],
          [ 0.1066, -0.3017, -0.1134],
          [ 0.2552, -0.0809,  0.0082]],

         [[-0.0081, -0.0919,  0.2610],
          [ 0.0177, -0.1081,  0.0423],
          [ 0.0616,  0.0067, -0.0678]],

         [[ 0.0147, -0.0393,  0.2003],
          [ 0.0929, -0.1979,  0.1609],
          [-0.1131,  0.0248, -0.1871]]],


        [[[ 0.0974, -0.0773, -0.0042],
          [ 0.1362,  0.1717,  0.1806],
          [ 0.0088, -0.0676,  0.1126]],

         [[-0.1535, -0.1809,  0.1551],
          [ 0.0332, -0.0501,  0.1041],
          [ 0.1230, -0.2045,  0.1306]],

         [[ 0.1006, -0.0507, -0.0137],
          [ 0.0126,  0.0205, -0.1164],
          [-0.0299, -0.1603, -0.0183]]],


        [[[-0.0712,  0.0890, -0.0424],
          [-0.1244,  0.0649,  0.1163],
          [ 0.1110,  0.1657,  0.0017]],

         [[ 0.0996,  0.1321,  0.1128],
          [ 0.0695, -0.0385, -0.0357],
          [-0.0464, -0.0300,  0.1387]],

         [[-0.0449,  0.1437,  0.0733],
          [-0.0417, -0.0456, -0.0606],
          [-0.0390, -0.0081,  0.0420]]],


        ...,


        [[[ 0.2258, -0.1155, -0.1197],
          [-0.0502, -0.1142, -0.0964],
          [-0.0945,  0.0241,  0.0673]],

         [[ 0.1673,  0.1936, -0.1154],
          [ 0.2050, -0.1396, -0.1133],
          [-0.0592, -0.0740, -0.0270]],

         [[ 0.1833,  0.1030, -0.1014],
          [-0.0673, -0.1539, -0.0736],
          [ 0.0702,  0.1808, -0.1281]]],


        [[[-0.1037,  0.0879, -0.1470],
          [-0.1404, -0.1449,  0.1164],
          [-0.1483,  0.1547, -0.1338]],

         [[ 0.0936, -0.1188,  0.0828],
          [ 0.1484,  0.0013, -0.0868],
          [-0.1427,  0.1168, -0.1969]],

         [[ 0.1244, -0.0207,  0.0003],
          [ 0.1398, -0.1760,  0.0103],
          [-0.0250,  0.0178, -0.1224]]],


        [[[ 0.0712,  0.1591,  0.1094],
          [-0.0873, -0.1871,  0.2127],
          [-0.1752, -0.0226, -0.0663]],

         [[-0.0244, -0.0780,  0.1493],
          [ 0.0001, -0.2132,  0.0550],
          [-0.0073,  0.0765,  0.0296]],

         [[-0.1695, -0.1992,  0.1221],
          [ 0.1453, -0.1915, -0.0132],
          [ 0.0089, -0.0151,  0.2936]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3479e-05,  6.5576e-06,  9.1799e-06],
          [ 1.0659e-05, -3.0170e-05, -1.1338e-05],
          [ 2.5515e-05, -8.0881e-06,  8.1607e-07]],

         [[-8.1424e-07, -9.1916e-06,  2.6104e-05],
          [ 1.7672e-06, -1.0807e-05,  4.2287e-06],
          [ 6.1640e-06,  6.7441e-07, -6.7801e-06]],

         [[ 1.4748e-06, -3.9347e-06,  2.0029e-05],
          [ 9.2923e-06, -1.9788e-05,  1.6088e-05],
          [-1.1308e-05,  2.4789e-06, -1.8709e-05]]],


        [[[ 9.7390e-06, -7.7269e-06, -4.2410e-07],
          [ 1.3616e-05,  1.7168e-05,  1.8055e-05],
          [ 8.7692e-07, -6.7619e-06,  1.1258e-05]],

         [[-1.5350e-05, -1.8095e-05,  1.5512e-05],
          [ 3.3230e-06, -5.0114e-06,  1.0407e-05],
          [ 1.2302e-05, -2.0451e-05,  1.3061e-05]],

         [[ 1.0063e-05, -5.0670e-06, -1.3668e-06],
          [ 1.2563e-06,  2.0547e-06, -1.1642e-05],
          [-2.9885e-06, -1.6027e-05, -1.8346e-06]]],


        [[[-7.1246e-06,  8.9037e-06, -4.2385e-06],
          [-1.2440e-05,  6.4894e-06,  1.1630e-05],
          [ 1.1098e-05,  1.6570e-05,  1.7126e-07]],

         [[ 9.9593e-06,  1.3214e-05,  1.1284e-05],
          [ 6.9530e-06, -3.8463e-06, -3.5655e-06],
          [-4.6397e-06, -2.9965e-06,  1.3875e-05]],

         [[-4.4920e-06,  1.4370e-05,  7.3260e-06],
          [-4.1731e-06, -4.5637e-06, -6.0563e-06],
          [-3.8954e-06, -8.1455e-07,  4.2036e-06]]],


        ...,


        [[[ 2.2580e-05, -1.1547e-05, -1.1970e-05],
          [-5.0249e-06, -1.1416e-05, -9.6439e-06],
          [-9.4468e-06,  2.4107e-06,  6.7306e-06]],

         [[ 1.6728e-05,  1.9356e-05, -1.1540e-05],
          [ 2.0502e-05, -1.3964e-05, -1.1333e-05],
          [-5.9237e-06, -7.3978e-06, -2.7044e-06]],

         [[ 1.8333e-05,  1.0297e-05, -1.0135e-05],
          [-6.7299e-06, -1.5394e-05, -7.3584e-06],
          [ 7.0250e-06,  1.8077e-05, -1.2809e-05]]],


        [[[-1.0370e-05,  8.7855e-06, -1.4701e-05],
          [-1.4037e-05, -1.4486e-05,  1.1638e-05],
          [-1.4827e-05,  1.5470e-05, -1.3377e-05]],

         [[ 9.3649e-06, -1.1876e-05,  8.2835e-06],
          [ 1.4844e-05,  1.3354e-07, -8.6827e-06],
          [-1.4271e-05,  1.1679e-05, -1.9686e-05]],

         [[ 1.2443e-05, -2.0690e-06,  3.3017e-08],
          [ 1.3983e-05, -1.7604e-05,  1.0266e-06],
          [-2.5042e-06,  1.7795e-06, -1.2243e-05]]],


        [[[ 7.1240e-06,  1.5906e-05,  1.0936e-05],
          [-8.7338e-06, -1.8706e-05,  2.1274e-05],
          [-1.7519e-05, -2.2564e-06, -6.6295e-06]],

         [[-2.4409e-06, -7.8021e-06,  1.4925e-05],
          [ 1.2108e-08, -2.1318e-05,  5.4979e-06],
          [-7.2986e-07,  7.6475e-06,  2.9603e-06]],

         [[-1.6947e-05, -1.9916e-05,  1.2214e-05],
          [ 1.4535e-05, -1.9149e-05, -1.3169e-06],
          [ 8.8965e-07, -1.5069e-06,  2.9355e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8432]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0525]], device='cuda:0')

Epoch: 109 | Batch_idx: 0 |  Loss: (0.4448) |  Loss2: (0.2376) | Acc: (90.00%) (116/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.4119) |  Loss2: (0.2375) | Acc: (93.00%) (1321/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.4157) |  Loss2: (0.2375) | Acc: (93.00%) (2525/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.4268) |  Loss2: (0.2374) | Acc: (93.00%) (3714/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.4310) |  Loss2: (0.2374) | Acc: (93.00%) (4900/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.4300) |  Loss2: (0.2373) | Acc: (93.00%) (6088/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.4279) |  Loss2: (0.2373) | Acc: (93.00%) (7287/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.4286) |  Loss2: (0.2372) | Acc: (93.00%) (8492/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.4280) |  Loss2: (0.2372) | Acc: (93.00%) (9695/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.4270) |  Loss2: (0.2372) | Acc: (93.00%) (10892/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.4264) |  Loss2: (0.2371) | Acc: (93.00%) (12098/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.4243) |  Loss2: (0.2371) | Acc: (93.00%) (13304/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.4246) |  Loss2: (0.2370) | Acc: (93.00%) (14490/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.4241) |  Loss2: (0.2370) | Acc: (93.00%) (15697/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.4254) |  Loss2: (0.2369) | Acc: (93.00%) (16882/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.4259) |  Loss2: (0.2369) | Acc: (93.00%) (18075/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.4275) |  Loss2: (0.2369) | Acc: (93.00%) (19257/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.4271) |  Loss2: (0.2368) | Acc: (93.00%) (20458/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.4271) |  Loss2: (0.2368) | Acc: (93.00%) (21650/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.4267) |  Loss2: (0.2368) | Acc: (93.00%) (22847/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.4267) |  Loss2: (0.2367) | Acc: (93.00%) (24047/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.4267) |  Loss2: (0.2367) | Acc: (93.00%) (25236/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.4259) |  Loss2: (0.2366) | Acc: (93.00%) (26434/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.4269) |  Loss2: (0.2366) | Acc: (93.00%) (27621/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.4270) |  Loss2: (0.2366) | Acc: (93.00%) (28824/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.4276) |  Loss2: (0.2365) | Acc: (93.00%) (30013/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.4280) |  Loss2: (0.2365) | Acc: (93.00%) (31202/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.4278) |  Loss2: (0.2365) | Acc: (93.00%) (32402/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.4282) |  Loss2: (0.2364) | Acc: (93.00%) (33598/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.4285) |  Loss2: (0.2364) | Acc: (93.00%) (34789/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.4290) |  Loss2: (0.2364) | Acc: (93.00%) (35981/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.4291) |  Loss2: (0.2363) | Acc: (93.00%) (37177/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.4288) |  Loss2: (0.2363) | Acc: (93.00%) (38380/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.4287) |  Loss2: (0.2362) | Acc: (93.00%) (39578/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.4279) |  Loss2: (0.2362) | Acc: (93.00%) (40781/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.4282) |  Loss2: (0.2362) | Acc: (93.00%) (41975/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.4279) |  Loss2: (0.2361) | Acc: (93.00%) (43167/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.4287) |  Loss2: (0.2361) | Acc: (93.00%) (44344/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.4295) |  Loss2: (0.2361) | Acc: (93.00%) (45522/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.4296) |  Loss2: (0.2360) | Acc: (93.00%) (46664/50000)
# TEST : Loss: (0.3859) | Acc: (87.00%) (8761/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1347,  0.0656,  0.0918],
          [ 0.1066, -0.3016, -0.1133],
          [ 0.2550, -0.0808,  0.0082]],

         [[-0.0081, -0.0919,  0.2609],
          [ 0.0177, -0.1080,  0.0423],
          [ 0.0616,  0.0067, -0.0678]],

         [[ 0.0147, -0.0393,  0.2002],
          [ 0.0929, -0.1978,  0.1608],
          [-0.1130,  0.0248, -0.1870]]],


        [[[ 0.0974, -0.0772, -0.0042],
          [ 0.1361,  0.1716,  0.1805],
          [ 0.0088, -0.0676,  0.1125]],

         [[-0.1534, -0.1809,  0.1551],
          [ 0.0332, -0.0501,  0.1040],
          [ 0.1230, -0.2044,  0.1306]],

         [[ 0.1006, -0.0506, -0.0137],
          [ 0.0126,  0.0205, -0.1164],
          [-0.0299, -0.1602, -0.0183]]],


        [[[-0.0712,  0.0890, -0.0424],
          [-0.1244,  0.0649,  0.1163],
          [ 0.1109,  0.1656,  0.0017]],

         [[ 0.0996,  0.1321,  0.1128],
          [ 0.0695, -0.0384, -0.0356],
          [-0.0464, -0.0300,  0.1387]],

         [[-0.0449,  0.1436,  0.0732],
          [-0.0417, -0.0456, -0.0605],
          [-0.0389, -0.0081,  0.0420]]],


        ...,


        [[[ 0.2257, -0.1154, -0.1197],
          [-0.0502, -0.1141, -0.0964],
          [-0.0944,  0.0241,  0.0673]],

         [[ 0.1672,  0.1935, -0.1154],
          [ 0.2049, -0.1396, -0.1133],
          [-0.0592, -0.0739, -0.0270]],

         [[ 0.1833,  0.1029, -0.1013],
          [-0.0673, -0.1539, -0.0736],
          [ 0.0702,  0.1807, -0.1280]]],


        [[[-0.1037,  0.0878, -0.1469],
          [-0.1403, -0.1448,  0.1163],
          [-0.1482,  0.1546, -0.1337]],

         [[ 0.0936, -0.1187,  0.0828],
          [ 0.1484,  0.0013, -0.0868],
          [-0.1427,  0.1167, -0.1968]],

         [[ 0.1244, -0.0207,  0.0003],
          [ 0.1398, -0.1760,  0.0103],
          [-0.0250,  0.0178, -0.1224]]],


        [[[ 0.0712,  0.1590,  0.1093],
          [-0.0873, -0.1870,  0.2127],
          [-0.1751, -0.0226, -0.0663]],

         [[-0.0244, -0.0780,  0.1492],
          [ 0.0001, -0.2131,  0.0550],
          [-0.0073,  0.0764,  0.0296]],

         [[-0.1694, -0.1991,  0.1221],
          [ 0.1453, -0.1914, -0.0132],
          [ 0.0089, -0.0151,  0.2934]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3474e-05,  6.5550e-06,  9.1764e-06],
          [ 1.0655e-05, -3.0159e-05, -1.1333e-05],
          [ 2.5505e-05, -8.0849e-06,  8.1575e-07]],

         [[-8.1392e-07, -9.1881e-06,  2.6094e-05],
          [ 1.7665e-06, -1.0803e-05,  4.2271e-06],
          [ 6.1615e-06,  6.7416e-07, -6.7775e-06]],

         [[ 1.4742e-06, -3.9331e-06,  2.0021e-05],
          [ 9.2888e-06, -1.9780e-05,  1.6081e-05],
          [-1.1303e-05,  2.4779e-06, -1.8702e-05]]],


        [[[ 9.7352e-06, -7.7240e-06, -4.2394e-07],
          [ 1.3610e-05,  1.7161e-05,  1.8048e-05],
          [ 8.7659e-07, -6.7593e-06,  1.1254e-05]],

         [[-1.5344e-05, -1.8088e-05,  1.5506e-05],
          [ 3.3217e-06, -5.0095e-06,  1.0403e-05],
          [ 1.2297e-05, -2.0443e-05,  1.3056e-05]],

         [[ 1.0059e-05, -5.0650e-06, -1.3662e-06],
          [ 1.2559e-06,  2.0539e-06, -1.1637e-05],
          [-2.9873e-06, -1.6021e-05, -1.8339e-06]]],


        [[[-7.1217e-06,  8.9002e-06, -4.2369e-06],
          [-1.2436e-05,  6.4868e-06,  1.1625e-05],
          [ 1.1093e-05,  1.6563e-05,  1.7119e-07]],

         [[ 9.9555e-06,  1.3209e-05,  1.1280e-05],
          [ 6.9504e-06, -3.8448e-06, -3.5641e-06],
          [-4.6379e-06, -2.9954e-06,  1.3870e-05]],

         [[-4.4903e-06,  1.4364e-05,  7.3230e-06],
          [-4.1715e-06, -4.5620e-06, -6.0540e-06],
          [-3.8939e-06, -8.1423e-07,  4.2020e-06]]],


        ...,


        [[[ 2.2571e-05, -1.1543e-05, -1.1966e-05],
          [-5.0230e-06, -1.1412e-05, -9.6401e-06],
          [-9.4430e-06,  2.4097e-06,  6.7280e-06]],

         [[ 1.6722e-05,  1.9348e-05, -1.1536e-05],
          [ 2.0494e-05, -1.3959e-05, -1.1329e-05],
          [-5.9214e-06, -7.3949e-06, -2.7033e-06]],

         [[ 1.8326e-05,  1.0293e-05, -1.0131e-05],
          [-6.7273e-06, -1.5388e-05, -7.3555e-06],
          [ 7.0224e-06,  1.8070e-05, -1.2804e-05]]],


        [[[-1.0366e-05,  8.7821e-06, -1.4695e-05],
          [-1.4032e-05, -1.4480e-05,  1.1634e-05],
          [-1.4821e-05,  1.5464e-05, -1.3372e-05]],

         [[ 9.3611e-06, -1.1871e-05,  8.2803e-06],
          [ 1.4838e-05,  1.3349e-07, -8.6793e-06],
          [-1.4266e-05,  1.1675e-05, -1.9678e-05]],

         [[ 1.2438e-05, -2.0682e-06,  3.3004e-08],
          [ 1.3978e-05, -1.7597e-05,  1.0262e-06],
          [-2.5033e-06,  1.7788e-06, -1.2238e-05]]],


        [[[ 7.1211e-06,  1.5900e-05,  1.0931e-05],
          [-8.7303e-06, -1.8699e-05,  2.1265e-05],
          [-1.7512e-05, -2.2555e-06, -6.6268e-06]],

         [[-2.4399e-06, -7.7991e-06,  1.4919e-05],
          [ 1.2103e-08, -2.1310e-05,  5.4957e-06],
          [-7.2957e-07,  7.6446e-06,  2.9591e-06]],

         [[-1.6941e-05, -1.9908e-05,  1.2209e-05],
          [ 1.4529e-05, -1.9141e-05, -1.3164e-06],
          [ 8.8928e-07, -1.5063e-06,  2.9343e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8609]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0128]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (3700/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (4889/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (6068/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (93.00%) (7268/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (8466/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (9646/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (93.00%) (10834/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (93.00%) (12038/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (13208/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (14400/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (15584/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (16760/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (17953/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (92.00%) (19134/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (20312/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (21489/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (22691/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (23881/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (25080/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (26287/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (27472/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (28659/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (29855/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (31044/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (32232/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (33418/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (34615/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (35812/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (36983/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (38168/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (39350/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2054) |  Loss2: (0.0000) | Acc: (92.00%) (40541/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (41733/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2055) |  Loss2: (0.0000) | Acc: (92.00%) (42923/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2061) |  Loss2: (0.0000) | Acc: (92.00%) (44114/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (45305/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (46439/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
# TEST : Loss: (0.4454) | Acc: (86.00%) (8631/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1313,  0.0686,  0.0937],
          [ 0.1112, -0.3006, -0.1117],
          [ 0.2587, -0.0801,  0.0079]],

         [[-0.0087, -0.0911,  0.2608],
          [ 0.0195, -0.1084,  0.0423],
          [ 0.0629,  0.0062, -0.0700]],

         [[ 0.0135, -0.0405,  0.1984],
          [ 0.0930, -0.1995,  0.1595],
          [-0.1134,  0.0229, -0.1901]]],


        [[[ 0.0983, -0.0753, -0.0025],
          [ 0.1371,  0.1739,  0.1823],
          [ 0.0099, -0.0654,  0.1142]],

         [[-0.1517, -0.1787,  0.1564],
          [ 0.0348, -0.0477,  0.1051],
          [ 0.1241, -0.2025,  0.1314]],

         [[ 0.1026, -0.0484, -0.0119],
          [ 0.0145,  0.0227, -0.1152],
          [-0.0280, -0.1583, -0.0176]]],


        [[[-0.0714,  0.0890, -0.0424],
          [-0.1245,  0.0648,  0.1160],
          [ 0.1108,  0.1655,  0.0013]],

         [[ 0.0996,  0.1323,  0.1129],
          [ 0.0695, -0.0383, -0.0357],
          [-0.0462, -0.0299,  0.1382]],

         [[-0.0448,  0.1438,  0.0733],
          [-0.0417, -0.0456, -0.0606],
          [-0.0388, -0.0082,  0.0416]]],


        ...,


        [[[ 0.2263, -0.1154, -0.1215],
          [-0.0492, -0.1137, -0.0970],
          [-0.0934,  0.0247,  0.0674]],

         [[ 0.1672,  0.1938, -0.1159],
          [ 0.2058, -0.1387, -0.1131],
          [-0.0584, -0.0733, -0.0266]],

         [[ 0.1831,  0.1035, -0.1011],
          [-0.0660, -0.1526, -0.0729],
          [ 0.0712,  0.1814, -0.1277]]],


        [[[-0.1040,  0.0877, -0.1465],
          [-0.1419, -0.1463,  0.1158],
          [-0.1488,  0.1534, -0.1341]],

         [[ 0.0943, -0.1177,  0.0842],
          [ 0.1483,  0.0012, -0.0861],
          [-0.1416,  0.1171, -0.1958]],

         [[ 0.1249, -0.0200,  0.0013],
          [ 0.1398, -0.1760,  0.0108],
          [-0.0240,  0.0181, -0.1216]]],


        [[[ 0.0723,  0.1601,  0.1095],
          [-0.0887, -0.1897,  0.2107],
          [-0.1784, -0.0278, -0.0730]],

         [[-0.0231, -0.0778,  0.1489],
          [-0.0001, -0.2157,  0.0531],
          [-0.0080,  0.0732,  0.0248]],

         [[-0.1672, -0.1990,  0.1219],
          [ 0.1460, -0.1936, -0.0141],
          [ 0.0085, -0.0176,  0.2908]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0313, -0.0215, -0.0276],
          [-0.0845, -0.0644, -0.0502],
          [-0.0716, -0.0607, -0.0321]],

         [[-0.0279, -0.0237, -0.0335],
          [-0.0795, -0.0636, -0.0513],
          [-0.0676, -0.0579, -0.0328]],

         [[ 0.0082,  0.0200,  0.0156],
          [-0.0436, -0.0164,  0.0007],
          [-0.0379, -0.0170,  0.0110]]],


        [[[-0.0064, -0.0005,  0.0028],
          [-0.0035, -0.0015, -0.0003],
          [ 0.0116,  0.0102,  0.0067]],

         [[-0.0087, -0.0022,  0.0015],
          [-0.0079, -0.0058, -0.0038],
          [ 0.0048,  0.0033,  0.0009]],

         [[-0.0137, -0.0071, -0.0025],
          [-0.0125, -0.0100, -0.0077],
          [ 0.0006, -0.0008, -0.0033]]],


        [[[-0.0014, -0.0008, -0.0030],
          [-0.0022, -0.0003, -0.0004],
          [-0.0044, -0.0019, -0.0009]],

         [[-0.0000,  0.0012,  0.0001],
          [-0.0016,  0.0007,  0.0018],
          [-0.0036, -0.0011,  0.0007]],

         [[-0.0037, -0.0022, -0.0026],
          [-0.0046, -0.0023, -0.0008],
          [-0.0061, -0.0034, -0.0015]]],


        ...,


        [[[-0.0237, -0.0297, -0.0307],
          [-0.0145, -0.0168, -0.0140],
          [-0.0284, -0.0290, -0.0273]],

         [[-0.0278, -0.0308, -0.0306],
          [-0.0209, -0.0210, -0.0137],
          [-0.0347, -0.0320, -0.0241]],

         [[-0.0431, -0.0482, -0.0509],
          [-0.0341, -0.0353, -0.0306],
          [-0.0439, -0.0418, -0.0360]]],


        [[[ 0.0013,  0.0086,  0.0111],
          [ 0.0044,  0.0068,  0.0095],
          [ 0.0009,  0.0018, -0.0007]],

         [[-0.0017,  0.0043,  0.0052],
          [-0.0009,  0.0014,  0.0044],
          [-0.0066, -0.0044, -0.0056]],

         [[-0.0024,  0.0030,  0.0048],
          [-0.0065, -0.0026,  0.0016],
          [-0.0132, -0.0095, -0.0098]]],


        [[[-0.0242, -0.0481, -0.0494],
          [-0.0154, -0.0232, -0.0107],
          [-0.0151, -0.0239, -0.0191]],

         [[-0.0152, -0.0392, -0.0262],
          [-0.0027, -0.0146,  0.0103],
          [ 0.0056, -0.0104,  0.0011]],

         [[ 0.0164, -0.0000,  0.0059],
          [ 0.0328,  0.0218,  0.0350],
          [ 0.0370,  0.0195,  0.0211]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8605]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 111 | Batch_idx: 0 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (2510/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (3702/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (4888/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (6062/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (7253/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (8460/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (9655/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (10845/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (12047/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (13251/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (14444/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (15647/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (16839/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (18006/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (19204/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (20397/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (21581/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (22763/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (23940/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (25120/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (26307/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (27489/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (28680/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (29863/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (31048/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (32235/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (92.00%) (33430/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (34603/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (35772/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (92.00%) (36958/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (38153/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (39322/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (40492/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (41663/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (42859/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (44063/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (45257/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (46406/50000)
# TEST : Loss: (0.4633) | Acc: (86.00%) (8611/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1326,  0.0697,  0.0963],
          [ 0.1085, -0.3021, -0.1112],
          [ 0.2554, -0.0823,  0.0055]],

         [[-0.0123, -0.0916,  0.2622],
          [ 0.0143, -0.1118,  0.0410],
          [ 0.0567,  0.0008, -0.0756]],

         [[ 0.0134, -0.0384,  0.2009],
          [ 0.0919, -0.1996,  0.1602],
          [-0.1155,  0.0210, -0.1930]]],


        [[[ 0.0967, -0.0771, -0.0043],
          [ 0.1359,  0.1725,  0.1809],
          [ 0.0088, -0.0668,  0.1129]],

         [[-0.1539, -0.1810,  0.1536],
          [ 0.0326, -0.0499,  0.1028],
          [ 0.1220, -0.2045,  0.1295]],

         [[ 0.0999, -0.0513, -0.0151],
          [ 0.0115,  0.0196, -0.1182],
          [-0.0308, -0.1611, -0.0199]]],


        [[[-0.0715,  0.0890, -0.0425],
          [-0.1245,  0.0649,  0.1160],
          [ 0.1105,  0.1656,  0.0015]],

         [[ 0.0992,  0.1321,  0.1126],
          [ 0.0692, -0.0384, -0.0358],
          [-0.0467, -0.0300,  0.1381]],

         [[-0.0451,  0.1435,  0.0729],
          [-0.0421, -0.0459, -0.0610],
          [-0.0395, -0.0086,  0.0412]]],


        ...,


        [[[ 0.2292, -0.1118, -0.1184],
          [-0.0488, -0.1130, -0.0961],
          [-0.0938,  0.0252,  0.0689]],

         [[ 0.1687,  0.1965, -0.1132],
          [ 0.2049, -0.1386, -0.1127],
          [-0.0601, -0.0737, -0.0259]],

         [[ 0.1836,  0.1047, -0.0999],
          [-0.0667, -0.1527, -0.0730],
          [ 0.0700,  0.1813, -0.1271]]],


        [[[-0.1029,  0.0881, -0.1462],
          [-0.1407, -0.1457,  0.1160],
          [-0.1477,  0.1538, -0.1346]],

         [[ 0.0962, -0.1161,  0.0856],
          [ 0.1496,  0.0024, -0.0851],
          [-0.1404,  0.1177, -0.1959]],

         [[ 0.1260, -0.0192,  0.0020],
          [ 0.1408, -0.1751,  0.0114],
          [-0.0231,  0.0186, -0.1217]]],


        [[[ 0.0763,  0.1625,  0.1119],
          [-0.0857, -0.1883,  0.2155],
          [-0.1764, -0.0268, -0.0704]],

         [[-0.0182, -0.0750,  0.1517],
          [ 0.0046, -0.2129,  0.0579],
          [-0.0040,  0.0763,  0.0280]],

         [[-0.1634, -0.1980,  0.1228],
          [ 0.1488, -0.1925, -0.0106],
          [ 0.0101, -0.0161,  0.2934]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0089,  0.0405,  0.0491],
          [ 0.0234,  0.0460,  0.0508],
          [ 0.0126,  0.0256,  0.0345]],

         [[ 0.0225,  0.0505,  0.0574],
          [ 0.0353,  0.0555,  0.0599],
          [ 0.0201,  0.0340,  0.0419]],

         [[-0.0336, -0.0087,  0.0020],
          [-0.0162, -0.0006,  0.0053],
          [-0.0244, -0.0183, -0.0115]]],


        [[[-0.0146, -0.0181, -0.0134],
          [-0.0163, -0.0196, -0.0152],
          [-0.0190, -0.0214, -0.0164]],

         [[-0.0136, -0.0170, -0.0113],
          [-0.0153, -0.0182, -0.0135],
          [-0.0175, -0.0195, -0.0148]],

         [[-0.0168, -0.0206, -0.0166],
          [-0.0200, -0.0230, -0.0198],
          [-0.0239, -0.0252, -0.0214]]],


        [[[-0.0001, -0.0009, -0.0027],
          [-0.0008, -0.0006, -0.0016],
          [-0.0016, -0.0002, -0.0005]],

         [[ 0.0016,  0.0010, -0.0002],
          [ 0.0011,  0.0013,  0.0008],
          [ 0.0007,  0.0019,  0.0020]],

         [[ 0.0017,  0.0005, -0.0009],
          [ 0.0012,  0.0008,  0.0000],
          [ 0.0014,  0.0021,  0.0020]]],


        ...,


        [[[ 0.0082,  0.0157,  0.0176],
          [ 0.0090,  0.0139,  0.0147],
          [-0.0046, -0.0019, -0.0028]],

         [[ 0.0267,  0.0296,  0.0285],
          [ 0.0265,  0.0265,  0.0237],
          [ 0.0107,  0.0086,  0.0047]],

         [[ 0.0356,  0.0365,  0.0340],
          [ 0.0299,  0.0280,  0.0245],
          [ 0.0115,  0.0072,  0.0045]]],


        [[[ 0.0092,  0.0075,  0.0074],
          [ 0.0077,  0.0071,  0.0064],
          [-0.0020, -0.0013, -0.0005]],

         [[ 0.0048,  0.0033,  0.0033],
          [ 0.0040,  0.0039,  0.0036],
          [-0.0048, -0.0029, -0.0016]],

         [[-0.0006, -0.0045, -0.0050],
          [-0.0005, -0.0023, -0.0034],
          [-0.0091, -0.0085, -0.0080]]],


        [[[ 0.0669,  0.0772,  0.0686],
          [ 0.0555,  0.0672,  0.0534],
          [ 0.0552,  0.0566,  0.0342]],

         [[ 0.0544,  0.0649,  0.0581],
          [ 0.0483,  0.0588,  0.0476],
          [ 0.0565,  0.0581,  0.0379]],

         [[ 0.0635,  0.0718,  0.0700],
          [ 0.0574,  0.0671,  0.0615],
          [ 0.0702,  0.0721,  0.0583]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8601]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 112 | Batch_idx: 0 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (3717/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (4916/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1892) |  Loss2: (0.0000) | Acc: (93.00%) (6105/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (7310/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (8509/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (9723/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (10903/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (12092/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (13287/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (14474/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (15672/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (16876/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (18066/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (19247/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1916) |  Loss2: (0.0000) | Acc: (93.00%) (20443/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (21638/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (22830/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (24026/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (25204/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (26395/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (27579/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (28769/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (29964/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (31154/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (32339/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (33532/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (34721/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (35909/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (37110/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (38305/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (39482/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (40662/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (41853/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (43038/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (44223/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (45418/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (46575/50000)
# TEST : Loss: (0.4391) | Acc: (86.00%) (8644/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1326,  0.0706,  0.0988],
          [ 0.1103, -0.3027, -0.1109],
          [ 0.2586, -0.0802,  0.0084]],

         [[-0.0137, -0.0920,  0.2627],
          [ 0.0143, -0.1140,  0.0391],
          [ 0.0587,  0.0015, -0.0747]],

         [[ 0.0074, -0.0433,  0.1964],
          [ 0.0867, -0.2061,  0.1543],
          [-0.1179,  0.0179, -0.1953]]],


        [[[ 0.0982, -0.0741, -0.0018],
          [ 0.1367,  0.1743,  0.1823],
          [ 0.0093, -0.0650,  0.1143]],

         [[-0.1533, -0.1787,  0.1555],
          [ 0.0324, -0.0488,  0.1035],
          [ 0.1213, -0.2037,  0.1300]],

         [[ 0.1000, -0.0496, -0.0131],
          [ 0.0110,  0.0202, -0.1175],
          [-0.0312, -0.1604, -0.0193]]],


        [[[-0.0718,  0.0887, -0.0426],
          [-0.1250,  0.0646,  0.1158],
          [ 0.1101,  0.1653,  0.0013]],

         [[ 0.0991,  0.1320,  0.1127],
          [ 0.0688, -0.0385, -0.0358],
          [-0.0469, -0.0302,  0.1381]],

         [[-0.0450,  0.1436,  0.0732],
          [-0.0423, -0.0459, -0.0608],
          [-0.0396, -0.0086,  0.0414]]],


        ...,


        [[[ 0.2267, -0.1151, -0.1207],
          [-0.0513, -0.1158, -0.0964],
          [-0.0961,  0.0225,  0.0689]],

         [[ 0.1682,  0.1946, -0.1141],
          [ 0.2043, -0.1399, -0.1118],
          [-0.0604, -0.0749, -0.0250]],

         [[ 0.1861,  0.1062, -0.0980],
          [-0.0649, -0.1515, -0.0702],
          [ 0.0714,  0.1820, -0.1249]]],


        [[[-0.1033,  0.0884, -0.1452],
          [-0.1411, -0.1454,  0.1164],
          [-0.1473,  0.1541, -0.1347]],

         [[ 0.0957, -0.1159,  0.0863],
          [ 0.1489,  0.0023, -0.0851],
          [-0.1403,  0.1176, -0.1965]],

         [[ 0.1249, -0.0199,  0.0015],
          [ 0.1398, -0.1757,  0.0107],
          [-0.0234,  0.0181, -0.1226]]],


        [[[ 0.0738,  0.1618,  0.1098],
          [-0.0857, -0.1904,  0.2153],
          [-0.1735, -0.0253, -0.0692]],

         [[-0.0197, -0.0755,  0.1499],
          [ 0.0043, -0.2155,  0.0576],
          [-0.0018,  0.0773,  0.0290]],

         [[-0.1665, -0.2020,  0.1184],
          [ 0.1471, -0.1972, -0.0129],
          [ 0.0121, -0.0160,  0.2939]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0501, -0.0399, -0.0243],
          [-0.0385, -0.0270, -0.0178],
          [-0.0437, -0.0308, -0.0309]],

         [[-0.0302, -0.0195, -0.0094],
          [-0.0161, -0.0065, -0.0027],
          [-0.0212, -0.0108, -0.0144]],

         [[-0.0520, -0.0404, -0.0334],
          [-0.0343, -0.0209, -0.0202],
          [-0.0340, -0.0218, -0.0263]]],


        [[[ 0.0029, -0.0030,  0.0035],
          [ 0.0026, -0.0047, -0.0014],
          [-0.0041, -0.0124, -0.0092]],

         [[ 0.0012, -0.0031,  0.0055],
          [ 0.0033, -0.0032,  0.0021],
          [-0.0008, -0.0092, -0.0052]],

         [[ 0.0046,  0.0057,  0.0158],
          [ 0.0087,  0.0072,  0.0139],
          [ 0.0076,  0.0028,  0.0078]]],


        [[[ 0.0002, -0.0015, -0.0010],
          [ 0.0038,  0.0022,  0.0028],
          [ 0.0016, -0.0001,  0.0004]],

         [[ 0.0000, -0.0006,  0.0012],
          [ 0.0044,  0.0036,  0.0056],
          [ 0.0024,  0.0015,  0.0031]],

         [[ 0.0023,  0.0031,  0.0059],
          [ 0.0063,  0.0069,  0.0098],
          [ 0.0043,  0.0044,  0.0068]]],


        ...,


        [[[-0.0343, -0.0232, -0.0238],
          [-0.0356, -0.0234, -0.0178],
          [-0.0463, -0.0374, -0.0249]],

         [[-0.0349, -0.0250, -0.0281],
          [-0.0346, -0.0252, -0.0222],
          [-0.0429, -0.0380, -0.0285]],

         [[-0.0282, -0.0241, -0.0302],
          [-0.0229, -0.0178, -0.0179],
          [-0.0268, -0.0256, -0.0205]]],


        [[[-0.0108, -0.0105, -0.0080],
          [-0.0079, -0.0059, -0.0024],
          [-0.0110, -0.0069, -0.0025]],

         [[-0.0085, -0.0030,  0.0019],
          [-0.0049,  0.0001,  0.0049],
          [-0.0065, -0.0017,  0.0037]],

         [[-0.0024,  0.0033,  0.0080],
          [ 0.0013,  0.0064,  0.0119],
          [ 0.0001,  0.0052,  0.0110]]],


        [[[ 0.0458,  0.0434,  0.0330],
          [ 0.0134,  0.0168,  0.0201],
          [ 0.0062,  0.0077,  0.0119]],

         [[ 0.0475,  0.0457,  0.0332],
          [ 0.0226,  0.0252,  0.0231],
          [ 0.0194,  0.0174,  0.0132]],

         [[ 0.0416,  0.0320,  0.0223],
          [ 0.0220,  0.0145,  0.0131],
          [ 0.0215,  0.0082,  0.0008]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8598]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 113 | Batch_idx: 0 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (3696/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (4899/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (6094/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (7286/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (8489/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (9682/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (10888/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (12091/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (13291/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (14486/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (15689/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (16879/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (18071/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (19266/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (20448/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (21644/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (22832/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (24033/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (25216/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (26404/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (27593/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (28803/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (30004/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (31199/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (32393/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (33585/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (34776/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (35957/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (37145/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (38339/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (39550/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (40746/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (41939/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (43147/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (44345/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (45552/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (46714/50000)
# TEST : Loss: (0.4504) | Acc: (86.00%) (8621/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1354,  0.0668,  0.0973],
          [ 0.1081, -0.3063, -0.1111],
          [ 0.2588, -0.0804,  0.0116]],

         [[-0.0147, -0.0934,  0.2637],
          [ 0.0137, -0.1153,  0.0412],
          [ 0.0602,  0.0035, -0.0698]],

         [[ 0.0089, -0.0405,  0.2024],
          [ 0.0878, -0.2036,  0.1613],
          [-0.1165,  0.0217, -0.1873]]],


        [[[ 0.0988, -0.0741, -0.0024],
          [ 0.1379,  0.1750,  0.1825],
          [ 0.0100, -0.0645,  0.1146]],

         [[-0.1518, -0.1777,  0.1556],
          [ 0.0344, -0.0471,  0.1043],
          [ 0.1223, -0.2026,  0.1306]],

         [[ 0.1012, -0.0488, -0.0128],
          [ 0.0126,  0.0214, -0.1169],
          [-0.0302, -0.1595, -0.0188]]],


        [[[-0.0716,  0.0888, -0.0427],
          [-0.1249,  0.0647,  0.1158],
          [ 0.1098,  0.1651,  0.0011]],

         [[ 0.0992,  0.1321,  0.1127],
          [ 0.0688, -0.0383, -0.0356],
          [-0.0472, -0.0302,  0.1379]],

         [[-0.0451,  0.1435,  0.0732],
          [-0.0424, -0.0458, -0.0607],
          [-0.0400, -0.0088,  0.0411]]],


        ...,


        [[[ 0.2287, -0.1141, -0.1206],
          [-0.0503, -0.1157, -0.0961],
          [-0.0939,  0.0229,  0.0693]],

         [[ 0.1684,  0.1946, -0.1141],
          [ 0.2038, -0.1405, -0.1117],
          [-0.0599, -0.0754, -0.0249]],

         [[ 0.1835,  0.1036, -0.1005],
          [-0.0674, -0.1542, -0.0725],
          [ 0.0703,  0.1798, -0.1267]]],


        [[[-0.1048,  0.0874, -0.1450],
          [-0.1423, -0.1462,  0.1167],
          [-0.1481,  0.1533, -0.1348]],

         [[ 0.0952, -0.1158,  0.0876],
          [ 0.1486,  0.0026, -0.0834],
          [-0.1403,  0.1178, -0.1955]],

         [[ 0.1261, -0.0182,  0.0043],
          [ 0.1412, -0.1739,  0.0136],
          [-0.0219,  0.0195, -0.1206]]],


        [[[ 0.0727,  0.1650,  0.1112],
          [-0.0879, -0.1912,  0.2156],
          [-0.1777, -0.0283, -0.0722]],

         [[-0.0197, -0.0728,  0.1507],
          [ 0.0023, -0.2170,  0.0571],
          [-0.0062,  0.0749,  0.0263]],

         [[-0.1665, -0.1993,  0.1192],
          [ 0.1466, -0.1972, -0.0120],
          [ 0.0104, -0.0161,  0.2936]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0780, -0.0970, -0.0678],
          [-0.0806, -0.0763, -0.0410],
          [-0.0872, -0.0631, -0.0215]],

         [[-0.0699, -0.0911, -0.0640],
          [-0.0745, -0.0709, -0.0383],
          [-0.0877, -0.0652, -0.0275]],

         [[-0.0888, -0.1023, -0.0728],
          [-0.0942, -0.0816, -0.0442],
          [-0.1052, -0.0742, -0.0301]]],


        [[[ 0.0139,  0.0081,  0.0032],
          [ 0.0072,  0.0028,  0.0004],
          [ 0.0006, -0.0014, -0.0025]],

         [[ 0.0161,  0.0099,  0.0040],
          [ 0.0083,  0.0041,  0.0017],
          [ 0.0006, -0.0000, -0.0002]],

         [[ 0.0147,  0.0090,  0.0047],
          [ 0.0072,  0.0035,  0.0031],
          [-0.0012, -0.0019, -0.0003]]],


        [[[ 0.0001, -0.0006, -0.0017],
          [-0.0016, -0.0018, -0.0027],
          [-0.0013, -0.0008, -0.0008]],

         [[ 0.0009,  0.0005, -0.0002],
          [-0.0010, -0.0007, -0.0011],
          [-0.0009,  0.0000,  0.0006]],

         [[-0.0057, -0.0061, -0.0069],
          [-0.0084, -0.0082, -0.0088],
          [-0.0092, -0.0083, -0.0080]]],


        ...,


        [[[ 0.0405,  0.0357,  0.0402],
          [ 0.0381,  0.0304,  0.0283],
          [ 0.0365,  0.0338,  0.0302]],

         [[ 0.0340,  0.0277,  0.0320],
          [ 0.0302,  0.0214,  0.0199],
          [ 0.0281,  0.0253,  0.0235]],

         [[ 0.0475,  0.0371,  0.0376],
          [ 0.0472,  0.0348,  0.0296],
          [ 0.0482,  0.0404,  0.0356]]],


        [[[ 0.0137,  0.0061,  0.0015],
          [ 0.0185,  0.0091,  0.0042],
          [ 0.0103,  0.0030, -0.0033]],

         [[ 0.0136,  0.0064,  0.0017],
          [ 0.0196,  0.0105,  0.0048],
          [ 0.0120,  0.0043, -0.0025]],

         [[ 0.0143,  0.0065,  0.0025],
          [ 0.0163,  0.0077,  0.0039],
          [ 0.0105,  0.0039, -0.0011]]],


        [[[-0.0318, -0.0316, -0.0193],
          [-0.0272, -0.0309, -0.0330],
          [-0.0101, -0.0348, -0.0318]],

         [[-0.0367, -0.0381, -0.0306],
          [-0.0316, -0.0390, -0.0461],
          [-0.0132, -0.0393, -0.0420]],

         [[ 0.0048,  0.0072,  0.0188],
          [ 0.0106,  0.0090,  0.0081],
          [ 0.0278,  0.0066,  0.0098]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8595]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 114 | Batch_idx: 0 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (2521/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (4907/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (6124/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (7328/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (8531/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (9733/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (10953/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (12162/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (13365/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (94.00%) (14569/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (94.00%) (15765/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (94.00%) (16968/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (18159/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (19338/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (20537/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (21735/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (22930/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (24125/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (25322/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (26509/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (27709/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (28904/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1828) |  Loss2: (0.0000) | Acc: (93.00%) (30104/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (31295/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (32493/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (33688/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (34892/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (36071/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (37286/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (38500/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (39692/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (40903/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (42090/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (43280/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1854) |  Loss2: (0.0000) | Acc: (93.00%) (44471/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (45663/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (46805/50000)
# TEST : Loss: (0.4248) | Acc: (86.00%) (8691/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1305,  0.0693,  0.0975],
          [ 0.1120, -0.3052, -0.1112],
          [ 0.2644, -0.0781,  0.0111]],

         [[-0.0083, -0.0889,  0.2650],
          [ 0.0193, -0.1122,  0.0424],
          [ 0.0667,  0.0068, -0.0695]],

         [[ 0.0134, -0.0377,  0.2021],
          [ 0.0889, -0.2043,  0.1588],
          [-0.1168,  0.0185, -0.1919]]],


        [[[ 0.0976, -0.0751, -0.0036],
          [ 0.1385,  0.1752,  0.1818],
          [ 0.0113, -0.0641,  0.1140]],

         [[-0.1530, -0.1788,  0.1542],
          [ 0.0343, -0.0478,  0.1027],
          [ 0.1225, -0.2032,  0.1291]],

         [[ 0.1012, -0.0488, -0.0131],
          [ 0.0132,  0.0213, -0.1178],
          [-0.0294, -0.1597, -0.0198]]],


        [[[-0.0718,  0.0884, -0.0429],
          [-0.1249,  0.0647,  0.1157],
          [ 0.1101,  0.1654,  0.0012]],

         [[ 0.0992,  0.1319,  0.1125],
          [ 0.0690, -0.0383, -0.0356],
          [-0.0467, -0.0298,  0.1380]],

         [[-0.0448,  0.1435,  0.0732],
          [-0.0420, -0.0457, -0.0606],
          [-0.0394, -0.0083,  0.0413]]],


        ...,


        [[[ 0.2277, -0.1148, -0.1211],
          [-0.0521, -0.1169, -0.0959],
          [-0.0948,  0.0232,  0.0707]],

         [[ 0.1676,  0.1938, -0.1156],
          [ 0.2024, -0.1415, -0.1123],
          [-0.0602, -0.0751, -0.0242]],

         [[ 0.1828,  0.1032, -0.1018],
          [-0.0687, -0.1549, -0.0730],
          [ 0.0698,  0.1801, -0.1256]]],


        [[[-0.1046,  0.0870, -0.1453],
          [-0.1423, -0.1469,  0.1170],
          [-0.1478,  0.1530, -0.1344]],

         [[ 0.0956, -0.1159,  0.0874],
          [ 0.1485,  0.0019, -0.0831],
          [-0.1399,  0.1176, -0.1951]],

         [[ 0.1263, -0.0185,  0.0040],
          [ 0.1409, -0.1748,  0.0135],
          [-0.0216,  0.0191, -0.1205]]],


        [[[ 0.0750,  0.1672,  0.1117],
          [-0.0851, -0.1906,  0.2174],
          [-0.1743, -0.0238, -0.0677]],

         [[-0.0193, -0.0738,  0.1481],
          [ 0.0036, -0.2183,  0.0569],
          [-0.0046,  0.0778,  0.0293]],

         [[-0.1666, -0.1997,  0.1183],
          [ 0.1479, -0.1975, -0.0108],
          [ 0.0124, -0.0124,  0.2980]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1443,  0.1325,  0.0902],
          [ 0.0988,  0.0933,  0.0701],
          [ 0.0645,  0.0655,  0.0604]],

         [[ 0.1404,  0.1240,  0.0900],
          [ 0.1042,  0.0859,  0.0653],
          [ 0.0716,  0.0533,  0.0462]],

         [[ 0.0626,  0.0713,  0.0723],
          [ 0.0429,  0.0469,  0.0559],
          [ 0.0294,  0.0307,  0.0446]]],


        [[[-0.0024, -0.0070, -0.0012],
          [ 0.0003, -0.0058,  0.0004],
          [-0.0022, -0.0095, -0.0062]],

         [[ 0.0023, -0.0072, -0.0049],
          [ 0.0023, -0.0060, -0.0005],
          [-0.0008, -0.0069, -0.0032]],

         [[ 0.0158,  0.0035,  0.0045],
          [ 0.0158,  0.0047,  0.0078],
          [ 0.0137,  0.0053,  0.0075]]],


        [[[ 0.0007,  0.0001,  0.0022],
          [-0.0004, -0.0010,  0.0004],
          [ 0.0003, -0.0003,  0.0003]],

         [[-0.0008, -0.0012,  0.0013],
          [-0.0026, -0.0030, -0.0015],
          [-0.0020, -0.0023, -0.0016]],

         [[-0.0024, -0.0028, -0.0009],
          [-0.0043, -0.0049, -0.0036],
          [-0.0044, -0.0048, -0.0040]]],


        ...,


        [[[-0.0084, -0.0072, -0.0070],
          [-0.0011,  0.0027,  0.0019],
          [-0.0095,  0.0032, -0.0007]],

         [[ 0.0015,  0.0004, -0.0022],
          [ 0.0072,  0.0106,  0.0078],
          [-0.0049,  0.0070,  0.0025]],

         [[ 0.0261,  0.0204,  0.0132],
          [ 0.0353,  0.0339,  0.0277],
          [ 0.0270,  0.0337,  0.0268]]],


        [[[ 0.0213,  0.0137, -0.0042],
          [ 0.0147,  0.0102, -0.0053],
          [ 0.0118,  0.0079, -0.0006]],

         [[ 0.0272,  0.0190,  0.0023],
          [ 0.0180,  0.0133, -0.0010],
          [ 0.0140,  0.0095,  0.0025]],

         [[ 0.0230,  0.0148,  0.0021],
          [ 0.0124,  0.0075, -0.0024],
          [ 0.0069,  0.0028, -0.0010]]],


        [[[-0.0491, -0.0899, -0.0909],
          [-0.0753, -0.1147, -0.1117],
          [-0.0897, -0.1291, -0.1223]],

         [[-0.0365, -0.0819, -0.0859],
          [-0.0635, -0.1007, -0.0978],
          [-0.0741, -0.1088, -0.1024]],

         [[ 0.0185, -0.0182, -0.0188],
          [-0.0039, -0.0337, -0.0241],
          [-0.0102, -0.0323, -0.0220]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.8592]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.4320) |  Loss2: (0.2349) | Acc: (92.00%) (119/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.4261) |  Loss2: (0.2349) | Acc: (93.00%) (1322/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.4559) |  Loss2: (0.2348) | Acc: (92.00%) (2490/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.4673) |  Loss2: (0.2348) | Acc: (92.00%) (3655/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.4840) |  Loss2: (0.2348) | Acc: (91.00%) (4811/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.4860) |  Loss2: (0.2347) | Acc: (91.00%) (5970/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.4886) |  Loss2: (0.2347) | Acc: (91.00%) (7128/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.4879) |  Loss2: (0.2347) | Acc: (91.00%) (8298/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.4880) |  Loss2: (0.2346) | Acc: (91.00%) (9459/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.4859) |  Loss2: (0.2346) | Acc: (91.00%) (10632/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.4874) |  Loss2: (0.2346) | Acc: (91.00%) (11785/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.4847) |  Loss2: (0.2345) | Acc: (91.00%) (12967/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.4835) |  Loss2: (0.2345) | Acc: (91.00%) (14151/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.4836) |  Loss2: (0.2344) | Acc: (91.00%) (15327/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.4853) |  Loss2: (0.2344) | Acc: (91.00%) (16489/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.4857) |  Loss2: (0.2343) | Acc: (91.00%) (17650/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.4845) |  Loss2: (0.2343) | Acc: (91.00%) (18817/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.4822) |  Loss2: (0.2342) | Acc: (91.00%) (20007/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.4816) |  Loss2: (0.2342) | Acc: (91.00%) (21178/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.4805) |  Loss2: (0.2341) | Acc: (91.00%) (22363/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.4805) |  Loss2: (0.2341) | Acc: (91.00%) (23528/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.4795) |  Loss2: (0.2340) | Acc: (91.00%) (24715/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.4788) |  Loss2: (0.2340) | Acc: (91.00%) (25887/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.4780) |  Loss2: (0.2339) | Acc: (91.00%) (27061/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.4764) |  Loss2: (0.2339) | Acc: (91.00%) (28255/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.4761) |  Loss2: (0.2338) | Acc: (91.00%) (29431/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.4749) |  Loss2: (0.2338) | Acc: (91.00%) (30613/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.4733) |  Loss2: (0.2337) | Acc: (91.00%) (31802/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.4729) |  Loss2: (0.2337) | Acc: (91.00%) (32984/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.4730) |  Loss2: (0.2336) | Acc: (91.00%) (34148/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.4728) |  Loss2: (0.2336) | Acc: (91.00%) (35318/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.4717) |  Loss2: (0.2336) | Acc: (91.00%) (36509/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.4716) |  Loss2: (0.2335) | Acc: (91.00%) (37684/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.4711) |  Loss2: (0.2335) | Acc: (91.00%) (38865/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.4710) |  Loss2: (0.2334) | Acc: (91.00%) (40038/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.4702) |  Loss2: (0.2334) | Acc: (91.00%) (41230/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.4687) |  Loss2: (0.2333) | Acc: (91.00%) (42425/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.4676) |  Loss2: (0.2333) | Acc: (91.00%) (43619/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.4671) |  Loss2: (0.2332) | Acc: (91.00%) (44808/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.4663) |  Loss2: (0.2332) | Acc: (91.00%) (45959/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_115.pth.tar'
# TEST : Loss: (0.4306) | Acc: (86.00%) (8653/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1311,  0.0679,  0.0963],
          [ 0.1113, -0.3062, -0.1122],
          [ 0.2637, -0.0791,  0.0101]],

         [[-0.0085, -0.0898,  0.2641],
          [ 0.0191, -0.1128,  0.0417],
          [ 0.0666,  0.0063, -0.0699]],

         [[ 0.0139, -0.0381,  0.2016],
          [ 0.0895, -0.2043,  0.1585],
          [-0.1162,  0.0185, -0.1919]]],


        [[[ 0.0975, -0.0750, -0.0035],
          [ 0.1384,  0.1751,  0.1817],
          [ 0.0113, -0.0639,  0.1140]],

         [[-0.1531, -0.1788,  0.1541],
          [ 0.0340, -0.0479,  0.1025],
          [ 0.1224, -0.2031,  0.1290]],

         [[ 0.1009, -0.0489, -0.0132],
          [ 0.0128,  0.0210, -0.1180],
          [-0.0297, -0.1598, -0.0200]]],


        [[[-0.0718,  0.0884, -0.0430],
          [-0.1248,  0.0647,  0.1156],
          [ 0.1101,  0.1654,  0.0012]],

         [[ 0.0992,  0.1319,  0.1124],
          [ 0.0690, -0.0382, -0.0356],
          [-0.0466, -0.0297,  0.1380]],

         [[-0.0448,  0.1435,  0.0731],
          [-0.0420, -0.0456, -0.0606],
          [-0.0393, -0.0083,  0.0413]]],


        ...,


        [[[ 0.2267, -0.1156, -0.1218],
          [-0.0529, -0.1175, -0.0966],
          [-0.0955,  0.0224,  0.0698]],

         [[ 0.1669,  0.1933, -0.1160],
          [ 0.2018, -0.1418, -0.1126],
          [-0.0607, -0.0756, -0.0247]],

         [[ 0.1818,  0.1026, -0.1023],
          [-0.0695, -0.1554, -0.0735],
          [ 0.0689,  0.1794, -0.1263]]],


        [[[-0.1046,  0.0869, -0.1452],
          [-0.1422, -0.1470,  0.1169],
          [-0.1477,  0.1528, -0.1345]],

         [[ 0.0955, -0.1159,  0.0874],
          [ 0.1485,  0.0018, -0.0831],
          [-0.1398,  0.1175, -0.1950]],

         [[ 0.1262, -0.0184,  0.0041],
          [ 0.1410, -0.1747,  0.0135],
          [-0.0214,  0.0192, -0.1204]]],


        [[[ 0.0766,  0.1690,  0.1135],
          [-0.0832, -0.1885,  0.2192],
          [-0.1722, -0.0216, -0.0659]],

         [[-0.0180, -0.0721,  0.1497],
          [ 0.0051, -0.2164,  0.0587],
          [-0.0027,  0.0799,  0.0311]],

         [[-0.1658, -0.1987,  0.1191],
          [ 0.1489, -0.1963, -0.0097],
          [ 0.0138, -0.0109,  0.2990]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3105e-05,  6.7946e-06,  9.6269e-06],
          [ 1.1131e-05, -3.0623e-05, -1.1220e-05],
          [ 2.6369e-05, -7.9128e-06,  1.0139e-06]],

         [[-8.4533e-07, -8.9797e-06,  2.6413e-05],
          [ 1.9078e-06, -1.1284e-05,  4.1709e-06],
          [ 6.6607e-06,  6.2955e-07, -6.9890e-06]],

         [[ 1.3938e-06, -3.8079e-06,  2.0160e-05],
          [ 8.9489e-06, -2.0430e-05,  1.5849e-05],
          [-1.1617e-05,  1.8464e-06, -1.9187e-05]]],


        [[[ 9.7506e-06, -7.5037e-06, -3.5182e-07],
          [ 1.3840e-05,  1.7508e-05,  1.8171e-05],
          [ 1.1322e-06, -6.3918e-06,  1.1400e-05]],

         [[-1.5310e-05, -1.7879e-05,  1.5412e-05],
          [ 3.4006e-06, -4.7946e-06,  1.0251e-05],
          [ 1.2235e-05, -2.0314e-05,  1.2895e-05]],

         [[ 1.0088e-05, -4.8878e-06, -1.3184e-06],
          [ 1.2821e-06,  2.1013e-06, -1.1800e-05],
          [-2.9654e-06, -1.5983e-05, -1.9988e-06]]],


        [[[-7.1788e-06,  8.8381e-06, -4.2971e-06],
          [-1.2482e-05,  6.4674e-06,  1.1564e-05],
          [ 1.1008e-05,  1.6536e-05,  1.2207e-07]],

         [[ 9.9168e-06,  1.3188e-05,  1.1238e-05],
          [ 6.8982e-06, -3.8195e-06, -3.5595e-06],
          [-4.6647e-06, -2.9720e-06,  1.3803e-05]],

         [[-4.4756e-06,  1.4347e-05,  7.3122e-06],
          [-4.1998e-06, -4.5607e-06, -6.0577e-06],
          [-3.9325e-06, -8.2831e-07,  4.1311e-06]]],


        ...,


        [[[ 2.2670e-05, -1.1556e-05, -1.2182e-05],
          [-5.2926e-06, -1.1755e-05, -9.6565e-06],
          [-9.5550e-06,  2.2404e-06,  6.9767e-06]],

         [[ 1.6687e-05,  1.9327e-05, -1.1597e-05],
          [ 2.0183e-05, -1.4183e-05, -1.1263e-05],
          [-6.0682e-06, -7.5579e-06, -2.4719e-06]],

         [[ 1.8181e-05,  1.0255e-05, -1.0232e-05],
          [-6.9489e-06, -1.5543e-05, -7.3495e-06],
          [ 6.8889e-06,  1.7936e-05, -1.2628e-05]]],


        [[[-1.0460e-05,  8.6854e-06, -1.4523e-05],
          [-1.4221e-05, -1.4699e-05,  1.1687e-05],
          [-1.4774e-05,  1.5282e-05, -1.3446e-05]],

         [[ 9.5524e-06, -1.1586e-05,  8.7439e-06],
          [ 1.4850e-05,  1.8017e-07, -8.3053e-06],
          [-1.3979e-05,  1.1752e-05, -1.9502e-05]],

         [[ 1.2624e-05, -1.8435e-06,  4.0587e-07],
          [ 1.4097e-05, -1.7466e-05,  1.3531e-06],
          [-2.1387e-06,  1.9239e-06, -1.2035e-05]]],


        [[[ 7.6596e-06,  1.6895e-05,  1.1345e-05],
          [-8.3176e-06, -1.8854e-05,  2.1915e-05],
          [-1.7225e-05, -2.1618e-06, -6.5896e-06]],

         [[-1.8028e-06, -7.2136e-06,  1.4969e-05],
          [ 5.1060e-07, -2.1642e-05,  5.8654e-06],
          [-2.7242e-07,  7.9897e-06,  3.1069e-06]],

         [[-1.6583e-05, -1.9870e-05,  1.1914e-05],
          [ 1.4891e-05, -1.9626e-05, -9.7313e-07],
          [ 1.3848e-06, -1.0917e-06,  2.9900e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.9296]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0232]], device='cuda:0')

Epoch: 116 | Batch_idx: 0 |  Loss: (0.4575) |  Loss2: (0.2312) | Acc: (93.00%) (120/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.4158) |  Loss2: (0.2311) | Acc: (93.00%) (1320/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.4279) |  Loss2: (0.2311) | Acc: (92.00%) (2494/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.4276) |  Loss2: (0.2310) | Acc: (92.00%) (3674/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.4342) |  Loss2: (0.2310) | Acc: (92.00%) (4849/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.4311) |  Loss2: (0.2309) | Acc: (92.00%) (6042/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.4329) |  Loss2: (0.2309) | Acc: (92.00%) (7237/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.4331) |  Loss2: (0.2308) | Acc: (92.00%) (8425/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.4342) |  Loss2: (0.2308) | Acc: (92.00%) (9601/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.4341) |  Loss2: (0.2307) | Acc: (92.00%) (10791/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.4333) |  Loss2: (0.2307) | Acc: (92.00%) (11976/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.4365) |  Loss2: (0.2306) | Acc: (92.00%) (13157/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.4352) |  Loss2: (0.2306) | Acc: (92.00%) (14347/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.4355) |  Loss2: (0.2306) | Acc: (92.00%) (15541/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.4359) |  Loss2: (0.2305) | Acc: (92.00%) (16724/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.4379) |  Loss2: (0.2305) | Acc: (92.00%) (17893/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.4366) |  Loss2: (0.2304) | Acc: (92.00%) (19096/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.4365) |  Loss2: (0.2304) | Acc: (92.00%) (20286/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.4366) |  Loss2: (0.2304) | Acc: (92.00%) (21480/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.4363) |  Loss2: (0.2303) | Acc: (92.00%) (22669/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.4362) |  Loss2: (0.2303) | Acc: (92.00%) (23854/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.4364) |  Loss2: (0.2302) | Acc: (92.00%) (25036/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.4366) |  Loss2: (0.2302) | Acc: (92.00%) (26221/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.4372) |  Loss2: (0.2301) | Acc: (92.00%) (27405/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.4369) |  Loss2: (0.2301) | Acc: (92.00%) (28599/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.4360) |  Loss2: (0.2300) | Acc: (92.00%) (29801/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.4352) |  Loss2: (0.2300) | Acc: (92.00%) (30992/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.4349) |  Loss2: (0.2299) | Acc: (92.00%) (32180/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.4346) |  Loss2: (0.2299) | Acc: (92.00%) (33369/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.4346) |  Loss2: (0.2298) | Acc: (92.00%) (34554/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.4347) |  Loss2: (0.2298) | Acc: (92.00%) (35742/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.4348) |  Loss2: (0.2297) | Acc: (92.00%) (36922/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.4342) |  Loss2: (0.2297) | Acc: (92.00%) (38119/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.4341) |  Loss2: (0.2296) | Acc: (92.00%) (39312/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.4341) |  Loss2: (0.2296) | Acc: (92.00%) (40500/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.4341) |  Loss2: (0.2295) | Acc: (92.00%) (41696/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.4349) |  Loss2: (0.2295) | Acc: (92.00%) (42865/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.4347) |  Loss2: (0.2294) | Acc: (92.00%) (44045/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.4347) |  Loss2: (0.2294) | Acc: (92.00%) (45231/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.4349) |  Loss2: (0.2293) | Acc: (92.00%) (46374/50000)
# TEST : Loss: (0.4109) | Acc: (87.00%) (8701/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1310,  0.0679,  0.0962],
          [ 0.1113, -0.3061, -0.1122],
          [ 0.2636, -0.0791,  0.0101]],

         [[-0.0084, -0.0898,  0.2640],
          [ 0.0191, -0.1128,  0.0417],
          [ 0.0666,  0.0063, -0.0699]],

         [[ 0.0139, -0.0381,  0.2015],
          [ 0.0895, -0.2042,  0.1584],
          [-0.1161,  0.0185, -0.1918]]],


        [[[ 0.0975, -0.0750, -0.0035],
          [ 0.1383,  0.1750,  0.1816],
          [ 0.0113, -0.0639,  0.1140]],

         [[-0.1530, -0.1787,  0.1541],
          [ 0.0340, -0.0479,  0.1025],
          [ 0.1223, -0.2031,  0.1289]],

         [[ 0.1008, -0.0489, -0.0132],
          [ 0.0128,  0.0210, -0.1180],
          [-0.0296, -0.1598, -0.0200]]],


        [[[-0.0718,  0.0883, -0.0430],
          [-0.1248,  0.0646,  0.1156],
          [ 0.1100,  0.1653,  0.0012]],

         [[ 0.0991,  0.1318,  0.1123],
          [ 0.0690, -0.0382, -0.0356],
          [-0.0466, -0.0297,  0.1380]],

         [[-0.0447,  0.1434,  0.0731],
          [-0.0420, -0.0456, -0.0606],
          [-0.0393, -0.0083,  0.0413]]],


        ...,


        [[[ 0.2266, -0.1155, -0.1218],
          [-0.0529, -0.1175, -0.0965],
          [-0.0955,  0.0224,  0.0697]],

         [[ 0.1668,  0.1932, -0.1159],
          [ 0.2017, -0.1418, -0.1126],
          [-0.0607, -0.0755, -0.0247]],

         [[ 0.1817,  0.1025, -0.1023],
          [-0.0695, -0.1554, -0.0735],
          [ 0.0689,  0.1793, -0.1262]]],


        [[[-0.1046,  0.0868, -0.1452],
          [-0.1422, -0.1469,  0.1168],
          [-0.1477,  0.1528, -0.1344]],

         [[ 0.0955, -0.1158,  0.0874],
          [ 0.1484,  0.0018, -0.0830],
          [-0.1397,  0.1175, -0.1949]],

         [[ 0.1262, -0.0184,  0.0041],
          [ 0.1409, -0.1746,  0.0135],
          [-0.0214,  0.0192, -0.1203]]],


        [[[ 0.0766,  0.1689,  0.1134],
          [-0.0831, -0.1885,  0.2191],
          [-0.1722, -0.0216, -0.0659]],

         [[-0.0180, -0.0721,  0.1496],
          [ 0.0051, -0.2163,  0.0586],
          [-0.0027,  0.0799,  0.0311]],

         [[-0.1658, -0.1986,  0.1191],
          [ 0.1488, -0.1962, -0.0097],
          [ 0.0138, -0.0109,  0.2989]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3100e-05,  6.7920e-06,  9.6231e-06],
          [ 1.1126e-05, -3.0612e-05, -1.1216e-05],
          [ 2.6359e-05, -7.9096e-06,  1.0135e-06]],

         [[-8.4500e-07, -8.9762e-06,  2.6403e-05],
          [ 1.9070e-06, -1.1280e-05,  4.1693e-06],
          [ 6.6581e-06,  6.2929e-07, -6.9864e-06]],

         [[ 1.3933e-06, -3.8064e-06,  2.0152e-05],
          [ 8.9454e-06, -2.0422e-05,  1.5842e-05],
          [-1.1612e-05,  1.8457e-06, -1.9180e-05]]],


        [[[ 9.7469e-06, -7.5007e-06, -3.5169e-07],
          [ 1.3835e-05,  1.7501e-05,  1.8164e-05],
          [ 1.1318e-06, -6.3891e-06,  1.1395e-05]],

         [[-1.5304e-05, -1.7872e-05,  1.5406e-05],
          [ 3.3993e-06, -4.7927e-06,  1.0247e-05],
          [ 1.2231e-05, -2.0306e-05,  1.2890e-05]],

         [[ 1.0084e-05, -4.8859e-06, -1.3178e-06],
          [ 1.2816e-06,  2.1005e-06, -1.1795e-05],
          [-2.9642e-06, -1.5977e-05, -1.9980e-06]]],


        [[[-7.1759e-06,  8.8346e-06, -4.2953e-06],
          [-1.2477e-05,  6.4648e-06,  1.1559e-05],
          [ 1.1004e-05,  1.6530e-05,  1.2202e-07]],

         [[ 9.9130e-06,  1.3183e-05,  1.1234e-05],
          [ 6.8956e-06, -3.8180e-06, -3.5580e-06],
          [-4.6628e-06, -2.9709e-06,  1.3797e-05]],

         [[-4.4738e-06,  1.4341e-05,  7.3093e-06],
          [-4.1982e-06, -4.5590e-06, -6.0553e-06],
          [-3.9309e-06, -8.2799e-07,  4.1295e-06]]],


        ...,


        [[[ 2.2662e-05, -1.1551e-05, -1.2177e-05],
          [-5.2906e-06, -1.1750e-05, -9.6527e-06],
          [-9.5512e-06,  2.2395e-06,  6.9741e-06]],

         [[ 1.6680e-05,  1.9319e-05, -1.1592e-05],
          [ 2.0175e-05, -1.4177e-05, -1.1258e-05],
          [-6.0659e-06, -7.5550e-06, -2.4710e-06]],

         [[ 1.8174e-05,  1.0251e-05, -1.0228e-05],
          [-6.9463e-06, -1.5537e-05, -7.3466e-06],
          [ 6.8863e-06,  1.7929e-05, -1.2624e-05]]],


        [[[-1.0456e-05,  8.6819e-06, -1.4517e-05],
          [-1.4215e-05, -1.4693e-05,  1.1683e-05],
          [-1.4768e-05,  1.5276e-05, -1.3441e-05]],

         [[ 9.5486e-06, -1.1582e-05,  8.7404e-06],
          [ 1.4844e-05,  1.8011e-07, -8.3021e-06],
          [-1.3974e-05,  1.1747e-05, -1.9495e-05]],

         [[ 1.2619e-05, -1.8428e-06,  4.0570e-07],
          [ 1.4092e-05, -1.7459e-05,  1.3526e-06],
          [-2.1379e-06,  1.9232e-06, -1.2031e-05]]],


        [[[ 7.6567e-06,  1.6889e-05,  1.1341e-05],
          [-8.3144e-06, -1.8846e-05,  2.1906e-05],
          [-1.7218e-05, -2.1609e-06, -6.5870e-06]],

         [[-1.8021e-06, -7.2107e-06,  1.4963e-05],
          [ 5.1040e-07, -2.1634e-05,  5.8631e-06],
          [-2.7231e-07,  7.9865e-06,  3.1056e-06]],

         [[-1.6577e-05, -1.9862e-05,  1.1909e-05],
          [ 1.4885e-05, -1.9618e-05, -9.7277e-07],
          [ 1.3842e-06, -1.0913e-06,  2.9888e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[0.9738]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0325]], device='cuda:0')

Epoch: 117 | Batch_idx: 0 |  Loss: (0.4399) |  Loss2: (0.2275) | Acc: (92.00%) (118/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.4120) |  Loss2: (0.2274) | Acc: (93.00%) (1320/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.4068) |  Loss2: (0.2274) | Acc: (93.00%) (2526/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.4119) |  Loss2: (0.2273) | Acc: (93.00%) (3721/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.4204) |  Loss2: (0.2273) | Acc: (93.00%) (4908/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.4199) |  Loss2: (0.2272) | Acc: (93.00%) (6103/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.4217) |  Loss2: (0.2272) | Acc: (93.00%) (7288/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.4195) |  Loss2: (0.2271) | Acc: (93.00%) (8482/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.4215) |  Loss2: (0.2271) | Acc: (93.00%) (9668/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.4237) |  Loss2: (0.2270) | Acc: (93.00%) (10854/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.4191) |  Loss2: (0.2270) | Acc: (93.00%) (12073/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.4209) |  Loss2: (0.2270) | Acc: (93.00%) (13267/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.4198) |  Loss2: (0.2269) | Acc: (93.00%) (14460/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.4206) |  Loss2: (0.2269) | Acc: (93.00%) (15645/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.4204) |  Loss2: (0.2268) | Acc: (93.00%) (16838/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.4202) |  Loss2: (0.2268) | Acc: (93.00%) (18041/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.4207) |  Loss2: (0.2267) | Acc: (93.00%) (19230/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.4213) |  Loss2: (0.2267) | Acc: (93.00%) (20414/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.4203) |  Loss2: (0.2267) | Acc: (93.00%) (21619/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.4209) |  Loss2: (0.2266) | Acc: (93.00%) (22810/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.4212) |  Loss2: (0.2266) | Acc: (93.00%) (23996/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.4204) |  Loss2: (0.2265) | Acc: (93.00%) (25194/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.4206) |  Loss2: (0.2265) | Acc: (93.00%) (26394/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.4218) |  Loss2: (0.2264) | Acc: (93.00%) (27576/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.4211) |  Loss2: (0.2264) | Acc: (93.00%) (28782/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.4200) |  Loss2: (0.2264) | Acc: (93.00%) (29990/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.4201) |  Loss2: (0.2263) | Acc: (93.00%) (31192/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.4198) |  Loss2: (0.2263) | Acc: (93.00%) (32381/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.4199) |  Loss2: (0.2262) | Acc: (93.00%) (33577/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.4201) |  Loss2: (0.2262) | Acc: (93.00%) (34774/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.4207) |  Loss2: (0.2262) | Acc: (93.00%) (35965/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.4209) |  Loss2: (0.2261) | Acc: (93.00%) (37158/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.4206) |  Loss2: (0.2261) | Acc: (93.00%) (38348/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.4203) |  Loss2: (0.2260) | Acc: (93.00%) (39545/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.4204) |  Loss2: (0.2260) | Acc: (93.00%) (40740/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.4201) |  Loss2: (0.2259) | Acc: (93.00%) (41940/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.4201) |  Loss2: (0.2259) | Acc: (93.00%) (43127/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.4201) |  Loss2: (0.2259) | Acc: (93.00%) (44317/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.4210) |  Loss2: (0.2258) | Acc: (93.00%) (45496/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.4201) |  Loss2: (0.2258) | Acc: (93.00%) (46661/50000)
# TEST : Loss: (0.4042) | Acc: (87.00%) (8712/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1309,  0.0679,  0.0962],
          [ 0.1112, -0.3060, -0.1121],
          [ 0.2635, -0.0791,  0.0101]],

         [[-0.0084, -0.0897,  0.2639],
          [ 0.0191, -0.1128,  0.0417],
          [ 0.0666,  0.0063, -0.0698]],

         [[ 0.0139, -0.0380,  0.2014],
          [ 0.0894, -0.2041,  0.1584],
          [-0.1161,  0.0184, -0.1917]]],


        [[[ 0.0974, -0.0750, -0.0035],
          [ 0.1383,  0.1749,  0.1816],
          [ 0.0113, -0.0639,  0.1139]],

         [[-0.1530, -0.1787,  0.1540],
          [ 0.0340, -0.0479,  0.1024],
          [ 0.1223, -0.2030,  0.1288]],

         [[ 0.1008, -0.0488, -0.0132],
          [ 0.0128,  0.0210, -0.1179],
          [-0.0296, -0.1597, -0.0200]]],


        [[[-0.0717,  0.0883, -0.0429],
          [-0.1247,  0.0646,  0.1155],
          [ 0.1100,  0.1652,  0.0012]],

         [[ 0.0991,  0.1318,  0.1123],
          [ 0.0689, -0.0382, -0.0356],
          [-0.0466, -0.0297,  0.1379]],

         [[-0.0447,  0.1434,  0.0731],
          [-0.0420, -0.0456, -0.0605],
          [-0.0393, -0.0083,  0.0413]]],


        ...,


        [[[ 0.2265, -0.1155, -0.1217],
          [-0.0529, -0.1175, -0.0965],
          [-0.0955,  0.0224,  0.0697]],

         [[ 0.1667,  0.1931, -0.1159],
          [ 0.2017, -0.1417, -0.1125],
          [-0.0606, -0.0755, -0.0247]],

         [[ 0.1817,  0.1025, -0.1022],
          [-0.0694, -0.1553, -0.0734],
          [ 0.0688,  0.1792, -0.1262]]],


        [[[-0.1045,  0.0868, -0.1451],
          [-0.1421, -0.1469,  0.1168],
          [-0.1476,  0.1527, -0.1344]],

         [[ 0.0954, -0.1158,  0.0874],
          [ 0.1484,  0.0018, -0.0830],
          [-0.1397,  0.1174, -0.1949]],

         [[ 0.1261, -0.0184,  0.0041],
          [ 0.1409, -0.1745,  0.0135],
          [-0.0214,  0.0192, -0.1203]]],


        [[[ 0.0765,  0.1688,  0.1134],
          [-0.0831, -0.1884,  0.2190],
          [-0.1721, -0.0216, -0.0658]],

         [[-0.0180, -0.0721,  0.1496],
          [ 0.0051, -0.2162,  0.0586],
          [-0.0027,  0.0798,  0.0310]],

         [[-0.1657, -0.1985,  0.1190],
          [ 0.1488, -0.1961, -0.0097],
          [ 0.0138, -0.0109,  0.2988]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3095e-05,  6.7893e-06,  9.6193e-06],
          [ 1.1122e-05, -3.0600e-05, -1.1211e-05],
          [ 2.6348e-05, -7.9064e-06,  1.0131e-06]],

         [[-8.4467e-07, -8.9727e-06,  2.6392e-05],
          [ 1.9063e-06, -1.1275e-05,  4.1677e-06],
          [ 6.6555e-06,  6.2904e-07, -6.9837e-06]],

         [[ 1.3927e-06, -3.8050e-06,  2.0144e-05],
          [ 8.9419e-06, -2.0413e-05,  1.5836e-05],
          [-1.1607e-05,  1.8449e-06, -1.9172e-05]]],


        [[[ 9.7431e-06, -7.4978e-06, -3.5155e-07],
          [ 1.3829e-05,  1.7494e-05,  1.8157e-05],
          [ 1.1314e-06, -6.3865e-06,  1.1391e-05]],

         [[-1.5298e-05, -1.7865e-05,  1.5400e-05],
          [ 3.3980e-06, -4.7908e-06,  1.0243e-05],
          [ 1.2226e-05, -2.0297e-05,  1.2885e-05]],

         [[ 1.0080e-05, -4.8840e-06, -1.3173e-06],
          [ 1.2811e-06,  2.0997e-06, -1.1791e-05],
          [-2.9630e-06, -1.5970e-05, -1.9972e-06]]],


        [[[-7.1730e-06,  8.8311e-06, -4.2936e-06],
          [-1.2472e-05,  6.4621e-06,  1.1555e-05],
          [ 1.0999e-05,  1.6524e-05,  1.2198e-07]],

         [[ 9.9092e-06,  1.3177e-05,  1.1229e-05],
          [ 6.8929e-06, -3.8166e-06, -3.5566e-06],
          [-4.6609e-06, -2.9697e-06,  1.3792e-05]],

         [[-4.4721e-06,  1.4335e-05,  7.3064e-06],
          [-4.1966e-06, -4.5573e-06, -6.0530e-06],
          [-3.9293e-06, -8.2766e-07,  4.1279e-06]]],


        ...,


        [[[ 2.2653e-05, -1.1547e-05, -1.2172e-05],
          [-5.2886e-06, -1.1746e-05, -9.6489e-06],
          [-9.5474e-06,  2.2387e-06,  6.9715e-06]],

         [[ 1.6674e-05,  1.9312e-05, -1.1588e-05],
          [ 2.0166e-05, -1.4171e-05, -1.1254e-05],
          [-6.0635e-06, -7.5520e-06, -2.4700e-06]],

         [[ 1.8167e-05,  1.0247e-05, -1.0224e-05],
          [-6.9437e-06, -1.5531e-05, -7.3437e-06],
          [ 6.8837e-06,  1.7922e-05, -1.2619e-05]]],


        [[[-1.0452e-05,  8.6784e-06, -1.4511e-05],
          [-1.4209e-05, -1.4688e-05,  1.1678e-05],
          [-1.4762e-05,  1.5271e-05, -1.3436e-05]],

         [[ 9.5448e-06, -1.1577e-05,  8.7369e-06],
          [ 1.4838e-05,  1.8004e-07, -8.2989e-06],
          [-1.3969e-05,  1.1743e-05, -1.9487e-05]],

         [[ 1.2614e-05, -1.8420e-06,  4.0554e-07],
          [ 1.4087e-05, -1.7452e-05,  1.3520e-06],
          [-2.1371e-06,  1.9225e-06, -1.2026e-05]]],


        [[[ 7.6537e-06,  1.6882e-05,  1.1337e-05],
          [-8.3112e-06, -1.8839e-05,  2.1898e-05],
          [-1.7211e-05, -2.1600e-06, -6.5844e-06]],

         [[-1.8014e-06, -7.2078e-06,  1.4957e-05],
          [ 5.1020e-07, -2.1625e-05,  5.8607e-06],
          [-2.7220e-07,  7.9833e-06,  3.1044e-06]],

         [[-1.6570e-05, -1.9854e-05,  1.1904e-05],
          [ 1.4879e-05, -1.9611e-05, -9.7240e-07],
          [ 1.3837e-06, -1.0908e-06,  2.9876e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0043]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0136]], device='cuda:0')

Epoch: 118 | Batch_idx: 0 |  Loss: (0.5419) |  Loss2: (0.2241) | Acc: (85.00%) (110/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.4084) |  Loss2: (0.2240) | Acc: (92.00%) (1309/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.4206) |  Loss2: (0.2240) | Acc: (92.00%) (2498/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.4121) |  Loss2: (0.2240) | Acc: (93.00%) (3701/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.4158) |  Loss2: (0.2239) | Acc: (93.00%) (4892/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.4174) |  Loss2: (0.2239) | Acc: (93.00%) (6089/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.4124) |  Loss2: (0.2238) | Acc: (93.00%) (7302/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.4123) |  Loss2: (0.2238) | Acc: (93.00%) (8509/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.4106) |  Loss2: (0.2238) | Acc: (93.00%) (9717/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.4127) |  Loss2: (0.2237) | Acc: (93.00%) (10895/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.4097) |  Loss2: (0.2237) | Acc: (93.00%) (12106/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.4097) |  Loss2: (0.2236) | Acc: (93.00%) (13306/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.4094) |  Loss2: (0.2236) | Acc: (93.00%) (14516/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.4105) |  Loss2: (0.2236) | Acc: (93.00%) (15705/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.4117) |  Loss2: (0.2235) | Acc: (93.00%) (16902/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.4112) |  Loss2: (0.2235) | Acc: (93.00%) (18105/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.4114) |  Loss2: (0.2234) | Acc: (93.00%) (19298/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.4110) |  Loss2: (0.2234) | Acc: (93.00%) (20495/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.4104) |  Loss2: (0.2234) | Acc: (93.00%) (21696/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.4100) |  Loss2: (0.2233) | Acc: (93.00%) (22906/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.4105) |  Loss2: (0.2233) | Acc: (93.00%) (24088/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.4110) |  Loss2: (0.2232) | Acc: (93.00%) (25275/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.4110) |  Loss2: (0.2232) | Acc: (93.00%) (26478/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.4119) |  Loss2: (0.2231) | Acc: (93.00%) (27660/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.4117) |  Loss2: (0.2231) | Acc: (93.00%) (28859/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.4116) |  Loss2: (0.2230) | Acc: (93.00%) (30045/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.4125) |  Loss2: (0.2230) | Acc: (93.00%) (31223/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.4125) |  Loss2: (0.2230) | Acc: (93.00%) (32417/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.4116) |  Loss2: (0.2229) | Acc: (93.00%) (33629/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.4109) |  Loss2: (0.2229) | Acc: (93.00%) (34836/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.4118) |  Loss2: (0.2228) | Acc: (93.00%) (36021/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.4113) |  Loss2: (0.2228) | Acc: (93.00%) (37227/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.4111) |  Loss2: (0.2227) | Acc: (93.00%) (38423/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.4104) |  Loss2: (0.2227) | Acc: (93.00%) (39628/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.4100) |  Loss2: (0.2226) | Acc: (93.00%) (40829/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.4100) |  Loss2: (0.2226) | Acc: (93.00%) (42030/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.4100) |  Loss2: (0.2226) | Acc: (93.00%) (43230/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.4101) |  Loss2: (0.2225) | Acc: (93.00%) (44417/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.4094) |  Loss2: (0.2225) | Acc: (93.00%) (45621/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.4095) |  Loss2: (0.2224) | Acc: (93.00%) (46782/50000)
# TEST : Loss: (0.4022) | Acc: (87.00%) (8718/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1309,  0.0679,  0.0962],
          [ 0.1112, -0.3059, -0.1121],
          [ 0.2634, -0.0790,  0.0101]],

         [[-0.0084, -0.0897,  0.2638],
          [ 0.0191, -0.1127,  0.0417],
          [ 0.0665,  0.0063, -0.0698]],

         [[ 0.0139, -0.0380,  0.2014],
          [ 0.0894, -0.2041,  0.1583],
          [-0.1160,  0.0184, -0.1916]]],


        [[[ 0.0974, -0.0749, -0.0035],
          [ 0.1382,  0.1749,  0.1815],
          [ 0.0113, -0.0638,  0.1139]],

         [[-0.1529, -0.1786,  0.1539],
          [ 0.0340, -0.0479,  0.1024],
          [ 0.1222, -0.2029,  0.1288]],

         [[ 0.1008, -0.0488, -0.0132],
          [ 0.0128,  0.0210, -0.1179],
          [-0.0296, -0.1596, -0.0200]]],


        [[[-0.0717,  0.0883, -0.0429],
          [-0.1247,  0.0646,  0.1155],
          [ 0.1099,  0.1652,  0.0012]],

         [[ 0.0991,  0.1317,  0.1123],
          [ 0.0689, -0.0382, -0.0356],
          [-0.0466, -0.0297,  0.1379]],

         [[-0.0447,  0.1433,  0.0730],
          [-0.0420, -0.0456, -0.0605],
          [-0.0393, -0.0083,  0.0413]]],


        ...,


        [[[ 0.2264, -0.1154, -0.1217],
          [-0.0529, -0.1174, -0.0965],
          [-0.0954,  0.0224,  0.0697]],

         [[ 0.1667,  0.1930, -0.1158],
          [ 0.2016, -0.1417, -0.1125],
          [-0.0606, -0.0755, -0.0247]],

         [[ 0.1816,  0.1024, -0.1022],
          [-0.0694, -0.1553, -0.0734],
          [ 0.0688,  0.1791, -0.1261]]],


        [[[-0.1045,  0.0867, -0.1451],
          [-0.1420, -0.1468,  0.1167],
          [-0.1476,  0.1526, -0.1343]],

         [[ 0.0954, -0.1157,  0.0873],
          [ 0.1483,  0.0018, -0.0830],
          [-0.1396,  0.1174, -0.1948]],

         [[ 0.1261, -0.0184,  0.0041],
          [ 0.1408, -0.1745,  0.0135],
          [-0.0214,  0.0192, -0.1202]]],


        [[[ 0.0765,  0.1688,  0.1133],
          [-0.0831, -0.1883,  0.2189],
          [-0.1720, -0.0216, -0.0658]],

         [[-0.0180, -0.0720,  0.1495],
          [ 0.0051, -0.2162,  0.0586],
          [-0.0027,  0.0798,  0.0310]],

         [[-0.1656, -0.1985,  0.1190],
          [ 0.1487, -0.1960, -0.0097],
          [ 0.0138, -0.0109,  0.2986]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3089e-05,  6.7867e-06,  9.6155e-06],
          [ 1.1118e-05, -3.0588e-05, -1.1207e-05],
          [ 2.6338e-05, -7.9032e-06,  1.0127e-06]],

         [[-8.4434e-07, -8.9692e-06,  2.6382e-05],
          [ 1.9056e-06, -1.1271e-05,  4.1661e-06],
          [ 6.6529e-06,  6.2878e-07, -6.9811e-06]],

         [[ 1.3922e-06, -3.8035e-06,  2.0136e-05],
          [ 8.9385e-06, -2.0405e-05,  1.5829e-05],
          [-1.1603e-05,  1.8442e-06, -1.9164e-05]]],


        [[[ 9.7393e-06, -7.4949e-06, -3.5141e-07],
          [ 1.3824e-05,  1.7487e-05,  1.8150e-05],
          [ 1.1309e-06, -6.3839e-06,  1.1387e-05]],

         [[-1.5292e-05, -1.7858e-05,  1.5394e-05],
          [ 3.3966e-06, -4.7889e-06,  1.0239e-05],
          [ 1.2221e-05, -2.0289e-05,  1.2880e-05]],

         [[ 1.0076e-05, -4.8821e-06, -1.3168e-06],
          [ 1.2806e-06,  2.0989e-06, -1.1786e-05],
          [-2.9619e-06, -1.5964e-05, -1.9964e-06]]],


        [[[-7.1700e-06,  8.8276e-06, -4.2918e-06],
          [-1.2467e-05,  6.4595e-06,  1.1550e-05],
          [ 1.0995e-05,  1.6517e-05,  1.2193e-07]],

         [[ 9.9054e-06,  1.3172e-05,  1.1225e-05],
          [ 6.8903e-06, -3.8151e-06, -3.5551e-06],
          [-4.6590e-06, -2.9686e-06,  1.3787e-05]],

         [[-4.4703e-06,  1.4330e-05,  7.3034e-06],
          [-4.1950e-06, -4.5555e-06, -6.0507e-06],
          [-3.9277e-06, -8.2733e-07,  4.1263e-06]]],


        ...,


        [[[ 2.2644e-05, -1.1542e-05, -1.2168e-05],
          [-5.2865e-06, -1.1741e-05, -9.6451e-06],
          [-9.5436e-06,  2.2378e-06,  6.9688e-06]],

         [[ 1.6667e-05,  1.9304e-05, -1.1583e-05],
          [ 2.0158e-05, -1.4165e-05, -1.1250e-05],
          [-6.0612e-06, -7.5491e-06, -2.4691e-06]],

         [[ 1.8160e-05,  1.0243e-05, -1.0220e-05],
          [-6.9411e-06, -1.5525e-05, -7.3408e-06],
          [ 6.8810e-06,  1.7915e-05, -1.2614e-05]]],


        [[[-1.0448e-05,  8.6749e-06, -1.4505e-05],
          [-1.4204e-05, -1.4682e-05,  1.1673e-05],
          [-1.4756e-05,  1.5265e-05, -1.3430e-05]],

         [[ 9.5410e-06, -1.1572e-05,  8.7334e-06],
          [ 1.4832e-05,  1.7997e-07, -8.2957e-06],
          [-1.3963e-05,  1.1738e-05, -1.9479e-05]],

         [[ 1.2610e-05, -1.8413e-06,  4.0537e-07],
          [ 1.4081e-05, -1.7445e-05,  1.3515e-06],
          [-2.1363e-06,  1.9217e-06, -1.2021e-05]]],


        [[[ 7.6508e-06,  1.6876e-05,  1.1332e-05],
          [-8.3080e-06, -1.8831e-05,  2.1889e-05],
          [-1.7204e-05, -2.1591e-06, -6.5817e-06]],

         [[-1.8006e-06, -7.2049e-06,  1.4951e-05],
          [ 5.1000e-07, -2.1616e-05,  5.8584e-06],
          [-2.7209e-07,  7.9801e-06,  3.1032e-06]],

         [[-1.6564e-05, -1.9847e-05,  1.1900e-05],
          [ 1.4873e-05, -1.9603e-05, -9.7204e-07],
          [ 1.3832e-06, -1.0904e-06,  2.9865e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0269]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0458]], device='cuda:0')

Epoch: 119 | Batch_idx: 0 |  Loss: (0.3570) |  Loss2: (0.2208) | Acc: (96.00%) (123/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.3981) |  Loss2: (0.2208) | Acc: (93.00%) (1323/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.3967) |  Loss2: (0.2207) | Acc: (93.00%) (2524/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.3999) |  Loss2: (0.2207) | Acc: (93.00%) (3723/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.4071) |  Loss2: (0.2206) | Acc: (93.00%) (4906/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.4158) |  Loss2: (0.2206) | Acc: (93.00%) (6084/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.4135) |  Loss2: (0.2206) | Acc: (93.00%) (7282/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.4146) |  Loss2: (0.2205) | Acc: (93.00%) (8471/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.4138) |  Loss2: (0.2205) | Acc: (93.00%) (9661/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.4124) |  Loss2: (0.2205) | Acc: (93.00%) (10863/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.4124) |  Loss2: (0.2205) | Acc: (93.00%) (12053/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.4118) |  Loss2: (0.2204) | Acc: (93.00%) (13256/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.4098) |  Loss2: (0.2204) | Acc: (93.00%) (14461/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.4064) |  Loss2: (0.2204) | Acc: (93.00%) (15672/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.4070) |  Loss2: (0.2203) | Acc: (93.00%) (16866/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.4075) |  Loss2: (0.2203) | Acc: (93.00%) (18051/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.4060) |  Loss2: (0.2203) | Acc: (93.00%) (19258/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.4066) |  Loss2: (0.2202) | Acc: (93.00%) (20454/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.4073) |  Loss2: (0.2202) | Acc: (93.00%) (21642/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.4050) |  Loss2: (0.2202) | Acc: (93.00%) (22868/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.4058) |  Loss2: (0.2201) | Acc: (93.00%) (24057/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.4059) |  Loss2: (0.2201) | Acc: (93.00%) (25257/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.4059) |  Loss2: (0.2200) | Acc: (93.00%) (26465/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.4058) |  Loss2: (0.2200) | Acc: (93.00%) (27669/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.4064) |  Loss2: (0.2200) | Acc: (93.00%) (28859/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.4078) |  Loss2: (0.2199) | Acc: (93.00%) (30042/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.4080) |  Loss2: (0.2199) | Acc: (93.00%) (31231/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.4076) |  Loss2: (0.2198) | Acc: (93.00%) (32428/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.4071) |  Loss2: (0.2198) | Acc: (93.00%) (33637/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.4071) |  Loss2: (0.2198) | Acc: (93.00%) (34829/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.4072) |  Loss2: (0.2197) | Acc: (93.00%) (36017/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.4073) |  Loss2: (0.2197) | Acc: (93.00%) (37208/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.4078) |  Loss2: (0.2197) | Acc: (93.00%) (38394/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.4078) |  Loss2: (0.2196) | Acc: (93.00%) (39586/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.4074) |  Loss2: (0.2196) | Acc: (93.00%) (40792/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.4065) |  Loss2: (0.2195) | Acc: (93.00%) (42001/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.4074) |  Loss2: (0.2195) | Acc: (93.00%) (43181/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.4070) |  Loss2: (0.2195) | Acc: (93.00%) (44389/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.4067) |  Loss2: (0.2194) | Acc: (93.00%) (45589/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.4069) |  Loss2: (0.2194) | Acc: (93.00%) (46740/50000)
# TEST : Loss: (0.3969) | Acc: (87.00%) (8729/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1308,  0.0678,  0.0961],
          [ 0.1111, -0.3058, -0.1120],
          [ 0.2633, -0.0790,  0.0101]],

         [[-0.0084, -0.0897,  0.2637],
          [ 0.0190, -0.1127,  0.0416],
          [ 0.0665,  0.0063, -0.0698]],

         [[ 0.0139, -0.0380,  0.2013],
          [ 0.0893, -0.2040,  0.1582],
          [-0.1160,  0.0184, -0.1916]]],


        [[[ 0.0974, -0.0749, -0.0035],
          [ 0.1382,  0.1748,  0.1814],
          [ 0.0113, -0.0638,  0.1138]],

         [[-0.1529, -0.1785,  0.1539],
          [ 0.0340, -0.0479,  0.1024],
          [ 0.1222, -0.2028,  0.1287]],

         [[ 0.1007, -0.0488, -0.0132],
          [ 0.0128,  0.0210, -0.1178],
          [-0.0296, -0.1596, -0.0200]]],


        [[[-0.0717,  0.0882, -0.0429],
          [-0.1246,  0.0646,  0.1155],
          [ 0.1099,  0.1651,  0.0012]],

         [[ 0.0990,  0.1317,  0.1122],
          [ 0.0689, -0.0381, -0.0355],
          [-0.0466, -0.0297,  0.1378]],

         [[-0.0447,  0.1432,  0.0730],
          [-0.0419, -0.0455, -0.0605],
          [-0.0393, -0.0083,  0.0412]]],


        ...,


        [[[ 0.2264, -0.1154, -0.1216],
          [-0.0528, -0.1174, -0.0964],
          [-0.0954,  0.0224,  0.0697]],

         [[ 0.1666,  0.1930, -0.1158],
          [ 0.2015, -0.1416, -0.1125],
          [-0.0606, -0.0755, -0.0247]],

         [[ 0.1815,  0.1024, -0.1022],
          [-0.0694, -0.1552, -0.0734],
          [ 0.0688,  0.1791, -0.1261]]],


        [[[-0.1044,  0.0867, -0.1450],
          [-0.1420, -0.1468,  0.1167],
          [-0.1475,  0.1526, -0.1343]],

         [[ 0.0954, -0.1157,  0.0873],
          [ 0.1483,  0.0018, -0.0829],
          [-0.1396,  0.1173, -0.1947]],

         [[ 0.1261, -0.0184,  0.0041],
          [ 0.1408, -0.1744,  0.0135],
          [-0.0214,  0.0192, -0.1202]]],


        [[[ 0.0765,  0.1687,  0.1133],
          [-0.0830, -0.1882,  0.2188],
          [-0.1720, -0.0216, -0.0658]],

         [[-0.0180, -0.0720,  0.1495],
          [ 0.0051, -0.2161,  0.0586],
          [-0.0027,  0.0798,  0.0310]],

         [[-0.1656, -0.1984,  0.1189],
          [ 0.1487, -0.1960, -0.0097],
          [ 0.0138, -0.0109,  0.2985]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.3084e-05,  6.7841e-06,  9.6117e-06],
          [ 1.1113e-05, -3.0577e-05, -1.1203e-05],
          [ 2.6327e-05, -7.9000e-06,  1.0123e-06]],

         [[-8.4401e-07, -8.9657e-06,  2.6371e-05],
          [ 1.9049e-06, -1.1266e-05,  4.1645e-06],
          [ 6.6502e-06,  6.2853e-07, -6.9785e-06]],

         [[ 1.3916e-06, -3.8020e-06,  2.0128e-05],
          [ 8.9350e-06, -2.0397e-05,  1.5823e-05],
          [-1.1598e-05,  1.8435e-06, -1.9157e-05]]],


        [[[ 9.7355e-06, -7.4920e-06, -3.5128e-07],
          [ 1.3819e-05,  1.7480e-05,  1.8143e-05],
          [ 1.1305e-06, -6.3813e-06,  1.1382e-05]],

         [[-1.5286e-05, -1.7851e-05,  1.5389e-05],
          [ 3.3953e-06, -4.7870e-06,  1.0235e-05],
          [ 1.2217e-05, -2.0281e-05,  1.2874e-05]],

         [[ 1.0072e-05, -4.8802e-06, -1.3163e-06],
          [ 1.2801e-06,  2.0981e-06, -1.1781e-05],
          [-2.9607e-06, -1.5958e-05, -1.9956e-06]]],


        [[[-7.1671e-06,  8.8241e-06, -4.2901e-06],
          [-1.2462e-05,  6.4569e-06,  1.1546e-05],
          [ 1.0990e-05,  1.6511e-05,  1.2189e-07]],

         [[ 9.9017e-06,  1.3167e-05,  1.1221e-05],
          [ 6.8877e-06, -3.8137e-06, -3.5537e-06],
          [-4.6571e-06, -2.9674e-06,  1.3782e-05]],

         [[-4.4686e-06,  1.4324e-05,  7.3005e-06],
          [-4.1934e-06, -4.5538e-06, -6.0483e-06],
          [-3.9261e-06, -8.2700e-07,  4.1247e-06]]],


        ...,


        [[[ 2.2635e-05, -1.1538e-05, -1.2163e-05],
          [-5.2845e-06, -1.1736e-05, -9.6413e-06],
          [-9.5398e-06,  2.2369e-06,  6.9662e-06]],

         [[ 1.6661e-05,  1.9297e-05, -1.1578e-05],
          [ 2.0150e-05, -1.4159e-05, -1.1245e-05],
          [-6.0589e-06, -7.5462e-06, -2.4681e-06]],

         [[ 1.8153e-05,  1.0239e-05, -1.0216e-05],
          [-6.9384e-06, -1.5519e-05, -7.3379e-06],
          [ 6.8784e-06,  1.7908e-05, -1.2610e-05]]],


        [[[-1.0444e-05,  8.6714e-06, -1.4499e-05],
          [-1.4198e-05, -1.4676e-05,  1.1669e-05],
          [-1.4750e-05,  1.5259e-05, -1.3425e-05]],

         [[ 9.5372e-06, -1.1568e-05,  8.7299e-06],
          [ 1.4827e-05,  1.7990e-07, -8.2925e-06],
          [-1.3958e-05,  1.1733e-05, -1.9472e-05]],

         [[ 1.2605e-05, -1.8406e-06,  4.0521e-07],
          [ 1.4076e-05, -1.7438e-05,  1.3509e-06],
          [-2.1355e-06,  1.9210e-06, -1.2017e-05]]],


        [[[ 7.6479e-06,  1.6870e-05,  1.1328e-05],
          [-8.3048e-06, -1.8823e-05,  2.1880e-05],
          [-1.7197e-05, -2.1583e-06, -6.5791e-06]],

         [[-1.7999e-06, -7.2020e-06,  1.4946e-05],
          [ 5.0980e-07, -2.1607e-05,  5.8561e-06],
          [-2.7198e-07,  7.9769e-06,  3.1019e-06]],

         [[-1.6557e-05, -1.9839e-05,  1.1895e-05],
          [ 1.4867e-05, -1.9596e-05, -9.7168e-07],
          [ 1.3826e-06, -1.0899e-06,  2.9853e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0535]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0683]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (2522/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (4919/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (7303/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (8496/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (9700/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (10896/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (12085/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (13280/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (14479/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (15663/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (16854/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (18044/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (19230/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (20424/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (21614/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (22813/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (24027/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (25225/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (26421/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (27624/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (28808/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (30006/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (31195/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (32375/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (33567/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (34777/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (35970/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (37171/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (38370/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (39563/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (40747/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (41930/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (43129/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (44308/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (45511/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (46649/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_120.pth.tar'
# TEST : Loss: (0.4423) | Acc: (86.00%) (8664/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1304,  0.0695,  0.0983],
          [ 0.1104, -0.3079, -0.1118],
          [ 0.2615, -0.0820,  0.0084]],

         [[-0.0094, -0.0893,  0.2648],
          [ 0.0183, -0.1141,  0.0423],
          [ 0.0655,  0.0042, -0.0714]],

         [[ 0.0128, -0.0382,  0.2014],
          [ 0.0883, -0.2060,  0.1578],
          [-0.1168,  0.0160, -0.1933]]],


        [[[ 0.0987, -0.0726, -0.0030],
          [ 0.1386,  0.1764,  0.1814],
          [ 0.0125, -0.0615,  0.1149]],

         [[-0.1531, -0.1782,  0.1520],
          [ 0.0326, -0.0482,  0.1002],
          [ 0.1211, -0.2027,  0.1277]],

         [[ 0.0995, -0.0494, -0.0153],
          [ 0.0106,  0.0195, -0.1205],
          [-0.0318, -0.1608, -0.0220]]],


        [[[-0.0715,  0.0883, -0.0428],
          [-0.1244,  0.0648,  0.1156],
          [ 0.1103,  0.1655,  0.0017]],

         [[ 0.0986,  0.1311,  0.1118],
          [ 0.0684, -0.0385, -0.0358],
          [-0.0467, -0.0298,  0.1378]],

         [[-0.0451,  0.1427,  0.0727],
          [-0.0423, -0.0459, -0.0606],
          [-0.0396, -0.0086,  0.0412]]],


        ...,


        [[[ 0.2273, -0.1141, -0.1211],
          [-0.0515, -0.1162, -0.0949],
          [-0.0935,  0.0239,  0.0708]],

         [[ 0.1658,  0.1927, -0.1167],
          [ 0.2009, -0.1421, -0.1130],
          [-0.0601, -0.0757, -0.0256]],

         [[ 0.1846,  0.1053, -0.1007],
          [-0.0660, -0.1527, -0.0718],
          [ 0.0727,  0.1819, -0.1245]]],


        [[[-0.1026,  0.0875, -0.1447],
          [-0.1411, -0.1468,  0.1161],
          [-0.1460,  0.1530, -0.1345]],

         [[ 0.0961, -0.1158,  0.0863],
          [ 0.1482,  0.0011, -0.0844],
          [-0.1390,  0.1171, -0.1957]],

         [[ 0.1267, -0.0189,  0.0025],
          [ 0.1416, -0.1743,  0.0125],
          [-0.0193,  0.0202, -0.1204]]],


        [[[ 0.0715,  0.1657,  0.1119],
          [-0.0877, -0.1955,  0.2138],
          [-0.1757, -0.0297, -0.0739]],

         [[-0.0185, -0.0705,  0.1529],
          [ 0.0039, -0.2193,  0.0573],
          [-0.0042,  0.0746,  0.0253]],

         [[-0.1673, -0.1991,  0.1200],
          [ 0.1462, -0.2007, -0.0124],
          [ 0.0114, -0.0162,  0.2930]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0373, -0.0173,  0.0195],
          [-0.0743, -0.0400, -0.0100],
          [-0.0734, -0.0567, -0.0227]],

         [[-0.0717, -0.0649, -0.0285],
          [-0.0845, -0.0545, -0.0294],
          [-0.0655, -0.0510, -0.0193]],

         [[-0.0638, -0.0615, -0.0245],
          [-0.0760, -0.0514, -0.0247],
          [-0.0517, -0.0422, -0.0138]]],


        [[[ 0.0164,  0.0134,  0.0044],
          [ 0.0181,  0.0175,  0.0072],
          [ 0.0102,  0.0129,  0.0020]],

         [[ 0.0242,  0.0214,  0.0104],
          [ 0.0221,  0.0213,  0.0092],
          [ 0.0112,  0.0135,  0.0012]],

         [[ 0.0241,  0.0208,  0.0115],
          [ 0.0197,  0.0172,  0.0063],
          [ 0.0080,  0.0082, -0.0031]]],


        [[[-0.0018, -0.0008, -0.0025],
          [-0.0020,  0.0000, -0.0009],
          [-0.0036, -0.0006, -0.0011]],

         [[-0.0009,  0.0007, -0.0010],
          [-0.0008,  0.0018,  0.0007],
          [-0.0016,  0.0019,  0.0011]],

         [[ 0.0022,  0.0035,  0.0016],
          [ 0.0009,  0.0029,  0.0013],
          [-0.0003,  0.0024,  0.0009]]],


        ...,


        [[[-0.0581, -0.0588, -0.0410],
          [-0.0580, -0.0569, -0.0414],
          [-0.0543, -0.0630, -0.0595]],

         [[-0.0597, -0.0596, -0.0429],
          [-0.0588, -0.0587, -0.0451],
          [-0.0577, -0.0692, -0.0674]],

         [[-0.0782, -0.0778, -0.0585],
          [-0.0757, -0.0759, -0.0586],
          [-0.0748, -0.0855, -0.0783]]],


        [[[-0.0022, -0.0036, -0.0017],
          [-0.0030, -0.0058, -0.0013],
          [-0.0000, -0.0095, -0.0032]],

         [[-0.0046, -0.0042, -0.0052],
          [-0.0052, -0.0059, -0.0038],
          [-0.0031, -0.0104, -0.0050]],

         [[-0.0076, -0.0072, -0.0096],
          [-0.0113, -0.0110, -0.0094],
          [-0.0110, -0.0145, -0.0097]]],


        [[[-0.0641, -0.0705, -0.0575],
          [-0.0909, -0.1092, -0.1170],
          [-0.0952, -0.1210, -0.1291]],

         [[-0.1642, -0.1619, -0.1590],
          [-0.1846, -0.1943, -0.2118],
          [-0.1835, -0.2071, -0.2204]],

         [[-0.2092, -0.2123, -0.2259],
          [-0.2282, -0.2436, -0.2779],
          [-0.2320, -0.2591, -0.2859]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0544]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 121 | Batch_idx: 0 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (94.00%) (2530/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (94.00%) (3733/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (94.00%) (4936/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (6155/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (7358/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (8544/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (9732/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (12134/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (13324/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (14509/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (15704/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (16894/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (18102/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (19303/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1849) |  Loss2: (0.0000) | Acc: (93.00%) (20506/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (21702/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (22897/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (24097/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (25304/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (26514/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (27712/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1837) |  Loss2: (0.0000) | Acc: (93.00%) (28909/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (30100/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (31277/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (32456/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (33641/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (34846/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (36047/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (37246/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (38437/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (39621/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (40804/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (42022/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (43218/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (44427/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (45647/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (46794/50000)
# TEST : Loss: (0.4126) | Acc: (87.00%) (8737/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1309,  0.0664,  0.0956],
          [ 0.1123, -0.3112, -0.1160],
          [ 0.2651, -0.0811,  0.0065]],

         [[-0.0087, -0.0895,  0.2639],
          [ 0.0218, -0.1153,  0.0396],
          [ 0.0702,  0.0057, -0.0734]],

         [[ 0.0134, -0.0379,  0.2014],
          [ 0.0917, -0.2057,  0.1570],
          [-0.1123,  0.0184, -0.1927]]],


        [[[ 0.1005, -0.0711, -0.0017],
          [ 0.1400,  0.1777,  0.1826],
          [ 0.0141, -0.0600,  0.1166]],

         [[-0.1508, -0.1758,  0.1542],
          [ 0.0345, -0.0462,  0.1023],
          [ 0.1233, -0.2005,  0.1302]],

         [[ 0.1018, -0.0465, -0.0120],
          [ 0.0126,  0.0220, -0.1176],
          [-0.0292, -0.1583, -0.0192]]],


        [[[-0.0719,  0.0879, -0.0432],
          [-0.1247,  0.0645,  0.1153],
          [ 0.1097,  0.1652,  0.0014]],

         [[ 0.0990,  0.1314,  0.1118],
          [ 0.0689, -0.0381, -0.0357],
          [-0.0465, -0.0295,  0.1379]],

         [[-0.0441,  0.1434,  0.0731],
          [-0.0415, -0.0452, -0.0602],
          [-0.0390, -0.0080,  0.0416]]],


        ...,


        [[[ 0.2263, -0.1157, -0.1225],
          [-0.0493, -0.1156, -0.0946],
          [-0.0912,  0.0253,  0.0731]],

         [[ 0.1648,  0.1906, -0.1191],
          [ 0.2030, -0.1421, -0.1140],
          [-0.0583, -0.0754, -0.0252]],

         [[ 0.1835,  0.1030, -0.1037],
          [-0.0643, -0.1531, -0.0735],
          [ 0.0741,  0.1816, -0.1250]]],


        [[[-0.1042,  0.0868, -0.1444],
          [-0.1438, -0.1485,  0.1158],
          [-0.1473,  0.1524, -0.1342]],

         [[ 0.0967, -0.1144,  0.0887],
          [ 0.1474,  0.0013, -0.0823],
          [-0.1387,  0.1181, -0.1935]],

         [[ 0.1262, -0.0189,  0.0036],
          [ 0.1396, -0.1752,  0.0133],
          [-0.0207,  0.0196, -0.1195]]],


        [[[ 0.0699,  0.1635,  0.1094],
          [-0.0865, -0.1950,  0.2176],
          [-0.1747, -0.0276, -0.0702]],

         [[-0.0190, -0.0715,  0.1522],
          [ 0.0057, -0.2184,  0.0622],
          [-0.0035,  0.0762,  0.0294]],

         [[-0.1706, -0.2014,  0.1186],
          [ 0.1464, -0.2002, -0.0085],
          [ 0.0109, -0.0152,  0.2963]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0012, -0.0088, -0.0183],
          [ 0.0018, -0.0145, -0.0211],
          [-0.0043, -0.0170, -0.0228]],

         [[ 0.0129,  0.0025, -0.0082],
          [ 0.0185,  0.0045, -0.0058],
          [ 0.0117,  0.0019, -0.0069]],

         [[ 0.0171,  0.0087,  0.0079],
          [ 0.0277,  0.0189,  0.0162],
          [ 0.0288,  0.0234,  0.0178]]],


        [[[-0.0003, -0.0065, -0.0086],
          [-0.0004, -0.0071, -0.0111],
          [ 0.0004, -0.0061, -0.0093]],

         [[ 0.0002, -0.0050, -0.0072],
          [-0.0034, -0.0079, -0.0131],
          [-0.0030, -0.0070, -0.0101]],

         [[-0.0101, -0.0119, -0.0125],
          [-0.0162, -0.0179, -0.0204],
          [-0.0163, -0.0186, -0.0189]]],


        [[[ 0.0027,  0.0021,  0.0022],
          [ 0.0036,  0.0020,  0.0018],
          [ 0.0053,  0.0026,  0.0018]],

         [[-0.0014, -0.0017, -0.0018],
          [-0.0010, -0.0022, -0.0022],
          [ 0.0002, -0.0015, -0.0018]],

         [[-0.0042, -0.0044, -0.0054],
          [-0.0046, -0.0050, -0.0059],
          [-0.0037, -0.0045, -0.0049]]],


        ...,


        [[[ 0.0177,  0.0150,  0.0143],
          [ 0.0110,  0.0046,  0.0067],
          [ 0.0139,  0.0088,  0.0073]],

         [[-0.0014, -0.0041, -0.0046],
          [-0.0038, -0.0102, -0.0090],
          [-0.0011, -0.0065, -0.0088]],

         [[-0.0138, -0.0133, -0.0108],
          [-0.0128, -0.0158, -0.0125],
          [-0.0110, -0.0146, -0.0146]]],


        [[[-0.0054, -0.0051, -0.0047],
          [ 0.0030,  0.0029,  0.0021],
          [ 0.0073,  0.0080,  0.0040]],

         [[-0.0144, -0.0152, -0.0149],
          [-0.0063, -0.0075, -0.0089],
          [-0.0022, -0.0021, -0.0058]],

         [[-0.0165, -0.0189, -0.0177],
          [-0.0108, -0.0130, -0.0137],
          [-0.0095, -0.0093, -0.0121]]],


        [[[-0.0289, -0.0288, -0.0404],
          [-0.0505, -0.0480, -0.0589],
          [-0.0564, -0.0538, -0.0596]],

         [[-0.0275, -0.0281, -0.0391],
          [-0.0492, -0.0454, -0.0522],
          [-0.0536, -0.0549, -0.0531]],

         [[-0.0194, -0.0225, -0.0282],
          [-0.0246, -0.0281, -0.0322],
          [-0.0196, -0.0316, -0.0329]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0540]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 122 | Batch_idx: 0 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (4951/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (6171/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (7380/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (8589/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (9794/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (10987/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (12184/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (94.00%) (13392/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (14593/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (94.00%) (15806/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (17003/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (94.00%) (18192/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (94.00%) (19383/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (94.00%) (20578/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (21771/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (22968/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (24181/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (25376/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (26582/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (27793/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (28989/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (30190/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (93.00%) (31403/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (32602/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (33800/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (34993/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (36199/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (37406/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (38609/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (39811/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1784) |  Loss2: (0.0000) | Acc: (93.00%) (41003/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (42204/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (43402/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (44600/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45794/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (46940/50000)
# TEST : Loss: (0.4202) | Acc: (87.00%) (8730/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1320,  0.0663,  0.0971],
          [ 0.1108, -0.3126, -0.1164],
          [ 0.2653, -0.0807,  0.0071]],

         [[-0.0091, -0.0884,  0.2669],
          [ 0.0218, -0.1150,  0.0415],
          [ 0.0714,  0.0067, -0.0711]],

         [[ 0.0135, -0.0369,  0.2043],
          [ 0.0927, -0.2049,  0.1596],
          [-0.1093,  0.0209, -0.1881]]],


        [[[ 0.0998, -0.0723, -0.0020],
          [ 0.1395,  0.1767,  0.1819],
          [ 0.0134, -0.0613,  0.1159]],

         [[-0.1507, -0.1760,  0.1548],
          [ 0.0348, -0.0461,  0.1026],
          [ 0.1230, -0.2008,  0.1306]],

         [[ 0.1008, -0.0475, -0.0122],
          [ 0.0119,  0.0213, -0.1181],
          [-0.0298, -0.1590, -0.0194]]],


        [[[-0.0722,  0.0878, -0.0431],
          [-0.1252,  0.0643,  0.1152],
          [ 0.1092,  0.1650,  0.0015]],

         [[ 0.0987,  0.1313,  0.1119],
          [ 0.0684, -0.0383, -0.0357],
          [-0.0469, -0.0296,  0.1378]],

         [[-0.0440,  0.1436,  0.0735],
          [-0.0414, -0.0450, -0.0599],
          [-0.0387, -0.0077,  0.0419]]],


        ...,


        [[[ 0.2249, -0.1164, -0.1229],
          [-0.0517, -0.1171, -0.0950],
          [-0.0933,  0.0237,  0.0724]],

         [[ 0.1649,  0.1918, -0.1178],
          [ 0.2024, -0.1419, -0.1130],
          [-0.0586, -0.0757, -0.0250]],

         [[ 0.1838,  0.1044, -0.1026],
          [-0.0643, -0.1526, -0.0728],
          [ 0.0748,  0.1821, -0.1243]]],


        [[[-0.1046,  0.0868, -0.1440],
          [-0.1443, -0.1486,  0.1161],
          [-0.1474,  0.1524, -0.1338]],

         [[ 0.0979, -0.1130,  0.0902],
          [ 0.1483,  0.0027, -0.0807],
          [-0.1374,  0.1194, -0.1919]],

         [[ 0.1290, -0.0163,  0.0059],
          [ 0.1420, -0.1731,  0.0154],
          [-0.0182,  0.0216, -0.1175]]],


        [[[ 0.0676,  0.1650,  0.1112],
          [-0.0858, -0.1967,  0.2167],
          [-0.1723, -0.0274, -0.0714]],

         [[-0.0200, -0.0697,  0.1532],
          [ 0.0076, -0.2186,  0.0622],
          [-0.0011,  0.0776,  0.0286]],

         [[-0.1745, -0.2020,  0.1173],
          [ 0.1470, -0.2015, -0.0096],
          [ 0.0130, -0.0141,  0.2956]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0391,  0.0575,  0.0947],
          [ 0.0369,  0.0565,  0.0760],
          [-0.0019,  0.0212,  0.0267]],

         [[ 0.0141,  0.0245,  0.0524],
          [ 0.0139,  0.0289,  0.0380],
          [-0.0247, -0.0025, -0.0047]],

         [[ 0.0389,  0.0380,  0.0627],
          [ 0.0455,  0.0483,  0.0579],
          [ 0.0176,  0.0308,  0.0344]]],


        [[[-0.0174, -0.0126, -0.0068],
          [-0.0136, -0.0093,  0.0006],
          [-0.0168, -0.0133, -0.0012]],

         [[-0.0109, -0.0035,  0.0031],
          [-0.0087, -0.0026,  0.0075],
          [-0.0143, -0.0108,  0.0013]],

         [[-0.0085, -0.0011,  0.0045],
          [-0.0059,  0.0002,  0.0082],
          [-0.0128, -0.0090,  0.0010]]],


        [[[ 0.0029,  0.0021,  0.0020],
          [ 0.0035,  0.0023,  0.0024],
          [ 0.0040,  0.0027,  0.0027]],

         [[-0.0004, -0.0017, -0.0019],
          [ 0.0005, -0.0011, -0.0012],
          [ 0.0013, -0.0003, -0.0003]],

         [[ 0.0046,  0.0031,  0.0025],
          [ 0.0056,  0.0038,  0.0033],
          [ 0.0062,  0.0044,  0.0038]]],


        ...,


        [[[ 0.0539,  0.0467,  0.0490],
          [ 0.0421,  0.0357,  0.0423],
          [ 0.0346,  0.0323,  0.0459]],

         [[ 0.0608,  0.0523,  0.0524],
          [ 0.0494,  0.0415,  0.0452],
          [ 0.0412,  0.0376,  0.0489]],

         [[ 0.0516,  0.0408,  0.0391],
          [ 0.0399,  0.0304,  0.0334],
          [ 0.0291,  0.0249,  0.0358]]],


        [[[ 0.0113,  0.0070,  0.0029],
          [ 0.0086,  0.0063, -0.0019],
          [ 0.0113,  0.0054, -0.0025]],

         [[ 0.0086,  0.0029, -0.0016],
          [ 0.0073,  0.0037, -0.0047],
          [ 0.0128,  0.0051, -0.0026]],

         [[ 0.0035, -0.0044, -0.0106],
          [ 0.0005, -0.0052, -0.0140],
          [ 0.0024, -0.0071, -0.0148]]],


        [[[ 0.0463,  0.0525,  0.0483],
          [ 0.0595,  0.0687,  0.0524],
          [ 0.0755,  0.0811,  0.0552]],

         [[ 0.0204,  0.0417,  0.0491],
          [ 0.0366,  0.0563,  0.0508],
          [ 0.0581,  0.0696,  0.0522]],

         [[-0.0046,  0.0234,  0.0334],
          [ 0.0150,  0.0375,  0.0334],
          [ 0.0356,  0.0538,  0.0438]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0536]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 123 | Batch_idx: 0 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (1328/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (3715/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (4908/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (6100/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (7299/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (8506/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (9709/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (10902/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (12089/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (13305/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (14499/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (15698/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (16901/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (18100/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (19300/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (20505/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (21710/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (22909/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (24108/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (25297/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (26500/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (27698/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (28884/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (30094/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (31293/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (32498/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (33718/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (34939/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (36145/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (37353/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (38545/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (39738/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (40931/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (42134/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (93.00%) (43339/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (44526/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (45727/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (46872/50000)
# TEST : Loss: (0.4342) | Acc: (86.00%) (8691/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1307,  0.0674,  0.0957],
          [ 0.1119, -0.3134, -0.1180],
          [ 0.2668, -0.0800,  0.0065]],

         [[-0.0090, -0.0865,  0.2671],
          [ 0.0223, -0.1146,  0.0419],
          [ 0.0721,  0.0078, -0.0707]],

         [[ 0.0105, -0.0371,  0.2036],
          [ 0.0890, -0.2077,  0.1579],
          [-0.1132,  0.0175, -0.1914]]],


        [[[ 0.0990, -0.0730, -0.0030],
          [ 0.1389,  0.1762,  0.1809],
          [ 0.0134, -0.0610,  0.1162]],

         [[-0.1518, -0.1770,  0.1536],
          [ 0.0335, -0.0471,  0.1012],
          [ 0.1222, -0.2012,  0.1303]],

         [[ 0.0994, -0.0493, -0.0141],
          [ 0.0105,  0.0195, -0.1203],
          [-0.0303, -0.1599, -0.0206]]],


        [[[-0.0724,  0.0876, -0.0432],
          [-0.1256,  0.0640,  0.1149],
          [ 0.1087,  0.1648,  0.0012]],

         [[ 0.0988,  0.1314,  0.1120],
          [ 0.0682, -0.0382, -0.0356],
          [-0.0471, -0.0296,  0.1377]],

         [[-0.0443,  0.1433,  0.0732],
          [-0.0418, -0.0453, -0.0602],
          [-0.0393, -0.0081,  0.0414]]],


        ...,


        [[[ 0.2245, -0.1163, -0.1231],
          [-0.0519, -0.1167, -0.0949],
          [-0.0938,  0.0241,  0.0723]],

         [[ 0.1646,  0.1911, -0.1190],
          [ 0.2024, -0.1421, -0.1141],
          [-0.0591, -0.0760, -0.0260]],

         [[ 0.1821,  0.1024, -0.1050],
          [-0.0655, -0.1541, -0.0749],
          [ 0.0730,  0.1804, -0.1261]]],


        [[[-0.1041,  0.0873, -0.1435],
          [-0.1438, -0.1478,  0.1173],
          [-0.1471,  0.1526, -0.1335]],

         [[ 0.0981, -0.1127,  0.0901],
          [ 0.1487,  0.0032, -0.0799],
          [-0.1375,  0.1190, -0.1925]],

         [[ 0.1288, -0.0165,  0.0055],
          [ 0.1420, -0.1729,  0.0160],
          [-0.0184,  0.0211, -0.1181]]],


        [[[ 0.0671,  0.1628,  0.1094],
          [-0.0873, -0.2012,  0.2156],
          [-0.1774, -0.0326, -0.0745]],

         [[-0.0189, -0.0713,  0.1515],
          [ 0.0083, -0.2212,  0.0615],
          [-0.0036,  0.0752,  0.0269]],

         [[-0.1715, -0.2010,  0.1180],
          [ 0.1491, -0.2015, -0.0073],
          [ 0.0118, -0.0142,  0.2965]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1044,  0.0951,  0.0976],
          [ 0.1130,  0.0990,  0.0920],
          [ 0.1307,  0.1148,  0.1259]],

         [[ 0.1320,  0.1240,  0.1243],
          [ 0.1388,  0.1255,  0.1153],
          [ 0.1575,  0.1405,  0.1454]],

         [[ 0.1296,  0.1202,  0.1225],
          [ 0.1414,  0.1248,  0.1171],
          [ 0.1619,  0.1393,  0.1459]]],


        [[[ 0.0391,  0.0379,  0.0272],
          [ 0.0300,  0.0304,  0.0191],
          [ 0.0393,  0.0382,  0.0264]],

         [[ 0.0475,  0.0464,  0.0361],
          [ 0.0392,  0.0401,  0.0290],
          [ 0.0470,  0.0467,  0.0355]],

         [[ 0.0394,  0.0398,  0.0307],
          [ 0.0306,  0.0346,  0.0258],
          [ 0.0364,  0.0402,  0.0320]]],


        [[[ 0.0009,  0.0001, -0.0022],
          [-0.0017,  0.0002, -0.0014],
          [-0.0021,  0.0015,  0.0024]],

         [[-0.0006, -0.0013, -0.0030],
          [-0.0031, -0.0013, -0.0023],
          [-0.0036, -0.0002,  0.0013]],

         [[-0.0008, -0.0012, -0.0020],
          [-0.0031, -0.0014, -0.0016],
          [-0.0039, -0.0007,  0.0011]]],


        ...,


        [[[-0.0674, -0.0497, -0.0359],
          [-0.0763, -0.0581, -0.0430],
          [-0.0886, -0.0668, -0.0468]],

         [[-0.0585, -0.0429, -0.0296],
          [-0.0667, -0.0492, -0.0352],
          [-0.0787, -0.0573, -0.0380]],

         [[-0.0359, -0.0242, -0.0157],
          [-0.0420, -0.0260, -0.0170],
          [-0.0500, -0.0288, -0.0141]]],


        [[[-0.0010, -0.0094, -0.0124],
          [ 0.0049,  0.0004, -0.0010],
          [ 0.0041,  0.0036,  0.0048]],

         [[-0.0051, -0.0133, -0.0164],
          [-0.0010, -0.0053, -0.0066],
          [-0.0037, -0.0041, -0.0028]],

         [[ 0.0093,  0.0006, -0.0027],
          [ 0.0133,  0.0093,  0.0070],
          [ 0.0116,  0.0118,  0.0126]]],


        [[[-0.1913, -0.1630, -0.1559],
          [-0.1444, -0.1360, -0.1470],
          [-0.1115, -0.1121, -0.1261]],

         [[-0.2140, -0.1852, -0.1798],
          [-0.1762, -0.1657, -0.1775],
          [-0.1455, -0.1423, -0.1543]],

         [[-0.1617, -0.1451, -0.1466],
          [-0.1306, -0.1253, -0.1410],
          [-0.1111, -0.1026, -0.1103]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0532]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 124 | Batch_idx: 0 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (3743/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (4947/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (6150/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (7369/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (8575/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (9769/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1650) |  Loss2: (0.0000) | Acc: (94.00%) (10976/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1656) |  Loss2: (0.0000) | Acc: (94.00%) (12182/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (13380/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (14584/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (15782/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (16990/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (19405/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (20608/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (21809/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (23020/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (24224/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (25431/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (26631/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (27827/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (29037/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (30242/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (31452/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (32656/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (33872/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (35073/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (36275/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (37488/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (38693/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (39895/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (41098/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (42287/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (43482/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (44688/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (45906/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1690) |  Loss2: (0.0000) | Acc: (94.00%) (47070/50000)
# TEST : Loss: (0.4251) | Acc: (87.00%) (8722/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1301,  0.0690,  0.0966],
          [ 0.1116, -0.3142, -0.1198],
          [ 0.2683, -0.0785,  0.0044]],

         [[-0.0093, -0.0842,  0.2702],
          [ 0.0216, -0.1138,  0.0429],
          [ 0.0733,  0.0103, -0.0707]],

         [[ 0.0103, -0.0349,  0.2079],
          [ 0.0903, -0.2059,  0.1611],
          [-0.1094,  0.0213, -0.1887]]],


        [[[ 0.0989, -0.0725, -0.0033],
          [ 0.1384,  0.1761,  0.1804],
          [ 0.0140, -0.0604,  0.1162]],

         [[-0.1523, -0.1768,  0.1529],
          [ 0.0326, -0.0475,  0.1003],
          [ 0.1223, -0.2011,  0.1298]],

         [[ 0.0982, -0.0501, -0.0159],
          [ 0.0087,  0.0178, -0.1228],
          [-0.0313, -0.1611, -0.0229]]],


        [[[-0.0723,  0.0878, -0.0432],
          [-0.1256,  0.0641,  0.1147],
          [ 0.1086,  0.1650,  0.0013]],

         [[ 0.0986,  0.1312,  0.1117],
          [ 0.0680, -0.0384, -0.0360],
          [-0.0474, -0.0298,  0.1373]],

         [[-0.0444,  0.1431,  0.0730],
          [-0.0421, -0.0455, -0.0605],
          [-0.0396, -0.0082,  0.0412]]],


        ...,


        [[[ 0.2264, -0.1146, -0.1222],
          [-0.0508, -0.1157, -0.0935],
          [-0.0937,  0.0251,  0.0735]],

         [[ 0.1659,  0.1922, -0.1189],
          [ 0.2033, -0.1412, -0.1132],
          [-0.0591, -0.0752, -0.0253]],

         [[ 0.1806,  0.1010, -0.1069],
          [-0.0663, -0.1549, -0.0754],
          [ 0.0718,  0.1802, -0.1261]]],


        [[[-0.1055,  0.0872, -0.1449],
          [-0.1444, -0.1474,  0.1166],
          [-0.1469,  0.1538, -0.1334]],

         [[ 0.0976, -0.1120,  0.0893],
          [ 0.1490,  0.0043, -0.0797],
          [-0.1361,  0.1211, -0.1913]],

         [[ 0.1283, -0.0164,  0.0043],
          [ 0.1419, -0.1724,  0.0156],
          [-0.0179,  0.0222, -0.1178]]],


        [[[ 0.0703,  0.1681,  0.1128],
          [-0.0855, -0.1980,  0.2192],
          [-0.1781, -0.0310, -0.0752]],

         [[-0.0188, -0.0697,  0.1516],
          [ 0.0065, -0.2217,  0.0622],
          [-0.0074,  0.0742,  0.0245]],

         [[-0.1733, -0.2026,  0.1142],
          [ 0.1473, -0.2038, -0.0090],
          [ 0.0086, -0.0159,  0.2932]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0509,  0.0617,  0.0491],
          [ 0.0191,  0.0323,  0.0332],
          [-0.0108, -0.0098, -0.0042]],

         [[-0.0090,  0.0261,  0.0189],
          [-0.0326, -0.0048,  0.0042],
          [-0.0504, -0.0465, -0.0256]],

         [[ 0.1065,  0.1372,  0.1214],
          [ 0.0726,  0.1005,  0.1042],
          [ 0.0370,  0.0362,  0.0523]]],


        [[[ 0.0081,  0.0075,  0.0055],
          [-0.0004,  0.0001, -0.0040],
          [-0.0054, -0.0057, -0.0100]],

         [[ 0.0094,  0.0098,  0.0056],
          [ 0.0009,  0.0016, -0.0038],
          [-0.0051, -0.0035, -0.0072]],

         [[-0.0008, -0.0028, -0.0126],
          [-0.0083, -0.0084, -0.0179],
          [-0.0147, -0.0122, -0.0175]]],


        [[[ 0.0052,  0.0046, -0.0007],
          [ 0.0013,  0.0017, -0.0024],
          [-0.0030, -0.0019, -0.0034]],

         [[ 0.0047,  0.0038, -0.0006],
          [ 0.0006,  0.0011, -0.0019],
          [-0.0035, -0.0017, -0.0021]],

         [[-0.0034, -0.0060, -0.0111],
          [-0.0079, -0.0093, -0.0126],
          [-0.0129, -0.0127, -0.0133]]],


        ...,


        [[[-0.0443, -0.0266, -0.0227],
          [-0.0595, -0.0380, -0.0245],
          [-0.0531, -0.0364, -0.0211]],

         [[-0.0490, -0.0403, -0.0401],
          [-0.0652, -0.0531, -0.0455],
          [-0.0670, -0.0561, -0.0450]],

         [[-0.0334, -0.0155, -0.0087],
          [-0.0448, -0.0262, -0.0148],
          [-0.0465, -0.0296, -0.0125]]],


        [[[-0.0019,  0.0023,  0.0140],
          [-0.0047, -0.0059,  0.0063],
          [-0.0034, -0.0065,  0.0000]],

         [[-0.0326, -0.0259, -0.0113],
          [-0.0334, -0.0305, -0.0158],
          [-0.0319, -0.0307, -0.0222]],

         [[ 0.0020,  0.0040,  0.0114],
          [ 0.0038,  0.0018,  0.0093],
          [ 0.0056,  0.0018,  0.0033]]],


        [[[-0.0065, -0.0021,  0.0000],
          [-0.0444, -0.0417, -0.0339],
          [-0.0635, -0.0609, -0.0455]],

         [[ 0.1327,  0.1396,  0.1338],
          [ 0.0932,  0.0968,  0.0930],
          [ 0.0636,  0.0590,  0.0699]],

         [[ 0.0577,  0.0846,  0.0759],
          [ 0.0141,  0.0374,  0.0312],
          [-0.0096,  0.0012,  0.0177]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.0528]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.3529) |  Loss2: (0.2181) | Acc: (95.00%) (122/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.3922) |  Loss2: (0.2180) | Acc: (94.00%) (1325/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.4023) |  Loss2: (0.2180) | Acc: (93.00%) (2507/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.4138) |  Loss2: (0.2179) | Acc: (92.00%) (3676/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.4146) |  Loss2: (0.2179) | Acc: (92.00%) (4862/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.4207) |  Loss2: (0.2178) | Acc: (92.00%) (6040/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.4207) |  Loss2: (0.2177) | Acc: (92.00%) (7225/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.4240) |  Loss2: (0.2177) | Acc: (92.00%) (8410/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.4204) |  Loss2: (0.2176) | Acc: (92.00%) (9614/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.4222) |  Loss2: (0.2176) | Acc: (92.00%) (10801/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.4208) |  Loss2: (0.2175) | Acc: (92.00%) (11996/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.4227) |  Loss2: (0.2174) | Acc: (92.00%) (13167/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.4207) |  Loss2: (0.2174) | Acc: (92.00%) (14371/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.4194) |  Loss2: (0.2173) | Acc: (92.00%) (15559/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.4189) |  Loss2: (0.2173) | Acc: (92.00%) (16751/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.4187) |  Loss2: (0.2172) | Acc: (92.00%) (17942/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.4164) |  Loss2: (0.2171) | Acc: (92.00%) (19148/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.4170) |  Loss2: (0.2171) | Acc: (92.00%) (20332/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.4155) |  Loss2: (0.2170) | Acc: (92.00%) (21534/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.4146) |  Loss2: (0.2170) | Acc: (92.00%) (22728/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.4140) |  Loss2: (0.2169) | Acc: (92.00%) (23917/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.4145) |  Loss2: (0.2169) | Acc: (92.00%) (25095/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.4135) |  Loss2: (0.2168) | Acc: (92.00%) (26298/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.4128) |  Loss2: (0.2168) | Acc: (92.00%) (27490/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.4129) |  Loss2: (0.2167) | Acc: (92.00%) (28679/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.4125) |  Loss2: (0.2166) | Acc: (92.00%) (29876/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.4118) |  Loss2: (0.2166) | Acc: (93.00%) (31079/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.4107) |  Loss2: (0.2165) | Acc: (93.00%) (32289/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.4098) |  Loss2: (0.2165) | Acc: (93.00%) (33497/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.4089) |  Loss2: (0.2164) | Acc: (93.00%) (34705/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.4086) |  Loss2: (0.2163) | Acc: (93.00%) (35900/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.4078) |  Loss2: (0.2163) | Acc: (93.00%) (37100/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.4081) |  Loss2: (0.2162) | Acc: (93.00%) (38291/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.4076) |  Loss2: (0.2162) | Acc: (93.00%) (39491/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.4074) |  Loss2: (0.2161) | Acc: (93.00%) (40691/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.4070) |  Loss2: (0.2160) | Acc: (93.00%) (41892/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.4073) |  Loss2: (0.2160) | Acc: (93.00%) (43073/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.4066) |  Loss2: (0.2159) | Acc: (93.00%) (44282/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.4057) |  Loss2: (0.2159) | Acc: (93.00%) (45497/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.4057) |  Loss2: (0.2158) | Acc: (93.00%) (46650/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_125.pth.tar'
# TEST : Loss: (0.4308) | Acc: (86.00%) (8698/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1299,  0.0688,  0.0968],
          [ 0.1118, -0.3140, -0.1195],
          [ 0.2684, -0.0782,  0.0047]],

         [[-0.0084, -0.0837,  0.2709],
          [ 0.0225, -0.1130,  0.0438],
          [ 0.0742,  0.0114, -0.0696]],

         [[ 0.0102, -0.0355,  0.2076],
          [ 0.0904, -0.2060,  0.1609],
          [-0.1090,  0.0218, -0.1883]]],


        [[[ 0.0987, -0.0727, -0.0035],
          [ 0.1384,  0.1759,  0.1802],
          [ 0.0141, -0.0604,  0.1161]],

         [[-0.1525, -0.1770,  0.1526],
          [ 0.0326, -0.0477,  0.1001],
          [ 0.1224, -0.2010,  0.1296]],

         [[ 0.0980, -0.0502, -0.0160],
          [ 0.0087,  0.0177, -0.1228],
          [-0.0312, -0.1611, -0.0229]]],


        [[[-0.0723,  0.0877, -0.0431],
          [-0.1256,  0.0641,  0.1147],
          [ 0.1087,  0.1650,  0.0013]],

         [[ 0.0985,  0.1311,  0.1116],
          [ 0.0679, -0.0385, -0.0360],
          [-0.0474, -0.0297,  0.1373]],

         [[-0.0444,  0.1431,  0.0730],
          [-0.0421, -0.0455, -0.0604],
          [-0.0395, -0.0081,  0.0413]]],


        ...,


        [[[ 0.2264, -0.1146, -0.1221],
          [-0.0506, -0.1156, -0.0933],
          [-0.0936,  0.0251,  0.0735]],

         [[ 0.1659,  0.1921, -0.1189],
          [ 0.2034, -0.1410, -0.1130],
          [-0.0590, -0.0751, -0.0253]],

         [[ 0.1807,  0.1009, -0.1070],
          [-0.0661, -0.1548, -0.0753],
          [ 0.0719,  0.1801, -0.1261]]],


        [[[-0.1057,  0.0869, -0.1452],
          [-0.1445, -0.1475,  0.1164],
          [-0.1469,  0.1536, -0.1334]],

         [[ 0.0977, -0.1119,  0.0892],
          [ 0.1492,  0.0045, -0.0795],
          [-0.1357,  0.1214, -0.1909]],

         [[ 0.1282, -0.0164,  0.0041],
          [ 0.1419, -0.1723,  0.0156],
          [-0.0177,  0.0224, -0.1175]]],


        [[[ 0.0702,  0.1680,  0.1127],
          [-0.0854, -0.1978,  0.2190],
          [-0.1781, -0.0309, -0.0756]],

         [[-0.0198, -0.0708,  0.1502],
          [ 0.0056, -0.2224,  0.0612],
          [-0.0083,  0.0735,  0.0235]],

         [[-0.1736, -0.2031,  0.1133],
          [ 0.1472, -0.2040, -0.0094],
          [ 0.0086, -0.0158,  0.2928]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2991e-05,  6.8787e-06,  9.6831e-06],
          [ 1.1179e-05, -3.1404e-05, -1.1952e-05],
          [ 2.6844e-05, -7.8183e-06,  4.6684e-07]],

         [[-8.3647e-07, -8.3688e-06,  2.7089e-05],
          [ 2.2527e-06, -1.1300e-05,  4.3774e-06],
          [ 7.4164e-06,  1.1422e-06, -6.9641e-06]],

         [[ 1.0210e-06, -3.5519e-06,  2.0757e-05],
          [ 9.0351e-06, -2.0600e-05,  1.6095e-05],
          [-1.0897e-05,  2.1808e-06, -1.8827e-05]]],


        [[[ 9.8724e-06, -7.2696e-06, -3.4702e-07],
          [ 1.3835e-05,  1.7594e-05,  1.8024e-05],
          [ 1.4089e-06, -6.0399e-06,  1.1614e-05]],

         [[-1.5246e-05, -1.7702e-05,  1.5255e-05],
          [ 3.2579e-06, -4.7670e-06,  1.0013e-05],
          [ 1.2236e-05, -2.0104e-05,  1.2964e-05]],

         [[ 9.8046e-06, -5.0238e-06, -1.5975e-06],
          [ 8.6685e-07,  1.7672e-06, -1.2276e-05],
          [-3.1152e-06, -1.6106e-05, -2.2859e-06]]],


        [[[-7.2280e-06,  8.7720e-06, -4.3131e-06],
          [-1.2557e-05,  6.4053e-06,  1.1473e-05],
          [ 1.0866e-05,  1.6498e-05,  1.3205e-07]],

         [[ 9.8491e-06,  1.3113e-05,  1.1162e-05],
          [ 6.7895e-06, -3.8468e-06, -3.5967e-06],
          [-4.7411e-06, -2.9746e-06,  1.3727e-05]],

         [[-4.4426e-06,  1.4308e-05,  7.2968e-06],
          [-4.2098e-06, -4.5473e-06, -6.0382e-06],
          [-3.9493e-06, -8.0743e-07,  4.1262e-06]]],


        ...,


        [[[ 2.2640e-05, -1.1455e-05, -1.2212e-05],
          [-5.0627e-06, -1.1561e-05, -9.3332e-06],
          [-9.3558e-06,  2.5111e-06,  7.3476e-06]],

         [[ 1.6592e-05,  1.9210e-05, -1.1889e-05],
          [ 2.0340e-05, -1.4101e-05, -1.1303e-05],
          [-5.8977e-06, -7.5076e-06, -2.5274e-06]],

         [[ 1.8069e-05,  1.0091e-05, -1.0698e-05],
          [-6.6098e-06, -1.5477e-05, -7.5307e-06],
          [ 7.1901e-06,  1.8011e-05, -1.2607e-05]]],


        [[[-1.0571e-05,  8.6901e-06, -1.4519e-05],
          [-1.4453e-05, -1.4751e-05,  1.1637e-05],
          [-1.4688e-05,  1.5359e-05, -1.3342e-05]],

         [[ 9.7724e-06, -1.1193e-05,  8.9236e-06],
          [ 1.4917e-05,  4.5192e-07, -7.9526e-06],
          [-1.3572e-05,  1.2140e-05, -1.9091e-05]],

         [[ 1.2815e-05, -1.6431e-06,  4.1364e-07],
          [ 1.4186e-05, -1.7229e-05,  1.5589e-06],
          [-1.7732e-06,  2.2367e-06, -1.1754e-05]]],


        [[[ 7.0218e-06,  1.6801e-05,  1.1265e-05],
          [-8.5386e-06, -1.9785e-05,  2.1896e-05],
          [-1.7807e-05, -3.0945e-06, -7.5600e-06]],

         [[-1.9826e-06, -7.0794e-06,  1.5022e-05],
          [ 5.5972e-07, -2.2244e-05,  6.1158e-06],
          [-8.3193e-07,  7.3543e-06,  2.3498e-06]],

         [[-1.7358e-05, -2.0313e-05,  1.1333e-05],
          [ 1.4718e-05, -2.0397e-05, -9.4272e-07],
          [ 8.6168e-07, -1.5793e-06,  2.9281e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.1077]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0065]], device='cuda:0')

Epoch: 126 | Batch_idx: 0 |  Loss: (0.3888) |  Loss2: (0.2134) | Acc: (92.00%) (118/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.3821) |  Loss2: (0.2134) | Acc: (93.00%) (1319/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.3878) |  Loss2: (0.2133) | Acc: (93.00%) (2522/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.3906) |  Loss2: (0.2133) | Acc: (93.00%) (3726/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.3897) |  Loss2: (0.2132) | Acc: (93.00%) (4924/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.3855) |  Loss2: (0.2131) | Acc: (94.00%) (6137/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.3842) |  Loss2: (0.2131) | Acc: (94.00%) (7344/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.3845) |  Loss2: (0.2130) | Acc: (94.00%) (8550/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.3873) |  Loss2: (0.2130) | Acc: (93.00%) (9745/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.3868) |  Loss2: (0.2129) | Acc: (93.00%) (10945/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.3864) |  Loss2: (0.2129) | Acc: (93.00%) (12147/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.3872) |  Loss2: (0.2128) | Acc: (93.00%) (13350/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.3879) |  Loss2: (0.2128) | Acc: (93.00%) (14550/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.3893) |  Loss2: (0.2127) | Acc: (93.00%) (15743/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.3901) |  Loss2: (0.2126) | Acc: (93.00%) (16941/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.3892) |  Loss2: (0.2126) | Acc: (93.00%) (18152/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.3902) |  Loss2: (0.2125) | Acc: (93.00%) (19348/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.3908) |  Loss2: (0.2125) | Acc: (93.00%) (20543/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.3896) |  Loss2: (0.2124) | Acc: (93.00%) (21754/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.3895) |  Loss2: (0.2124) | Acc: (93.00%) (22951/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.3888) |  Loss2: (0.2123) | Acc: (93.00%) (24160/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.3878) |  Loss2: (0.2123) | Acc: (93.00%) (25375/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.3880) |  Loss2: (0.2122) | Acc: (93.00%) (26575/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.3881) |  Loss2: (0.2122) | Acc: (93.00%) (27777/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.3877) |  Loss2: (0.2121) | Acc: (93.00%) (28976/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.3880) |  Loss2: (0.2121) | Acc: (93.00%) (30171/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.3877) |  Loss2: (0.2120) | Acc: (93.00%) (31380/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.3878) |  Loss2: (0.2120) | Acc: (93.00%) (32581/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.3882) |  Loss2: (0.2119) | Acc: (93.00%) (33768/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.3881) |  Loss2: (0.2118) | Acc: (93.00%) (34978/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.3881) |  Loss2: (0.2118) | Acc: (93.00%) (36176/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.3882) |  Loss2: (0.2117) | Acc: (93.00%) (37371/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.3875) |  Loss2: (0.2117) | Acc: (93.00%) (38576/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.3873) |  Loss2: (0.2116) | Acc: (93.00%) (39782/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.3871) |  Loss2: (0.2116) | Acc: (93.00%) (40987/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.3870) |  Loss2: (0.2115) | Acc: (93.00%) (42191/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.3866) |  Loss2: (0.2115) | Acc: (93.00%) (43403/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.3865) |  Loss2: (0.2114) | Acc: (93.00%) (44604/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.3864) |  Loss2: (0.2114) | Acc: (93.00%) (45813/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.3862) |  Loss2: (0.2113) | Acc: (93.00%) (46962/50000)
# TEST : Loss: (0.4156) | Acc: (87.00%) (8732/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1299,  0.0688,  0.0968],
          [ 0.1117, -0.3139, -0.1195],
          [ 0.2683, -0.0782,  0.0047]],

         [[-0.0084, -0.0837,  0.2708],
          [ 0.0225, -0.1130,  0.0438],
          [ 0.0741,  0.0114, -0.0696]],

         [[ 0.0102, -0.0355,  0.2075],
          [ 0.0903, -0.2059,  0.1609],
          [-0.1089,  0.0218, -0.1882]]],


        [[[ 0.0987, -0.0727, -0.0035],
          [ 0.1383,  0.1759,  0.1802],
          [ 0.0141, -0.0604,  0.1161]],

         [[-0.1524, -0.1769,  0.1525],
          [ 0.0326, -0.0477,  0.1001],
          [ 0.1223, -0.2010,  0.1296]],

         [[ 0.0980, -0.0502, -0.0160],
          [ 0.0087,  0.0177, -0.1227],
          [-0.0311, -0.1610, -0.0229]]],


        [[[-0.0723,  0.0877, -0.0431],
          [-0.1255,  0.0640,  0.1147],
          [ 0.1086,  0.1649,  0.0013]],

         [[ 0.0985,  0.1311,  0.1116],
          [ 0.0679, -0.0385, -0.0360],
          [-0.0474, -0.0297,  0.1372]],

         [[-0.0444,  0.1430,  0.0729],
          [-0.0421, -0.0455, -0.0604],
          [-0.0395, -0.0081,  0.0412]]],


        ...,


        [[[ 0.2263, -0.1145, -0.1221],
          [-0.0506, -0.1156, -0.0933],
          [-0.0935,  0.0251,  0.0734]],

         [[ 0.1659,  0.1920, -0.1188],
          [ 0.2033, -0.1410, -0.1130],
          [-0.0590, -0.0750, -0.0253]],

         [[ 0.1806,  0.1009, -0.1069],
          [-0.0661, -0.1547, -0.0753],
          [ 0.0719,  0.1800, -0.1260]]],


        [[[-0.1057,  0.0869, -0.1451],
          [-0.1445, -0.1475,  0.1163],
          [-0.1468,  0.1535, -0.1334]],

         [[ 0.0977, -0.1119,  0.0892],
          [ 0.1491,  0.0045, -0.0795],
          [-0.1357,  0.1214, -0.1908]],

         [[ 0.1281, -0.0164,  0.0041],
          [ 0.1418, -0.1722,  0.0156],
          [-0.0177,  0.0224, -0.1175]]],


        [[[ 0.0702,  0.1679,  0.1126],
          [-0.0854, -0.1978,  0.2189],
          [-0.1780, -0.0309, -0.0756]],

         [[-0.0198, -0.0708,  0.1502],
          [ 0.0056, -0.2223,  0.0611],
          [-0.0083,  0.0735,  0.0235]],

         [[-0.1735, -0.2030,  0.1133],
          [ 0.1471, -0.2039, -0.0094],
          [ 0.0086, -0.0158,  0.2927]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2986e-05,  6.8761e-06,  9.6794e-06],
          [ 1.1175e-05, -3.1391e-05, -1.1947e-05],
          [ 2.6833e-05, -7.8154e-06,  4.6666e-07]],

         [[-8.3615e-07, -8.3656e-06,  2.7078e-05],
          [ 2.2518e-06, -1.1296e-05,  4.3757e-06],
          [ 7.4135e-06,  1.1417e-06, -6.9615e-06]],

         [[ 1.0206e-06, -3.5504e-06,  2.0749e-05],
          [ 9.0316e-06, -2.0592e-05,  1.6088e-05],
          [-1.0892e-05,  2.1799e-06, -1.8820e-05]]],


        [[[ 9.8686e-06, -7.2667e-06, -3.4688e-07],
          [ 1.3830e-05,  1.7587e-05,  1.8017e-05],
          [ 1.4084e-06, -6.0375e-06,  1.1610e-05]],

         [[-1.5240e-05, -1.7695e-05,  1.5249e-05],
          [ 3.2566e-06, -4.7651e-06,  1.0009e-05],
          [ 1.2232e-05, -2.0096e-05,  1.2959e-05]],

         [[ 9.8008e-06, -5.0219e-06, -1.5968e-06],
          [ 8.6652e-07,  1.7666e-06, -1.2271e-05],
          [-3.1140e-06, -1.6099e-05, -2.2850e-06]]],


        [[[-7.2250e-06,  8.7685e-06, -4.3113e-06],
          [-1.2553e-05,  6.4027e-06,  1.1469e-05],
          [ 1.0861e-05,  1.6492e-05,  1.3200e-07]],

         [[ 9.8453e-06,  1.3108e-05,  1.1158e-05],
          [ 6.7869e-06, -3.8453e-06, -3.5953e-06],
          [-4.7392e-06, -2.9734e-06,  1.3721e-05]],

         [[-4.4408e-06,  1.4302e-05,  7.2938e-06],
          [-4.2082e-06, -4.5456e-06, -6.0359e-06],
          [-3.9477e-06, -8.0710e-07,  4.1246e-06]]],


        ...,


        [[[ 2.2631e-05, -1.1451e-05, -1.2207e-05],
          [-5.0607e-06, -1.1556e-05, -9.3295e-06],
          [-9.3520e-06,  2.5102e-06,  7.3447e-06]],

         [[ 1.6586e-05,  1.9203e-05, -1.1884e-05],
          [ 2.0332e-05, -1.4096e-05, -1.1299e-05],
          [-5.8954e-06, -7.5047e-06, -2.5264e-06]],

         [[ 1.8062e-05,  1.0087e-05, -1.0694e-05],
          [-6.6071e-06, -1.5472e-05, -7.5278e-06],
          [ 7.1872e-06,  1.8004e-05, -1.2603e-05]]],


        [[[-1.0567e-05,  8.6866e-06, -1.4513e-05],
          [-1.4448e-05, -1.4745e-05,  1.1632e-05],
          [-1.4682e-05,  1.5353e-05, -1.3337e-05]],

         [[ 9.7686e-06, -1.1189e-05,  8.9201e-06],
          [ 1.4911e-05,  4.5174e-07, -7.9494e-06],
          [-1.3567e-05,  1.2135e-05, -1.9083e-05]],

         [[ 1.2810e-05, -1.6424e-06,  4.1347e-07],
          [ 1.4180e-05, -1.7222e-05,  1.5583e-06],
          [-1.7725e-06,  2.2358e-06, -1.1749e-05]]],


        [[[ 7.0192e-06,  1.6794e-05,  1.1261e-05],
          [-8.5353e-06, -1.9777e-05,  2.1888e-05],
          [-1.7800e-05, -3.0933e-06, -7.5571e-06]],

         [[-1.9818e-06, -7.0766e-06,  1.5016e-05],
          [ 5.5950e-07, -2.2235e-05,  6.1135e-06],
          [-8.3160e-07,  7.3514e-06,  2.3489e-06]],

         [[-1.7351e-05, -2.0305e-05,  1.1329e-05],
          [ 1.4713e-05, -2.0389e-05, -9.4235e-07],
          [ 8.6136e-07, -1.5787e-06,  2.9269e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.1507]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0421]], device='cuda:0')

Epoch: 127 | Batch_idx: 0 |  Loss: (0.3655) |  Loss2: (0.2093) | Acc: (94.00%) (121/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.3560) |  Loss2: (0.2093) | Acc: (95.00%) (1340/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.3710) |  Loss2: (0.2092) | Acc: (94.00%) (2543/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.3700) |  Loss2: (0.2091) | Acc: (94.00%) (3757/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.3650) |  Loss2: (0.2091) | Acc: (94.00%) (4972/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.3673) |  Loss2: (0.2090) | Acc: (94.00%) (6171/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.3716) |  Loss2: (0.2090) | Acc: (94.00%) (7373/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.3695) |  Loss2: (0.2089) | Acc: (94.00%) (8586/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.3741) |  Loss2: (0.2089) | Acc: (94.00%) (9777/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.3747) |  Loss2: (0.2088) | Acc: (94.00%) (10972/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.3754) |  Loss2: (0.2088) | Acc: (94.00%) (12177/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.3771) |  Loss2: (0.2087) | Acc: (94.00%) (13374/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.3791) |  Loss2: (0.2087) | Acc: (94.00%) (14560/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.3805) |  Loss2: (0.2087) | Acc: (93.00%) (15756/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.3785) |  Loss2: (0.2086) | Acc: (94.00%) (16978/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.3777) |  Loss2: (0.2086) | Acc: (94.00%) (18187/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.3777) |  Loss2: (0.2085) | Acc: (94.00%) (19389/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.3784) |  Loss2: (0.2085) | Acc: (94.00%) (20589/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.3764) |  Loss2: (0.2085) | Acc: (94.00%) (21811/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.3760) |  Loss2: (0.2084) | Acc: (94.00%) (23020/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.3757) |  Loss2: (0.2084) | Acc: (94.00%) (24232/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.3756) |  Loss2: (0.2083) | Acc: (94.00%) (25447/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.3755) |  Loss2: (0.2083) | Acc: (94.00%) (26666/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.3752) |  Loss2: (0.2082) | Acc: (94.00%) (27870/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.3751) |  Loss2: (0.2082) | Acc: (94.00%) (29075/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.3735) |  Loss2: (0.2081) | Acc: (94.00%) (30299/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.3742) |  Loss2: (0.2081) | Acc: (94.00%) (31509/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.3740) |  Loss2: (0.2081) | Acc: (94.00%) (32721/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.3734) |  Loss2: (0.2080) | Acc: (94.00%) (33940/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.3734) |  Loss2: (0.2080) | Acc: (94.00%) (35141/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.3728) |  Loss2: (0.2079) | Acc: (94.00%) (36353/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.3727) |  Loss2: (0.2079) | Acc: (94.00%) (37558/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.3732) |  Loss2: (0.2078) | Acc: (94.00%) (38753/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.3726) |  Loss2: (0.2078) | Acc: (94.00%) (39966/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.3721) |  Loss2: (0.2077) | Acc: (94.00%) (41190/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.3722) |  Loss2: (0.2077) | Acc: (94.00%) (42390/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.3725) |  Loss2: (0.2076) | Acc: (94.00%) (43587/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.3728) |  Loss2: (0.2076) | Acc: (94.00%) (44783/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.3724) |  Loss2: (0.2076) | Acc: (94.00%) (45995/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.3725) |  Loss2: (0.2075) | Acc: (94.00%) (47150/50000)
# TEST : Loss: (0.4065) | Acc: (87.00%) (8754/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1298,  0.0687,  0.0968],
          [ 0.1117, -0.3138, -0.1194],
          [ 0.2682, -0.0781,  0.0047]],

         [[-0.0084, -0.0836,  0.2707],
          [ 0.0225, -0.1129,  0.0437],
          [ 0.0741,  0.0114, -0.0696]],

         [[ 0.0102, -0.0355,  0.2074],
          [ 0.0903, -0.2058,  0.1608],
          [-0.1089,  0.0218, -0.1881]]],


        [[[ 0.0986, -0.0726, -0.0035],
          [ 0.1382,  0.1758,  0.1801],
          [ 0.0141, -0.0604,  0.1160]],

         [[-0.1523, -0.1769,  0.1524],
          [ 0.0326, -0.0476,  0.1000],
          [ 0.1223, -0.2009,  0.1295]],

         [[ 0.0980, -0.0502, -0.0160],
          [ 0.0087,  0.0177, -0.1227],
          [-0.0311, -0.1609, -0.0228]]],


        [[[-0.0722,  0.0877, -0.0431],
          [-0.1255,  0.0640,  0.1146],
          [ 0.1086,  0.1649,  0.0013]],

         [[ 0.0984,  0.1310,  0.1115],
          [ 0.0678, -0.0384, -0.0359],
          [-0.0474, -0.0297,  0.1372]],

         [[-0.0444,  0.1430,  0.0729],
          [-0.0421, -0.0454, -0.0603],
          [-0.0395, -0.0081,  0.0412]]],


        ...,


        [[[ 0.2262, -0.1145, -0.1220],
          [-0.0506, -0.1155, -0.0933],
          [-0.0935,  0.0251,  0.0734]],

         [[ 0.1658,  0.1919, -0.1188],
          [ 0.2032, -0.1409, -0.1129],
          [-0.0589, -0.0750, -0.0253]],

         [[ 0.1806,  0.1008, -0.1069],
          [-0.0660, -0.1547, -0.0752],
          [ 0.0718,  0.1800, -0.1260]]],


        [[[-0.1056,  0.0868, -0.1451],
          [-0.1444, -0.1474,  0.1163],
          [-0.1468,  0.1535, -0.1333]],

         [[ 0.0976, -0.1118,  0.0892],
          [ 0.1491,  0.0045, -0.0795],
          [-0.1356,  0.1213, -0.1908]],

         [[ 0.1280, -0.0164,  0.0041],
          [ 0.1417, -0.1721,  0.0156],
          [-0.0177,  0.0223, -0.1174]]],


        [[[ 0.0702,  0.1679,  0.1126],
          [-0.0853, -0.1977,  0.2188],
          [-0.1779, -0.0309, -0.0755]],

         [[-0.0198, -0.0707,  0.1501],
          [ 0.0056, -0.2223,  0.0611],
          [-0.0083,  0.0735,  0.0235]],

         [[-0.1734, -0.2030,  0.1132],
          [ 0.1471, -0.2038, -0.0094],
          [ 0.0086, -0.0158,  0.2926]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2981e-05,  6.8735e-06,  9.6756e-06],
          [ 1.1170e-05, -3.1378e-05, -1.1942e-05],
          [ 2.6823e-05, -7.8125e-06,  4.6648e-07]],

         [[-8.3582e-07, -8.3624e-06,  2.7068e-05],
          [ 2.2509e-06, -1.1291e-05,  4.3739e-06],
          [ 7.4106e-06,  1.1413e-06, -6.9588e-06]],

         [[ 1.0202e-06, -3.5490e-06,  2.0741e-05],
          [ 9.0281e-06, -2.0584e-05,  1.6082e-05],
          [-1.0888e-05,  2.1791e-06, -1.8812e-05]]],


        [[[ 9.8648e-06, -7.2638e-06, -3.4675e-07],
          [ 1.3825e-05,  1.7580e-05,  1.8010e-05],
          [ 1.4078e-06, -6.0352e-06,  1.1605e-05]],

         [[-1.5234e-05, -1.7688e-05,  1.5243e-05],
          [ 3.2553e-06, -4.7632e-06,  1.0005e-05],
          [ 1.2227e-05, -2.0089e-05,  1.2954e-05]],

         [[ 9.7970e-06, -5.0200e-06, -1.5962e-06],
          [ 8.6619e-07,  1.7659e-06, -1.2267e-05],
          [-3.1127e-06, -1.6093e-05, -2.2842e-06]]],


        [[[-7.2221e-06,  8.7650e-06, -4.3096e-06],
          [-1.2548e-05,  6.4000e-06,  1.1465e-05],
          [ 1.0857e-05,  1.6485e-05,  1.3195e-07]],

         [[ 9.8415e-06,  1.3102e-05,  1.1154e-05],
          [ 6.7842e-06, -3.8439e-06, -3.5938e-06],
          [-4.7373e-06, -2.9722e-06,  1.3716e-05]],

         [[-4.4391e-06,  1.4296e-05,  7.2909e-06],
          [-4.2066e-06, -4.5438e-06, -6.0335e-06],
          [-3.9461e-06, -8.0678e-07,  4.1229e-06]]],


        ...,


        [[[ 2.2623e-05, -1.1447e-05, -1.2203e-05],
          [-5.0586e-06, -1.1551e-05, -9.3257e-06],
          [-9.3482e-06,  2.5092e-06,  7.3418e-06]],

         [[ 1.6579e-05,  1.9195e-05, -1.1879e-05],
          [ 2.0324e-05, -1.4091e-05, -1.1294e-05],
          [-5.8930e-06, -7.5018e-06, -2.5254e-06]],

         [[ 1.8055e-05,  1.0083e-05, -1.0690e-05],
          [-6.6045e-06, -1.5466e-05, -7.5249e-06],
          [ 7.1843e-06,  1.7997e-05, -1.2598e-05]]],


        [[[-1.0562e-05,  8.6831e-06, -1.4507e-05],
          [-1.4442e-05, -1.4739e-05,  1.1628e-05],
          [-1.4676e-05,  1.5347e-05, -1.3332e-05]],

         [[ 9.7648e-06, -1.1184e-05,  8.9166e-06],
          [ 1.4906e-05,  4.5156e-07, -7.9462e-06],
          [-1.3562e-05,  1.2131e-05, -1.9076e-05]],

         [[ 1.2805e-05, -1.6418e-06,  4.1331e-07],
          [ 1.4174e-05, -1.7215e-05,  1.5577e-06],
          [-1.7718e-06,  2.2349e-06, -1.1745e-05]]],


        [[[ 7.0166e-06,  1.6788e-05,  1.1257e-05],
          [-8.5321e-06, -1.9769e-05,  2.1879e-05],
          [-1.7794e-05, -3.0921e-06, -7.5541e-06]],

         [[-1.9810e-06, -7.0740e-06,  1.5010e-05],
          [ 5.5928e-07, -2.2226e-05,  6.1112e-06],
          [-8.3127e-07,  7.3485e-06,  2.3479e-06]],

         [[-1.7344e-05, -2.0297e-05,  1.1325e-05],
          [ 1.4707e-05, -2.0381e-05, -9.4199e-07],
          [ 8.6103e-07, -1.5781e-06,  2.9258e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.1820]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0374]], device='cuda:0')

Epoch: 128 | Batch_idx: 0 |  Loss: (0.4171) |  Loss2: (0.2056) | Acc: (91.00%) (117/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.3837) |  Loss2: (0.2056) | Acc: (94.00%) (1329/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.3701) |  Loss2: (0.2055) | Acc: (94.00%) (2541/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.3676) |  Loss2: (0.2055) | Acc: (94.00%) (3745/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.3733) |  Loss2: (0.2055) | Acc: (94.00%) (4942/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.3724) |  Loss2: (0.2054) | Acc: (94.00%) (6148/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.3717) |  Loss2: (0.2054) | Acc: (94.00%) (7360/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.3752) |  Loss2: (0.2054) | Acc: (94.00%) (8548/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.3770) |  Loss2: (0.2053) | Acc: (94.00%) (9760/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.3776) |  Loss2: (0.2053) | Acc: (94.00%) (10958/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.3785) |  Loss2: (0.2052) | Acc: (94.00%) (12164/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.3798) |  Loss2: (0.2052) | Acc: (94.00%) (13356/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.3801) |  Loss2: (0.2052) | Acc: (94.00%) (14561/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.3781) |  Loss2: (0.2051) | Acc: (94.00%) (15773/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.3777) |  Loss2: (0.2051) | Acc: (94.00%) (16989/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.3773) |  Loss2: (0.2050) | Acc: (94.00%) (18202/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.3767) |  Loss2: (0.2050) | Acc: (94.00%) (19407/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.3778) |  Loss2: (0.2050) | Acc: (94.00%) (20594/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.3776) |  Loss2: (0.2049) | Acc: (94.00%) (21798/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.3766) |  Loss2: (0.2049) | Acc: (94.00%) (23015/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.3760) |  Loss2: (0.2048) | Acc: (94.00%) (24233/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.3755) |  Loss2: (0.2048) | Acc: (94.00%) (25443/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.3747) |  Loss2: (0.2048) | Acc: (94.00%) (26652/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.3743) |  Loss2: (0.2047) | Acc: (94.00%) (27862/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.3736) |  Loss2: (0.2047) | Acc: (94.00%) (29072/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.3731) |  Loss2: (0.2046) | Acc: (94.00%) (30282/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.3726) |  Loss2: (0.2046) | Acc: (94.00%) (31493/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.3724) |  Loss2: (0.2045) | Acc: (94.00%) (32699/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.3724) |  Loss2: (0.2045) | Acc: (94.00%) (33902/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.3720) |  Loss2: (0.2045) | Acc: (94.00%) (35113/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.3721) |  Loss2: (0.2044) | Acc: (94.00%) (36314/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.3713) |  Loss2: (0.2044) | Acc: (94.00%) (37529/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.3710) |  Loss2: (0.2043) | Acc: (94.00%) (38739/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.3715) |  Loss2: (0.2043) | Acc: (94.00%) (39938/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.3715) |  Loss2: (0.2042) | Acc: (94.00%) (41139/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.3712) |  Loss2: (0.2042) | Acc: (94.00%) (42348/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.3719) |  Loss2: (0.2041) | Acc: (94.00%) (43539/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.3720) |  Loss2: (0.2041) | Acc: (94.00%) (44748/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.3715) |  Loss2: (0.2041) | Acc: (94.00%) (45974/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.3715) |  Loss2: (0.2040) | Acc: (94.00%) (47137/50000)
# TEST : Loss: (0.4020) | Acc: (87.00%) (8775/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1298,  0.0687,  0.0967],
          [ 0.1117, -0.3137, -0.1194],
          [ 0.2681, -0.0781,  0.0047]],

         [[-0.0084, -0.0836,  0.2706],
          [ 0.0225, -0.1129,  0.0437],
          [ 0.0741,  0.0114, -0.0696]],

         [[ 0.0102, -0.0355,  0.2073],
          [ 0.0902, -0.2058,  0.1608],
          [-0.1088,  0.0218, -0.1880]]],


        [[[ 0.0986, -0.0726, -0.0035],
          [ 0.1382,  0.1757,  0.1800],
          [ 0.0141, -0.0603,  0.1160]],

         [[-0.1523, -0.1768,  0.1524],
          [ 0.0325, -0.0476,  0.1000],
          [ 0.1222, -0.2008,  0.1295]],

         [[ 0.0979, -0.0502, -0.0160],
          [ 0.0087,  0.0177, -0.1226],
          [-0.0311, -0.1609, -0.0228]]],


        [[[-0.0722,  0.0876, -0.0431],
          [-0.1254,  0.0640,  0.1146],
          [ 0.1085,  0.1648,  0.0013]],

         [[ 0.0984,  0.1310,  0.1115],
          [ 0.0678, -0.0384, -0.0359],
          [-0.0474, -0.0297,  0.1371]],

         [[-0.0444,  0.1429,  0.0729],
          [-0.0420, -0.0454, -0.0603],
          [-0.0394, -0.0081,  0.0412]]],


        ...,


        [[[ 0.2261, -0.1144, -0.1220],
          [-0.0506, -0.1155, -0.0932],
          [-0.0934,  0.0251,  0.0734]],

         [[ 0.1657,  0.1919, -0.1187],
          [ 0.2032, -0.1409, -0.1129],
          [-0.0589, -0.0750, -0.0252]],

         [[ 0.1805,  0.1008, -0.1069],
          [-0.0660, -0.1546, -0.0752],
          [ 0.0718,  0.1799, -0.1259]]],


        [[[-0.1056,  0.0868, -0.1450],
          [-0.1444, -0.1473,  0.1162],
          [-0.1467,  0.1534, -0.1333]],

         [[ 0.0976, -0.1118,  0.0891],
          [ 0.1490,  0.0045, -0.0794],
          [-0.1356,  0.1213, -0.1907]],

         [[ 0.1280, -0.0164,  0.0041],
          [ 0.1417, -0.1721,  0.0156],
          [-0.0177,  0.0223, -0.1174]]],


        [[[ 0.0701,  0.1678,  0.1125],
          [-0.0853, -0.1976,  0.2187],
          [-0.1779, -0.0309, -0.0755]],

         [[-0.0198, -0.0707,  0.1500],
          [ 0.0056, -0.2222,  0.0611],
          [-0.0083,  0.0735,  0.0235]],

         [[-0.1734, -0.2029,  0.1132],
          [ 0.1470, -0.2037, -0.0094],
          [ 0.0086, -0.0158,  0.2925]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2976e-05,  6.8708e-06,  9.6718e-06],
          [ 1.1166e-05, -3.1365e-05, -1.1938e-05],
          [ 2.6812e-05, -7.8096e-06,  4.6629e-07]],

         [[-8.3549e-07, -8.3592e-06,  2.7057e-05],
          [ 2.2500e-06, -1.1287e-05,  4.3722e-06],
          [ 7.4076e-06,  1.1409e-06, -6.9562e-06]],

         [[ 1.0198e-06, -3.5475e-06,  2.0732e-05],
          [ 9.0246e-06, -2.0576e-05,  1.6075e-05],
          [-1.0883e-05,  2.1782e-06, -1.8805e-05]]],


        [[[ 9.8610e-06, -7.2609e-06, -3.4661e-07],
          [ 1.3819e-05,  1.7573e-05,  1.8003e-05],
          [ 1.4073e-06, -6.0329e-06,  1.1600e-05]],

         [[-1.5228e-05, -1.7681e-05,  1.5238e-05],
          [ 3.2540e-06, -4.7613e-06,  1.0001e-05],
          [ 1.2223e-05, -2.0081e-05,  1.2948e-05]],

         [[ 9.7933e-06, -5.0181e-06, -1.5955e-06],
          [ 8.6587e-07,  1.7653e-06, -1.2262e-05],
          [-3.1115e-06, -1.6086e-05, -2.2833e-06]]],


        [[[-7.2192e-06,  8.7615e-06, -4.3078e-06],
          [-1.2543e-05,  6.3974e-06,  1.1460e-05],
          [ 1.0853e-05,  1.6479e-05,  1.3190e-07]],

         [[ 9.8377e-06,  1.3097e-05,  1.1149e-05],
          [ 6.7816e-06, -3.8424e-06, -3.5924e-06],
          [-4.7354e-06, -2.9711e-06,  1.3711e-05]],

         [[-4.4373e-06,  1.4290e-05,  7.2880e-06],
          [-4.2050e-06, -4.5421e-06, -6.0312e-06],
          [-3.9445e-06, -8.0645e-07,  4.1213e-06]]],


        ...,


        [[[ 2.2614e-05, -1.1442e-05, -1.2198e-05],
          [-5.0566e-06, -1.1547e-05, -9.3219e-06],
          [-9.3444e-06,  2.5083e-06,  7.3389e-06]],

         [[ 1.6573e-05,  1.9187e-05, -1.1875e-05],
          [ 2.0316e-05, -1.4086e-05, -1.1290e-05],
          [-5.8907e-06, -7.4989e-06, -2.5244e-06]],

         [[ 1.8048e-05,  1.0079e-05, -1.0685e-05],
          [-6.6019e-06, -1.5460e-05, -7.5220e-06],
          [ 7.1814e-06,  1.7990e-05, -1.2593e-05]]],


        [[[-1.0558e-05,  8.6796e-06, -1.4502e-05],
          [-1.4436e-05, -1.4733e-05,  1.1623e-05],
          [-1.4671e-05,  1.5341e-05, -1.3327e-05]],

         [[ 9.7610e-06, -1.1180e-05,  8.9131e-06],
          [ 1.4900e-05,  4.5137e-07, -7.9429e-06],
          [-1.3556e-05,  1.2126e-05, -1.9068e-05]],

         [[ 1.2800e-05, -1.6411e-06,  4.1314e-07],
          [ 1.4168e-05, -1.7208e-05,  1.5571e-06],
          [-1.7711e-06,  2.2340e-06, -1.1740e-05]]],


        [[[ 7.0139e-06,  1.6782e-05,  1.1252e-05],
          [-8.5289e-06, -1.9762e-05,  2.1870e-05],
          [-1.7787e-05, -3.0908e-06, -7.5512e-06]],

         [[-1.9802e-06, -7.0714e-06,  1.5004e-05],
          [ 5.5906e-07, -2.2217e-05,  6.1089e-06],
          [-8.3094e-07,  7.3456e-06,  2.3470e-06]],

         [[-1.7337e-05, -2.0288e-05,  1.1320e-05],
          [ 1.4701e-05, -2.0373e-05, -9.4163e-07],
          [ 8.6070e-07, -1.5775e-06,  2.9246e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2142]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0047]], device='cuda:0')

Epoch: 129 | Batch_idx: 0 |  Loss: (0.3809) |  Loss2: (0.2024) | Acc: (91.00%) (117/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.3602) |  Loss2: (0.2023) | Acc: (93.00%) (1322/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.3600) |  Loss2: (0.2023) | Acc: (94.00%) (2536/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.3557) |  Loss2: (0.2022) | Acc: (94.00%) (3752/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.3538) |  Loss2: (0.2022) | Acc: (94.00%) (4978/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.3509) |  Loss2: (0.2021) | Acc: (94.00%) (6195/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.3525) |  Loss2: (0.2021) | Acc: (94.00%) (7404/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.3539) |  Loss2: (0.2021) | Acc: (94.00%) (8613/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.3567) |  Loss2: (0.2020) | Acc: (94.00%) (9819/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.3572) |  Loss2: (0.2020) | Acc: (94.00%) (11033/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.3575) |  Loss2: (0.2019) | Acc: (94.00%) (12253/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.3584) |  Loss2: (0.2019) | Acc: (94.00%) (13457/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.3575) |  Loss2: (0.2019) | Acc: (94.00%) (14674/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.3576) |  Loss2: (0.2018) | Acc: (94.00%) (15883/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.3570) |  Loss2: (0.2018) | Acc: (94.00%) (17095/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.3568) |  Loss2: (0.2018) | Acc: (94.00%) (18312/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.3564) |  Loss2: (0.2017) | Acc: (94.00%) (19523/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.3571) |  Loss2: (0.2017) | Acc: (94.00%) (20730/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.3582) |  Loss2: (0.2017) | Acc: (94.00%) (21939/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.3581) |  Loss2: (0.2016) | Acc: (94.00%) (23158/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.3579) |  Loss2: (0.2016) | Acc: (94.00%) (24373/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.3583) |  Loss2: (0.2015) | Acc: (94.00%) (25579/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.3587) |  Loss2: (0.2015) | Acc: (94.00%) (26786/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.3587) |  Loss2: (0.2015) | Acc: (94.00%) (27996/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.3594) |  Loss2: (0.2014) | Acc: (94.00%) (29203/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.3592) |  Loss2: (0.2014) | Acc: (94.00%) (30405/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.3596) |  Loss2: (0.2013) | Acc: (94.00%) (31614/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.3592) |  Loss2: (0.2013) | Acc: (94.00%) (32831/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.3591) |  Loss2: (0.2013) | Acc: (94.00%) (34051/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.3584) |  Loss2: (0.2012) | Acc: (94.00%) (35271/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.3588) |  Loss2: (0.2012) | Acc: (94.00%) (36476/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.3581) |  Loss2: (0.2012) | Acc: (94.00%) (37701/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.3583) |  Loss2: (0.2011) | Acc: (94.00%) (38908/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.3586) |  Loss2: (0.2011) | Acc: (94.00%) (40118/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.3588) |  Loss2: (0.2010) | Acc: (94.00%) (41323/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.3594) |  Loss2: (0.2010) | Acc: (94.00%) (42514/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.3591) |  Loss2: (0.2010) | Acc: (94.00%) (43728/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.3590) |  Loss2: (0.2009) | Acc: (94.00%) (44941/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.3591) |  Loss2: (0.2009) | Acc: (94.00%) (46149/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.3591) |  Loss2: (0.2009) | Acc: (94.00%) (47320/50000)
# TEST : Loss: (0.3934) | Acc: (87.00%) (8791/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1297,  0.0687,  0.0967],
          [ 0.1116, -0.3135, -0.1193],
          [ 0.2680, -0.0781,  0.0047]],

         [[-0.0084, -0.0836,  0.2705],
          [ 0.0225, -0.1128,  0.0437],
          [ 0.0740,  0.0114, -0.0695]],

         [[ 0.0102, -0.0355,  0.2072],
          [ 0.0902, -0.2057,  0.1607],
          [-0.1088,  0.0218, -0.1880]]],


        [[[ 0.0986, -0.0726, -0.0035],
          [ 0.1381,  0.1757,  0.1800],
          [ 0.0141, -0.0603,  0.1160]],

         [[-0.1522, -0.1767,  0.1523],
          [ 0.0325, -0.0476,  0.1000],
          [ 0.1222, -0.2007,  0.1294]],

         [[ 0.0979, -0.0502, -0.0159],
          [ 0.0087,  0.0176, -0.1226],
          [-0.0311, -0.1608, -0.0228]]],


        [[[-0.0722,  0.0876, -0.0431],
          [-0.1254,  0.0639,  0.1146],
          [ 0.1085,  0.1647,  0.0013]],

         [[ 0.0983,  0.1309,  0.1114],
          [ 0.0678, -0.0384, -0.0359],
          [-0.0473, -0.0297,  0.1371]],

         [[-0.0444,  0.1428,  0.0729],
          [-0.0420, -0.0454, -0.0603],
          [-0.0394, -0.0081,  0.0412]]],


        ...,


        [[[ 0.2261, -0.1144, -0.1219],
          [-0.0505, -0.1154, -0.0932],
          [-0.0934,  0.0251,  0.0734]],

         [[ 0.1657,  0.1918, -0.1187],
          [ 0.2031, -0.1408, -0.1129],
          [-0.0589, -0.0750, -0.0252]],

         [[ 0.1804,  0.1007, -0.1068],
          [-0.0660, -0.1545, -0.0752],
          [ 0.0718,  0.1798, -0.1259]]],


        [[[-0.1055,  0.0868, -0.1450],
          [-0.1443, -0.1473,  0.1162],
          [-0.1466,  0.1534, -0.1332]],

         [[ 0.0976, -0.1118,  0.0891],
          [ 0.1489,  0.0045, -0.0794],
          [-0.1355,  0.1212, -0.1906]],

         [[ 0.1279, -0.0164,  0.0041],
          [ 0.1416, -0.1720,  0.0156],
          [-0.0177,  0.0223, -0.1174]]],


        [[[ 0.0701,  0.1678,  0.1125],
          [-0.0853, -0.1975,  0.2186],
          [-0.1778, -0.0309, -0.0755]],

         [[-0.0198, -0.0707,  0.1500],
          [ 0.0056, -0.2221,  0.0611],
          [-0.0083,  0.0734,  0.0235]],

         [[-0.1733, -0.2028,  0.1132],
          [ 0.1470, -0.2036, -0.0094],
          [ 0.0086, -0.0158,  0.2923]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2970e-05,  6.8682e-06,  9.6680e-06],
          [ 1.1162e-05, -3.1352e-05, -1.1933e-05],
          [ 2.6802e-05, -7.8067e-06,  4.6611e-07]],

         [[-8.3516e-07, -8.3560e-06,  2.7047e-05],
          [ 2.2492e-06, -1.1282e-05,  4.3704e-06],
          [ 7.4047e-06,  1.1404e-06, -6.9536e-06]],

         [[ 1.0194e-06, -3.5460e-06,  2.0724e-05],
          [ 9.0211e-06, -2.0567e-05,  1.6069e-05],
          [-1.0879e-05,  2.1773e-06, -1.8797e-05]]],


        [[[ 9.8572e-06, -7.2580e-06, -3.4647e-07],
          [ 1.3814e-05,  1.7566e-05,  1.7996e-05],
          [ 1.4067e-06, -6.0306e-06,  1.1596e-05]],

         [[-1.5222e-05, -1.7674e-05,  1.5232e-05],
          [ 3.2527e-06, -4.7594e-06,  9.9974e-06],
          [ 1.2218e-05, -2.0074e-05,  1.2943e-05]],

         [[ 9.7895e-06, -5.0163e-06, -1.5949e-06],
          [ 8.6554e-07,  1.7646e-06, -1.2257e-05],
          [-3.1102e-06, -1.6080e-05, -2.2824e-06]]],


        [[[-7.2163e-06,  8.7580e-06, -4.3061e-06],
          [-1.2539e-05,  6.3948e-06,  1.1456e-05],
          [ 1.0848e-05,  1.6472e-05,  1.3185e-07]],

         [[ 9.8339e-06,  1.3092e-05,  1.1145e-05],
          [ 6.7790e-06, -3.8410e-06, -3.5909e-06],
          [-4.7335e-06, -2.9699e-06,  1.3706e-05]],

         [[-4.4356e-06,  1.4285e-05,  7.2851e-06],
          [-4.2034e-06, -4.5403e-06, -6.0289e-06],
          [-3.9429e-06, -8.0612e-07,  4.1197e-06]]],


        ...,


        [[[ 2.2605e-05, -1.1438e-05, -1.2193e-05],
          [-5.0545e-06, -1.1542e-05, -9.3181e-06],
          [-9.3406e-06,  2.5073e-06,  7.3359e-06]],

         [[ 1.6567e-05,  1.9180e-05, -1.1870e-05],
          [ 2.0307e-05, -1.4080e-05, -1.1286e-05],
          [-5.8884e-06, -7.4959e-06, -2.5234e-06]],

         [[ 1.8041e-05,  1.0075e-05, -1.0681e-05],
          [-6.5993e-06, -1.5454e-05, -7.5191e-06],
          [ 7.1785e-06,  1.7983e-05, -1.2589e-05]]],


        [[[-1.0554e-05,  8.6761e-06, -1.4496e-05],
          [-1.4430e-05, -1.4728e-05,  1.1618e-05],
          [-1.4665e-05,  1.5335e-05, -1.3321e-05]],

         [[ 9.7572e-06, -1.1176e-05,  8.9096e-06],
          [ 1.4894e-05,  4.5119e-07, -7.9397e-06],
          [-1.3551e-05,  1.2121e-05, -1.9061e-05]],

         [[ 1.2794e-05, -1.6405e-06,  4.1298e-07],
          [ 1.4162e-05, -1.7201e-05,  1.5564e-06],
          [-1.7703e-06,  2.2332e-06, -1.1735e-05]]],


        [[[ 7.0113e-06,  1.6775e-05,  1.1248e-05],
          [-8.5257e-06, -1.9754e-05,  2.1861e-05],
          [-1.7780e-05, -3.0896e-06, -7.5483e-06]],

         [[-1.9794e-06, -7.0688e-06,  1.4999e-05],
          [ 5.5884e-07, -2.2209e-05,  6.1065e-06],
          [-8.3062e-07,  7.3427e-06,  2.3460e-06]],

         [[-1.7330e-05, -2.0280e-05,  1.1316e-05],
          [ 1.4695e-05, -2.0365e-05, -9.4126e-07],
          [ 8.6037e-07, -1.5769e-06,  2.9234e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2372]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0198]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (2538/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (6151/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1664) |  Loss2: (0.0000) | Acc: (94.00%) (7359/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (8576/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (9781/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (10966/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (12164/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (13371/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (14573/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (15785/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (16982/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1692) |  Loss2: (0.0000) | Acc: (94.00%) (18180/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (19382/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (20580/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (21782/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (22991/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (24194/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (93.00%) (25373/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (93.00%) (26569/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (27772/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (93.00%) (28972/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (30165/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (93.00%) (31387/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (93.00%) (32588/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (33803/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (93.00%) (35008/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (36218/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (37441/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (38647/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (39829/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (41019/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (93.00%) (42216/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (43416/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (44615/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (45821/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (46976/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_130.pth.tar'
# TEST : Loss: (0.4323) | Acc: (87.00%) (8707/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1337,  0.0662,  0.0971],
          [ 0.1071, -0.3170, -0.1190],
          [ 0.2655, -0.0781,  0.0049]],

         [[-0.0110, -0.0848,  0.2709],
          [ 0.0191, -0.1152,  0.0439],
          [ 0.0716,  0.0106, -0.0710]],

         [[ 0.0087, -0.0362,  0.2072],
          [ 0.0873, -0.2082,  0.1596],
          [-0.1107,  0.0200, -0.1914]]],


        [[[ 0.1015, -0.0692, -0.0010],
          [ 0.1406,  0.1787,  0.1824],
          [ 0.0171, -0.0566,  0.1191]],

         [[-0.1484, -0.1728,  0.1550],
          [ 0.0355, -0.0442,  0.1024],
          [ 0.1256, -0.1968,  0.1324]],

         [[ 0.1021, -0.0458, -0.0122],
          [ 0.0121,  0.0213, -0.1196],
          [-0.0267, -0.1563, -0.0194]]],


        [[[-0.0717,  0.0878, -0.0431],
          [-0.1252,  0.0640,  0.1145],
          [ 0.1083,  0.1646,  0.0012]],

         [[ 0.0986,  0.1310,  0.1113],
          [ 0.0679, -0.0384, -0.0360],
          [-0.0474, -0.0297,  0.1369]],

         [[-0.0444,  0.1427,  0.0726],
          [-0.0423, -0.0456, -0.0606],
          [-0.0398, -0.0084,  0.0407]]],


        ...,


        [[[ 0.2256, -0.1151, -0.1221],
          [-0.0517, -0.1164, -0.0934],
          [-0.0958,  0.0230,  0.0720]],

         [[ 0.1654,  0.1914, -0.1181],
          [ 0.2024, -0.1413, -0.1128],
          [-0.0603, -0.0761, -0.0259]],

         [[ 0.1825,  0.1023, -0.1050],
          [-0.0641, -0.1533, -0.0739],
          [ 0.0724,  0.1798, -0.1255]]],


        [[[-0.1058,  0.0860, -0.1449],
          [-0.1447, -0.1485,  0.1161],
          [-0.1465,  0.1528, -0.1327]],

         [[ 0.0962, -0.1137,  0.0876],
          [ 0.1474,  0.0020, -0.0809],
          [-0.1364,  0.1195, -0.1914]],

         [[ 0.1269, -0.0177,  0.0028],
          [ 0.1408, -0.1735,  0.0146],
          [-0.0178,  0.0215, -0.1176]]],


        [[[ 0.0692,  0.1696,  0.1146],
          [-0.0848, -0.1975,  0.2204],
          [-0.1779, -0.0317, -0.0746]],

         [[-0.0183, -0.0676,  0.1524],
          [ 0.0076, -0.2208,  0.0632],
          [-0.0067,  0.0755,  0.0262]],

         [[-0.1714, -0.1997,  0.1151],
          [ 0.1498, -0.2021, -0.0077],
          [ 0.0118, -0.0125,  0.2954]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0331, -0.0566, -0.0576],
          [-0.0531, -0.0628, -0.0530],
          [-0.0327, -0.0454, -0.0353]],

         [[-0.0234, -0.0397, -0.0468],
          [-0.0418, -0.0429, -0.0382],
          [-0.0246, -0.0278, -0.0221]],

         [[-0.0318, -0.0488, -0.0733],
          [-0.0583, -0.0606, -0.0681],
          [-0.0451, -0.0507, -0.0518]]],


        [[[-0.0191, -0.0121, -0.0060],
          [-0.0144, -0.0063,  0.0011],
          [-0.0082, -0.0005,  0.0066]],

         [[-0.0174, -0.0143, -0.0078],
          [-0.0099, -0.0054,  0.0018],
          [-0.0067, -0.0006,  0.0075]],

         [[-0.0206, -0.0161, -0.0074],
          [-0.0141, -0.0095, -0.0010],
          [-0.0081, -0.0039,  0.0034]]],


        [[[-0.0004, -0.0008, -0.0025],
          [-0.0004, -0.0004, -0.0014],
          [ 0.0002,  0.0001, -0.0009]],

         [[ 0.0008,  0.0016, -0.0001],
          [ 0.0019,  0.0033,  0.0024],
          [ 0.0037,  0.0045,  0.0034]],

         [[ 0.0011,  0.0017, -0.0003],
          [ 0.0024,  0.0033,  0.0018],
          [ 0.0047,  0.0042,  0.0023]]],


        ...,


        [[[ 0.0039, -0.0007,  0.0058],
          [ 0.0038, -0.0026,  0.0117],
          [-0.0085, -0.0137,  0.0018]],

         [[ 0.0011, -0.0034, -0.0045],
          [ 0.0036, -0.0032,  0.0032],
          [-0.0042, -0.0111, -0.0029]],

         [[ 0.0011, -0.0004,  0.0005],
          [ 0.0073,  0.0034,  0.0105],
          [ 0.0076,  0.0023,  0.0095]]],


        [[[ 0.0131,  0.0089, -0.0023],
          [ 0.0123,  0.0082,  0.0006],
          [ 0.0103,  0.0065,  0.0031]],

         [[ 0.0096,  0.0042, -0.0097],
          [ 0.0098,  0.0052, -0.0032],
          [ 0.0063,  0.0024, -0.0017]],

         [[ 0.0145,  0.0052, -0.0116],
          [ 0.0122,  0.0047, -0.0070],
          [ 0.0063, -0.0000, -0.0061]]],


        [[[ 0.0640,  0.0809,  0.0813],
          [ 0.0612,  0.0628,  0.0416],
          [ 0.0633,  0.0535,  0.0272]],

         [[ 0.0545,  0.0815,  0.0962],
          [ 0.0563,  0.0615,  0.0533],
          [ 0.0578,  0.0428,  0.0236]],

         [[ 0.0264,  0.0611,  0.0883],
          [ 0.0464,  0.0502,  0.0427],
          [ 0.0638,  0.0397,  0.0135]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2366]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 131 | Batch_idx: 0 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1688) |  Loss2: (0.0000) | Acc: (94.00%) (7363/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (8575/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (9782/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (10992/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (12210/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (13416/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (14638/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (15836/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (17046/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (18261/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (19470/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (20678/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (21893/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (23098/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (24292/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (25506/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (26718/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (27927/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (29123/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (30322/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (31517/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (32715/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (33918/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1657) |  Loss2: (0.0000) | Acc: (94.00%) (35135/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (36324/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (37533/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1670) |  Loss2: (0.0000) | Acc: (94.00%) (38743/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (39937/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (41149/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1681) |  Loss2: (0.0000) | Acc: (94.00%) (42342/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (43549/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1689) |  Loss2: (0.0000) | Acc: (94.00%) (44747/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1687) |  Loss2: (0.0000) | Acc: (94.00%) (45948/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (47114/50000)
# TEST : Loss: (0.4471) | Acc: (86.00%) (8657/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1356,  0.0651,  0.0966],
          [ 0.1070, -0.3183, -0.1197],
          [ 0.2654, -0.0810,  0.0040]],

         [[-0.0133, -0.0846,  0.2712],
          [ 0.0181, -0.1157,  0.0432],
          [ 0.0695,  0.0071, -0.0731]],

         [[ 0.0064, -0.0368,  0.2064],
          [ 0.0854, -0.2092,  0.1581],
          [-0.1135,  0.0168, -0.1935]]],


        [[[ 0.0998, -0.0719, -0.0036],
          [ 0.1392,  0.1758,  0.1797],
          [ 0.0145, -0.0607,  0.1155]],

         [[-0.1502, -0.1758,  0.1522],
          [ 0.0339, -0.0474,  0.0995],
          [ 0.1227, -0.2011,  0.1288]],

         [[ 0.0991, -0.0496, -0.0153],
          [ 0.0092,  0.0171, -0.1229],
          [-0.0305, -0.1612, -0.0233]]],


        [[[-0.0721,  0.0877, -0.0429],
          [-0.1256,  0.0638,  0.1145],
          [ 0.1077,  0.1641,  0.0011]],

         [[ 0.0983,  0.1310,  0.1117],
          [ 0.0677, -0.0381, -0.0355],
          [-0.0475, -0.0297,  0.1371]],

         [[-0.0449,  0.1425,  0.0728],
          [-0.0425, -0.0456, -0.0602],
          [-0.0398, -0.0083,  0.0411]]],


        ...,


        [[[ 0.2256, -0.1144, -0.1216],
          [-0.0521, -0.1165, -0.0932],
          [-0.0960,  0.0230,  0.0728]],

         [[ 0.1660,  0.1925, -0.1170],
          [ 0.2026, -0.1409, -0.1122],
          [-0.0596, -0.0755, -0.0247]],

         [[ 0.1838,  0.1039, -0.1037],
          [-0.0633, -0.1525, -0.0730],
          [ 0.0735,  0.1809, -0.1238]]],


        [[[-0.1057,  0.0864, -0.1456],
          [-0.1450, -0.1482,  0.1160],
          [-0.1465,  0.1525, -0.1330]],

         [[ 0.0956, -0.1136,  0.0866],
          [ 0.1463,  0.0018, -0.0813],
          [-0.1370,  0.1187, -0.1922]],

         [[ 0.1259, -0.0181,  0.0016],
          [ 0.1393, -0.1741,  0.0140],
          [-0.0189,  0.0203, -0.1187]]],


        [[[ 0.0684,  0.1693,  0.1133],
          [-0.0855, -0.2003,  0.2188],
          [-0.1777, -0.0342, -0.0762]],

         [[-0.0185, -0.0679,  0.1522],
          [ 0.0064, -0.2240,  0.0623],
          [-0.0074,  0.0727,  0.0251]],

         [[-0.1719, -0.2006,  0.1139],
          [ 0.1487, -0.2048, -0.0088],
          [ 0.0116, -0.0136,  0.2955]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.1049,  0.0504,  0.0310],
          [ 0.0898,  0.0404,  0.0340],
          [ 0.0476,  0.0094,  0.0172]],

         [[ 0.1527,  0.0917,  0.0781],
          [ 0.1495,  0.0918,  0.0826],
          [ 0.1288,  0.0775,  0.0743]],

         [[ 0.1439,  0.0896,  0.0748],
          [ 0.1422,  0.0914,  0.0777],
          [ 0.1295,  0.0797,  0.0694]]],


        [[[ 0.0044,  0.0072,  0.0043],
          [ 0.0011,  0.0010, -0.0024],
          [ 0.0005,  0.0017, -0.0051]],

         [[-0.0069, -0.0010,  0.0008],
          [-0.0116, -0.0074, -0.0047],
          [-0.0128, -0.0085, -0.0096]],

         [[ 0.0029,  0.0074,  0.0076],
          [-0.0056, -0.0024,  0.0003],
          [-0.0098, -0.0065, -0.0065]]],


        [[[ 0.0014, -0.0018, -0.0056],
          [ 0.0021, -0.0016, -0.0053],
          [ 0.0027, -0.0011, -0.0046]],

         [[ 0.0028,  0.0032,  0.0025],
          [ 0.0043,  0.0040,  0.0033],
          [ 0.0056,  0.0044,  0.0035]],

         [[ 0.0026,  0.0028,  0.0023],
          [ 0.0040,  0.0034,  0.0029],
          [ 0.0054,  0.0040,  0.0034]]],


        ...,


        [[[-0.0611, -0.0677, -0.0647],
          [-0.0523, -0.0589, -0.0542],
          [-0.0377, -0.0459, -0.0450]],

         [[-0.0803, -0.0780, -0.0706],
          [-0.0721, -0.0678, -0.0560],
          [-0.0571, -0.0553, -0.0456]],

         [[-0.0743, -0.0708, -0.0606],
          [-0.0687, -0.0633, -0.0494],
          [-0.0562, -0.0541, -0.0421]]],


        [[[-0.0074, -0.0292, -0.0295],
          [ 0.0081, -0.0126, -0.0190],
          [ 0.0210,  0.0063,  0.0014]],

         [[ 0.0230,  0.0070,  0.0100],
          [ 0.0288,  0.0136,  0.0119],
          [ 0.0330,  0.0227,  0.0225]],

         [[ 0.0092, -0.0045, -0.0036],
          [ 0.0142,  0.0017, -0.0020],
          [ 0.0188,  0.0096,  0.0068]]],


        [[[-0.0383, -0.0139, -0.0210],
          [-0.0489, -0.0275, -0.0351],
          [-0.0884, -0.0612, -0.0576]],

         [[-0.0537, -0.0396, -0.0483],
          [-0.0542, -0.0354, -0.0527],
          [-0.0901, -0.0673, -0.0706]],

         [[-0.0223, -0.0153, -0.0250],
          [-0.0224, -0.0117, -0.0303],
          [-0.0573, -0.0434, -0.0503]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2361]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 132 | Batch_idx: 0 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (3747/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (6174/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (7387/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (8598/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (9810/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (11011/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (12223/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (13435/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (14636/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (15838/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (17055/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (18266/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (19478/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (20693/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (21905/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (23109/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (24310/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (25531/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (26742/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (29153/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (30365/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (31567/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (32781/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (33977/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (35191/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (36394/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (37606/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (38825/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (40032/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (41236/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (42437/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (43645/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (44859/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (46067/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (47224/50000)
# TEST : Loss: (0.4421) | Acc: (87.00%) (8716/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1338,  0.0693,  0.1026],
          [ 0.1113, -0.3136, -0.1133],
          [ 0.2706, -0.0742,  0.0124]],

         [[-0.0167, -0.0848,  0.2742],
          [ 0.0173, -0.1155,  0.0458],
          [ 0.0696,  0.0091, -0.0696]],

         [[ 0.0037, -0.0370,  0.2084],
          [ 0.0837, -0.2098,  0.1590],
          [-0.1157,  0.0166, -0.1922]]],


        [[[ 0.1018, -0.0701, -0.0031],
          [ 0.1406,  0.1776,  0.1802],
          [ 0.0167, -0.0582,  0.1171]],

         [[-0.1478, -0.1736,  0.1530],
          [ 0.0358, -0.0452,  0.1005],
          [ 0.1252, -0.1982,  0.1308]],

         [[ 0.1009, -0.0477, -0.0145],
          [ 0.0099,  0.0182, -0.1226],
          [-0.0295, -0.1599, -0.0225]]],


        [[[-0.0724,  0.0874, -0.0430],
          [-0.1256,  0.0638,  0.1144],
          [ 0.1080,  0.1643,  0.0012]],

         [[ 0.0981,  0.1309,  0.1115],
          [ 0.0678, -0.0381, -0.0356],
          [-0.0470, -0.0295,  0.1371]],

         [[-0.0447,  0.1425,  0.0727],
          [-0.0422, -0.0455, -0.0603],
          [-0.0395, -0.0083,  0.0409]]],


        ...,


        [[[ 0.2224, -0.1172, -0.1241],
          [-0.0548, -0.1188, -0.0949],
          [-0.0972,  0.0219,  0.0721]],

         [[ 0.1641,  0.1912, -0.1183],
          [ 0.2012, -0.1418, -0.1129],
          [-0.0595, -0.0752, -0.0245]],

         [[ 0.1826,  0.1030, -0.1049],
          [-0.0637, -0.1529, -0.0735],
          [ 0.0746,  0.1817, -0.1231]]],


        [[[-0.1062,  0.0858, -0.1451],
          [-0.1446, -0.1483,  0.1164],
          [-0.1452,  0.1527, -0.1336]],

         [[ 0.0955, -0.1139,  0.0872],
          [ 0.1467,  0.0017, -0.0808],
          [-0.1357,  0.1191, -0.1924]],

         [[ 0.1260, -0.0187,  0.0014],
          [ 0.1400, -0.1740,  0.0141],
          [-0.0173,  0.0211, -0.1185]]],


        [[[ 0.0691,  0.1712,  0.1135],
          [-0.0843, -0.2008,  0.2195],
          [-0.1744, -0.0331, -0.0748]],

         [[-0.0179, -0.0666,  0.1522],
          [ 0.0072, -0.2249,  0.0628],
          [-0.0056,  0.0734,  0.0262]],

         [[-0.1734, -0.2018,  0.1116],
          [ 0.1479, -0.2066, -0.0091],
          [ 0.0117, -0.0132,  0.2971]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0655,  0.0785,  0.0683],
          [ 0.0744,  0.0863,  0.0802],
          [ 0.0860,  0.0913,  0.0843]],

         [[ 0.0532,  0.0801,  0.0881],
          [ 0.0495,  0.0774,  0.0907],
          [ 0.0570,  0.0737,  0.0843]],

         [[ 0.0746,  0.0972,  0.1163],
          [ 0.0710,  0.0977,  0.1195],
          [ 0.0824,  0.1025,  0.1159]]],


        [[[-0.0175, -0.0135, -0.0076],
          [-0.0069, -0.0046, -0.0037],
          [ 0.0024,  0.0035,  0.0005]],

         [[-0.0136, -0.0112, -0.0036],
          [-0.0028, -0.0027, -0.0004],
          [ 0.0065,  0.0061,  0.0033]],

         [[-0.0142, -0.0144, -0.0072],
          [-0.0059, -0.0080, -0.0057],
          [ 0.0012, -0.0005, -0.0035]]],


        [[[ 0.0001,  0.0002,  0.0023],
          [-0.0019, -0.0026, -0.0012],
          [-0.0006, -0.0009,  0.0006]],

         [[-0.0002, -0.0010,  0.0023],
          [-0.0009, -0.0022,  0.0002],
          [ 0.0016,  0.0013,  0.0035]],

         [[-0.0066, -0.0077, -0.0042],
          [-0.0062, -0.0074, -0.0048],
          [-0.0028, -0.0032, -0.0008]]],


        ...,


        [[[ 0.0233,  0.0316,  0.0339],
          [ 0.0038,  0.0142,  0.0168],
          [ 0.0065,  0.0185,  0.0239]],

         [[ 0.0315,  0.0351,  0.0369],
          [ 0.0114,  0.0176,  0.0185],
          [ 0.0141,  0.0225,  0.0241]],

         [[ 0.0280,  0.0309,  0.0314],
          [ 0.0129,  0.0191,  0.0190],
          [ 0.0169,  0.0234,  0.0227]]],


        [[[ 0.0026,  0.0003, -0.0004],
          [-0.0033, -0.0030, -0.0043],
          [-0.0061, -0.0037, -0.0033]],

         [[ 0.0088,  0.0079,  0.0096],
          [ 0.0030,  0.0050,  0.0083],
          [-0.0004,  0.0036,  0.0081]],

         [[ 0.0063,  0.0059,  0.0067],
          [ 0.0033,  0.0041,  0.0062],
          [ 0.0045,  0.0062,  0.0086]]],


        [[[-0.0393, -0.0455, -0.0374],
          [-0.0156, -0.0418, -0.0394],
          [-0.0457, -0.0636, -0.0620]],

         [[-0.0612, -0.0630, -0.0568],
          [-0.0347, -0.0556, -0.0557],
          [-0.0535, -0.0707, -0.0761]],

         [[-0.0440, -0.0479, -0.0457],
          [-0.0269, -0.0462, -0.0466],
          [-0.0393, -0.0556, -0.0615]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2357]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 133 | Batch_idx: 0 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (2537/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (4963/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (6178/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (7394/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (8595/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (9816/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (11040/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (12261/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (13469/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (14676/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (15892/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (17107/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (18305/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (19509/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (20719/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (21927/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (23141/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (24347/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (25557/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (26766/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (27971/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (29191/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1542) |  Loss2: (0.0000) | Acc: (94.00%) (30402/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (31600/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (32804/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (34009/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (35228/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (36450/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (37657/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (38862/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (40088/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (41294/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (42496/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (43701/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (44915/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (46128/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (47295/50000)
# TEST : Loss: (0.4323) | Acc: (87.00%) (8718/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1361,  0.0644,  0.0975],
          [ 0.1082, -0.3195, -0.1163],
          [ 0.2691, -0.0779,  0.0105]],

         [[-0.0207, -0.0900,  0.2694],
          [ 0.0147, -0.1206,  0.0437],
          [ 0.0686,  0.0056, -0.0717]],

         [[ 0.0034, -0.0383,  0.2065],
          [ 0.0833, -0.2120,  0.1591],
          [-0.1161,  0.0145, -0.1923]]],


        [[[ 0.1026, -0.0685, -0.0014],
          [ 0.1408,  0.1784,  0.1811],
          [ 0.0174, -0.0573,  0.1178]],

         [[-0.1476, -0.1731,  0.1532],
          [ 0.0354, -0.0451,  0.1002],
          [ 0.1252, -0.1979,  0.1305]],

         [[ 0.1016, -0.0470, -0.0138],
          [ 0.0104,  0.0189, -0.1223],
          [-0.0286, -0.1590, -0.0225]]],


        [[[-0.0725,  0.0874, -0.0433],
          [-0.1254,  0.0639,  0.1143],
          [ 0.1081,  0.1644,  0.0012]],

         [[ 0.0980,  0.1309,  0.1113],
          [ 0.0679, -0.0379, -0.0355],
          [-0.0470, -0.0294,  0.1370]],

         [[-0.0447,  0.1427,  0.0728],
          [-0.0419, -0.0452, -0.0601],
          [-0.0392, -0.0080,  0.0410]]],


        ...,


        [[[ 0.2251, -0.1153, -0.1221],
          [-0.0528, -0.1183, -0.0929],
          [-0.0959,  0.0220,  0.0728]],

         [[ 0.1648,  0.1913, -0.1180],
          [ 0.2010, -0.1432, -0.1132],
          [-0.0602, -0.0770, -0.0259]],

         [[ 0.1807,  0.1014, -0.1059],
          [-0.0653, -0.1552, -0.0744],
          [ 0.0733,  0.1797, -0.1245]]],


        [[[-0.1068,  0.0853, -0.1453],
          [-0.1444, -0.1487,  0.1161],
          [-0.1445,  0.1525, -0.1338]],

         [[ 0.0939, -0.1155,  0.0856],
          [ 0.1462,  0.0005, -0.0820],
          [-0.1355,  0.1183, -0.1933]],

         [[ 0.1244, -0.0201,  0.0002],
          [ 0.1391, -0.1752,  0.0132],
          [-0.0175,  0.0202, -0.1192]]],


        [[[ 0.0709,  0.1733,  0.1147],
          [-0.0823, -0.2001,  0.2217],
          [-0.1749, -0.0347, -0.0756]],

         [[-0.0208, -0.0679,  0.1519],
          [ 0.0055, -0.2271,  0.0635],
          [-0.0099,  0.0688,  0.0230]],

         [[-0.1746, -0.2023,  0.1112],
          [ 0.1481, -0.2073, -0.0076],
          [ 0.0092, -0.0155,  0.2961]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1230, -0.1429, -0.1543],
          [-0.1020, -0.1117, -0.1188],
          [-0.1516, -0.1556, -0.1751]],

         [[-0.1450, -0.1602, -0.1736],
          [-0.1264, -0.1333, -0.1439],
          [-0.1827, -0.1780, -0.1925]],

         [[-0.1219, -0.1348, -0.1414],
          [-0.1052, -0.1128, -0.1226],
          [-0.1598, -0.1535, -0.1669]]],


        [[[-0.0361, -0.0389, -0.0247],
          [-0.0177, -0.0241, -0.0122],
          [-0.0099, -0.0141, -0.0038]],

         [[-0.0290, -0.0323, -0.0172],
          [-0.0112, -0.0194, -0.0070],
          [-0.0062, -0.0119, -0.0003]],

         [[-0.0225, -0.0259, -0.0134],
          [-0.0066, -0.0152, -0.0057],
          [-0.0037, -0.0105, -0.0017]]],


        [[[-0.0024,  0.0000, -0.0016],
          [ 0.0007,  0.0058,  0.0051],
          [-0.0025,  0.0032,  0.0035]],

         [[-0.0045, -0.0024, -0.0045],
          [-0.0016,  0.0029,  0.0020],
          [-0.0038,  0.0013,  0.0012]],

         [[ 0.0001,  0.0027,  0.0005],
          [ 0.0019,  0.0066,  0.0058],
          [-0.0003,  0.0048,  0.0048]]],


        ...,


        [[[ 0.0084,  0.0120,  0.0019],
          [ 0.0164,  0.0171,  0.0072],
          [ 0.0265,  0.0220,  0.0021]],

         [[ 0.0078,  0.0082, -0.0061],
          [ 0.0161,  0.0157,  0.0035],
          [ 0.0283,  0.0235,  0.0026]],

         [[-0.0000,  0.0007, -0.0136],
          [ 0.0104,  0.0107, -0.0015],
          [ 0.0240,  0.0201,  0.0000]]],


        [[[-0.0256, -0.0135, -0.0093],
          [-0.0121, -0.0013, -0.0024],
          [-0.0022,  0.0068, -0.0000]],

         [[-0.0369, -0.0267, -0.0229],
          [-0.0239, -0.0150, -0.0150],
          [-0.0116, -0.0029, -0.0073]],

         [[-0.0172, -0.0088, -0.0060],
          [-0.0045,  0.0030,  0.0022],
          [ 0.0036,  0.0137,  0.0092]]],


        [[[-0.0986, -0.1189, -0.1612],
          [-0.1411, -0.1584, -0.1718],
          [-0.1717, -0.1829, -0.1787]],

         [[-0.1269, -0.1471, -0.1859],
          [-0.1692, -0.1846, -0.1958],
          [-0.1964, -0.2057, -0.1970]],

         [[-0.1478, -0.1674, -0.1998],
          [-0.1868, -0.2010, -0.2091],
          [-0.2123, -0.2210, -0.2079]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2352]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 134 | Batch_idx: 0 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (3761/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (4959/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (6180/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (7389/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (9805/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (11020/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (12220/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (13430/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (14641/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (15857/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (17077/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (18288/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (19510/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1509) |  Loss2: (0.0000) | Acc: (94.00%) (20723/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (21919/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (23140/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (24335/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (25543/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (26747/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (27973/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (29179/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (30389/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (31593/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (32819/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (34029/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (35246/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (36460/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (37679/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (38890/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (40105/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (41319/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (42532/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (43727/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (44935/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (46138/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (47297/50000)
# TEST : Loss: (0.4147) | Acc: (87.00%) (8738/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1291,  0.0681,  0.1002],
          [ 0.1099, -0.3220, -0.1162],
          [ 0.2670, -0.0812,  0.0102]],

         [[-0.0142, -0.0854,  0.2738],
          [ 0.0165, -0.1219,  0.0461],
          [ 0.0684,  0.0045, -0.0691]],

         [[ 0.0093, -0.0349,  0.2075],
          [ 0.0855, -0.2132,  0.1599],
          [-0.1150,  0.0145, -0.1895]]],


        [[[ 0.1024, -0.0684, -0.0017],
          [ 0.1403,  0.1785,  0.1807],
          [ 0.0177, -0.0568,  0.1184]],

         [[-0.1472, -0.1728,  0.1527],
          [ 0.0356, -0.0446,  0.0998],
          [ 0.1260, -0.1972,  0.1309]],

         [[ 0.1004, -0.0481, -0.0155],
          [ 0.0092,  0.0178, -0.1242],
          [-0.0291, -0.1597, -0.0237]]],


        [[[-0.0725,  0.0872, -0.0433],
          [-0.1255,  0.0637,  0.1142],
          [ 0.1078,  0.1643,  0.0013]],

         [[ 0.0978,  0.1307,  0.1112],
          [ 0.0676, -0.0382, -0.0356],
          [-0.0474, -0.0295,  0.1371]],

         [[-0.0450,  0.1423,  0.0726],
          [-0.0424, -0.0456, -0.0603],
          [-0.0397, -0.0083,  0.0410]]],


        ...,


        [[[ 0.2231, -0.1165, -0.1228],
          [-0.0545, -0.1197, -0.0931],
          [-0.0972,  0.0206,  0.0728]],

         [[ 0.1640,  0.1908, -0.1184],
          [ 0.2006, -0.1438, -0.1135],
          [-0.0605, -0.0782, -0.0263]],

         [[ 0.1816,  0.1027, -0.1048],
          [-0.0630, -0.1536, -0.0732],
          [ 0.0755,  0.1805, -0.1236]]],


        [[[-0.1056,  0.0866, -0.1439],
          [-0.1444, -0.1484,  0.1165],
          [-0.1447,  0.1528, -0.1332]],

         [[ 0.0965, -0.1127,  0.0881],
          [ 0.1470,  0.0018, -0.0806],
          [-0.1349,  0.1195, -0.1921]],

         [[ 0.1271, -0.0176,  0.0022],
          [ 0.1406, -0.1737,  0.0142],
          [-0.0162,  0.0215, -0.1182]]],


        [[[ 0.0730,  0.1751,  0.1146],
          [-0.0815, -0.2016,  0.2202],
          [-0.1730, -0.0316, -0.0732]],

         [[-0.0197, -0.0678,  0.1502],
          [ 0.0045, -0.2304,  0.0603],
          [-0.0104,  0.0704,  0.0243]],

         [[-0.1744, -0.2019,  0.1106],
          [ 0.1479, -0.2091, -0.0084],
          [ 0.0105, -0.0123,  0.3000]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0400, -0.0307, -0.0430],
          [-0.0570, -0.0427, -0.0559],
          [-0.0777, -0.0680, -0.0818]],

         [[-0.0615, -0.0517, -0.0621],
          [-0.0813, -0.0661, -0.0785],
          [-0.0998, -0.0886, -0.1021]],

         [[-0.0881, -0.0794, -0.0934],
          [-0.1062, -0.0891, -0.1033],
          [-0.1244, -0.1101, -0.1218]]],


        [[[ 0.0045,  0.0017,  0.0059],
          [ 0.0040,  0.0019,  0.0070],
          [ 0.0031,  0.0010,  0.0054]],

         [[ 0.0121,  0.0092,  0.0134],
          [ 0.0103,  0.0082,  0.0131],
          [ 0.0076,  0.0056,  0.0098]],

         [[ 0.0182,  0.0145,  0.0178],
          [ 0.0171,  0.0144,  0.0187],
          [ 0.0167,  0.0142,  0.0175]]],


        [[[-0.0033, -0.0016, -0.0015],
          [-0.0002,  0.0012,  0.0006],
          [ 0.0017,  0.0026,  0.0016]],

         [[-0.0040, -0.0020, -0.0015],
          [-0.0005,  0.0012,  0.0010],
          [ 0.0016,  0.0028,  0.0023]],

         [[-0.0036, -0.0014, -0.0014],
          [-0.0010,  0.0008,  0.0003],
          [ 0.0007,  0.0018,  0.0010]]],


        ...,


        [[[ 0.0254,  0.0174,  0.0076],
          [ 0.0328,  0.0263,  0.0134],
          [ 0.0358,  0.0266,  0.0162]],

         [[ 0.0212,  0.0135,  0.0044],
          [ 0.0325,  0.0256,  0.0121],
          [ 0.0368,  0.0278,  0.0171]],

         [[ 0.0213,  0.0153,  0.0070],
          [ 0.0299,  0.0260,  0.0138],
          [ 0.0336,  0.0272,  0.0176]]],


        [[[ 0.0181,  0.0093,  0.0048],
          [ 0.0150,  0.0084,  0.0017],
          [ 0.0157,  0.0071, -0.0021]],

         [[ 0.0173,  0.0077,  0.0023],
          [ 0.0138,  0.0066, -0.0008],
          [ 0.0121,  0.0037, -0.0055]],

         [[ 0.0109,  0.0036, -0.0000],
          [ 0.0068,  0.0010, -0.0049],
          [ 0.0043, -0.0023, -0.0092]]],


        [[[-0.0627, -0.0642, -0.0745],
          [-0.0927, -0.0939, -0.0962],
          [-0.1070, -0.1132, -0.1149]],

         [[-0.0411, -0.0419, -0.0508],
          [-0.0725, -0.0743, -0.0759],
          [-0.0896, -0.0975, -0.0989]],

         [[-0.0464, -0.0473, -0.0556],
          [-0.0807, -0.0782, -0.0789],
          [-0.0948, -0.0954, -0.0945]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2347]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.3434) |  Loss2: (0.1996) | Acc: (95.00%) (122/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.3922) |  Loss2: (0.1996) | Acc: (93.00%) (1311/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.3773) |  Loss2: (0.1995) | Acc: (93.00%) (2526/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.3775) |  Loss2: (0.1995) | Acc: (93.00%) (3718/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.3849) |  Loss2: (0.1994) | Acc: (93.00%) (4898/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.3880) |  Loss2: (0.1993) | Acc: (93.00%) (6090/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.3872) |  Loss2: (0.1993) | Acc: (93.00%) (7290/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.3900) |  Loss2: (0.1992) | Acc: (93.00%) (8481/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.3933) |  Loss2: (0.1991) | Acc: (93.00%) (9664/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.3903) |  Loss2: (0.1991) | Acc: (93.00%) (10867/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.3874) |  Loss2: (0.1990) | Acc: (93.00%) (12075/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.3862) |  Loss2: (0.1989) | Acc: (93.00%) (13271/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.3870) |  Loss2: (0.1988) | Acc: (93.00%) (14464/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.3870) |  Loss2: (0.1988) | Acc: (93.00%) (15663/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.3871) |  Loss2: (0.1987) | Acc: (93.00%) (16858/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.3872) |  Loss2: (0.1986) | Acc: (93.00%) (18050/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.3874) |  Loss2: (0.1985) | Acc: (93.00%) (19237/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.3877) |  Loss2: (0.1985) | Acc: (93.00%) (20433/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.3883) |  Loss2: (0.1984) | Acc: (93.00%) (21609/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.3901) |  Loss2: (0.1983) | Acc: (93.00%) (22783/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.3908) |  Loss2: (0.1983) | Acc: (93.00%) (23963/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.3899) |  Loss2: (0.1982) | Acc: (93.00%) (25159/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.3894) |  Loss2: (0.1981) | Acc: (93.00%) (26356/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.3889) |  Loss2: (0.1980) | Acc: (93.00%) (27552/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.3886) |  Loss2: (0.1980) | Acc: (93.00%) (28751/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.3887) |  Loss2: (0.1979) | Acc: (93.00%) (29944/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.3879) |  Loss2: (0.1978) | Acc: (93.00%) (31145/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.3866) |  Loss2: (0.1978) | Acc: (93.00%) (32355/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.3860) |  Loss2: (0.1977) | Acc: (93.00%) (33557/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.3848) |  Loss2: (0.1976) | Acc: (93.00%) (34769/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.3849) |  Loss2: (0.1975) | Acc: (93.00%) (35960/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.3841) |  Loss2: (0.1975) | Acc: (93.00%) (37159/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.3833) |  Loss2: (0.1974) | Acc: (93.00%) (38366/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.3837) |  Loss2: (0.1973) | Acc: (93.00%) (39548/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.3823) |  Loss2: (0.1973) | Acc: (93.00%) (40761/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.3817) |  Loss2: (0.1972) | Acc: (93.00%) (41965/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.3808) |  Loss2: (0.1971) | Acc: (93.00%) (43177/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.3799) |  Loss2: (0.1971) | Acc: (93.00%) (44393/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.3791) |  Loss2: (0.1970) | Acc: (93.00%) (45600/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.3780) |  Loss2: (0.1969) | Acc: (93.00%) (46776/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_135.pth.tar'
# TEST : Loss: (0.4390) | Acc: (86.00%) (8696/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1283,  0.0684,  0.1001],
          [ 0.1108, -0.3216, -0.1161],
          [ 0.2683, -0.0802,  0.0107]],

         [[-0.0131, -0.0846,  0.2742],
          [ 0.0179, -0.1209,  0.0468],
          [ 0.0702,  0.0059, -0.0680]],

         [[ 0.0102, -0.0342,  0.2081],
          [ 0.0868, -0.2122,  0.1607],
          [-0.1131,  0.0159, -0.1883]]],


        [[[ 0.1026, -0.0683, -0.0016],
          [ 0.1403,  0.1785,  0.1807],
          [ 0.0178, -0.0568,  0.1184]],

         [[-0.1470, -0.1726,  0.1527],
          [ 0.0357, -0.0445,  0.0997],
          [ 0.1260, -0.1971,  0.1308]],

         [[ 0.1006, -0.0480, -0.0154],
          [ 0.0093,  0.0179, -0.1241],
          [-0.0290, -0.1596, -0.0237]]],


        [[[-0.0725,  0.0872, -0.0432],
          [-0.1255,  0.0636,  0.1141],
          [ 0.1077,  0.1642,  0.0013]],

         [[ 0.0977,  0.1306,  0.1112],
          [ 0.0675, -0.0382, -0.0356],
          [-0.0474, -0.0295,  0.1370]],

         [[-0.0450,  0.1422,  0.0726],
          [-0.0424, -0.0457, -0.0603],
          [-0.0397, -0.0083,  0.0410]]],


        ...,


        [[[ 0.2224, -0.1170, -0.1233],
          [-0.0549, -0.1201, -0.0935],
          [-0.0977,  0.0201,  0.0723]],

         [[ 0.1633,  0.1903, -0.1189],
          [ 0.2001, -0.1441, -0.1138],
          [-0.0610, -0.0786, -0.0267]],

         [[ 0.1810,  0.1022, -0.1053],
          [-0.0633, -0.1538, -0.0735],
          [ 0.0750,  0.1801, -0.1239]]],


        [[[-0.1057,  0.0864, -0.1439],
          [-0.1444, -0.1484,  0.1165],
          [-0.1446,  0.1527, -0.1332]],

         [[ 0.0964, -0.1128,  0.0881],
          [ 0.1470,  0.0019, -0.0805],
          [-0.1348,  0.1195, -0.1920]],

         [[ 0.1271, -0.0176,  0.0022],
          [ 0.1406, -0.1736,  0.0143],
          [-0.0159,  0.0216, -0.1181]]],


        [[[ 0.0739,  0.1754,  0.1150],
          [-0.0804, -0.2008,  0.2207],
          [-0.1718, -0.0306, -0.0724]],

         [[-0.0195, -0.0680,  0.1501],
          [ 0.0050, -0.2300,  0.0605],
          [-0.0098,  0.0711,  0.0248]],

         [[-0.1743, -0.2021,  0.1104],
          [ 0.1484, -0.2086, -0.0080],
          [ 0.0113, -0.0114,  0.3005]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2827e-05,  6.8351e-06,  1.0007e-05],
          [ 1.1084e-05, -3.2156e-05, -1.1614e-05],
          [ 2.6825e-05, -8.0234e-06,  1.0657e-06]],

         [[-1.3102e-06, -8.4626e-06,  2.7424e-05],
          [ 1.7905e-06, -1.2094e-05,  4.6839e-06],
          [ 7.0175e-06,  5.9088e-07, -6.8014e-06]],

         [[ 1.0246e-06, -3.4238e-06,  2.0807e-05],
          [ 8.6798e-06, -2.1219e-05,  1.6074e-05],
          [-1.1308e-05,  1.5914e-06, -1.8830e-05]]],


        [[[ 1.0260e-05, -6.8260e-06, -1.6102e-07],
          [ 1.4035e-05,  1.7847e-05,  1.8066e-05],
          [ 1.7826e-06, -5.6752e-06,  1.1836e-05]],

         [[-1.4700e-05, -1.7257e-05,  1.5270e-05],
          [ 3.5706e-06, -4.4537e-06,  9.9725e-06],
          [ 1.2602e-05, -1.9706e-05,  1.3084e-05]],

         [[ 1.0057e-05, -4.7957e-06, -1.5430e-06],
          [ 9.2823e-07,  1.7873e-06, -1.2415e-05],
          [-2.8989e-06, -1.5959e-05, -2.3653e-06]]],


        [[[-7.2484e-06,  8.7178e-06, -4.3250e-06],
          [-1.2551e-05,  6.3610e-06,  1.1412e-05],
          [ 1.0775e-05,  1.6422e-05,  1.3029e-07]],

         [[ 9.7748e-06,  1.3064e-05,  1.1123e-05],
          [ 6.7549e-06, -3.8199e-06, -3.5574e-06],
          [-4.7369e-06, -2.9505e-06,  1.3702e-05]],

         [[-4.5033e-06,  1.4220e-05,  7.2563e-06],
          [-4.2423e-06, -4.5660e-06, -6.0265e-06],
          [-3.9722e-06, -8.3166e-07,  4.0993e-06]]],


        ...,


        [[[ 2.2236e-05, -1.1697e-05, -1.2334e-05],
          [-5.4871e-06, -1.2006e-05, -9.3508e-06],
          [-9.7684e-06,  2.0076e-06,  7.2275e-06]],

         [[ 1.6327e-05,  1.9026e-05, -1.1889e-05],
          [ 2.0009e-05, -1.4410e-05, -1.1379e-05],
          [-6.0994e-06, -7.8611e-06, -2.6738e-06]],

         [[ 1.8103e-05,  1.0225e-05, -1.0526e-05],
          [-6.3327e-06, -1.5380e-05, -7.3457e-06],
          [ 7.5034e-06,  1.8007e-05, -1.2390e-05]]],


        [[[-1.0574e-05,  8.6380e-06, -1.4391e-05],
          [-1.4439e-05, -1.4838e-05,  1.1651e-05],
          [-1.4462e-05,  1.5267e-05, -1.3316e-05]],

         [[ 9.6416e-06, -1.1279e-05,  8.8065e-06],
          [ 1.4699e-05,  1.8576e-07, -8.0456e-06],
          [-1.3476e-05,  1.1952e-05, -1.9198e-05]],

         [[ 1.2711e-05, -1.7569e-06,  2.2317e-07],
          [ 1.4064e-05, -1.7357e-05,  1.4306e-06],
          [-1.5948e-06,  2.1625e-06, -1.1807e-05]]],


        [[[ 7.3879e-06,  1.7538e-05,  1.1498e-05],
          [-8.0396e-06, -2.0079e-05,  2.2071e-05],
          [-1.7185e-05, -3.0578e-06, -7.2397e-06]],

         [[-1.9496e-06, -6.7989e-06,  1.5007e-05],
          [ 5.0361e-07, -2.2998e-05,  6.0514e-06],
          [-9.7628e-07,  7.1090e-06,  2.4765e-06]],

         [[-1.7433e-05, -2.0213e-05,  1.1038e-05],
          [ 1.4840e-05, -2.0861e-05, -8.0420e-07],
          [ 1.1269e-06, -1.1411e-06,  3.0051e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.2855]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0293]], device='cuda:0')

Epoch: 136 | Batch_idx: 0 |  Loss: (0.3796) |  Loss2: (0.1942) | Acc: (93.00%) (120/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.3704) |  Loss2: (0.1942) | Acc: (93.00%) (1317/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.3683) |  Loss2: (0.1941) | Acc: (93.00%) (2520/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.3737) |  Loss2: (0.1941) | Acc: (93.00%) (3711/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.3720) |  Loss2: (0.1940) | Acc: (93.00%) (4905/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.3710) |  Loss2: (0.1939) | Acc: (93.00%) (6104/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.3688) |  Loss2: (0.1939) | Acc: (93.00%) (7304/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.3700) |  Loss2: (0.1938) | Acc: (93.00%) (8504/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.3694) |  Loss2: (0.1938) | Acc: (93.00%) (9708/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.3695) |  Loss2: (0.1937) | Acc: (93.00%) (10916/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.3666) |  Loss2: (0.1937) | Acc: (93.00%) (12127/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.3654) |  Loss2: (0.1936) | Acc: (93.00%) (13343/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.3648) |  Loss2: (0.1935) | Acc: (93.00%) (14545/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.3642) |  Loss2: (0.1935) | Acc: (93.00%) (15757/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.3639) |  Loss2: (0.1934) | Acc: (94.00%) (16973/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.3651) |  Loss2: (0.1934) | Acc: (94.00%) (18170/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.3630) |  Loss2: (0.1933) | Acc: (94.00%) (19380/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.3618) |  Loss2: (0.1932) | Acc: (94.00%) (20596/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.3611) |  Loss2: (0.1932) | Acc: (94.00%) (21815/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.3622) |  Loss2: (0.1931) | Acc: (94.00%) (23007/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.3616) |  Loss2: (0.1931) | Acc: (94.00%) (24215/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.3608) |  Loss2: (0.1930) | Acc: (94.00%) (25439/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.3602) |  Loss2: (0.1930) | Acc: (94.00%) (26650/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.3598) |  Loss2: (0.1929) | Acc: (94.00%) (27860/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.3595) |  Loss2: (0.1928) | Acc: (94.00%) (29078/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.3594) |  Loss2: (0.1928) | Acc: (94.00%) (30288/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.3591) |  Loss2: (0.1927) | Acc: (94.00%) (31499/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.3591) |  Loss2: (0.1927) | Acc: (94.00%) (32703/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.3586) |  Loss2: (0.1926) | Acc: (94.00%) (33915/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.3581) |  Loss2: (0.1926) | Acc: (94.00%) (35129/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.3575) |  Loss2: (0.1925) | Acc: (94.00%) (36346/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.3575) |  Loss2: (0.1925) | Acc: (94.00%) (37557/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.3569) |  Loss2: (0.1924) | Acc: (94.00%) (38779/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.3567) |  Loss2: (0.1923) | Acc: (94.00%) (39985/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.3559) |  Loss2: (0.1923) | Acc: (94.00%) (41208/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.3553) |  Loss2: (0.1922) | Acc: (94.00%) (42423/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.3550) |  Loss2: (0.1922) | Acc: (94.00%) (43632/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.3552) |  Loss2: (0.1921) | Acc: (94.00%) (44836/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.3550) |  Loss2: (0.1921) | Acc: (94.00%) (46052/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.3551) |  Loss2: (0.1920) | Acc: (94.00%) (47215/50000)
# TEST : Loss: (0.4174) | Acc: (87.00%) (8735/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1282,  0.0683,  0.1000],
          [ 0.1108, -0.3214, -0.1161],
          [ 0.2681, -0.0802,  0.0107]],

         [[-0.0131, -0.0846,  0.2741],
          [ 0.0179, -0.1209,  0.0468],
          [ 0.0701,  0.0059, -0.0680]],

         [[ 0.0102, -0.0342,  0.2080],
          [ 0.0868, -0.2121,  0.1607],
          [-0.1130,  0.0159, -0.1882]]],


        [[[ 0.1026, -0.0682, -0.0016],
          [ 0.1403,  0.1784,  0.1806],
          [ 0.0178, -0.0567,  0.1183]],

         [[-0.1469, -0.1725,  0.1526],
          [ 0.0357, -0.0445,  0.0997],
          [ 0.1260, -0.1970,  0.1308]],

         [[ 0.1005, -0.0479, -0.0154],
          [ 0.0093,  0.0179, -0.1241],
          [-0.0290, -0.1595, -0.0236]]],


        [[[-0.0725,  0.0871, -0.0432],
          [-0.1255,  0.0636,  0.1141],
          [ 0.1077,  0.1642,  0.0013]],

         [[ 0.0977,  0.1306,  0.1112],
          [ 0.0675, -0.0382, -0.0356],
          [-0.0473, -0.0295,  0.1370]],

         [[-0.0450,  0.1421,  0.0725],
          [-0.0424, -0.0456, -0.0602],
          [-0.0397, -0.0083,  0.0410]]],


        ...,


        [[[ 0.2223, -0.1169, -0.1233],
          [-0.0548, -0.1200, -0.0935],
          [-0.0976,  0.0201,  0.0722]],

         [[ 0.1632,  0.1902, -0.1188],
          [ 0.2000, -0.1440, -0.1137],
          [-0.0610, -0.0786, -0.0267]],

         [[ 0.1810,  0.1022, -0.1052],
          [-0.0633, -0.1537, -0.0734],
          [ 0.0750,  0.1800, -0.1238]]],


        [[[-0.1057,  0.0863, -0.1439],
          [-0.1443, -0.1483,  0.1165],
          [-0.1446,  0.1526, -0.1331]],

         [[ 0.0964, -0.1127,  0.0880],
          [ 0.1469,  0.0019, -0.0804],
          [-0.1347,  0.1195, -0.1919]],

         [[ 0.1271, -0.0176,  0.0022],
          [ 0.1406, -0.1735,  0.0143],
          [-0.0159,  0.0216, -0.1180]]],


        [[[ 0.0738,  0.1753,  0.1149],
          [-0.0804, -0.2007,  0.2206],
          [-0.1718, -0.0306, -0.0724]],

         [[-0.0195, -0.0680,  0.1500],
          [ 0.0050, -0.2299,  0.0605],
          [-0.0098,  0.0711,  0.0248]],

         [[-0.1743, -0.2020,  0.1103],
          [ 0.1483, -0.2085, -0.0080],
          [ 0.0113, -0.0114,  0.3004]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2822e-05,  6.8325e-06,  1.0003e-05],
          [ 1.1079e-05, -3.2143e-05, -1.1609e-05],
          [ 2.6815e-05, -8.0202e-06,  1.0653e-06]],

         [[-1.3097e-06, -8.4594e-06,  2.7413e-05],
          [ 1.7897e-06, -1.2089e-05,  4.6820e-06],
          [ 7.0148e-06,  5.9065e-07, -6.7988e-06]],

         [[ 1.0242e-06, -3.4225e-06,  2.0799e-05],
          [ 8.6763e-06, -2.1211e-05,  1.6067e-05],
          [-1.1304e-05,  1.5908e-06, -1.8823e-05]]],


        [[[ 1.0256e-05, -6.8234e-06, -1.6096e-07],
          [ 1.4030e-05,  1.7840e-05,  1.8059e-05],
          [ 1.7818e-06, -5.6730e-06,  1.1831e-05]],

         [[-1.4694e-05, -1.7250e-05,  1.5264e-05],
          [ 3.5691e-06, -4.4520e-06,  9.9687e-06],
          [ 1.2597e-05, -1.9698e-05,  1.3079e-05]],

         [[ 1.0053e-05, -4.7938e-06, -1.5423e-06],
          [ 9.2787e-07,  1.7866e-06, -1.2410e-05],
          [-2.8977e-06, -1.5953e-05, -2.3643e-06]]],


        [[[-7.2455e-06,  8.7143e-06, -4.3232e-06],
          [-1.2547e-05,  6.3584e-06,  1.1408e-05],
          [ 1.0771e-05,  1.6416e-05,  1.3024e-07]],

         [[ 9.7710e-06,  1.3059e-05,  1.1118e-05],
          [ 6.7522e-06, -3.8185e-06, -3.5560e-06],
          [-4.7350e-06, -2.9493e-06,  1.3697e-05]],

         [[-4.5015e-06,  1.4214e-05,  7.2534e-06],
          [-4.2407e-06, -4.5642e-06, -6.0242e-06],
          [-3.9706e-06, -8.3133e-07,  4.0977e-06]]],


        ...,


        [[[ 2.2227e-05, -1.1692e-05, -1.2329e-05],
          [-5.4849e-06, -1.2001e-05, -9.3470e-06],
          [-9.7646e-06,  2.0068e-06,  7.2246e-06]],

         [[ 1.6321e-05,  1.9019e-05, -1.1884e-05],
          [ 2.0001e-05, -1.4404e-05, -1.1375e-05],
          [-6.0971e-06, -7.8579e-06, -2.6727e-06]],

         [[ 1.8096e-05,  1.0221e-05, -1.0522e-05],
          [-6.3303e-06, -1.5374e-05, -7.3428e-06],
          [ 7.5005e-06,  1.8000e-05, -1.2385e-05]]],


        [[[-1.0570e-05,  8.6345e-06, -1.4385e-05],
          [-1.4433e-05, -1.4832e-05,  1.1646e-05],
          [-1.4456e-05,  1.5261e-05, -1.3311e-05]],

         [[ 9.6378e-06, -1.1274e-05,  8.8030e-06],
          [ 1.4693e-05,  1.8568e-07, -8.0424e-06],
          [-1.3471e-05,  1.1948e-05, -1.9190e-05]],

         [[ 1.2706e-05, -1.7562e-06,  2.2308e-07],
          [ 1.4059e-05, -1.7350e-05,  1.4301e-06],
          [-1.5941e-06,  2.1617e-06, -1.1802e-05]]],


        [[[ 7.3850e-06,  1.7531e-05,  1.1493e-05],
          [-8.0364e-06, -2.0071e-05,  2.2062e-05],
          [-1.7178e-05, -3.0566e-06, -7.2368e-06]],

         [[-1.9489e-06, -6.7963e-06,  1.5001e-05],
          [ 5.0341e-07, -2.2989e-05,  6.0491e-06],
          [-9.7591e-07,  7.1061e-06,  2.4756e-06]],

         [[-1.7426e-05, -2.0205e-05,  1.1034e-05],
          [ 1.4834e-05, -2.0852e-05, -8.0387e-07],
          [ 1.1265e-06, -1.1406e-06,  3.0040e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.3227]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0107]], device='cuda:0')

Epoch: 137 | Batch_idx: 0 |  Loss: (0.3627) |  Loss2: (0.1897) | Acc: (92.00%) (119/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.3384) |  Loss2: (0.1897) | Acc: (94.00%) (1330/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.3354) |  Loss2: (0.1896) | Acc: (94.00%) (2541/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.3408) |  Loss2: (0.1896) | Acc: (94.00%) (3751/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.3422) |  Loss2: (0.1895) | Acc: (94.00%) (4966/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.3477) |  Loss2: (0.1895) | Acc: (94.00%) (6160/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.3473) |  Loss2: (0.1894) | Acc: (94.00%) (7374/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.3487) |  Loss2: (0.1894) | Acc: (94.00%) (8579/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.3475) |  Loss2: (0.1893) | Acc: (94.00%) (9792/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.3476) |  Loss2: (0.1893) | Acc: (94.00%) (10997/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.3469) |  Loss2: (0.1892) | Acc: (94.00%) (12217/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.3474) |  Loss2: (0.1892) | Acc: (94.00%) (13417/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.3479) |  Loss2: (0.1891) | Acc: (94.00%) (14624/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.3462) |  Loss2: (0.1891) | Acc: (94.00%) (15847/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.3468) |  Loss2: (0.1890) | Acc: (94.00%) (17054/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.3474) |  Loss2: (0.1890) | Acc: (94.00%) (18261/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.3477) |  Loss2: (0.1889) | Acc: (94.00%) (19469/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.3471) |  Loss2: (0.1889) | Acc: (94.00%) (20687/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.3473) |  Loss2: (0.1888) | Acc: (94.00%) (21886/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.3472) |  Loss2: (0.1888) | Acc: (94.00%) (23103/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.3475) |  Loss2: (0.1887) | Acc: (94.00%) (24300/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.3469) |  Loss2: (0.1887) | Acc: (94.00%) (25517/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.3470) |  Loss2: (0.1886) | Acc: (94.00%) (26724/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.3465) |  Loss2: (0.1886) | Acc: (94.00%) (27944/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.3461) |  Loss2: (0.1885) | Acc: (94.00%) (29162/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.3440) |  Loss2: (0.1885) | Acc: (94.00%) (30399/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.3433) |  Loss2: (0.1884) | Acc: (94.00%) (31621/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.3432) |  Loss2: (0.1884) | Acc: (94.00%) (32837/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.3433) |  Loss2: (0.1883) | Acc: (94.00%) (34053/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.3425) |  Loss2: (0.1883) | Acc: (94.00%) (35272/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.3425) |  Loss2: (0.1882) | Acc: (94.00%) (36488/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.3421) |  Loss2: (0.1882) | Acc: (94.00%) (37709/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.3424) |  Loss2: (0.1881) | Acc: (94.00%) (38914/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.3416) |  Loss2: (0.1881) | Acc: (94.00%) (40140/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.3420) |  Loss2: (0.1880) | Acc: (94.00%) (41345/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.3417) |  Loss2: (0.1880) | Acc: (94.00%) (42549/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.3414) |  Loss2: (0.1879) | Acc: (94.00%) (43763/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.3411) |  Loss2: (0.1879) | Acc: (94.00%) (44977/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.3415) |  Loss2: (0.1878) | Acc: (94.00%) (46181/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.3410) |  Loss2: (0.1878) | Acc: (94.00%) (47354/50000)
# TEST : Loss: (0.4053) | Acc: (87.00%) (8776/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1282,  0.0683,  0.1000],
          [ 0.1107, -0.3213, -0.1160],
          [ 0.2680, -0.0802,  0.0106]],

         [[-0.0131, -0.0846,  0.2740],
          [ 0.0179, -0.1208,  0.0468],
          [ 0.0701,  0.0059, -0.0680]],

         [[ 0.0102, -0.0342,  0.2079],
          [ 0.0867, -0.2120,  0.1606],
          [-0.1130,  0.0159, -0.1882]]],


        [[[ 0.1025, -0.0682, -0.0016],
          [ 0.1402,  0.1783,  0.1805],
          [ 0.0178, -0.0567,  0.1183]],

         [[-0.1469, -0.1724,  0.1526],
          [ 0.0357, -0.0445,  0.0996],
          [ 0.1259, -0.1969,  0.1307]],

         [[ 0.1005, -0.0479, -0.0154],
          [ 0.0093,  0.0179, -0.1240],
          [-0.0290, -0.1595, -0.0236]]],


        [[[-0.0724,  0.0871, -0.0432],
          [-0.1254,  0.0636,  0.1140],
          [ 0.1077,  0.1641,  0.0013]],

         [[ 0.0977,  0.1305,  0.1111],
          [ 0.0675, -0.0382, -0.0355],
          [-0.0473, -0.0295,  0.1369]],

         [[-0.0450,  0.1421,  0.0725],
          [-0.0424, -0.0456, -0.0602],
          [-0.0397, -0.0083,  0.0410]]],


        ...,


        [[[ 0.2222, -0.1169, -0.1232],
          [-0.0548, -0.1200, -0.0934],
          [-0.0976,  0.0201,  0.0722]],

         [[ 0.1631,  0.1901, -0.1188],
          [ 0.1999, -0.1440, -0.1137],
          [-0.0609, -0.0785, -0.0267]],

         [[ 0.1809,  0.1022, -0.1052],
          [-0.0633, -0.1537, -0.0734],
          [ 0.0750,  0.1799, -0.1238]]],


        [[[-0.1057,  0.0863, -0.1438],
          [-0.1443, -0.1483,  0.1164],
          [-0.1445,  0.1526, -0.1331]],

         [[ 0.0963, -0.1127,  0.0880],
          [ 0.1469,  0.0019, -0.0804],
          [-0.1347,  0.1194, -0.1918]],

         [[ 0.1270, -0.0176,  0.0022],
          [ 0.1405, -0.1734,  0.0143],
          [-0.0159,  0.0216, -0.1180]]],


        [[[ 0.0738,  0.1752,  0.1149],
          [-0.0803, -0.2006,  0.2205],
          [-0.1717, -0.0306, -0.0723]],

         [[-0.0195, -0.0679,  0.1500],
          [ 0.0050, -0.2298,  0.0605],
          [-0.0098,  0.0710,  0.0247]],

         [[-0.1742, -0.2020,  0.1103],
          [ 0.1483, -0.2084, -0.0080],
          [ 0.0113, -0.0114,  0.3003]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2817e-05,  6.8299e-06,  9.9992e-06],
          [ 1.1075e-05, -3.2130e-05, -1.1604e-05],
          [ 2.6804e-05, -8.0170e-06,  1.0649e-06]],

         [[-1.3092e-06, -8.4562e-06,  2.7403e-05],
          [ 1.7890e-06, -1.2084e-05,  4.6802e-06],
          [ 7.0122e-06,  5.9041e-07, -6.7962e-06]],

         [[ 1.0238e-06, -3.4212e-06,  2.0791e-05],
          [ 8.6728e-06, -2.1203e-05,  1.6061e-05],
          [-1.1300e-05,  1.5901e-06, -1.8815e-05]]],


        [[[ 1.0252e-05, -6.8208e-06, -1.6089e-07],
          [ 1.4024e-05,  1.7833e-05,  1.8052e-05],
          [ 1.7811e-06, -5.6708e-06,  1.1826e-05]],

         [[-1.4688e-05, -1.7243e-05,  1.5258e-05],
          [ 3.5676e-06, -4.4502e-06,  9.9649e-06],
          [ 1.2593e-05, -1.9691e-05,  1.3074e-05]],

         [[ 1.0049e-05, -4.7919e-06, -1.5417e-06],
          [ 9.2750e-07,  1.7859e-06, -1.2405e-05],
          [-2.8965e-06, -1.5946e-05, -2.3634e-06]]],


        [[[-7.2426e-06,  8.7108e-06, -4.3215e-06],
          [-1.2542e-05,  6.3557e-06,  1.1403e-05],
          [ 1.0767e-05,  1.6409e-05,  1.3019e-07]],

         [[ 9.7672e-06,  1.3053e-05,  1.1114e-05],
          [ 6.7496e-06, -3.8170e-06, -3.5545e-06],
          [-4.7331e-06, -2.9482e-06,  1.3692e-05]],

         [[-4.4998e-06,  1.4208e-05,  7.2505e-06],
          [-4.2390e-06, -4.5625e-06, -6.0218e-06],
          [-3.9690e-06, -8.3100e-07,  4.0961e-06]]],


        ...,


        [[[ 2.2219e-05, -1.1688e-05, -1.2324e-05],
          [-5.4827e-06, -1.1996e-05, -9.3432e-06],
          [-9.7608e-06,  2.0060e-06,  7.2217e-06]],

         [[ 1.6315e-05,  1.9011e-05, -1.1880e-05],
          [ 1.9994e-05, -1.4398e-05, -1.1371e-05],
          [-6.0947e-06, -7.8547e-06, -2.6717e-06]],

         [[ 1.8089e-05,  1.0217e-05, -1.0518e-05],
          [-6.3280e-06, -1.5368e-05, -7.3399e-06],
          [ 7.4975e-06,  1.7993e-05, -1.2380e-05]]],


        [[[-1.0566e-05,  8.6310e-06, -1.4380e-05],
          [-1.4428e-05, -1.4826e-05,  1.1642e-05],
          [-1.4450e-05,  1.5256e-05, -1.3306e-05]],

         [[ 9.6340e-06, -1.1270e-05,  8.7995e-06],
          [ 1.4688e-05,  1.8561e-07, -8.0392e-06],
          [-1.3465e-05,  1.1943e-05, -1.9183e-05]],

         [[ 1.2701e-05, -1.7556e-06,  2.2299e-07],
          [ 1.4054e-05, -1.7343e-05,  1.4295e-06],
          [-1.5935e-06,  2.1608e-06, -1.1797e-05]]],


        [[[ 7.3821e-06,  1.7524e-05,  1.1489e-05],
          [-8.0332e-06, -2.0064e-05,  2.2054e-05],
          [-1.7171e-05, -3.0554e-06, -7.2339e-06]],

         [[-1.9482e-06, -6.7936e-06,  1.4995e-05],
          [ 5.0321e-07, -2.2981e-05,  6.0468e-06],
          [-9.7555e-07,  7.1031e-06,  2.4746e-06]],

         [[-1.7419e-05, -2.0197e-05,  1.1029e-05],
          [ 1.4828e-05, -2.0844e-05, -8.0354e-07],
          [ 1.1261e-06, -1.1402e-06,  3.0028e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.3557]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0056]], device='cuda:0')

Epoch: 138 | Batch_idx: 0 |  Loss: (0.3482) |  Loss2: (0.1859) | Acc: (93.00%) (120/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.3652) |  Loss2: (0.1859) | Acc: (93.00%) (1317/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.3527) |  Loss2: (0.1858) | Acc: (93.00%) (2526/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.3467) |  Loss2: (0.1858) | Acc: (94.00%) (3744/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.3457) |  Loss2: (0.1857) | Acc: (94.00%) (4956/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.3459) |  Loss2: (0.1857) | Acc: (94.00%) (6170/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.3455) |  Loss2: (0.1856) | Acc: (94.00%) (7379/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.3453) |  Loss2: (0.1856) | Acc: (94.00%) (8581/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.3422) |  Loss2: (0.1855) | Acc: (94.00%) (9805/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.3406) |  Loss2: (0.1855) | Acc: (94.00%) (11032/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.3396) |  Loss2: (0.1854) | Acc: (94.00%) (12253/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.3400) |  Loss2: (0.1854) | Acc: (94.00%) (13468/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.3389) |  Loss2: (0.1853) | Acc: (94.00%) (14683/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.3381) |  Loss2: (0.1853) | Acc: (94.00%) (15905/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.3391) |  Loss2: (0.1852) | Acc: (94.00%) (17109/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.3390) |  Loss2: (0.1852) | Acc: (94.00%) (18333/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.3383) |  Loss2: (0.1851) | Acc: (94.00%) (19553/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.3380) |  Loss2: (0.1851) | Acc: (94.00%) (20760/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.3370) |  Loss2: (0.1850) | Acc: (94.00%) (21978/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.3365) |  Loss2: (0.1850) | Acc: (94.00%) (23193/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.3372) |  Loss2: (0.1849) | Acc: (94.00%) (24403/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.3373) |  Loss2: (0.1849) | Acc: (94.00%) (25611/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.3379) |  Loss2: (0.1848) | Acc: (94.00%) (26820/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.3387) |  Loss2: (0.1848) | Acc: (94.00%) (28023/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.3386) |  Loss2: (0.1848) | Acc: (94.00%) (29242/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.3381) |  Loss2: (0.1847) | Acc: (94.00%) (30461/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.3380) |  Loss2: (0.1847) | Acc: (94.00%) (31673/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.3369) |  Loss2: (0.1846) | Acc: (94.00%) (32901/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.3364) |  Loss2: (0.1846) | Acc: (94.00%) (34124/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.3363) |  Loss2: (0.1845) | Acc: (94.00%) (35341/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.3359) |  Loss2: (0.1845) | Acc: (94.00%) (36567/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.3353) |  Loss2: (0.1844) | Acc: (94.00%) (37791/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.3351) |  Loss2: (0.1844) | Acc: (94.00%) (39005/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.3350) |  Loss2: (0.1843) | Acc: (94.00%) (40213/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.3347) |  Loss2: (0.1843) | Acc: (94.00%) (41425/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.3342) |  Loss2: (0.1843) | Acc: (94.00%) (42645/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.3342) |  Loss2: (0.1842) | Acc: (94.00%) (43862/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.3342) |  Loss2: (0.1842) | Acc: (94.00%) (45076/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.3340) |  Loss2: (0.1841) | Acc: (94.00%) (46299/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.3335) |  Loss2: (0.1841) | Acc: (94.00%) (47479/50000)
# TEST : Loss: (0.4020) | Acc: (87.00%) (8799/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1281,  0.0683,  0.1000],
          [ 0.1107, -0.3212, -0.1160],
          [ 0.2679, -0.0801,  0.0106]],

         [[-0.0131, -0.0845,  0.2739],
          [ 0.0179, -0.1208,  0.0468],
          [ 0.0701,  0.0059, -0.0679]],

         [[ 0.0102, -0.0342,  0.2078],
          [ 0.0867, -0.2119,  0.1605],
          [-0.1130,  0.0159, -0.1881]]],


        [[[ 0.1025, -0.0682, -0.0016],
          [ 0.1402,  0.1783,  0.1804],
          [ 0.0178, -0.0567,  0.1182]],

         [[-0.1468, -0.1724,  0.1525],
          [ 0.0357, -0.0445,  0.0996],
          [ 0.1259, -0.1968,  0.1307]],

         [[ 0.1005, -0.0479, -0.0154],
          [ 0.0093,  0.0179, -0.1240],
          [-0.0290, -0.1594, -0.0236]]],


        [[[-0.0724,  0.0871, -0.0432],
          [-0.1254,  0.0635,  0.1140],
          [ 0.1076,  0.1640,  0.0013]],

         [[ 0.0976,  0.1305,  0.1111],
          [ 0.0675, -0.0382, -0.0355],
          [-0.0473, -0.0295,  0.1369]],

         [[-0.0450,  0.1420,  0.0725],
          [-0.0424, -0.0456, -0.0602],
          [-0.0397, -0.0083,  0.0409]]],


        ...,


        [[[ 0.2221, -0.1168, -0.1232],
          [-0.0548, -0.1199, -0.0934],
          [-0.0976,  0.0201,  0.0722]],

         [[ 0.1631,  0.1900, -0.1187],
          [ 0.1999, -0.1439, -0.1137],
          [-0.0609, -0.0785, -0.0267]],

         [[ 0.1808,  0.1021, -0.1051],
          [-0.0633, -0.1536, -0.0734],
          [ 0.0749,  0.1799, -0.1237]]],


        [[[-0.1056,  0.0863, -0.1437],
          [-0.1442, -0.1482,  0.1164],
          [-0.1444,  0.1525, -0.1330]],

         [[ 0.0963, -0.1127,  0.0880],
          [ 0.1468,  0.0019, -0.0804],
          [-0.1346,  0.1194, -0.1917]],

         [[ 0.1270, -0.0175,  0.0022],
          [ 0.1405, -0.1734,  0.0143],
          [-0.0159,  0.0216, -0.1179]]],


        [[[ 0.0738,  0.1752,  0.1148],
          [-0.0803, -0.2006,  0.2204],
          [-0.1716, -0.0305, -0.0723]],

         [[-0.0195, -0.0679,  0.1499],
          [ 0.0050, -0.2297,  0.0604],
          [-0.0098,  0.0710,  0.0247]],

         [[-0.1741, -0.2019,  0.1103],
          [ 0.1482, -0.2084, -0.0080],
          [ 0.0113, -0.0114,  0.3002]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2812e-05,  6.8272e-06,  9.9955e-06],
          [ 1.1070e-05, -3.2117e-05, -1.1600e-05],
          [ 2.6794e-05, -8.0138e-06,  1.0645e-06]],

         [[-1.3087e-06, -8.4530e-06,  2.7392e-05],
          [ 1.7883e-06, -1.2080e-05,  4.6783e-06],
          [ 7.0096e-06,  5.9017e-07, -6.7936e-06]],

         [[ 1.0234e-06, -3.4198e-06,  2.0783e-05],
          [ 8.6693e-06, -2.1195e-05,  1.6055e-05],
          [-1.1295e-05,  1.5895e-06, -1.8808e-05]]],


        [[[ 1.0248e-05, -6.8182e-06, -1.6083e-07],
          [ 1.4019e-05,  1.7826e-05,  1.8045e-05],
          [ 1.7804e-06, -5.6686e-06,  1.1822e-05]],

         [[-1.4683e-05, -1.7236e-05,  1.5252e-05],
          [ 3.5662e-06, -4.4485e-06,  9.9611e-06],
          [ 1.2588e-05, -1.9683e-05,  1.3068e-05]],

         [[ 1.0045e-05, -4.7900e-06, -1.5411e-06],
          [ 9.2714e-07,  1.7852e-06, -1.2400e-05],
          [-2.8954e-06, -1.5940e-05, -2.3625e-06]]],


        [[[-7.2397e-06,  8.7073e-06, -4.3197e-06],
          [-1.2537e-05,  6.3531e-06,  1.1399e-05],
          [ 1.0762e-05,  1.6403e-05,  1.3014e-07]],

         [[ 9.7634e-06,  1.3048e-05,  1.1109e-05],
          [ 6.7470e-06, -3.8155e-06, -3.5531e-06],
          [-4.7312e-06, -2.9470e-06,  1.3687e-05]],

         [[-4.4980e-06,  1.4202e-05,  7.2476e-06],
          [-4.2374e-06, -4.5607e-06, -6.0195e-06],
          [-3.9674e-06, -8.3068e-07,  4.0945e-06]]],


        ...,


        [[[ 2.2210e-05, -1.1683e-05, -1.2319e-05],
          [-5.4806e-06, -1.1992e-05, -9.3394e-06],
          [-9.7570e-06,  2.0052e-06,  7.2187e-06]],

         [[ 1.6308e-05,  1.9003e-05, -1.1875e-05],
          [ 1.9986e-05, -1.4392e-05, -1.1366e-05],
          [-6.0924e-06, -7.8515e-06, -2.6707e-06]],

         [[ 1.8082e-05,  1.0212e-05, -1.0514e-05],
          [-6.3257e-06, -1.5362e-05, -7.3370e-06],
          [ 7.4946e-06,  1.7986e-05, -1.2375e-05]]],


        [[[-1.0562e-05,  8.6275e-06, -1.4374e-05],
          [-1.4422e-05, -1.4820e-05,  1.1637e-05],
          [-1.4444e-05,  1.5250e-05, -1.3300e-05]],

         [[ 9.6303e-06, -1.1266e-05,  8.7960e-06],
          [ 1.4682e-05,  1.8554e-07, -8.0360e-06],
          [-1.3460e-05,  1.1938e-05, -1.9175e-05]],

         [[ 1.2695e-05, -1.7549e-06,  2.2290e-07],
          [ 1.4048e-05, -1.7336e-05,  1.4290e-06],
          [-1.5928e-06,  2.1599e-06, -1.1793e-05]]],


        [[[ 7.3791e-06,  1.7517e-05,  1.1485e-05],
          [-8.0300e-06, -2.0056e-05,  2.2045e-05],
          [-1.7164e-05, -3.0543e-06, -7.2310e-06]],

         [[-1.9474e-06, -6.7910e-06,  1.4989e-05],
          [ 5.0301e-07, -2.2972e-05,  6.0444e-06],
          [-9.7519e-07,  7.1002e-06,  2.4737e-06]],

         [[-1.7412e-05, -2.0189e-05,  1.1025e-05],
          [ 1.4823e-05, -2.0836e-05, -8.0322e-07],
          [ 1.1256e-06, -1.1397e-06,  3.0016e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.3847]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0303]], device='cuda:0')

Epoch: 139 | Batch_idx: 0 |  Loss: (0.3068) |  Loss2: (0.1824) | Acc: (94.00%) (121/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.3196) |  Loss2: (0.1823) | Acc: (95.00%) (1348/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.3208) |  Loss2: (0.1823) | Acc: (95.00%) (2567/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.3224) |  Loss2: (0.1822) | Acc: (95.00%) (3791/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.3271) |  Loss2: (0.1822) | Acc: (95.00%) (5000/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.3279) |  Loss2: (0.1821) | Acc: (95.00%) (6212/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.3320) |  Loss2: (0.1821) | Acc: (94.00%) (7408/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.3315) |  Loss2: (0.1821) | Acc: (94.00%) (8627/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.3297) |  Loss2: (0.1820) | Acc: (95.00%) (9852/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.3317) |  Loss2: (0.1820) | Acc: (94.00%) (11063/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.3311) |  Loss2: (0.1820) | Acc: (95.00%) (12286/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.3307) |  Loss2: (0.1819) | Acc: (95.00%) (13507/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.3302) |  Loss2: (0.1819) | Acc: (95.00%) (14724/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.3302) |  Loss2: (0.1818) | Acc: (95.00%) (15945/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.3291) |  Loss2: (0.1818) | Acc: (95.00%) (17168/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.3276) |  Loss2: (0.1818) | Acc: (95.00%) (18403/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.3284) |  Loss2: (0.1817) | Acc: (95.00%) (19609/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.3280) |  Loss2: (0.1817) | Acc: (95.00%) (20841/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.3279) |  Loss2: (0.1817) | Acc: (95.00%) (22058/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.3285) |  Loss2: (0.1816) | Acc: (95.00%) (23264/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.3290) |  Loss2: (0.1816) | Acc: (95.00%) (24470/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.3291) |  Loss2: (0.1816) | Acc: (95.00%) (25674/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.3289) |  Loss2: (0.1815) | Acc: (95.00%) (26894/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.3302) |  Loss2: (0.1815) | Acc: (95.00%) (28099/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.3297) |  Loss2: (0.1815) | Acc: (95.00%) (29319/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.3293) |  Loss2: (0.1814) | Acc: (95.00%) (30538/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.3292) |  Loss2: (0.1814) | Acc: (95.00%) (31758/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.3287) |  Loss2: (0.1814) | Acc: (95.00%) (32972/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.3292) |  Loss2: (0.1813) | Acc: (95.00%) (34174/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.3299) |  Loss2: (0.1813) | Acc: (94.00%) (35382/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.3298) |  Loss2: (0.1812) | Acc: (94.00%) (36600/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.3294) |  Loss2: (0.1812) | Acc: (95.00%) (37823/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.3292) |  Loss2: (0.1812) | Acc: (95.00%) (39043/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.3289) |  Loss2: (0.1811) | Acc: (95.00%) (40266/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.3293) |  Loss2: (0.1811) | Acc: (95.00%) (41483/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.3288) |  Loss2: (0.1811) | Acc: (95.00%) (42706/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.3293) |  Loss2: (0.1810) | Acc: (95.00%) (43913/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.3295) |  Loss2: (0.1810) | Acc: (95.00%) (45127/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.3301) |  Loss2: (0.1809) | Acc: (94.00%) (46328/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.3301) |  Loss2: (0.1809) | Acc: (95.00%) (47503/50000)
# TEST : Loss: (0.3990) | Acc: (88.00%) (8818/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1281,  0.0682,  0.0999],
          [ 0.1107, -0.3210, -0.1160],
          [ 0.2678, -0.0801,  0.0106]],

         [[-0.0131, -0.0845,  0.2738],
          [ 0.0179, -0.1208,  0.0468],
          [ 0.0701,  0.0059, -0.0679]],

         [[ 0.0102, -0.0342,  0.2077],
          [ 0.0867, -0.2119,  0.1605],
          [-0.1129,  0.0159, -0.1880]]],


        [[[ 0.1024, -0.0682, -0.0016],
          [ 0.1401,  0.1782,  0.1804],
          [ 0.0178, -0.0567,  0.1182]],

         [[-0.1468, -0.1723,  0.1525],
          [ 0.0356, -0.0445,  0.0996],
          [ 0.1258, -0.1968,  0.1306]],

         [[ 0.1004, -0.0479, -0.0154],
          [ 0.0093,  0.0178, -0.1239],
          [-0.0289, -0.1593, -0.0236]]],


        [[[-0.0724,  0.0870, -0.0432],
          [-0.1253,  0.0635,  0.1139],
          [ 0.1076,  0.1640,  0.0013]],

         [[ 0.0976,  0.1304,  0.1111],
          [ 0.0674, -0.0381, -0.0355],
          [-0.0473, -0.0295,  0.1368]],

         [[-0.0450,  0.1420,  0.0724],
          [-0.0424, -0.0456, -0.0602],
          [-0.0397, -0.0083,  0.0409]]],


        ...,


        [[[ 0.2220, -0.1168, -0.1231],
          [-0.0548, -0.1199, -0.0934],
          [-0.0975,  0.0200,  0.0722]],

         [[ 0.1630,  0.1900, -0.1187],
          [ 0.1998, -0.1439, -0.1136],
          [-0.0609, -0.0785, -0.0267]],

         [[ 0.1807,  0.1021, -0.1051],
          [-0.0632, -0.1536, -0.0733],
          [ 0.0749,  0.1798, -0.1237]]],


        [[[-0.1056,  0.0862, -0.1437],
          [-0.1442, -0.1481,  0.1163],
          [-0.1444,  0.1524, -0.1330]],

         [[ 0.0963, -0.1126,  0.0879],
          [ 0.1468,  0.0019, -0.0803],
          [-0.1345,  0.1193, -0.1917]],

         [[ 0.1269, -0.0175,  0.0022],
          [ 0.1404, -0.1733,  0.0143],
          [-0.0159,  0.0216, -0.1179]]],


        [[[ 0.0738,  0.1751,  0.1148],
          [-0.0803, -0.2005,  0.2204],
          [-0.1716, -0.0305, -0.0723]],

         [[-0.0195, -0.0679,  0.1498],
          [ 0.0050, -0.2296,  0.0604],
          [-0.0097,  0.0710,  0.0247]],

         [[-0.1741, -0.2018,  0.1102],
          [ 0.1482, -0.2083, -0.0080],
          [ 0.0113, -0.0114,  0.3000]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2806e-05,  6.8246e-06,  9.9917e-06],
          [ 1.1066e-05, -3.2105e-05, -1.1595e-05],
          [ 2.6783e-05, -8.0106e-06,  1.0641e-06]],

         [[-1.3081e-06, -8.4498e-06,  2.7382e-05],
          [ 1.7876e-06, -1.2075e-05,  4.6764e-06],
          [ 7.0070e-06,  5.8994e-07, -6.7910e-06]],

         [[ 1.0230e-06, -3.4185e-06,  2.0775e-05],
          [ 8.6658e-06, -2.1187e-05,  1.6048e-05],
          [-1.1291e-05,  1.5888e-06, -1.8800e-05]]],


        [[[ 1.0244e-05, -6.8155e-06, -1.6076e-07],
          [ 1.4014e-05,  1.7819e-05,  1.8038e-05],
          [ 1.7797e-06, -5.6664e-06,  1.1817e-05]],

         [[-1.4677e-05, -1.7229e-05,  1.5246e-05],
          [ 3.5647e-06, -4.4467e-06,  9.9573e-06],
          [ 1.2583e-05, -1.9676e-05,  1.3063e-05]],

         [[ 1.0042e-05, -4.7881e-06, -1.5405e-06],
          [ 9.2677e-07,  1.7844e-06, -1.2395e-05],
          [-2.8942e-06, -1.5934e-05, -2.3615e-06]]],


        [[[-7.2368e-06,  8.7038e-06, -4.3180e-06],
          [-1.2533e-05,  6.3505e-06,  1.1395e-05],
          [ 1.0758e-05,  1.6397e-05,  1.3009e-07]],

         [[ 9.7596e-06,  1.3043e-05,  1.1105e-05],
          [ 6.7444e-06, -3.8141e-06, -3.5516e-06],
          [-4.7293e-06, -2.9458e-06,  1.3681e-05]],

         [[-4.4963e-06,  1.4197e-05,  7.2446e-06],
          [-4.2358e-06, -4.5590e-06, -6.0172e-06],
          [-3.9658e-06, -8.3035e-07,  4.0929e-06]]],


        ...,


        [[[ 2.2201e-05, -1.1678e-05, -1.2314e-05],
          [-5.4784e-06, -1.1987e-05, -9.3357e-06],
          [-9.7532e-06,  2.0044e-06,  7.2158e-06]],

         [[ 1.6302e-05,  1.8996e-05, -1.1870e-05],
          [ 1.9979e-05, -1.4387e-05, -1.1362e-05],
          [-6.0901e-06, -7.8483e-06, -2.6697e-06]],

         [[ 1.8075e-05,  1.0208e-05, -1.0510e-05],
          [-6.3233e-06, -1.5356e-05, -7.3341e-06],
          [ 7.4917e-06,  1.7979e-05, -1.2370e-05]]],


        [[[-1.0558e-05,  8.6240e-06, -1.4368e-05],
          [-1.4416e-05, -1.4814e-05,  1.1632e-05],
          [-1.4438e-05,  1.5244e-05, -1.3295e-05]],

         [[ 9.6265e-06, -1.1261e-05,  8.7925e-06],
          [ 1.4676e-05,  1.8546e-07, -8.0328e-06],
          [-1.3455e-05,  1.1934e-05, -1.9167e-05]],

         [[ 1.2690e-05, -1.7543e-06,  2.2281e-07],
          [ 1.4043e-05, -1.7329e-05,  1.4285e-06],
          [-1.5922e-06,  2.1590e-06, -1.1788e-05]]],


        [[[ 7.3762e-06,  1.7510e-05,  1.1480e-05],
          [-8.0268e-06, -2.0049e-05,  2.2036e-05],
          [-1.7157e-05, -3.0531e-06, -7.2281e-06]],

         [[-1.9467e-06, -6.7884e-06,  1.4983e-05],
          [ 5.0281e-07, -2.2963e-05,  6.0421e-06],
          [-9.7482e-07,  7.0973e-06,  2.4728e-06]],

         [[-1.7405e-05, -2.0180e-05,  1.1021e-05],
          [ 1.4817e-05, -2.0828e-05, -8.0289e-07],
          [ 1.1252e-06, -1.1393e-06,  3.0005e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4122]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[0.0728]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (2568/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (3781/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (6213/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (94.00%) (8633/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (9840/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (11043/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (12252/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (13459/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (14667/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (15882/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (17099/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (18312/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (19520/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (20728/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (21951/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (23155/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (24359/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (25563/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (26786/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (27989/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (29212/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (30421/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (31623/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (32829/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (34028/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (35248/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (36451/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (37645/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1587) |  Loss2: (0.0000) | Acc: (94.00%) (38845/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (40055/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (41247/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (42460/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (43675/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (44884/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (46084/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (47253/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_140.pth.tar'
# TEST : Loss: (0.4235) | Acc: (87.00%) (8732/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1295,  0.0681,  0.1000],
          [ 0.1098, -0.3212, -0.1176],
          [ 0.2680, -0.0809,  0.0068]],

         [[-0.0178, -0.0868,  0.2721],
          [ 0.0130, -0.1237,  0.0433],
          [ 0.0652,  0.0008, -0.0749]],

         [[ 0.0086, -0.0325,  0.2108],
          [ 0.0855, -0.2101,  0.1627],
          [-0.1137,  0.0162, -0.1882]]],


        [[[ 0.0996, -0.0712, -0.0032],
          [ 0.1387,  0.1767,  0.1805],
          [ 0.0156, -0.0594,  0.1164]],

         [[-0.1490, -0.1746,  0.1519],
          [ 0.0346, -0.0452,  0.1006],
          [ 0.1240, -0.1987,  0.1298]],

         [[ 0.0994, -0.0488, -0.0145],
          [ 0.0094,  0.0183, -0.1217],
          [-0.0293, -0.1600, -0.0230]]],


        [[[-0.0721,  0.0873, -0.0428],
          [-0.1251,  0.0638,  0.1142],
          [ 0.1075,  0.1641,  0.0015]],

         [[ 0.0975,  0.1304,  0.1109],
          [ 0.0674, -0.0382, -0.0357],
          [-0.0476, -0.0297,  0.1364]],

         [[-0.0452,  0.1417,  0.0722],
          [-0.0425, -0.0457, -0.0603],
          [-0.0400, -0.0086,  0.0406]]],


        ...,


        [[[ 0.2224, -0.1169, -0.1240],
          [-0.0548, -0.1196, -0.0931],
          [-0.0981,  0.0206,  0.0723]],

         [[ 0.1629,  0.1895, -0.1198],
          [ 0.1998, -0.1433, -0.1134],
          [-0.0612, -0.0776, -0.0267]],

         [[ 0.1796,  0.1006, -0.1066],
          [-0.0637, -0.1536, -0.0735],
          [ 0.0746,  0.1804, -0.1237]]],


        [[[-0.1015,  0.0886, -0.1431],
          [-0.1409, -0.1464,  0.1167],
          [-0.1413,  0.1537, -0.1327]],

         [[ 0.0986, -0.1121,  0.0867],
          [ 0.1484,  0.0017, -0.0817],
          [-0.1330,  0.1189, -0.1930]],

         [[ 0.1282, -0.0185, -0.0005],
          [ 0.1409, -0.1748,  0.0112],
          [-0.0156,  0.0199, -0.1205]]],


        [[[ 0.0739,  0.1777,  0.1170],
          [-0.0821, -0.2011,  0.2203],
          [-0.1745, -0.0338, -0.0765]],

         [[-0.0162, -0.0631,  0.1541],
          [ 0.0051, -0.2286,  0.0626],
          [-0.0119,  0.0692,  0.0236]],

         [[-0.1727, -0.1995,  0.1115],
          [ 0.1494, -0.2074, -0.0074],
          [ 0.0123, -0.0115,  0.2990]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1050, -0.1219, -0.1056],
          [-0.1136, -0.1147, -0.0905],
          [-0.1223, -0.1227, -0.0973]],

         [[-0.0810, -0.0925, -0.0877],
          [-0.0833, -0.0783, -0.0722],
          [-0.0847, -0.0730, -0.0630]],

         [[-0.0459, -0.0539, -0.0480],
          [-0.0539, -0.0445, -0.0402],
          [-0.0617, -0.0478, -0.0394]]],


        [[[ 0.0069,  0.0052,  0.0071],
          [ 0.0017, -0.0023, -0.0019],
          [ 0.0019,  0.0014, -0.0000]],

         [[ 0.0046,  0.0009,  0.0079],
          [-0.0032, -0.0088, -0.0019],
          [-0.0015, -0.0032,  0.0040]],

         [[ 0.0085,  0.0034,  0.0110],
          [ 0.0019, -0.0043,  0.0038],
          [ 0.0063,  0.0040,  0.0130]]],


        [[[-0.0002, -0.0006, -0.0008],
          [-0.0015, -0.0015, -0.0011],
          [-0.0019, -0.0015, -0.0007]],

         [[ 0.0011,  0.0011,  0.0002],
          [-0.0000,  0.0007,  0.0007],
          [-0.0001,  0.0010,  0.0017]],

         [[ 0.0031,  0.0024,  0.0009],
          [ 0.0013,  0.0016,  0.0010],
          [ 0.0010,  0.0016,  0.0016]]],


        ...,


        [[[-0.0251, -0.0282, -0.0330],
          [-0.0158, -0.0129, -0.0134],
          [-0.0264, -0.0308, -0.0284]],

         [[-0.0304, -0.0302, -0.0319],
          [-0.0199, -0.0160, -0.0171],
          [-0.0339, -0.0362, -0.0366]],

         [[-0.0201, -0.0182, -0.0185],
          [-0.0091, -0.0046, -0.0047],
          [-0.0198, -0.0230, -0.0229]]],


        [[[-0.0128, -0.0106, -0.0090],
          [-0.0091, -0.0017,  0.0067],
          [-0.0155, -0.0075,  0.0036]],

         [[-0.0139, -0.0135, -0.0096],
          [-0.0120, -0.0072,  0.0033],
          [-0.0194, -0.0131,  0.0004]],

         [[-0.0145, -0.0118, -0.0070],
          [-0.0116, -0.0057,  0.0055],
          [-0.0178, -0.0103,  0.0044]]],


        [[[ 0.0853,  0.0668,  0.0373],
          [ 0.0692,  0.0753,  0.0698],
          [ 0.0670,  0.0749,  0.0715]],

         [[ 0.0538,  0.0277,  0.0024],
          [ 0.0567,  0.0489,  0.0435],
          [ 0.0712,  0.0708,  0.0696]],

         [[-0.0173, -0.0466, -0.0673],
          [-0.0149, -0.0255, -0.0259],
          [-0.0059, -0.0061, -0.0028]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4114]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 141 | Batch_idx: 0 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (2533/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (3750/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (6193/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (7409/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (8625/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1494) |  Loss2: (0.0000) | Acc: (94.00%) (9841/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (11055/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (12256/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (13475/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (14691/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (15919/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (94.00%) (17130/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (18328/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (19540/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (20746/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (21959/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (23189/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (24411/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (25634/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (26861/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (28075/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (29295/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (30492/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (31714/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (32922/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (34134/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (35346/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (36569/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (37773/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1486) |  Loss2: (0.0000) | Acc: (94.00%) (38983/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (40194/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (41407/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (42629/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (43834/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (45043/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (46250/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (47413/50000)
# TEST : Loss: (0.4317) | Acc: (87.00%) (8735/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1315,  0.0690,  0.1017],
          [ 0.1085, -0.3203, -0.1154],
          [ 0.2702, -0.0785,  0.0090]],

         [[-0.0192, -0.0842,  0.2765],
          [ 0.0128, -0.1204,  0.0489],
          [ 0.0680,  0.0048, -0.0708]],

         [[ 0.0065, -0.0314,  0.2135],
          [ 0.0841, -0.2087,  0.1664],
          [-0.1132,  0.0171, -0.1862]]],


        [[[ 0.0982, -0.0728, -0.0043],
          [ 0.1381,  0.1763,  0.1801],
          [ 0.0151, -0.0603,  0.1157]],

         [[-0.1504, -0.1763,  0.1504],
          [ 0.0339, -0.0458,  0.0998],
          [ 0.1233, -0.1997,  0.1288]],

         [[ 0.0991, -0.0495, -0.0151],
          [ 0.0094,  0.0180, -0.1220],
          [-0.0294, -0.1605, -0.0235]]],


        [[[-0.0722,  0.0872, -0.0429],
          [-0.1254,  0.0637,  0.1141],
          [ 0.1072,  0.1640,  0.0017]],

         [[ 0.0973,  0.1301,  0.1108],
          [ 0.0669, -0.0384, -0.0358],
          [-0.0480, -0.0298,  0.1364]],

         [[-0.0455,  0.1414,  0.0720],
          [-0.0429, -0.0459, -0.0604],
          [-0.0403, -0.0087,  0.0406]]],


        ...,


        [[[ 0.2250, -0.1155, -0.1225],
          [-0.0526, -0.1182, -0.0910],
          [-0.0960,  0.0224,  0.0745]],

         [[ 0.1653,  0.1902, -0.1189],
          [ 0.2015, -0.1426, -0.1120],
          [-0.0592, -0.0762, -0.0248]],

         [[ 0.1809,  0.1009, -0.1062],
          [-0.0628, -0.1535, -0.0727],
          [ 0.0750,  0.1804, -0.1230]]],


        [[[-0.1005,  0.0889, -0.1421],
          [-0.1406, -0.1463,  0.1174],
          [-0.1408,  0.1536, -0.1327]],

         [[ 0.1012, -0.1101,  0.0890],
          [ 0.1500,  0.0032, -0.0797],
          [-0.1313,  0.1201, -0.1917]],

         [[ 0.1309, -0.0164,  0.0017],
          [ 0.1428, -0.1733,  0.0131],
          [-0.0140,  0.0210, -0.1193]]],


        [[[ 0.0695,  0.1755,  0.1154],
          [-0.0859, -0.2039,  0.2188],
          [-0.1785, -0.0368, -0.0792]],

         [[-0.0168, -0.0623,  0.1552],
          [ 0.0039, -0.2292,  0.0636],
          [-0.0137,  0.0686,  0.0230]],

         [[-0.1728, -0.1974,  0.1145],
          [ 0.1482, -0.2069, -0.0045],
          [ 0.0097, -0.0118,  0.3000]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0280, -0.0571, -0.0681],
          [-0.0323, -0.0461, -0.0658],
          [-0.0288, -0.0508, -0.0690]],

         [[-0.0216, -0.0500, -0.0597],
          [-0.0277, -0.0450, -0.0614],
          [-0.0228, -0.0496, -0.0635]],

         [[-0.0531, -0.0768, -0.0902],
          [-0.0584, -0.0734, -0.0962],
          [-0.0491, -0.0726, -0.0973]]],


        [[[-0.0135, -0.0141, -0.0047],
          [-0.0053, -0.0044,  0.0042],
          [-0.0103, -0.0080,  0.0020]],

         [[-0.0085, -0.0102, -0.0030],
          [-0.0012, -0.0013,  0.0056],
          [-0.0084, -0.0063,  0.0026]],

         [[-0.0035, -0.0046,  0.0023],
          [ 0.0074,  0.0081,  0.0149],
          [ 0.0037,  0.0066,  0.0150]]],


        [[[ 0.0079,  0.0054,  0.0014],
          [ 0.0101,  0.0075,  0.0032],
          [ 0.0105,  0.0082,  0.0038]],

         [[ 0.0003, -0.0024, -0.0060],
          [ 0.0027, -0.0001, -0.0044],
          [ 0.0043,  0.0019, -0.0025]],

         [[ 0.0058,  0.0029, -0.0008],
          [ 0.0068,  0.0036, -0.0011],
          [ 0.0073,  0.0046, -0.0004]]],


        ...,


        [[[-0.0059, -0.0026, -0.0053],
          [-0.0090, -0.0025, -0.0047],
          [-0.0145, -0.0047, -0.0044]],

         [[-0.0091, -0.0057, -0.0052],
          [-0.0095, -0.0014, -0.0017],
          [-0.0125, -0.0017, -0.0008]],

         [[ 0.0171,  0.0200,  0.0151],
          [ 0.0155,  0.0199,  0.0156],
          [ 0.0130,  0.0200,  0.0169]]],


        [[[ 0.0102, -0.0029, -0.0099],
          [-0.0001, -0.0121, -0.0159],
          [ 0.0053, -0.0110, -0.0125]],

         [[-0.0030, -0.0109, -0.0120],
          [-0.0070, -0.0138, -0.0130],
          [ 0.0043, -0.0080, -0.0060]],

         [[ 0.0024, -0.0034, -0.0066],
          [-0.0034, -0.0083, -0.0091],
          [ 0.0058, -0.0032, -0.0028]]],


        [[[-0.0779, -0.0859, -0.0792],
          [-0.0680, -0.0807, -0.0815],
          [-0.0696, -0.0896, -0.0847]],

         [[-0.0694, -0.0867, -0.0885],
          [-0.0641, -0.0828, -0.0862],
          [-0.0656, -0.0882, -0.0753]],

         [[-0.0323, -0.0476, -0.0594],
          [-0.0215, -0.0413, -0.0514],
          [-0.0234, -0.0427, -0.0354]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4109]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 142 | Batch_idx: 0 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (95.00%) (3776/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (6216/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (7427/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (8651/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (9866/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (11082/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (12299/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (13507/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (95.00%) (14725/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (15954/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (17163/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (18386/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (19600/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (20826/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (22038/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (23255/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (24480/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (25703/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (26918/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (28132/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (29349/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (30562/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (31776/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (33001/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (34217/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (35430/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (36629/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (37841/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (39054/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (40264/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (41480/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (42702/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (43918/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (45113/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1460) |  Loss2: (0.0000) | Acc: (94.00%) (46324/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (47492/50000)
# TEST : Loss: (0.4257) | Acc: (87.00%) (8768/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1306,  0.0684,  0.0997],
          [ 0.1084, -0.3223, -0.1188],
          [ 0.2700, -0.0806,  0.0056]],

         [[-0.0197, -0.0839,  0.2760],
          [ 0.0113, -0.1222,  0.0469],
          [ 0.0678,  0.0035, -0.0729]],

         [[ 0.0057, -0.0307,  0.2134],
          [ 0.0826, -0.2100,  0.1652],
          [-0.1134,  0.0156, -0.1881]]],


        [[[ 0.0988, -0.0719, -0.0031],
          [ 0.1383,  0.1769,  0.1806],
          [ 0.0159, -0.0594,  0.1166]],

         [[-0.1493, -0.1753,  0.1515],
          [ 0.0341, -0.0455,  0.0998],
          [ 0.1236, -0.1992,  0.1291]],

         [[ 0.1008, -0.0483, -0.0139],
          [ 0.0105,  0.0186, -0.1218],
          [-0.0283, -0.1597, -0.0230]]],


        [[[-0.0727,  0.0869, -0.0429],
          [-0.1259,  0.0634,  0.1139],
          [ 0.1065,  0.1636,  0.0014]],

         [[ 0.0970,  0.1301,  0.1110],
          [ 0.0665, -0.0384, -0.0357],
          [-0.0485, -0.0299,  0.1364]],

         [[-0.0459,  0.1413,  0.0721],
          [-0.0433, -0.0459, -0.0604],
          [-0.0409, -0.0089,  0.0405]]],


        ...,


        [[[ 0.2273, -0.1137, -0.1218],
          [-0.0514, -0.1171, -0.0908],
          [-0.0961,  0.0222,  0.0745]],

         [[ 0.1665,  0.1913, -0.1184],
          [ 0.2018, -0.1418, -0.1118],
          [-0.0597, -0.0768, -0.0253]],

         [[ 0.1811,  0.1016, -0.1057],
          [-0.0630, -0.1528, -0.0723],
          [ 0.0738,  0.1794, -0.1234]]],


        [[[-0.1016,  0.0878, -0.1420],
          [-0.1415, -0.1473,  0.1172],
          [-0.1409,  0.1541, -0.1317]],

         [[ 0.1002, -0.1107,  0.0893],
          [ 0.1487,  0.0023, -0.0800],
          [-0.1317,  0.1205, -0.1911]],

         [[ 0.1307, -0.0165,  0.0020],
          [ 0.1423, -0.1736,  0.0130],
          [-0.0138,  0.0217, -0.1188]]],


        [[[ 0.0745,  0.1797,  0.1193],
          [-0.0813, -0.2021,  0.2209],
          [-0.1725, -0.0329, -0.0757]],

         [[-0.0112, -0.0580,  0.1592],
          [ 0.0084, -0.2274,  0.0657],
          [-0.0084,  0.0723,  0.0265]],

         [[-0.1718, -0.1986,  0.1133],
          [ 0.1495, -0.2091, -0.0056],
          [ 0.0124, -0.0113,  0.3016]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0427,  0.0493,  0.0501],
          [ 0.0523,  0.0574,  0.0602],
          [ 0.0536,  0.0577,  0.0537]],

         [[ 0.0288,  0.0304,  0.0333],
          [ 0.0378,  0.0373,  0.0395],
          [ 0.0335,  0.0310,  0.0281]],

         [[ 0.0200,  0.0183,  0.0215],
          [ 0.0260,  0.0202,  0.0221],
          [ 0.0170,  0.0081,  0.0050]]],


        [[[-0.0200, -0.0203, -0.0211],
          [-0.0294, -0.0281, -0.0251],
          [-0.0316, -0.0289, -0.0239]],

         [[-0.0377, -0.0380, -0.0377],
          [-0.0470, -0.0453, -0.0405],
          [-0.0475, -0.0447, -0.0384]],

         [[-0.0509, -0.0514, -0.0495],
          [-0.0591, -0.0580, -0.0524],
          [-0.0580, -0.0563, -0.0504]]],


        [[[-0.0010,  0.0025,  0.0026],
          [-0.0006,  0.0024,  0.0028],
          [-0.0008,  0.0014,  0.0017]],

         [[-0.0061, -0.0019, -0.0015],
          [-0.0055, -0.0018, -0.0010],
          [-0.0058, -0.0028, -0.0018]],

         [[-0.0045, -0.0008, -0.0010],
          [-0.0032, -0.0000,  0.0002],
          [-0.0029, -0.0004,  0.0002]]],


        ...,


        [[[ 0.0049,  0.0012,  0.0115],
          [ 0.0049,  0.0034,  0.0112],
          [ 0.0086,  0.0066,  0.0117]],

         [[ 0.0146,  0.0079,  0.0139],
          [ 0.0104,  0.0070,  0.0106],
          [ 0.0104,  0.0076,  0.0101]],

         [[ 0.0135,  0.0061,  0.0102],
          [ 0.0087,  0.0043,  0.0064],
          [ 0.0087,  0.0049,  0.0057]]],


        [[[ 0.0026, -0.0061, -0.0071],
          [ 0.0027, -0.0066, -0.0084],
          [ 0.0103,  0.0006,  0.0024]],

         [[ 0.0082, -0.0008, -0.0025],
          [ 0.0077, -0.0027, -0.0051],
          [ 0.0133,  0.0032,  0.0048]],

         [[ 0.0076, -0.0022, -0.0045],
          [ 0.0069, -0.0034, -0.0053],
          [ 0.0109,  0.0011,  0.0038]]],


        [[[ 0.0171,  0.0196,  0.0189],
          [ 0.0315,  0.0236,  0.0174],
          [ 0.0412,  0.0194,  0.0062]],

         [[-0.0044,  0.0030,  0.0137],
          [ 0.0135,  0.0071,  0.0094],
          [ 0.0339,  0.0088, -0.0004]],

         [[-0.0105, -0.0023,  0.0149],
          [ 0.0024, -0.0038,  0.0064],
          [ 0.0165, -0.0057, -0.0071]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4103]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 143 | Batch_idx: 0 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (2561/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (4994/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (6216/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (7437/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (8666/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (9892/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (11118/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (12331/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (13543/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (14749/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (15974/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (17187/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (18394/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (19618/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (20838/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (22063/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (23276/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (24504/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (25731/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (26943/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (28155/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (29362/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (30577/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (31800/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (33007/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (34220/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (35441/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (36658/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (37878/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (39104/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (40321/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (41520/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (42734/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (43946/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (45158/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (46382/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (47550/50000)
# TEST : Loss: (0.4628) | Acc: (86.00%) (8671/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1286,  0.0726,  0.1049],
          [ 0.1100, -0.3203, -0.1152],
          [ 0.2734, -0.0769,  0.0107]],

         [[-0.0208, -0.0820,  0.2790],
          [ 0.0107, -0.1214,  0.0493],
          [ 0.0691,  0.0056, -0.0691]],

         [[ 0.0029, -0.0307,  0.2141],
          [ 0.0800, -0.2107,  0.1656],
          [-0.1145,  0.0155, -0.1870]]],


        [[[ 0.0984, -0.0721, -0.0041],
          [ 0.1385,  0.1772,  0.1805],
          [ 0.0160, -0.0594,  0.1161]],

         [[-0.1503, -0.1766,  0.1490],
          [ 0.0336, -0.0462,  0.0984],
          [ 0.1229, -0.2000,  0.1277]],

         [[ 0.1000, -0.0496, -0.0159],
          [ 0.0100,  0.0179, -0.1227],
          [-0.0288, -0.1601, -0.0236]]],


        [[[-0.0723,  0.0869, -0.0430],
          [-0.1256,  0.0635,  0.1139],
          [ 0.1067,  0.1637,  0.0014]],

         [[ 0.0975,  0.1302,  0.1108],
          [ 0.0670, -0.0382, -0.0357],
          [-0.0481, -0.0297,  0.1363]],

         [[-0.0453,  0.1415,  0.0723],
          [-0.0427, -0.0456, -0.0601],
          [-0.0405, -0.0086,  0.0406]]],


        ...,


        [[[ 0.2231, -0.1172, -0.1254],
          [-0.0536, -0.1190, -0.0932],
          [-0.0979,  0.0202,  0.0712]],

         [[ 0.1624,  0.1875, -0.1223],
          [ 0.1993, -0.1442, -0.1148],
          [-0.0617, -0.0792, -0.0287]],

         [[ 0.1767,  0.0973, -0.1104],
          [-0.0655, -0.1558, -0.0761],
          [ 0.0715,  0.1764, -0.1273]]],


        [[[-0.1024,  0.0879, -0.1413],
          [-0.1423, -0.1471,  0.1182],
          [-0.1427,  0.1531, -0.1316]],

         [[ 0.0992, -0.1112,  0.0889],
          [ 0.1475,  0.0017, -0.0797],
          [-0.1337,  0.1191, -0.1913]],

         [[ 0.1301, -0.0169,  0.0014],
          [ 0.1417, -0.1737,  0.0134],
          [-0.0149,  0.0211, -0.1180]]],


        [[[ 0.0689,  0.1776,  0.1167],
          [-0.0835, -0.2038,  0.2210],
          [-0.1731, -0.0339, -0.0762]],

         [[-0.0175, -0.0615,  0.1553],
          [ 0.0065, -0.2288,  0.0651],
          [-0.0086,  0.0721,  0.0257]],

         [[-0.1743, -0.1990,  0.1126],
          [ 0.1511, -0.2070, -0.0025],
          [ 0.0157, -0.0076,  0.3052]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.2177,  0.2623,  0.2766],
          [ 0.2316,  0.2577,  0.2590],
          [ 0.2375,  0.2425,  0.2311]],

         [[ 0.1792,  0.2122,  0.2282],
          [ 0.2164,  0.2303,  0.2287],
          [ 0.2301,  0.2256,  0.2080]],

         [[ 0.1836,  0.2116,  0.2205],
          [ 0.2319,  0.2403,  0.2316],
          [ 0.2396,  0.2313,  0.2078]]],


        [[[ 0.0101,  0.0113,  0.0104],
          [ 0.0187,  0.0197,  0.0158],
          [ 0.0239,  0.0228,  0.0184]],

         [[ 0.0236,  0.0248,  0.0218],
          [ 0.0287,  0.0295,  0.0241],
          [ 0.0300,  0.0290,  0.0243]],

         [[ 0.0285,  0.0297,  0.0270],
          [ 0.0322,  0.0339,  0.0300],
          [ 0.0319,  0.0320,  0.0291]]],


        [[[ 0.0047,  0.0018, -0.0007],
          [ 0.0069,  0.0036, -0.0004],
          [ 0.0070,  0.0026, -0.0020]],

         [[ 0.0041,  0.0018, -0.0003],
          [ 0.0057,  0.0032, -0.0005],
          [ 0.0059,  0.0022, -0.0021]],

         [[ 0.0039,  0.0026,  0.0007],
          [ 0.0056,  0.0045,  0.0014],
          [ 0.0063,  0.0046,  0.0009]]],


        ...,


        [[[ 0.0027,  0.0016,  0.0025],
          [ 0.0066, -0.0010, -0.0066],
          [-0.0020, -0.0059, -0.0135]],

         [[-0.0038, -0.0014,  0.0034],
          [-0.0008, -0.0051, -0.0061],
          [-0.0084, -0.0102, -0.0150]],

         [[-0.0174, -0.0127, -0.0080],
          [-0.0103, -0.0108, -0.0123],
          [-0.0172, -0.0141, -0.0199]]],


        [[[ 0.0055,  0.0019,  0.0174],
          [ 0.0059, -0.0071, -0.0033],
          [ 0.0098, -0.0028, -0.0102]],

         [[ 0.0136,  0.0096,  0.0246],
          [ 0.0176,  0.0049,  0.0079],
          [ 0.0215,  0.0099,  0.0020]],

         [[ 0.0140,  0.0116,  0.0234],
          [ 0.0253,  0.0163,  0.0172],
          [ 0.0325,  0.0234,  0.0125]]],


        [[[ 0.0159,  0.0186,  0.0190],
          [-0.0018,  0.0171,  0.0284],
          [-0.0120, -0.0007,  0.0066]],

         [[ 0.0439,  0.0414,  0.0452],
          [ 0.0182,  0.0366,  0.0536],
          [ 0.0020,  0.0148,  0.0276]],

         [[ 0.0580,  0.0627,  0.0671],
          [ 0.0249,  0.0470,  0.0655],
          [ 0.0012,  0.0167,  0.0310]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4097]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

Epoch: 144 | Batch_idx: 0 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (94.00%) (1336/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (5001/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (6232/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (8693/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (9912/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (11129/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (12353/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (13584/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (14786/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (16000/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (17235/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (18453/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (19661/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (20874/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (22100/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (23325/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (24542/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (25766/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (26990/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (28211/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (29440/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (30649/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (31860/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (33067/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (34285/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (35508/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (36720/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (37933/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (39147/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (40359/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (41576/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (42800/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (44006/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (45219/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (46431/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (47597/50000)
# TEST : Loss: (0.4293) | Acc: (87.00%) (8747/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1287,  0.0715,  0.1005],
          [ 0.1115, -0.3218, -0.1197],
          [ 0.2776, -0.0756,  0.0068]],

         [[-0.0212, -0.0823,  0.2765],
          [ 0.0132, -0.1212,  0.0479],
          [ 0.0737,  0.0076, -0.0709]],

         [[ 0.0036, -0.0294,  0.2138],
          [ 0.0818, -0.2104,  0.1647],
          [-0.1113,  0.0167, -0.1888]]],


        [[[ 0.0983, -0.0721, -0.0044],
          [ 0.1389,  0.1780,  0.1808],
          [ 0.0162, -0.0587,  0.1164]],

         [[-0.1495, -0.1758,  0.1493],
          [ 0.0348, -0.0446,  0.0993],
          [ 0.1241, -0.1983,  0.1287]],

         [[ 0.0997, -0.0498, -0.0163],
          [ 0.0099,  0.0181, -0.1228],
          [-0.0289, -0.1597, -0.0235]]],


        [[[-0.0727,  0.0868, -0.0431],
          [-0.1262,  0.0633,  0.1137],
          [ 0.1062,  0.1636,  0.0013]],

         [[ 0.0971,  0.1301,  0.1106],
          [ 0.0662, -0.0384, -0.0359],
          [-0.0485, -0.0298,  0.1361]],

         [[-0.0454,  0.1414,  0.0721],
          [-0.0433, -0.0458, -0.0604],
          [-0.0409, -0.0087,  0.0404]]],


        ...,


        [[[ 0.2251, -0.1153, -0.1227],
          [-0.0533, -0.1189, -0.0914],
          [-0.0977,  0.0216,  0.0746]],

         [[ 0.1662,  0.1914, -0.1180],
          [ 0.2016, -0.1418, -0.1115],
          [-0.0596, -0.0761, -0.0245]],

         [[ 0.1800,  0.1012, -0.1066],
          [-0.0628, -0.1527, -0.0727],
          [ 0.0743,  0.1803, -0.1229]]],


        [[[-0.1013,  0.0889, -0.1403],
          [-0.1424, -0.1470,  0.1190],
          [-0.1425,  0.1533, -0.1312]],

         [[ 0.0998, -0.1106,  0.0890],
          [ 0.1473,  0.0018, -0.0795],
          [-0.1336,  0.1191, -0.1915]],

         [[ 0.1299, -0.0170,  0.0009],
          [ 0.1409, -0.1742,  0.0127],
          [-0.0153,  0.0205, -0.1190]]],


        [[[ 0.0689,  0.1756,  0.1136],
          [-0.0882, -0.2099,  0.2155],
          [-0.1771, -0.0374, -0.0826]],

         [[-0.0170, -0.0628,  0.1545],
          [ 0.0028, -0.2335,  0.0625],
          [-0.0111,  0.0701,  0.0216]],

         [[-0.1728, -0.1988,  0.1129],
          [ 0.1488, -0.2103, -0.0043],
          [ 0.0146, -0.0084,  0.3032]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0050, -0.0268, -0.0214],
          [-0.0192, -0.0432, -0.0398],
          [-0.0543, -0.0803, -0.0702]],

         [[-0.0168, -0.0380, -0.0333],
          [-0.0317, -0.0529, -0.0491],
          [-0.0609, -0.0835, -0.0714]],

         [[ 0.0408,  0.0119,  0.0048],
          [ 0.0420,  0.0174,  0.0113],
          [ 0.0287,  0.0048,  0.0051]]],


        [[[-0.0062, -0.0026, -0.0027],
          [-0.0079, -0.0057, -0.0037],
          [-0.0039, -0.0043, -0.0008]],

         [[-0.0073, -0.0033, -0.0025],
          [-0.0075, -0.0048, -0.0018],
          [-0.0035, -0.0039,  0.0012]],

         [[-0.0094, -0.0048, -0.0043],
          [-0.0111, -0.0086, -0.0065],
          [-0.0070, -0.0084, -0.0044]]],


        [[[ 0.0008,  0.0015,  0.0010],
          [ 0.0028,  0.0029,  0.0026],
          [ 0.0037,  0.0030,  0.0019]],

         [[-0.0002, -0.0007, -0.0018],
          [ 0.0019,  0.0014,  0.0005],
          [ 0.0029,  0.0018,  0.0004]],

         [[ 0.0030,  0.0021,  0.0006],
          [ 0.0053,  0.0047,  0.0035],
          [ 0.0069,  0.0059,  0.0041]]],


        ...,


        [[[-0.0061, -0.0058, -0.0022],
          [-0.0115, -0.0134, -0.0059],
          [-0.0126, -0.0122, -0.0005]],

         [[-0.0059, -0.0034,  0.0018],
          [-0.0105, -0.0108, -0.0015],
          [-0.0097, -0.0078,  0.0042]],

         [[-0.0110, -0.0075, -0.0036],
          [-0.0130, -0.0126, -0.0047],
          [-0.0119, -0.0101,  0.0004]]],


        [[[-0.0077, -0.0098, -0.0177],
          [-0.0008, -0.0050, -0.0129],
          [ 0.0015,  0.0020,  0.0003]],

         [[-0.0099, -0.0105, -0.0177],
          [-0.0041, -0.0060, -0.0124],
          [-0.0023, -0.0006, -0.0007]],

         [[-0.0135, -0.0136, -0.0194],
          [-0.0058, -0.0073, -0.0132],
          [-0.0009,  0.0001, -0.0013]]],


        [[[-0.0148,  0.0044,  0.0038],
          [-0.0186, -0.0173, -0.0332],
          [-0.0236, -0.0297, -0.0511]],

         [[-0.0013,  0.0174,  0.0224],
          [-0.0084, -0.0071, -0.0155],
          [-0.0134, -0.0199, -0.0313]],

         [[ 0.0116,  0.0236,  0.0328],
          [ 0.0051,  0.0055,  0.0023],
          [-0.0074, -0.0111, -0.0181]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4092]], device='cuda:0')

percentage_weight_grad tensor([[0.0001]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.2555) |  Loss2: (0.1796) | Acc: (98.00%) (126/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.3516) |  Loss2: (0.1796) | Acc: (94.00%) (1331/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.3531) |  Loss2: (0.1796) | Acc: (94.00%) (2531/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.3548) |  Loss2: (0.1795) | Acc: (93.00%) (3727/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.3543) |  Loss2: (0.1795) | Acc: (94.00%) (4934/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.3541) |  Loss2: (0.1794) | Acc: (94.00%) (6137/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.3552) |  Loss2: (0.1794) | Acc: (93.00%) (7336/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.3578) |  Loss2: (0.1793) | Acc: (93.00%) (8529/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.3587) |  Loss2: (0.1793) | Acc: (93.00%) (9727/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.3585) |  Loss2: (0.1792) | Acc: (93.00%) (10920/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.3567) |  Loss2: (0.1792) | Acc: (93.00%) (12118/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.3547) |  Loss2: (0.1791) | Acc: (93.00%) (13328/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.3544) |  Loss2: (0.1791) | Acc: (93.00%) (14529/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.3544) |  Loss2: (0.1790) | Acc: (93.00%) (15725/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.3544) |  Loss2: (0.1790) | Acc: (93.00%) (16920/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.3555) |  Loss2: (0.1789) | Acc: (93.00%) (18106/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.3540) |  Loss2: (0.1789) | Acc: (93.00%) (19313/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.3544) |  Loss2: (0.1788) | Acc: (93.00%) (20509/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.3528) |  Loss2: (0.1788) | Acc: (93.00%) (21721/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.3531) |  Loss2: (0.1787) | Acc: (93.00%) (22922/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.3515) |  Loss2: (0.1787) | Acc: (93.00%) (24141/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.3519) |  Loss2: (0.1786) | Acc: (93.00%) (25342/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.3510) |  Loss2: (0.1786) | Acc: (93.00%) (26545/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.3509) |  Loss2: (0.1785) | Acc: (93.00%) (27739/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.3511) |  Loss2: (0.1785) | Acc: (93.00%) (28942/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.3497) |  Loss2: (0.1784) | Acc: (93.00%) (30165/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.3491) |  Loss2: (0.1784) | Acc: (93.00%) (31369/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.3489) |  Loss2: (0.1783) | Acc: (93.00%) (32579/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.3491) |  Loss2: (0.1783) | Acc: (93.00%) (33780/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.3485) |  Loss2: (0.1782) | Acc: (93.00%) (35000/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.3485) |  Loss2: (0.1782) | Acc: (93.00%) (36210/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.3475) |  Loss2: (0.1781) | Acc: (94.00%) (37425/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.3472) |  Loss2: (0.1781) | Acc: (94.00%) (38631/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.3467) |  Loss2: (0.1780) | Acc: (94.00%) (39841/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.3459) |  Loss2: (0.1780) | Acc: (94.00%) (41064/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.3462) |  Loss2: (0.1779) | Acc: (94.00%) (42273/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.3453) |  Loss2: (0.1779) | Acc: (94.00%) (43493/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.3447) |  Loss2: (0.1778) | Acc: (94.00%) (44707/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.3443) |  Loss2: (0.1778) | Acc: (94.00%) (45918/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.3437) |  Loss2: (0.1777) | Acc: (94.00%) (47090/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_145.pth.tar'
# TEST : Loss: (0.4250) | Acc: (87.00%) (8736/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1268,  0.0733,  0.1022],
          [ 0.1133, -0.3199, -0.1181],
          [ 0.2798, -0.0733,  0.0087]],

         [[-0.0190, -0.0803,  0.2781],
          [ 0.0152, -0.1193,  0.0495],
          [ 0.0761,  0.0099, -0.0690]],

         [[ 0.0051, -0.0279,  0.2150],
          [ 0.0831, -0.2091,  0.1655],
          [-0.1097,  0.0181, -0.1877]]],


        [[[ 0.0984, -0.0722, -0.0045],
          [ 0.1390,  0.1780,  0.1808],
          [ 0.0163, -0.0587,  0.1164]],

         [[-0.1493, -0.1758,  0.1492],
          [ 0.0350, -0.0446,  0.0993],
          [ 0.1241, -0.1982,  0.1288]],

         [[ 0.1000, -0.0496, -0.0162],
          [ 0.0102,  0.0183, -0.1225],
          [-0.0287, -0.1594, -0.0233]]],


        [[[-0.0726,  0.0868, -0.0432],
          [-0.1261,  0.0632,  0.1135],
          [ 0.1062,  0.1635,  0.0012]],

         [[ 0.0971,  0.1301,  0.1106],
          [ 0.0663, -0.0384, -0.0359],
          [-0.0484, -0.0298,  0.1361]],

         [[-0.0453,  0.1414,  0.0721],
          [-0.0433, -0.0458, -0.0604],
          [-0.0408, -0.0087,  0.0405]]],


        ...,


        [[[ 0.2253, -0.1150, -0.1225],
          [-0.0529, -0.1185, -0.0913],
          [-0.0974,  0.0217,  0.0744]],

         [[ 0.1662,  0.1914, -0.1180],
          [ 0.2018, -0.1415, -0.1115],
          [-0.0595, -0.0761, -0.0248]],

         [[ 0.1801,  0.1013, -0.1065],
          [-0.0626, -0.1524, -0.0726],
          [ 0.0744,  0.1803, -0.1230]]],


        [[[-0.1009,  0.0891, -0.1401],
          [-0.1420, -0.1467,  0.1190],
          [-0.1422,  0.1534, -0.1312]],

         [[ 0.1001, -0.1105,  0.0890],
          [ 0.1475,  0.0018, -0.0795],
          [-0.1333,  0.1190, -0.1916]],

         [[ 0.1301, -0.0169,  0.0010],
          [ 0.1411, -0.1740,  0.0127],
          [-0.0152,  0.0204, -0.1191]]],


        [[[ 0.0698,  0.1761,  0.1141],
          [-0.0874, -0.2092,  0.2162],
          [-0.1763, -0.0367, -0.0818]],

         [[-0.0167, -0.0626,  0.1545],
          [ 0.0030, -0.2331,  0.0628],
          [-0.0108,  0.0704,  0.0219]],

         [[-0.1722, -0.1983,  0.1131],
          [ 0.1493, -0.2096, -0.0036],
          [ 0.0153, -0.0076,  0.3038]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2678e-05,  7.3345e-06,  1.0219e-05],
          [ 1.1332e-05, -3.1990e-05, -1.1806e-05],
          [ 2.7979e-05, -7.3314e-06,  8.6521e-07]],

         [[-1.9039e-06, -8.0309e-06,  2.7806e-05],
          [ 1.5229e-06, -1.1926e-05,  4.9509e-06],
          [ 7.6070e-06,  9.9065e-07, -6.8956e-06]],

         [[ 5.1120e-07, -2.7894e-06,  2.1496e-05],
          [ 8.3118e-06, -2.0914e-05,  1.6554e-05],
          [-1.0967e-05,  1.8075e-06, -1.8769e-05]]],


        [[[ 9.8357e-06, -7.2190e-06, -4.5456e-07],
          [ 1.3900e-05,  1.7796e-05,  1.8077e-05],
          [ 1.6253e-06, -5.8655e-06,  1.1642e-05]],

         [[-1.4929e-05, -1.7578e-05,  1.4919e-05],
          [ 3.4960e-06, -4.4583e-06,  9.9346e-06],
          [ 1.2408e-05, -1.9818e-05,  1.2876e-05]],

         [[ 9.9951e-06, -4.9636e-06, -1.6189e-06],
          [ 1.0207e-06,  1.8331e-06, -1.2249e-05],
          [-2.8696e-06, -1.5945e-05, -2.3290e-06]]],


        [[[-7.2611e-06,  8.6769e-06, -4.3167e-06],
          [-1.2610e-05,  6.3230e-06,  1.1355e-05],
          [ 1.0619e-05,  1.6350e-05,  1.2307e-07]],

         [[ 9.7138e-06,  1.3006e-05,  1.1056e-05],
          [ 6.6280e-06, -3.8387e-06, -3.5926e-06],
          [-4.8449e-06, -2.9779e-06,  1.3607e-05]],

         [[-4.5344e-06,  1.4140e-05,  7.2109e-06],
          [-4.3254e-06, -4.5782e-06, -6.0367e-06],
          [-4.0785e-06, -8.6739e-07,  4.0476e-06]]],


        ...,


        [[[ 2.2529e-05, -1.1499e-05, -1.2254e-05],
          [-5.2882e-06, -1.1845e-05, -9.1281e-06],
          [-9.7359e-06,  2.1710e-06,  7.4405e-06]],

         [[ 1.6621e-05,  1.9140e-05, -1.1804e-05],
          [ 2.0180e-05, -1.4150e-05, -1.1147e-05],
          [-5.9498e-06, -7.6124e-06, -2.4776e-06]],

         [[ 1.8007e-05,  1.0131e-05, -1.0650e-05],
          [-6.2552e-06, -1.5237e-05, -7.2611e-06],
          [ 7.4376e-06,  1.8028e-05, -1.2299e-05]]],


        [[[-1.0087e-05,  8.9094e-06, -1.4012e-05],
          [-1.4196e-05, -1.4670e-05,  1.1898e-05],
          [-1.4215e-05,  1.5337e-05, -1.3117e-05]],

         [[ 1.0011e-05, -1.1047e-05,  8.8987e-06],
          [ 1.4750e-05,  1.8259e-07, -7.9525e-06],
          [-1.3333e-05,  1.1902e-05, -1.9161e-05]],

         [[ 1.3009e-05, -1.6875e-06,  9.5614e-08],
          [ 1.4106e-05, -1.7404e-05,  1.2700e-06],
          [-1.5212e-06,  2.0447e-06, -1.1910e-05]]],


        [[[ 6.9814e-06,  1.7611e-05,  1.1413e-05],
          [-8.7386e-06, -2.0916e-05,  2.1618e-05],
          [-1.7632e-05, -3.6674e-06, -8.1782e-06]],

         [[-1.6705e-06, -6.2642e-06,  1.5455e-05],
          [ 3.0221e-07, -2.3314e-05,  6.2803e-06],
          [-1.0778e-06,  7.0444e-06,  2.1878e-06]],

         [[-1.7219e-05, -1.9827e-05,  1.1311e-05],
          [ 1.4928e-05, -2.0961e-05, -3.6311e-07],
          [ 1.5292e-06, -7.5765e-07,  3.0378e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4509]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0192]], device='cuda:0')

Epoch: 146 | Batch_idx: 0 |  Loss: (0.3364) |  Loss2: (0.1758) | Acc: (92.00%) (118/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.3259) |  Loss2: (0.1758) | Acc: (94.00%) (1325/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.3257) |  Loss2: (0.1757) | Acc: (94.00%) (2546/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.3301) |  Loss2: (0.1757) | Acc: (94.00%) (3749/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.3275) |  Loss2: (0.1756) | Acc: (94.00%) (4966/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.3259) |  Loss2: (0.1756) | Acc: (94.00%) (6184/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.3256) |  Loss2: (0.1755) | Acc: (94.00%) (7396/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.3246) |  Loss2: (0.1754) | Acc: (94.00%) (8609/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.3254) |  Loss2: (0.1754) | Acc: (94.00%) (9812/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.3249) |  Loss2: (0.1753) | Acc: (94.00%) (11030/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.3234) |  Loss2: (0.1753) | Acc: (94.00%) (12255/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.3253) |  Loss2: (0.1752) | Acc: (94.00%) (13455/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.3263) |  Loss2: (0.1752) | Acc: (94.00%) (14665/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.3264) |  Loss2: (0.1751) | Acc: (94.00%) (15875/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.3255) |  Loss2: (0.1751) | Acc: (94.00%) (17093/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.3260) |  Loss2: (0.1750) | Acc: (94.00%) (18302/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.3256) |  Loss2: (0.1750) | Acc: (94.00%) (19527/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.3248) |  Loss2: (0.1749) | Acc: (94.00%) (20744/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.3235) |  Loss2: (0.1749) | Acc: (94.00%) (21974/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.3225) |  Loss2: (0.1748) | Acc: (94.00%) (23197/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.3222) |  Loss2: (0.1748) | Acc: (94.00%) (24422/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.3216) |  Loss2: (0.1747) | Acc: (94.00%) (25644/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.3221) |  Loss2: (0.1747) | Acc: (94.00%) (26852/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.3214) |  Loss2: (0.1746) | Acc: (94.00%) (28075/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.3215) |  Loss2: (0.1746) | Acc: (94.00%) (29291/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.3209) |  Loss2: (0.1745) | Acc: (94.00%) (30509/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.3203) |  Loss2: (0.1745) | Acc: (94.00%) (31736/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.3196) |  Loss2: (0.1744) | Acc: (95.00%) (32960/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.3200) |  Loss2: (0.1744) | Acc: (94.00%) (34163/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.3202) |  Loss2: (0.1743) | Acc: (94.00%) (35378/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.3201) |  Loss2: (0.1743) | Acc: (94.00%) (36592/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.3195) |  Loss2: (0.1743) | Acc: (94.00%) (37807/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.3194) |  Loss2: (0.1742) | Acc: (94.00%) (39025/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.3187) |  Loss2: (0.1742) | Acc: (94.00%) (40248/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.3179) |  Loss2: (0.1741) | Acc: (95.00%) (41480/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.3180) |  Loss2: (0.1741) | Acc: (95.00%) (42693/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.3183) |  Loss2: (0.1740) | Acc: (95.00%) (43911/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.3184) |  Loss2: (0.1740) | Acc: (95.00%) (45127/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.3189) |  Loss2: (0.1739) | Acc: (95.00%) (46334/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.3190) |  Loss2: (0.1739) | Acc: (95.00%) (47511/50000)
# TEST : Loss: (0.4098) | Acc: (87.00%) (8767/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1267,  0.0733,  0.1022],
          [ 0.1133, -0.3198, -0.1180],
          [ 0.2797, -0.0733,  0.0086]],

         [[-0.0190, -0.0803,  0.2780],
          [ 0.0152, -0.1192,  0.0495],
          [ 0.0760,  0.0099, -0.0689]],

         [[ 0.0051, -0.0279,  0.2149],
          [ 0.0831, -0.2091,  0.1655],
          [-0.1096,  0.0181, -0.1876]]],


        [[[ 0.0983, -0.0722, -0.0045],
          [ 0.1390,  0.1779,  0.1807],
          [ 0.0162, -0.0586,  0.1164]],

         [[-0.1492, -0.1757,  0.1491],
          [ 0.0349, -0.0446,  0.0993],
          [ 0.1240, -0.1981,  0.1287]],

         [[ 0.0999, -0.0496, -0.0162],
          [ 0.0102,  0.0183, -0.1224],
          [-0.0287, -0.1594, -0.0233]]],


        [[[-0.0726,  0.0867, -0.0431],
          [-0.1261,  0.0632,  0.1135],
          [ 0.1062,  0.1634,  0.0012]],

         [[ 0.0971,  0.1300,  0.1105],
          [ 0.0663, -0.0384, -0.0359],
          [-0.0484, -0.0298,  0.1360]],

         [[-0.0453,  0.1414,  0.0721],
          [-0.0432, -0.0458, -0.0603],
          [-0.0408, -0.0087,  0.0405]]],


        ...,


        [[[ 0.2252, -0.1149, -0.1225],
          [-0.0529, -0.1184, -0.0912],
          [-0.0973,  0.0217,  0.0744]],

         [[ 0.1661,  0.1913, -0.1180],
          [ 0.2017, -0.1415, -0.1114],
          [-0.0595, -0.0761, -0.0248]],

         [[ 0.1800,  0.1013, -0.1065],
          [-0.0625, -0.1523, -0.0726],
          [ 0.0743,  0.1802, -0.1229]]],


        [[[-0.1008,  0.0891, -0.1401],
          [-0.1419, -0.1466,  0.1189],
          [-0.1421,  0.1533, -0.1311]],

         [[ 0.1001, -0.1104,  0.0890],
          [ 0.1474,  0.0018, -0.0795],
          [-0.1333,  0.1190, -0.1915]],

         [[ 0.1300, -0.0169,  0.0010],
          [ 0.1410, -0.1740,  0.0127],
          [-0.0152,  0.0204, -0.1191]]],


        [[[ 0.0698,  0.1760,  0.1141],
          [-0.0874, -0.2091,  0.2161],
          [-0.1762, -0.0367, -0.0817]],

         [[-0.0167, -0.0626,  0.1545],
          [ 0.0030, -0.2330,  0.0628],
          [-0.0108,  0.0704,  0.0219]],

         [[-0.1721, -0.1982,  0.1131],
          [ 0.1492, -0.2095, -0.0036],
          [ 0.0153, -0.0076,  0.3037]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2673e-05,  7.3316e-06,  1.0215e-05],
          [ 1.1327e-05, -3.1978e-05, -1.1802e-05],
          [ 2.7968e-05, -7.3285e-06,  8.6488e-07]],

         [[-1.9032e-06, -8.0277e-06,  2.7795e-05],
          [ 1.5223e-06, -1.1921e-05,  4.9490e-06],
          [ 7.6041e-06,  9.9025e-07, -6.8930e-06]],

         [[ 5.1100e-07, -2.7883e-06,  2.1488e-05],
          [ 8.3086e-06, -2.0906e-05,  1.6547e-05],
          [-1.0963e-05,  1.8068e-06, -1.8761e-05]]],


        [[[ 9.8319e-06, -7.2160e-06, -4.5438e-07],
          [ 1.3895e-05,  1.7789e-05,  1.8070e-05],
          [ 1.6247e-06, -5.8631e-06,  1.1638e-05]],

         [[-1.4923e-05, -1.7571e-05,  1.4913e-05],
          [ 3.4947e-06, -4.4566e-06,  9.9308e-06],
          [ 1.2403e-05, -1.9810e-05,  1.2871e-05]],

         [[ 9.9913e-06, -4.9617e-06, -1.6182e-06],
          [ 1.0203e-06,  1.8324e-06, -1.2244e-05],
          [-2.8685e-06, -1.5938e-05, -2.3281e-06]]],


        [[[-7.2582e-06,  8.6734e-06, -4.3150e-06],
          [-1.2606e-05,  6.3207e-06,  1.1350e-05],
          [ 1.0615e-05,  1.6343e-05,  1.2302e-07]],

         [[ 9.7100e-06,  1.3001e-05,  1.1052e-05],
          [ 6.6254e-06, -3.8372e-06, -3.5911e-06],
          [-4.8430e-06, -2.9767e-06,  1.3602e-05]],

         [[-4.5327e-06,  1.4135e-05,  7.2080e-06],
          [-4.3236e-06, -4.5765e-06, -6.0344e-06],
          [-4.0769e-06, -8.6707e-07,  4.0460e-06]]],


        ...,


        [[[ 2.2521e-05, -1.1495e-05, -1.2249e-05],
          [-5.2861e-06, -1.1841e-05, -9.1246e-06],
          [-9.7321e-06,  2.1701e-06,  7.4376e-06]],

         [[ 1.6615e-05,  1.9133e-05, -1.1799e-05],
          [ 2.0172e-05, -1.4145e-05, -1.1143e-05],
          [-5.9474e-06, -7.6095e-06, -2.4767e-06]],

         [[ 1.8000e-05,  1.0127e-05, -1.0646e-05],
          [-6.2529e-06, -1.5231e-05, -7.2582e-06],
          [ 7.4347e-06,  1.8021e-05, -1.2294e-05]]],


        [[[-1.0083e-05,  8.9059e-06, -1.4007e-05],
          [-1.4191e-05, -1.4664e-05,  1.1894e-05],
          [-1.4209e-05,  1.5331e-05, -1.3112e-05]],

         [[ 1.0007e-05, -1.1043e-05,  8.8952e-06],
          [ 1.4744e-05,  1.8252e-07, -7.9493e-06],
          [-1.3328e-05,  1.1897e-05, -1.9154e-05]],

         [[ 1.3003e-05, -1.6869e-06,  9.5577e-08],
          [ 1.4100e-05, -1.7397e-05,  1.2695e-06],
          [-1.5206e-06,  2.0439e-06, -1.1905e-05]]],


        [[[ 6.9788e-06,  1.7604e-05,  1.1408e-05],
          [-8.7351e-06, -2.0908e-05,  2.1609e-05],
          [-1.7625e-05, -3.6659e-06, -8.1750e-06]],

         [[-1.6699e-06, -6.2618e-06,  1.5449e-05],
          [ 3.0209e-07, -2.3304e-05,  6.2779e-06],
          [-1.0773e-06,  7.0418e-06,  2.1870e-06]],

         [[-1.7212e-05, -1.9819e-05,  1.1306e-05],
          [ 1.4922e-05, -2.0952e-05, -3.6297e-07],
          [ 1.5286e-06, -7.5736e-07,  3.0367e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.4987]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0854]], device='cuda:0')

Epoch: 147 | Batch_idx: 0 |  Loss: (0.2868) |  Loss2: (0.1721) | Acc: (96.00%) (124/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.3010) |  Loss2: (0.1720) | Acc: (95.00%) (1351/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.3052) |  Loss2: (0.1720) | Acc: (95.00%) (2569/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.3023) |  Loss2: (0.1719) | Acc: (95.00%) (3794/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.3014) |  Loss2: (0.1719) | Acc: (95.00%) (5018/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.3037) |  Loss2: (0.1718) | Acc: (95.00%) (6241/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.3039) |  Loss2: (0.1718) | Acc: (95.00%) (7464/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.3060) |  Loss2: (0.1717) | Acc: (95.00%) (8675/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.3077) |  Loss2: (0.1717) | Acc: (95.00%) (9889/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.3074) |  Loss2: (0.1716) | Acc: (95.00%) (11111/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.3066) |  Loss2: (0.1716) | Acc: (95.00%) (12341/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.3084) |  Loss2: (0.1715) | Acc: (95.00%) (13552/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.3104) |  Loss2: (0.1715) | Acc: (95.00%) (14761/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.3104) |  Loss2: (0.1714) | Acc: (95.00%) (15980/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.3110) |  Loss2: (0.1714) | Acc: (95.00%) (17195/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.3105) |  Loss2: (0.1713) | Acc: (95.00%) (18418/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.3105) |  Loss2: (0.1713) | Acc: (95.00%) (19648/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.3091) |  Loss2: (0.1712) | Acc: (95.00%) (20879/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.3096) |  Loss2: (0.1712) | Acc: (95.00%) (22100/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.3102) |  Loss2: (0.1711) | Acc: (95.00%) (23313/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.3117) |  Loss2: (0.1711) | Acc: (95.00%) (24523/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.3112) |  Loss2: (0.1710) | Acc: (95.00%) (25743/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.3104) |  Loss2: (0.1710) | Acc: (95.00%) (26968/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.3103) |  Loss2: (0.1709) | Acc: (95.00%) (28184/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.3100) |  Loss2: (0.1709) | Acc: (95.00%) (29407/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.3093) |  Loss2: (0.1708) | Acc: (95.00%) (30638/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.3096) |  Loss2: (0.1708) | Acc: (95.00%) (31851/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.3090) |  Loss2: (0.1707) | Acc: (95.00%) (33083/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.3091) |  Loss2: (0.1707) | Acc: (95.00%) (34299/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.3091) |  Loss2: (0.1706) | Acc: (95.00%) (35522/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.3090) |  Loss2: (0.1706) | Acc: (95.00%) (36743/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.3085) |  Loss2: (0.1706) | Acc: (95.00%) (37976/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.3081) |  Loss2: (0.1705) | Acc: (95.00%) (39204/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.3087) |  Loss2: (0.1705) | Acc: (95.00%) (40410/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.3081) |  Loss2: (0.1704) | Acc: (95.00%) (41637/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.3081) |  Loss2: (0.1704) | Acc: (95.00%) (42860/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.3077) |  Loss2: (0.1703) | Acc: (95.00%) (44089/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.3077) |  Loss2: (0.1703) | Acc: (95.00%) (45308/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.3078) |  Loss2: (0.1702) | Acc: (95.00%) (46519/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.3078) |  Loss2: (0.1702) | Acc: (95.00%) (47687/50000)
# TEST : Loss: (0.4028) | Acc: (87.00%) (8782/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1267,  0.0733,  0.1021],
          [ 0.1132, -0.3196, -0.1180],
          [ 0.2796, -0.0733,  0.0086]],

         [[-0.0190, -0.0802,  0.2778],
          [ 0.0152, -0.1192,  0.0495],
          [ 0.0760,  0.0099, -0.0689]],

         [[ 0.0051, -0.0279,  0.2148],
          [ 0.0831, -0.2090,  0.1654],
          [-0.1096,  0.0181, -0.1875]]],


        [[[ 0.0983, -0.0721, -0.0045],
          [ 0.1389,  0.1778,  0.1806],
          [ 0.0162, -0.0586,  0.1163]],

         [[-0.1492, -0.1756,  0.1491],
          [ 0.0349, -0.0445,  0.0993],
          [ 0.1240, -0.1980,  0.1287]],

         [[ 0.0999, -0.0496, -0.0162],
          [ 0.0102,  0.0183, -0.1224],
          [-0.0287, -0.1593, -0.0233]]],


        [[[-0.0726,  0.0867, -0.0431],
          [-0.1260,  0.0632,  0.1135],
          [ 0.1061,  0.1634,  0.0012]],

         [[ 0.0971,  0.1300,  0.1105],
          [ 0.0662, -0.0384, -0.0359],
          [-0.0484, -0.0298,  0.1360]],

         [[-0.0453,  0.1413,  0.0721],
          [-0.0432, -0.0457, -0.0603],
          [-0.0408, -0.0087,  0.0404]]],


        ...,


        [[[ 0.2251, -0.1149, -0.1224],
          [-0.0528, -0.1184, -0.0912],
          [-0.0973,  0.0217,  0.0743]],

         [[ 0.1661,  0.1913, -0.1179],
          [ 0.2016, -0.1414, -0.1114],
          [-0.0595, -0.0761, -0.0248]],

         [[ 0.1799,  0.1012, -0.1064],
          [-0.0625, -0.1523, -0.0726],
          [ 0.0743,  0.1801, -0.1229]]],


        [[[-0.1008,  0.0890, -0.1400],
          [-0.1418, -0.1466,  0.1189],
          [-0.1420,  0.1533, -0.1311]],

         [[ 0.1000, -0.1104,  0.0889],
          [ 0.1474,  0.0018, -0.0795],
          [-0.1332,  0.1189, -0.1915]],

         [[ 0.1300, -0.0169,  0.0010],
          [ 0.1410, -0.1739,  0.0127],
          [-0.0152,  0.0204, -0.1190]]],


        [[[ 0.0698,  0.1760,  0.1140],
          [-0.0873, -0.2090,  0.2160],
          [-0.1762, -0.0366, -0.0817]],

         [[-0.0167, -0.0626,  0.1544],
          [ 0.0030, -0.2330,  0.0628],
          [-0.0108,  0.0704,  0.0219]],

         [[-0.1721, -0.1981,  0.1130],
          [ 0.1492, -0.2094, -0.0036],
          [ 0.0153, -0.0076,  0.3035]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2668e-05,  7.3286e-06,  1.0211e-05],
          [ 1.1323e-05, -3.1965e-05, -1.1797e-05],
          [ 2.7958e-05, -7.3255e-06,  8.6455e-07]],

         [[-1.9024e-06, -8.0245e-06,  2.7785e-05],
          [ 1.5217e-06, -1.1917e-05,  4.9471e-06],
          [ 7.6012e-06,  9.8985e-07, -6.8904e-06]],

         [[ 5.1080e-07, -2.7872e-06,  2.1480e-05],
          [ 8.3054e-06, -2.0897e-05,  1.6541e-05],
          [-1.0958e-05,  1.8061e-06, -1.8753e-05]]],


        [[[ 9.8281e-06, -7.2131e-06, -4.5420e-07],
          [ 1.3890e-05,  1.7782e-05,  1.8063e-05],
          [ 1.6240e-06, -5.8608e-06,  1.1633e-05]],

         [[-1.4917e-05, -1.7564e-05,  1.4907e-05],
          [ 3.4934e-06, -4.4548e-06,  9.9270e-06],
          [ 1.2398e-05, -1.9803e-05,  1.2865e-05]],

         [[ 9.9875e-06, -4.9598e-06, -1.6176e-06],
          [ 1.0199e-06,  1.8316e-06, -1.2239e-05],
          [-2.8674e-06, -1.5932e-05, -2.3272e-06]]],


        [[[-7.2553e-06,  8.6699e-06, -4.3132e-06],
          [-1.2601e-05,  6.3184e-06,  1.1346e-05],
          [ 1.0611e-05,  1.6337e-05,  1.2297e-07]],

         [[ 9.7062e-06,  1.2995e-05,  1.1048e-05],
          [ 6.6228e-06, -3.8358e-06, -3.5897e-06],
          [-4.8411e-06, -2.9756e-06,  1.3597e-05]],

         [[-4.5309e-06,  1.4130e-05,  7.2051e-06],
          [-4.3219e-06, -4.5747e-06, -6.0320e-06],
          [-4.0753e-06, -8.6674e-07,  4.0444e-06]]],


        ...,


        [[[ 2.2512e-05, -1.1490e-05, -1.2244e-05],
          [-5.2841e-06, -1.1836e-05, -9.1211e-06],
          [-9.7283e-06,  2.1693e-06,  7.4346e-06]],

         [[ 1.6608e-05,  1.9125e-05, -1.1795e-05],
          [ 2.0164e-05, -1.4140e-05, -1.1138e-05],
          [-5.9451e-06, -7.6065e-06, -2.4757e-06]],

         [[ 1.7993e-05,  1.0123e-05, -1.0642e-05],
          [-6.2506e-06, -1.5226e-05, -7.2553e-06],
          [ 7.4318e-06,  1.8014e-05, -1.2289e-05]]],


        [[[-1.0078e-05,  8.9024e-06, -1.4001e-05],
          [-1.4185e-05, -1.4658e-05,  1.1889e-05],
          [-1.4204e-05,  1.5326e-05, -1.3107e-05]],

         [[ 1.0004e-05, -1.1039e-05,  8.8917e-06],
          [ 1.4738e-05,  1.8245e-07, -7.9461e-06],
          [-1.3323e-05,  1.1893e-05, -1.9146e-05]],

         [[ 1.2998e-05, -1.6862e-06,  9.5541e-08],
          [ 1.4095e-05, -1.7390e-05,  1.2690e-06],
          [-1.5201e-06,  2.0431e-06, -1.1901e-05]]],


        [[[ 6.9761e-06,  1.7597e-05,  1.1404e-05],
          [-8.7316e-06, -2.0900e-05,  2.1601e-05],
          [-1.7618e-05, -3.6644e-06, -8.1718e-06]],

         [[-1.6692e-06, -6.2595e-06,  1.5443e-05],
          [ 3.0197e-07, -2.3295e-05,  6.2756e-06],
          [-1.0769e-06,  7.0391e-06,  2.1861e-06]],

         [[-1.7205e-05, -1.9812e-05,  1.1302e-05],
          [ 1.4917e-05, -2.0944e-05, -3.6282e-07],
          [ 1.5280e-06, -7.5707e-07,  3.0355e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.5444]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0041]], device='cuda:0')

Epoch: 148 | Batch_idx: 0 |  Loss: (0.2969) |  Loss2: (0.1685) | Acc: (96.00%) (124/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.3063) |  Loss2: (0.1685) | Acc: (96.00%) (1359/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.3120) |  Loss2: (0.1684) | Acc: (95.00%) (2570/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.3092) |  Loss2: (0.1684) | Acc: (95.00%) (3792/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.3070) |  Loss2: (0.1684) | Acc: (95.00%) (5018/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.3093) |  Loss2: (0.1683) | Acc: (95.00%) (6238/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.3055) |  Loss2: (0.1683) | Acc: (95.00%) (7470/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.3046) |  Loss2: (0.1682) | Acc: (95.00%) (8701/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.3072) |  Loss2: (0.1682) | Acc: (95.00%) (9911/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.3063) |  Loss2: (0.1682) | Acc: (95.00%) (11135/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.3059) |  Loss2: (0.1681) | Acc: (95.00%) (12352/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.3063) |  Loss2: (0.1681) | Acc: (95.00%) (13574/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.3053) |  Loss2: (0.1680) | Acc: (95.00%) (14800/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.3033) |  Loss2: (0.1680) | Acc: (95.00%) (16025/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.3023) |  Loss2: (0.1680) | Acc: (95.00%) (17250/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.3012) |  Loss2: (0.1679) | Acc: (95.00%) (18478/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.3002) |  Loss2: (0.1679) | Acc: (95.00%) (19704/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.3009) |  Loss2: (0.1678) | Acc: (95.00%) (20923/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.2999) |  Loss2: (0.1678) | Acc: (95.00%) (22149/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.3003) |  Loss2: (0.1677) | Acc: (95.00%) (23367/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.3010) |  Loss2: (0.1677) | Acc: (95.00%) (24589/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.3008) |  Loss2: (0.1677) | Acc: (95.00%) (25813/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.3020) |  Loss2: (0.1676) | Acc: (95.00%) (27020/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.3022) |  Loss2: (0.1676) | Acc: (95.00%) (28241/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.3030) |  Loss2: (0.1675) | Acc: (95.00%) (29453/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.3031) |  Loss2: (0.1675) | Acc: (95.00%) (30660/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.3027) |  Loss2: (0.1675) | Acc: (95.00%) (31894/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.3021) |  Loss2: (0.1674) | Acc: (95.00%) (33125/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.3024) |  Loss2: (0.1674) | Acc: (95.00%) (34337/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.3021) |  Loss2: (0.1673) | Acc: (95.00%) (35558/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.3023) |  Loss2: (0.1673) | Acc: (95.00%) (36772/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.3022) |  Loss2: (0.1672) | Acc: (95.00%) (37993/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.3020) |  Loss2: (0.1672) | Acc: (95.00%) (39212/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.3017) |  Loss2: (0.1672) | Acc: (95.00%) (40440/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.3014) |  Loss2: (0.1671) | Acc: (95.00%) (41662/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.3011) |  Loss2: (0.1671) | Acc: (95.00%) (42889/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.3007) |  Loss2: (0.1670) | Acc: (95.00%) (44124/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.3009) |  Loss2: (0.1670) | Acc: (95.00%) (45341/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.3009) |  Loss2: (0.1670) | Acc: (95.00%) (46562/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.3008) |  Loss2: (0.1669) | Acc: (95.00%) (47735/50000)
# TEST : Loss: (0.3975) | Acc: (88.00%) (8800/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1266,  0.0733,  0.1021],
          [ 0.1132, -0.3195, -0.1179],
          [ 0.2795, -0.0732,  0.0086]],

         [[-0.0190, -0.0802,  0.2777],
          [ 0.0152, -0.1191,  0.0495],
          [ 0.0760,  0.0099, -0.0689]],

         [[ 0.0051, -0.0279,  0.2147],
          [ 0.0830, -0.2089,  0.1653],
          [-0.1095,  0.0181, -0.1875]]],


        [[[ 0.0982, -0.0721, -0.0045],
          [ 0.1388,  0.1778,  0.1806],
          [ 0.0162, -0.0586,  0.1163]],

         [[-0.1491, -0.1756,  0.1490],
          [ 0.0349, -0.0445,  0.0992],
          [ 0.1239, -0.1980,  0.1286]],

         [[ 0.0998, -0.0496, -0.0162],
          [ 0.0102,  0.0183, -0.1223],
          [-0.0287, -0.1593, -0.0233]]],


        [[[-0.0725,  0.0867, -0.0431],
          [-0.1260,  0.0632,  0.1134],
          [ 0.1061,  0.1633,  0.0012]],

         [[ 0.0970,  0.1299,  0.1104],
          [ 0.0662, -0.0383, -0.0359],
          [-0.0484, -0.0297,  0.1359]],

         [[-0.0453,  0.1412,  0.0720],
          [-0.0432, -0.0457, -0.0603],
          [-0.0407, -0.0087,  0.0404]]],


        ...,


        [[[ 0.2250, -0.1149, -0.1224],
          [-0.0528, -0.1183, -0.0912],
          [-0.0972,  0.0217,  0.0743]],

         [[ 0.1660,  0.1912, -0.1179],
          [ 0.2016, -0.1413, -0.1113],
          [-0.0594, -0.0760, -0.0247]],

         [[ 0.1799,  0.1012, -0.1064],
          [-0.0625, -0.1522, -0.0725],
          [ 0.0743,  0.1801, -0.1228]]],


        [[[-0.1007,  0.0890, -0.1400],
          [-0.1418, -0.1465,  0.1188],
          [-0.1420,  0.1532, -0.1310]],

         [[ 0.1000, -0.1103,  0.0889],
          [ 0.1473,  0.0018, -0.0794],
          [-0.1332,  0.1189, -0.1914]],

         [[ 0.1299, -0.0169,  0.0010],
          [ 0.1409, -0.1738,  0.0127],
          [-0.0152,  0.0204, -0.1190]]],


        [[[ 0.0697,  0.1759,  0.1140],
          [-0.0873, -0.2089,  0.2159],
          [-0.1761, -0.0366, -0.0817]],

         [[-0.0167, -0.0626,  0.1544],
          [ 0.0030, -0.2329,  0.0627],
          [-0.0108,  0.0704,  0.0219]],

         [[-0.1720, -0.1980,  0.1130],
          [ 0.1491, -0.2094, -0.0036],
          [ 0.0153, -0.0076,  0.3034]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2663e-05,  7.3257e-06,  1.0207e-05],
          [ 1.1319e-05, -3.1952e-05, -1.1792e-05],
          [ 2.7947e-05, -7.3226e-06,  8.6422e-07]],

         [[-1.9017e-06, -8.0213e-06,  2.7774e-05],
          [ 1.5211e-06, -1.1912e-05,  4.9452e-06],
          [ 7.5982e-06,  9.8945e-07, -6.8878e-06]],

         [[ 5.1060e-07, -2.7861e-06,  2.1471e-05],
          [ 8.3022e-06, -2.0889e-05,  1.6534e-05],
          [-1.0954e-05,  1.8054e-06, -1.8746e-05]]],


        [[[ 9.8243e-06, -7.2102e-06, -4.5401e-07],
          [ 1.3885e-05,  1.7775e-05,  1.8056e-05],
          [ 1.6234e-06, -5.8585e-06,  1.1628e-05]],

         [[-1.4911e-05, -1.7557e-05,  1.4901e-05],
          [ 3.4921e-06, -4.4531e-06,  9.9232e-06],
          [ 1.2393e-05, -1.9795e-05,  1.2860e-05]],

         [[ 9.9837e-06, -4.9579e-06, -1.6169e-06],
          [ 1.0195e-06,  1.8309e-06, -1.2235e-05],
          [-2.8663e-06, -1.5925e-05, -2.3263e-06]]],


        [[[-7.2524e-06,  8.6664e-06, -4.3115e-06],
          [-1.2596e-05,  6.3160e-06,  1.1342e-05],
          [ 1.0607e-05,  1.6330e-05,  1.2292e-07]],

         [[ 9.7025e-06,  1.2990e-05,  1.1043e-05],
          [ 6.6202e-06, -3.8343e-06, -3.5882e-06],
          [-4.8392e-06, -2.9744e-06,  1.3592e-05]],

         [[-4.5292e-06,  1.4125e-05,  7.2022e-06],
          [-4.3201e-06, -4.5730e-06, -6.0297e-06],
          [-4.0737e-06, -8.6641e-07,  4.0428e-06]]],


        ...,


        [[[ 2.2503e-05, -1.1486e-05, -1.2240e-05],
          [-5.2820e-06, -1.1831e-05, -9.1177e-06],
          [-9.7245e-06,  2.1684e-06,  7.4317e-06]],

         [[ 1.6602e-05,  1.9118e-05, -1.1790e-05],
          [ 2.0156e-05, -1.4135e-05, -1.1134e-05],
          [-5.9428e-06, -7.6036e-06, -2.4748e-06]],

         [[ 1.7986e-05,  1.0119e-05, -1.0638e-05],
          [-6.2481e-06, -1.5220e-05, -7.2524e-06],
          [ 7.4289e-06,  1.8007e-05, -1.2284e-05]]],


        [[[-1.0074e-05,  8.8989e-06, -1.3996e-05],
          [-1.4179e-05, -1.4653e-05,  1.1884e-05],
          [-1.4198e-05,  1.5320e-05, -1.3101e-05]],

         [[ 9.9999e-06, -1.1034e-05,  8.8882e-06],
          [ 1.4733e-05,  1.8237e-07, -7.9429e-06],
          [-1.3318e-05,  1.1888e-05, -1.9139e-05]],

         [[ 1.2993e-05, -1.6856e-06,  9.5505e-08],
          [ 1.4090e-05, -1.7383e-05,  1.2685e-06],
          [-1.5195e-06,  2.0423e-06, -1.1896e-05]]],


        [[[ 6.9735e-06,  1.7590e-05,  1.1400e-05],
          [-8.7281e-06, -2.0892e-05,  2.1593e-05],
          [-1.7611e-05, -3.6630e-06, -8.1685e-06]],

         [[-1.6686e-06, -6.2572e-06,  1.5437e-05],
          [ 3.0185e-07, -2.3286e-05,  6.2733e-06],
          [-1.0765e-06,  7.0365e-06,  2.1852e-06]],

         [[-1.7198e-05, -1.9804e-05,  1.1298e-05],
          [ 1.4911e-05, -2.0936e-05, -3.6267e-07],
          [ 1.5274e-06, -7.5678e-07,  3.0343e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.5806]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0535]], device='cuda:0')

Epoch: 149 | Batch_idx: 0 |  Loss: (0.3576) |  Loss2: (0.1654) | Acc: (89.00%) (115/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.2999) |  Loss2: (0.1653) | Acc: (94.00%) (1334/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.2976) |  Loss2: (0.1653) | Acc: (95.00%) (2561/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.2985) |  Loss2: (0.1653) | Acc: (95.00%) (3784/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.2935) |  Loss2: (0.1652) | Acc: (95.00%) (5016/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.2938) |  Loss2: (0.1652) | Acc: (95.00%) (6246/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.2954) |  Loss2: (0.1651) | Acc: (95.00%) (7468/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.2967) |  Loss2: (0.1651) | Acc: (95.00%) (8687/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.2968) |  Loss2: (0.1651) | Acc: (95.00%) (9909/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.2931) |  Loss2: (0.1650) | Acc: (95.00%) (11150/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.2917) |  Loss2: (0.1650) | Acc: (95.00%) (12377/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.2929) |  Loss2: (0.1650) | Acc: (95.00%) (13595/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.2949) |  Loss2: (0.1649) | Acc: (95.00%) (14803/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.2933) |  Loss2: (0.1649) | Acc: (95.00%) (16036/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.2942) |  Loss2: (0.1648) | Acc: (95.00%) (17256/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.2942) |  Loss2: (0.1648) | Acc: (95.00%) (18481/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.2941) |  Loss2: (0.1648) | Acc: (95.00%) (19705/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.2942) |  Loss2: (0.1647) | Acc: (95.00%) (20930/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.2939) |  Loss2: (0.1647) | Acc: (95.00%) (22154/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.2940) |  Loss2: (0.1647) | Acc: (95.00%) (23365/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.2945) |  Loss2: (0.1646) | Acc: (95.00%) (24589/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.2954) |  Loss2: (0.1646) | Acc: (95.00%) (25798/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.2953) |  Loss2: (0.1646) | Acc: (95.00%) (27017/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.2966) |  Loss2: (0.1645) | Acc: (95.00%) (28219/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.2962) |  Loss2: (0.1645) | Acc: (95.00%) (29450/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.2962) |  Loss2: (0.1645) | Acc: (95.00%) (30675/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.2973) |  Loss2: (0.1644) | Acc: (95.00%) (31883/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.2978) |  Loss2: (0.1644) | Acc: (95.00%) (33104/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.2983) |  Loss2: (0.1644) | Acc: (95.00%) (34325/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.2984) |  Loss2: (0.1643) | Acc: (95.00%) (35539/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.2989) |  Loss2: (0.1643) | Acc: (95.00%) (36736/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.2984) |  Loss2: (0.1643) | Acc: (95.00%) (37968/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.2986) |  Loss2: (0.1642) | Acc: (95.00%) (39181/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.2981) |  Loss2: (0.1642) | Acc: (95.00%) (40413/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.2982) |  Loss2: (0.1641) | Acc: (95.00%) (41641/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.2979) |  Loss2: (0.1641) | Acc: (95.00%) (42864/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.2974) |  Loss2: (0.1641) | Acc: (95.00%) (44094/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.2971) |  Loss2: (0.1640) | Acc: (95.00%) (45322/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.2968) |  Loss2: (0.1640) | Acc: (95.00%) (46557/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.2964) |  Loss2: (0.1640) | Acc: (95.00%) (47734/50000)
# TEST : Loss: (0.3912) | Acc: (88.00%) (8819/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1266,  0.0732,  0.1020],
          [ 0.1131, -0.3194, -0.1179],
          [ 0.2794, -0.0732,  0.0086]],

         [[-0.0190, -0.0802,  0.2776],
          [ 0.0152, -0.1191,  0.0494],
          [ 0.0760,  0.0099, -0.0689]],

         [[ 0.0051, -0.0278,  0.2146],
          [ 0.0830, -0.2088,  0.1653],
          [-0.1095,  0.0180, -0.1874]]],


        [[[ 0.0982, -0.0721, -0.0045],
          [ 0.1388,  0.1777,  0.1805],
          [ 0.0162, -0.0586,  0.1162]],

         [[-0.1491, -0.1755,  0.1490],
          [ 0.0349, -0.0445,  0.0992],
          [ 0.1239, -0.1979,  0.1285]],

         [[ 0.0998, -0.0496, -0.0162],
          [ 0.0102,  0.0183, -0.1223],
          [-0.0287, -0.1592, -0.0233]]],


        [[[-0.0725,  0.0866, -0.0431],
          [-0.1259,  0.0631,  0.1134],
          [ 0.1060,  0.1632,  0.0012]],

         [[ 0.0970,  0.1298,  0.1104],
          [ 0.0662, -0.0383, -0.0359],
          [-0.0484, -0.0297,  0.1359]],

         [[-0.0453,  0.1412,  0.0720],
          [-0.0432, -0.0457, -0.0603],
          [-0.0407, -0.0087,  0.0404]]],


        ...,


        [[[ 0.2249, -0.1148, -0.1224],
          [-0.0528, -0.1183, -0.0911],
          [-0.0972,  0.0217,  0.0743]],

         [[ 0.1660,  0.1911, -0.1179],
          [ 0.2015, -0.1413, -0.1113],
          [-0.0594, -0.0760, -0.0247]],

         [[ 0.1798,  0.1012, -0.1063],
          [-0.0625, -0.1521, -0.0725],
          [ 0.0743,  0.1800, -0.1228]]],


        [[[-0.1007,  0.0890, -0.1399],
          [-0.1417, -0.1465,  0.1188],
          [-0.1419,  0.1531, -0.1310]],

         [[ 0.1000, -0.1103,  0.0888],
          [ 0.1473,  0.0018, -0.0794],
          [-0.1331,  0.1188, -0.1913]],

         [[ 0.1299, -0.0168,  0.0010],
          [ 0.1408, -0.1738,  0.0127],
          [-0.0152,  0.0204, -0.1189]]],


        [[[ 0.0697,  0.1758,  0.1140],
          [-0.0872, -0.2088,  0.2158],
          [-0.1760, -0.0366, -0.0817]],

         [[-0.0167, -0.0625,  0.1543],
          [ 0.0030, -0.2328,  0.0627],
          [-0.0108,  0.0703,  0.0218]],

         [[-0.1719, -0.1980,  0.1129],
          [ 0.1490, -0.2093, -0.0036],
          [ 0.0153, -0.0076,  0.3033]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2658e-05,  7.3228e-06,  1.0203e-05],
          [ 1.1314e-05, -3.1939e-05, -1.1788e-05],
          [ 2.7937e-05, -7.3197e-06,  8.6389e-07]],

         [[-1.9010e-06, -8.0181e-06,  2.7764e-05],
          [ 1.5206e-06, -1.1907e-05,  4.9433e-06],
          [ 7.5953e-06,  9.8905e-07, -6.8851e-06]],

         [[ 5.1040e-07, -2.7850e-06,  2.1463e-05],
          [ 8.2990e-06, -2.0881e-05,  1.6528e-05],
          [-1.0950e-05,  1.8046e-06, -1.8738e-05]]],


        [[[ 9.8205e-06, -7.2073e-06, -4.5383e-07],
          [ 1.3879e-05,  1.7768e-05,  1.8049e-05],
          [ 1.6227e-06, -5.8562e-06,  1.1624e-05]],

         [[-1.4905e-05, -1.7550e-05,  1.4896e-05],
          [ 3.4908e-06, -4.4513e-06,  9.9194e-06],
          [ 1.2388e-05, -1.9788e-05,  1.2855e-05]],

         [[ 9.9799e-06, -4.9560e-06, -1.6163e-06],
          [ 1.0191e-06,  1.8302e-06, -1.2230e-05],
          [-2.8652e-06, -1.5919e-05, -2.3255e-06]]],


        [[[-7.2495e-06,  8.6629e-06, -4.3097e-06],
          [-1.2592e-05,  6.3137e-06,  1.1337e-05],
          [ 1.0603e-05,  1.6324e-05,  1.2287e-07]],

         [[ 9.6987e-06,  1.2985e-05,  1.1039e-05],
          [ 6.6175e-06, -3.8328e-06, -3.5867e-06],
          [-4.8373e-06, -2.9732e-06,  1.3586e-05]],

         [[-4.5274e-06,  1.4119e-05,  7.1993e-06],
          [-4.3184e-06, -4.5712e-06, -6.0274e-06],
          [-4.0721e-06, -8.6608e-07,  4.0412e-06]]],


        ...,


        [[[ 2.2494e-05, -1.1482e-05, -1.2235e-05],
          [-5.2800e-06, -1.1827e-05, -9.1142e-06],
          [-9.7207e-06,  2.1675e-06,  7.4288e-06]],

         [[ 1.6595e-05,  1.9110e-05, -1.1785e-05],
          [ 2.0148e-05, -1.4129e-05, -1.1130e-05],
          [-5.9404e-06, -7.6007e-06, -2.4738e-06]],

         [[ 1.7979e-05,  1.0115e-05, -1.0634e-05],
          [-6.2456e-06, -1.5214e-05, -7.2495e-06],
          [ 7.4260e-06,  1.8000e-05, -1.2280e-05]]],


        [[[-1.0070e-05,  8.8954e-06, -1.3991e-05],
          [-1.4173e-05, -1.4647e-05,  1.1880e-05],
          [-1.4192e-05,  1.5314e-05, -1.3096e-05]],

         [[ 9.9961e-06, -1.1030e-05,  8.8847e-06],
          [ 1.4727e-05,  1.8230e-07, -7.9397e-06],
          [-1.3313e-05,  1.1883e-05, -1.9131e-05]],

         [[ 1.2988e-05, -1.6849e-06,  9.5468e-08],
          [ 1.4085e-05, -1.7376e-05,  1.2680e-06],
          [-1.5189e-06,  2.0415e-06, -1.1892e-05]]],


        [[[ 6.9709e-06,  1.7583e-05,  1.1395e-05],
          [-8.7247e-06, -2.0884e-05,  2.1585e-05],
          [-1.7604e-05, -3.6615e-06, -8.1653e-06]],

         [[-1.6679e-06, -6.2548e-06,  1.5431e-05],
          [ 3.0173e-07, -2.3276e-05,  6.2709e-06],
          [-1.0760e-06,  7.0339e-06,  2.1843e-06]],

         [[-1.7191e-05, -1.9797e-05,  1.1293e-05],
          [ 1.4905e-05, -2.0928e-05, -3.6253e-07],
          [ 1.5268e-06, -7.5649e-07,  3.0332e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6125]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0342]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (5024/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (6254/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (7481/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (9928/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (11147/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (12369/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (13584/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (14792/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (16017/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (17239/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (18470/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (19687/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (20908/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (22120/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (23345/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (24571/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (25794/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (27010/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (28228/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (29444/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (30648/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (31855/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (33080/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (34299/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (35515/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (36728/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (37947/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (39160/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (40375/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (41591/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (42807/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (44020/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (45229/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (46449/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (47622/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_150.pth.tar'
# TEST : Loss: (0.4130) | Acc: (88.00%) (8838/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1308,  0.0695,  0.0968],
          [ 0.1091, -0.3241, -0.1237],
          [ 0.2740, -0.0781,  0.0036]],

         [[-0.0232, -0.0826,  0.2749],
          [ 0.0115, -0.1230,  0.0454],
          [ 0.0706,  0.0047, -0.0740]],

         [[ 0.0047, -0.0267,  0.2156],
          [ 0.0822, -0.2093,  0.1648],
          [-0.1118,  0.0159, -0.1892]]],


        [[[ 0.0962, -0.0746, -0.0061],
          [ 0.1376,  0.1754,  0.1795],
          [ 0.0146, -0.0611,  0.1149]],

         [[-0.1508, -0.1784,  0.1464],
          [ 0.0340, -0.0470,  0.0975],
          [ 0.1225, -0.2006,  0.1267]],

         [[ 0.0987, -0.0514, -0.0174],
          [ 0.0099,  0.0166, -0.1229],
          [-0.0288, -0.1609, -0.0240]]],


        [[[-0.0729,  0.0862, -0.0432],
          [-0.1259,  0.0632,  0.1135],
          [ 0.1063,  0.1635,  0.0015]],

         [[ 0.0965,  0.1295,  0.1103],
          [ 0.0661, -0.0383, -0.0358],
          [-0.0482, -0.0296,  0.1359]],

         [[-0.0459,  0.1407,  0.0719],
          [-0.0434, -0.0458, -0.0602],
          [-0.0407, -0.0086,  0.0404]]],


        ...,


        [[[ 0.2218, -0.1169, -0.1251],
          [-0.0544, -0.1198, -0.0932],
          [-0.0988,  0.0198,  0.0716]],

         [[ 0.1626,  0.1889, -0.1208],
          [ 0.1993, -0.1432, -0.1139],
          [-0.0617, -0.0785, -0.0282]],

         [[ 0.1758,  0.0984, -0.1096],
          [-0.0652, -0.1548, -0.0756],
          [ 0.0714,  0.1767, -0.1268]]],


        [[[-0.1020,  0.0878, -0.1412],
          [-0.1418, -0.1471,  0.1184],
          [-0.1412,  0.1530, -0.1309]],

         [[ 0.0996, -0.1104,  0.0881],
          [ 0.1475,  0.0018, -0.0792],
          [-0.1322,  0.1194, -0.1908]],

         [[ 0.1295, -0.0171,  0.0002],
          [ 0.1411, -0.1738,  0.0126],
          [-0.0143,  0.0208, -0.1188]]],


        [[[ 0.0685,  0.1768,  0.1155],
          [-0.0860, -0.2068,  0.2190],
          [-0.1733, -0.0333, -0.0778]],

         [[-0.0167, -0.0622,  0.1546],
          [ 0.0057, -0.2308,  0.0654],
          [-0.0062,  0.0742,  0.0252]],

         [[-0.1750, -0.2007,  0.1113],
          [ 0.1492, -0.2099, -0.0031],
          [ 0.0175, -0.0062,  0.3037]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0128,  0.0122,  0.0213],
          [-0.0118, -0.0043,  0.0167],
          [-0.0157, -0.0044,  0.0212]],

         [[-0.0135, -0.0140, -0.0092],
          [-0.0415, -0.0363, -0.0181],
          [-0.0527, -0.0433, -0.0190]],

         [[ 0.0038, -0.0048,  0.0052],
          [-0.0129, -0.0184,  0.0007],
          [-0.0173, -0.0181,  0.0041]]],


        [[[ 0.0165,  0.0217,  0.0199],
          [ 0.0195,  0.0226,  0.0220],
          [ 0.0267,  0.0253,  0.0206]],

         [[ 0.0239,  0.0294,  0.0295],
          [ 0.0284,  0.0305,  0.0309],
          [ 0.0354,  0.0331,  0.0293]],

         [[ 0.0310,  0.0360,  0.0361],
          [ 0.0338,  0.0370,  0.0373],
          [ 0.0376,  0.0379,  0.0350]]],


        [[[-0.0018, -0.0028, -0.0033],
          [-0.0030, -0.0034, -0.0041],
          [-0.0034, -0.0030, -0.0041]],

         [[ 0.0030,  0.0026,  0.0025],
          [ 0.0013,  0.0016,  0.0015],
          [ 0.0005,  0.0017,  0.0014]],

         [[ 0.0044,  0.0043,  0.0048],
          [ 0.0035,  0.0039,  0.0044],
          [ 0.0032,  0.0044,  0.0048]]],


        ...,


        [[[ 0.0037,  0.0008,  0.0040],
          [ 0.0090,  0.0001,  0.0009],
          [ 0.0209,  0.0133,  0.0142]],

         [[-0.0029, -0.0048, -0.0020],
          [ 0.0016, -0.0061, -0.0050],
          [ 0.0117,  0.0053,  0.0079]],

         [[-0.0114, -0.0143, -0.0134],
          [-0.0051, -0.0135, -0.0139],
          [ 0.0070, -0.0002,  0.0006]]],


        [[[ 0.0273,  0.0271,  0.0173],
          [ 0.0146,  0.0209,  0.0164],
          [-0.0046,  0.0032,  0.0043]],

         [[ 0.0121,  0.0134,  0.0057],
          [-0.0001,  0.0069,  0.0037],
          [-0.0209, -0.0135, -0.0108]],

         [[ 0.0063,  0.0069, -0.0002],
          [-0.0038, -0.0003, -0.0045],
          [-0.0212, -0.0189, -0.0194]]],


        [[[ 0.1578,  0.1629,  0.1568],
          [ 0.1443,  0.1297,  0.1281],
          [ 0.1412,  0.1181,  0.1224]],

         [[ 0.0940,  0.1021,  0.0975],
          [ 0.0914,  0.0823,  0.0825],
          [ 0.0921,  0.0738,  0.0760]],

         [[ 0.0781,  0.0788,  0.0695],
          [ 0.0728,  0.0624,  0.0601],
          [ 0.0604,  0.0439,  0.0447]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6124]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 151 | Batch_idx: 0 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (2561/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (4989/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (6210/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (7422/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (8640/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (9877/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (11102/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (12309/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (13535/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (14755/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (15969/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (17176/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (18401/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (19619/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (20835/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (22058/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (23256/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (24483/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (25701/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (26925/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (28156/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (29369/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (30585/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (31808/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (33024/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (34240/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (35468/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (36682/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (37904/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (39124/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1363) |  Loss2: (0.0000) | Acc: (95.00%) (40346/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (41560/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (42769/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (95.00%) (43988/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (45204/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (46409/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (47581/50000)
# TEST : Loss: (0.4297) | Acc: (87.00%) (8788/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1272,  0.0732,  0.1004],
          [ 0.1134, -0.3213, -0.1226],
          [ 0.2768, -0.0760,  0.0010]],

         [[-0.0213, -0.0798,  0.2770],
          [ 0.0150, -0.1200,  0.0464],
          [ 0.0727,  0.0072, -0.0760]],

         [[ 0.0068, -0.0237,  0.2174],
          [ 0.0851, -0.2066,  0.1654],
          [-0.1100,  0.0181, -0.1911]]],


        [[[ 0.0970, -0.0722, -0.0032],
          [ 0.1382,  0.1769,  0.1813],
          [ 0.0153, -0.0599,  0.1160]],

         [[-0.1498, -0.1759,  0.1495],
          [ 0.0348, -0.0453,  0.0994],
          [ 0.1232, -0.1991,  0.1283]],

         [[ 0.1002, -0.0487, -0.0142],
          [ 0.0109,  0.0184, -0.1211],
          [-0.0275, -0.1588, -0.0220]]],


        [[[-0.0727,  0.0864, -0.0432],
          [-0.1258,  0.0633,  0.1133],
          [ 0.1060,  0.1632,  0.0012]],

         [[ 0.0969,  0.1298,  0.1102],
          [ 0.0664, -0.0380, -0.0358],
          [-0.0481, -0.0295,  0.1357]],

         [[-0.0456,  0.1408,  0.0716],
          [-0.0433, -0.0458, -0.0606],
          [-0.0409, -0.0090,  0.0399]]],


        ...,


        [[[ 0.2231, -0.1151, -0.1235],
          [-0.0543, -0.1192, -0.0925],
          [-0.0991,  0.0197,  0.0717]],

         [[ 0.1634,  0.1901, -0.1198],
          [ 0.1988, -0.1428, -0.1135],
          [-0.0624, -0.0786, -0.0284]],

         [[ 0.1774,  0.1004, -0.1081],
          [-0.0644, -0.1535, -0.0747],
          [ 0.0720,  0.1773, -0.1266]]],


        [[[-0.0993,  0.0887, -0.1400],
          [-0.1410, -0.1480,  0.1181],
          [-0.1415,  0.1521, -0.1310]],

         [[ 0.1009, -0.1105,  0.0883],
          [ 0.1469,  0.0002, -0.0797],
          [-0.1335,  0.1181, -0.1907]],

         [[ 0.1316, -0.0163,  0.0011],
          [ 0.1413, -0.1744,  0.0129],
          [-0.0149,  0.0204, -0.1179]]],


        [[[ 0.0657,  0.1765,  0.1148],
          [-0.0890, -0.2112,  0.2162],
          [-0.1745, -0.0367, -0.0816]],

         [[-0.0173, -0.0600,  0.1557],
          [ 0.0035, -0.2336,  0.0639],
          [-0.0084,  0.0710,  0.0222]],

         [[-0.1765, -0.2000,  0.1110],
          [ 0.1478, -0.2123, -0.0046],
          [ 0.0169, -0.0075,  0.3022]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0497,  0.0798,  0.0700],
          [ 0.0422,  0.0661,  0.0511],
          [ 0.0138,  0.0325,  0.0046]],

         [[ 0.0543,  0.0870,  0.0806],
          [ 0.0536,  0.0818,  0.0688],
          [ 0.0356,  0.0566,  0.0278]],

         [[ 0.0647,  0.0965,  0.0912],
          [ 0.0656,  0.0907,  0.0774],
          [ 0.0502,  0.0692,  0.0408]]],


        [[[ 0.0020,  0.0137,  0.0164],
          [ 0.0063,  0.0167,  0.0096],
          [ 0.0069,  0.0094,  0.0019]],

         [[ 0.0074,  0.0176,  0.0187],
          [ 0.0106,  0.0195,  0.0110],
          [ 0.0091,  0.0109,  0.0025]],

         [[ 0.0120,  0.0192,  0.0216],
          [ 0.0129,  0.0183,  0.0122],
          [ 0.0101,  0.0095,  0.0036]]],


        [[[-0.0076, -0.0028, -0.0043],
          [-0.0059, -0.0010, -0.0027],
          [-0.0054, -0.0016, -0.0052]],

         [[-0.0029,  0.0021,  0.0009],
          [-0.0015,  0.0035,  0.0020],
          [-0.0014,  0.0026, -0.0008]],

         [[ 0.0016,  0.0063,  0.0053],
          [ 0.0029,  0.0076,  0.0065],
          [ 0.0028,  0.0066,  0.0042]]],


        ...,


        [[[ 0.0527,  0.0433,  0.0257],
          [ 0.0554,  0.0542,  0.0425],
          [ 0.0422,  0.0519,  0.0488]],

         [[ 0.0385,  0.0282,  0.0098],
          [ 0.0336,  0.0308,  0.0201],
          [ 0.0228,  0.0306,  0.0273]],

         [[ 0.0248,  0.0152,  0.0001],
          [ 0.0203,  0.0164,  0.0062],
          [ 0.0094,  0.0146,  0.0104]]],


        [[[-0.0036,  0.0015,  0.0008],
          [ 0.0147,  0.0156,  0.0070],
          [ 0.0140,  0.0115, -0.0044]],

         [[-0.0055, -0.0000, -0.0015],
          [ 0.0041,  0.0057, -0.0031],
          [ 0.0036,  0.0027, -0.0124]],

         [[-0.0030,  0.0036,  0.0024],
          [ 0.0004,  0.0033, -0.0036],
          [-0.0012, -0.0005, -0.0127]]],


        [[[ 0.0725,  0.0577,  0.0512],
          [ 0.0549,  0.0542,  0.0574],
          [ 0.0827,  0.0919,  0.1088]],

         [[ 0.0590,  0.0475,  0.0397],
          [ 0.0403,  0.0431,  0.0477],
          [ 0.0662,  0.0800,  0.0961]],

         [[ 0.0222,  0.0052, -0.0015],
          [ 0.0089,  0.0005,  0.0021],
          [ 0.0197,  0.0253,  0.0357]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6118]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 152 | Batch_idx: 0 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (2569/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (3791/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (6235/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (7451/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (8669/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (9898/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (11128/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (12349/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (13565/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (14775/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (16005/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (17228/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (18446/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (19676/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (20889/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (22106/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (23328/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (24552/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (25768/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (26987/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (28216/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (29445/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (30657/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (31878/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (33089/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (34314/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (35537/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (36741/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (37949/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (39175/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (40398/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (41627/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (42856/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (44052/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (45270/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1343) |  Loss2: (0.0000) | Acc: (95.00%) (46468/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (47645/50000)
# TEST : Loss: (0.4426) | Acc: (87.00%) (8731/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1272,  0.0719,  0.0990],
          [ 0.1116, -0.3242, -0.1234],
          [ 0.2771, -0.0750,  0.0016]],

         [[-0.0233, -0.0810,  0.2767],
          [ 0.0116, -0.1231,  0.0463],
          [ 0.0719,  0.0073, -0.0752]],

         [[ 0.0027, -0.0263,  0.2157],
          [ 0.0794, -0.2112,  0.1643],
          [-0.1131,  0.0165, -0.1908]]],


        [[[ 0.0952, -0.0734, -0.0040],
          [ 0.1365,  0.1761,  0.1809],
          [ 0.0146, -0.0598,  0.1165]],

         [[-0.1519, -0.1773,  0.1482],
          [ 0.0327, -0.0463,  0.0988],
          [ 0.1218, -0.1991,  0.1288]],

         [[ 0.0976, -0.0509, -0.0165],
          [ 0.0081,  0.0164, -0.1229],
          [-0.0298, -0.1601, -0.0230]]],


        [[[-0.0729,  0.0863, -0.0432],
          [-0.1261,  0.0631,  0.1133],
          [ 0.1058,  0.1631,  0.0013]],

         [[ 0.0966,  0.1296,  0.1102],
          [ 0.0661, -0.0381, -0.0358],
          [-0.0482, -0.0295,  0.1357]],

         [[-0.0459,  0.1405,  0.0715],
          [-0.0436, -0.0460, -0.0607],
          [-0.0409, -0.0090,  0.0398]]],


        ...,


        [[[ 0.2215, -0.1157, -0.1236],
          [-0.0554, -0.1195, -0.0917],
          [-0.1008,  0.0190,  0.0715]],

         [[ 0.1634,  0.1908, -0.1197],
          [ 0.1987, -0.1424, -0.1128],
          [-0.0632, -0.0788, -0.0284]],

         [[ 0.1789,  0.1021, -0.1074],
          [-0.0630, -0.1522, -0.0737],
          [ 0.0724,  0.1779, -0.1261]]],


        [[[-0.0999,  0.0884, -0.1408],
          [-0.1424, -0.1484,  0.1177],
          [-0.1420,  0.1525, -0.1309]],

         [[ 0.1028, -0.1082,  0.0896],
          [ 0.1481,  0.0023, -0.0780],
          [-0.1314,  0.1208, -0.1882]],

         [[ 0.1339, -0.0138,  0.0028],
          [ 0.1432, -0.1718,  0.0153],
          [-0.0118,  0.0238, -0.1148]]],


        [[[ 0.0701,  0.1842,  0.1210],
          [-0.0869, -0.2088,  0.2198],
          [-0.1724, -0.0350, -0.0788]],

         [[-0.0112, -0.0520,  0.1625],
          [ 0.0066, -0.2306,  0.0685],
          [-0.0062,  0.0730,  0.0257]],

         [[-0.1724, -0.1929,  0.1163],
          [ 0.1490, -0.2101, -0.0016],
          [ 0.0176, -0.0055,  0.3053]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1604, -0.1836, -0.1961],
          [-0.1682, -0.1775, -0.1913],
          [-0.1386, -0.1497, -0.1423]],

         [[-0.1710, -0.1847, -0.1932],
          [-0.1773, -0.1802, -0.1907],
          [-0.1457, -0.1503, -0.1433]],

         [[-0.1829, -0.1929, -0.1978],
          [-0.1867, -0.1846, -0.1950],
          [-0.1627, -0.1648, -0.1594]]],


        [[[-0.0086, -0.0136, -0.0113],
          [-0.0199, -0.0229, -0.0152],
          [-0.0220, -0.0244, -0.0161]],

         [[-0.0172, -0.0218, -0.0185],
          [-0.0290, -0.0312, -0.0225],
          [-0.0309, -0.0320, -0.0237]],

         [[-0.0266, -0.0302, -0.0255],
          [-0.0335, -0.0340, -0.0244],
          [-0.0337, -0.0336, -0.0246]]],


        [[[ 0.0081,  0.0025, -0.0012],
          [ 0.0086,  0.0027, -0.0011],
          [ 0.0062, -0.0001, -0.0028]],

         [[ 0.0093,  0.0040,  0.0002],
          [ 0.0089,  0.0033, -0.0005],
          [ 0.0056, -0.0003, -0.0030]],

         [[ 0.0098,  0.0054,  0.0023],
          [ 0.0093,  0.0045,  0.0013],
          [ 0.0062,  0.0010, -0.0012]]],


        ...,


        [[[ 0.0616,  0.0467,  0.0402],
          [ 0.0514,  0.0405,  0.0365],
          [ 0.0490,  0.0384,  0.0282]],

         [[ 0.0588,  0.0478,  0.0450],
          [ 0.0473,  0.0415,  0.0420],
          [ 0.0443,  0.0383,  0.0316]],

         [[ 0.0826,  0.0711,  0.0666],
          [ 0.0711,  0.0649,  0.0653],
          [ 0.0691,  0.0636,  0.0598]]],


        [[[ 0.0162,  0.0133,  0.0152],
          [ 0.0155,  0.0137,  0.0106],
          [-0.0016, -0.0025, -0.0048]],

         [[ 0.0055,  0.0058,  0.0096],
          [ 0.0051,  0.0070,  0.0058],
          [-0.0099, -0.0073, -0.0089]],

         [[ 0.0300,  0.0277,  0.0283],
          [ 0.0337,  0.0319,  0.0266],
          [ 0.0214,  0.0207,  0.0156]]],


        [[[-0.0053,  0.0054, -0.0196],
          [-0.0102,  0.0137,  0.0178],
          [-0.0020,  0.0226,  0.0482]],

         [[-0.0222, -0.0052, -0.0313],
          [-0.0283, -0.0003,  0.0018],
          [-0.0161,  0.0092,  0.0291]],

         [[ 0.0093,  0.0330,  0.0194],
          [-0.0010,  0.0324,  0.0433],
          [ 0.0102,  0.0397,  0.0653]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6111]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 153 | Batch_idx: 0 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (3809/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (6266/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (7495/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (96.00%) (8725/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (9946/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (11163/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (12392/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (13621/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (14840/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (16079/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (17308/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (18534/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (19771/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (20997/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (22222/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (23436/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (24662/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (25892/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (27113/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (28337/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (29554/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (30787/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (32018/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (33242/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (34465/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (35696/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (36920/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (38136/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (39346/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (40563/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (41796/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (43029/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (44236/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (45457/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (46667/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (47845/50000)
# TEST : Loss: (0.4115) | Acc: (87.00%) (8798/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1277,  0.0732,  0.1029],
          [ 0.1111, -0.3238, -0.1204],
          [ 0.2756, -0.0728,  0.0059]],

         [[-0.0258, -0.0811,  0.2788],
          [ 0.0090, -0.1244,  0.0470],
          [ 0.0684,  0.0076, -0.0730]],

         [[-0.0001, -0.0276,  0.2155],
          [ 0.0754, -0.2144,  0.1619],
          [-0.1179,  0.0139, -0.1930]]],


        [[[ 0.0949, -0.0733, -0.0045],
          [ 0.1377,  0.1779,  0.1822],
          [ 0.0159, -0.0583,  0.1174]],

         [[-0.1517, -0.1770,  0.1473],
          [ 0.0341, -0.0444,  0.0997],
          [ 0.1231, -0.1974,  0.1292]],

         [[ 0.0979, -0.0504, -0.0169],
          [ 0.0094,  0.0181, -0.1220],
          [-0.0283, -0.1584, -0.0224]]],


        [[[-0.0732,  0.0862, -0.0433],
          [-0.1264,  0.0629,  0.1132],
          [ 0.1053,  0.1628,  0.0011]],

         [[ 0.0966,  0.1297,  0.1102],
          [ 0.0659, -0.0381, -0.0358],
          [-0.0485, -0.0297,  0.1355]],

         [[-0.0457,  0.1407,  0.0716],
          [-0.0436, -0.0459, -0.0606],
          [-0.0411, -0.0092,  0.0396]]],


        ...,


        [[[ 0.2218, -0.1143, -0.1232],
          [-0.0545, -0.1188, -0.0913],
          [-0.0992,  0.0201,  0.0716]],

         [[ 0.1625,  0.1910, -0.1205],
          [ 0.1983, -0.1429, -0.1137],
          [-0.0626, -0.0787, -0.0293]],

         [[ 0.1775,  0.1016, -0.1089],
          [-0.0639, -0.1535, -0.0755],
          [ 0.0727,  0.1774, -0.1278]]],


        [[[-0.1004,  0.0884, -0.1402],
          [-0.1436, -0.1486,  0.1181],
          [-0.1424,  0.1535, -0.1299]],

         [[ 0.1020, -0.1083,  0.0899],
          [ 0.1462,  0.0016, -0.0780],
          [-0.1323,  0.1214, -0.1876]],

         [[ 0.1335, -0.0140,  0.0025],
          [ 0.1419, -0.1725,  0.0147],
          [-0.0122,  0.0242, -0.1148]]],


        [[[ 0.0694,  0.1839,  0.1195],
          [-0.0860, -0.2099,  0.2211],
          [-0.1728, -0.0361, -0.0775]],

         [[-0.0142, -0.0547,  0.1582],
          [ 0.0054, -0.2333,  0.0676],
          [-0.0080,  0.0713,  0.0254]],

         [[-0.1797, -0.1995,  0.1070],
          [ 0.1440, -0.2158, -0.0075],
          [ 0.0138, -0.0090,  0.3014]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[ 0.0388,  0.0199,  0.0185],
          [ 0.0255,  0.0141,  0.0318],
          [ 0.0045, -0.0056,  0.0151]],

         [[ 0.0681,  0.0459,  0.0389],
          [ 0.0588,  0.0385,  0.0484],
          [ 0.0484,  0.0267,  0.0378]],

         [[ 0.0816,  0.0686,  0.0632],
          [ 0.0721,  0.0569,  0.0643],
          [ 0.0565,  0.0368,  0.0424]]],


        [[[ 0.0108,  0.0134,  0.0082],
          [ 0.0043,  0.0072,  0.0033],
          [ 0.0081,  0.0044, -0.0025]],

         [[ 0.0078,  0.0106,  0.0021],
          [ 0.0016,  0.0056, -0.0012],
          [ 0.0049,  0.0020, -0.0068]],

         [[-0.0032, -0.0018, -0.0071],
          [-0.0080, -0.0045, -0.0085],
          [-0.0024, -0.0038, -0.0102]]],


        [[[ 0.0022,  0.0015,  0.0034],
          [ 0.0055,  0.0037,  0.0045],
          [ 0.0079,  0.0055,  0.0039]],

         [[-0.0025, -0.0033, -0.0013],
          [ 0.0014, -0.0003,  0.0008],
          [ 0.0052,  0.0029,  0.0013]],

         [[-0.0013, -0.0023, -0.0008],
          [ 0.0019,  0.0002,  0.0011],
          [ 0.0060,  0.0037,  0.0021]]],


        ...,


        [[[ 0.0366,  0.0305,  0.0306],
          [ 0.0328,  0.0294,  0.0285],
          [ 0.0411,  0.0428,  0.0390]],

         [[ 0.0292,  0.0179,  0.0163],
          [ 0.0195,  0.0113,  0.0099],
          [ 0.0213,  0.0207,  0.0178]],

         [[ 0.0135,  0.0026,  0.0054],
          [ 0.0022, -0.0053, -0.0028],
          [ 0.0051,  0.0047,  0.0039]]],


        [[[-0.0137, -0.0071,  0.0030],
          [-0.0099, -0.0069,  0.0001],
          [-0.0027, -0.0014, -0.0010]],

         [[-0.0083, -0.0038,  0.0053],
          [-0.0015,  0.0013,  0.0059],
          [ 0.0080,  0.0097,  0.0063]],

         [[-0.0082, -0.0047,  0.0073],
          [-0.0033, -0.0026,  0.0061],
          [ 0.0034,  0.0029,  0.0040]]],


        [[[ 0.0372,  0.0045,  0.0011],
          [ 0.0000, -0.0151,  0.0029],
          [-0.0316, -0.0109,  0.0071]],

         [[ 0.0248, -0.0097, -0.0278],
          [-0.0188, -0.0359, -0.0301],
          [-0.0605, -0.0458, -0.0401]],

         [[-0.0703, -0.0876, -0.0989],
          [-0.1042, -0.1102, -0.1066],
          [-0.1390, -0.1265, -0.1307]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6105]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 154 | Batch_idx: 0 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (2582/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (3819/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (5047/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (6277/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (7498/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (8732/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (96.00%) (9958/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (11181/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (12404/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (13616/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (14849/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (16070/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (17297/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (18509/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (19739/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (20963/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (22187/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (23419/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (24630/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (25844/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (27057/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (28295/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (29517/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (30737/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (31950/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (33184/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (34402/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (35612/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (36827/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (38042/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (39263/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (40474/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (41694/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (42908/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (44134/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (45359/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (46572/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (47756/50000)
# TEST : Loss: (0.4072) | Acc: (88.00%) (8846/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1245,  0.0729,  0.0990],
          [ 0.1149, -0.3240, -0.1231],
          [ 0.2818, -0.0712,  0.0056]],

         [[-0.0247, -0.0819,  0.2758],
          [ 0.0116, -0.1248,  0.0452],
          [ 0.0728,  0.0076, -0.0742]],

         [[-0.0003, -0.0284,  0.2134],
          [ 0.0770, -0.2148,  0.1605],
          [-0.1152,  0.0125, -0.1953]]],


        [[[ 0.0954, -0.0730, -0.0044],
          [ 0.1383,  0.1776,  0.1822],
          [ 0.0153, -0.0601,  0.1161]],

         [[-0.1501, -0.1759,  0.1482],
          [ 0.0359, -0.0433,  0.1008],
          [ 0.1237, -0.1979,  0.1292]],

         [[ 0.0987, -0.0496, -0.0156],
          [ 0.0103,  0.0187, -0.1207],
          [-0.0282, -0.1591, -0.0225]]],


        [[[-0.0732,  0.0861, -0.0432],
          [-0.1262,  0.0631,  0.1132],
          [ 0.1056,  0.1629,  0.0012]],

         [[ 0.0965,  0.1296,  0.1101],
          [ 0.0660, -0.0379, -0.0356],
          [-0.0482, -0.0295,  0.1356]],

         [[-0.0456,  0.1408,  0.0719],
          [-0.0433, -0.0455, -0.0601],
          [-0.0407, -0.0089,  0.0399]]],


        ...,


        [[[ 0.2226, -0.1135, -0.1221],
          [-0.0537, -0.1181, -0.0904],
          [-0.0982,  0.0211,  0.0729]],

         [[ 0.1640,  0.1925, -0.1188],
          [ 0.1996, -0.1418, -0.1125],
          [-0.0607, -0.0772, -0.0276]],

         [[ 0.1784,  0.1022, -0.1086],
          [-0.0630, -0.1533, -0.0757],
          [ 0.0738,  0.1778, -0.1274]]],


        [[[-0.0993,  0.0882, -0.1406],
          [-0.1433, -0.1493,  0.1178],
          [-0.1416,  0.1529, -0.1300]],

         [[ 0.1030, -0.1084,  0.0894],
          [ 0.1467,  0.0009, -0.0784],
          [-0.1308,  0.1213, -0.1874]],

         [[ 0.1357, -0.0122,  0.0042],
          [ 0.1431, -0.1720,  0.0157],
          [-0.0102,  0.0251, -0.1133]]],


        [[[ 0.0699,  0.1841,  0.1210],
          [-0.0857, -0.2105,  0.2224],
          [-0.1727, -0.0367, -0.0763]],

         [[-0.0122, -0.0538,  0.1599],
          [ 0.0065, -0.2337,  0.0686],
          [-0.0077,  0.0704,  0.0259]],

         [[-0.1772, -0.1965,  0.1102],
          [ 0.1473, -0.2130, -0.0042],
          [ 0.0170, -0.0067,  0.3042]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0442, -0.0312, -0.0435],
          [-0.0264, -0.0277, -0.0460],
          [-0.0675, -0.0786, -0.0938]],

         [[-0.0171, -0.0088, -0.0213],
          [ 0.0021, -0.0081, -0.0250],
          [-0.0403, -0.0564, -0.0708]],

         [[ 0.0270,  0.0348,  0.0198],
          [ 0.0409,  0.0319,  0.0127],
          [ 0.0019, -0.0082, -0.0258]]],


        [[[-0.0188, -0.0195, -0.0103],
          [-0.0082, -0.0116, -0.0022],
          [-0.0012, -0.0106, -0.0035]],

         [[-0.0160, -0.0163, -0.0064],
          [-0.0068, -0.0105, -0.0009],
          [ 0.0004, -0.0094, -0.0028]],

         [[-0.0029, -0.0028,  0.0043],
          [ 0.0049,  0.0020,  0.0090],
          [ 0.0119,  0.0034,  0.0080]]],


        [[[ 0.0025,  0.0010,  0.0021],
          [ 0.0037,  0.0018,  0.0020],
          [ 0.0032,  0.0004,  0.0001]],

         [[ 0.0009, -0.0002,  0.0011],
          [ 0.0022,  0.0008,  0.0014],
          [ 0.0014, -0.0006, -0.0005]],

         [[-0.0015, -0.0021, -0.0009],
          [-0.0004, -0.0008,  0.0000],
          [-0.0017, -0.0026, -0.0016]]],


        ...,


        [[[ 0.0469,  0.0441,  0.0361],
          [ 0.0395,  0.0342,  0.0249],
          [ 0.0298,  0.0224,  0.0109]],

         [[ 0.0448,  0.0434,  0.0376],
          [ 0.0378,  0.0341,  0.0278],
          [ 0.0276,  0.0224,  0.0141]],

         [[ 0.0334,  0.0316,  0.0282],
          [ 0.0300,  0.0264,  0.0214],
          [ 0.0238,  0.0186,  0.0123]]],


        [[[ 0.0007,  0.0010, -0.0069],
          [ 0.0132,  0.0082, -0.0024],
          [ 0.0050,  0.0003, -0.0083]],

         [[-0.0016,  0.0007, -0.0050],
          [ 0.0095,  0.0057, -0.0012],
          [ 0.0005, -0.0029, -0.0082]],

         [[-0.0031,  0.0033,  0.0001],
          [ 0.0033,  0.0037, -0.0002],
          [-0.0056, -0.0053, -0.0071]]],


        [[[ 0.0340,  0.0288,  0.0268],
          [ 0.0068,  0.0005, -0.0023],
          [ 0.0188,  0.0093,  0.0054]],

         [[ 0.0445,  0.0341,  0.0360],
          [ 0.0133,  0.0022,  0.0014],
          [ 0.0220,  0.0098,  0.0024]],

         [[ 0.0425,  0.0355,  0.0349],
          [ 0.0168,  0.0073,  0.0031],
          [ 0.0258,  0.0136,  0.0004]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6098]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.3059) |  Loss2: (0.1629) | Acc: (93.00%) (120/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.2854) |  Loss2: (0.1629) | Acc: (95.00%) (1350/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.2937) |  Loss2: (0.1628) | Acc: (95.00%) (2566/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.2995) |  Loss2: (0.1628) | Acc: (95.00%) (3773/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.3015) |  Loss2: (0.1628) | Acc: (94.00%) (4985/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.3096) |  Loss2: (0.1627) | Acc: (94.00%) (6188/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.3149) |  Loss2: (0.1627) | Acc: (94.00%) (7387/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.3151) |  Loss2: (0.1626) | Acc: (94.00%) (8593/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.3148) |  Loss2: (0.1626) | Acc: (94.00%) (9807/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.3157) |  Loss2: (0.1625) | Acc: (94.00%) (11016/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.3168) |  Loss2: (0.1625) | Acc: (94.00%) (12222/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.3179) |  Loss2: (0.1624) | Acc: (94.00%) (13430/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.3166) |  Loss2: (0.1624) | Acc: (94.00%) (14653/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.3173) |  Loss2: (0.1624) | Acc: (94.00%) (15859/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.3157) |  Loss2: (0.1623) | Acc: (94.00%) (17082/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.3156) |  Loss2: (0.1623) | Acc: (94.00%) (18289/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.3143) |  Loss2: (0.1622) | Acc: (94.00%) (19518/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.3157) |  Loss2: (0.1622) | Acc: (94.00%) (20719/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.3153) |  Loss2: (0.1621) | Acc: (94.00%) (21935/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.3169) |  Loss2: (0.1621) | Acc: (94.00%) (23137/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.3160) |  Loss2: (0.1620) | Acc: (94.00%) (24353/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.3157) |  Loss2: (0.1620) | Acc: (94.00%) (25572/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.3148) |  Loss2: (0.1619) | Acc: (94.00%) (26791/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.3144) |  Loss2: (0.1619) | Acc: (94.00%) (28006/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.3132) |  Loss2: (0.1618) | Acc: (94.00%) (29226/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.3125) |  Loss2: (0.1618) | Acc: (94.00%) (30442/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.3120) |  Loss2: (0.1618) | Acc: (94.00%) (31658/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.3122) |  Loss2: (0.1617) | Acc: (94.00%) (32864/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.3118) |  Loss2: (0.1617) | Acc: (94.00%) (34080/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.3109) |  Loss2: (0.1616) | Acc: (94.00%) (35302/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.3110) |  Loss2: (0.1616) | Acc: (94.00%) (36509/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.3112) |  Loss2: (0.1615) | Acc: (94.00%) (37712/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.3108) |  Loss2: (0.1615) | Acc: (94.00%) (38930/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.3108) |  Loss2: (0.1614) | Acc: (94.00%) (40147/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.3111) |  Loss2: (0.1614) | Acc: (94.00%) (41357/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.3112) |  Loss2: (0.1613) | Acc: (94.00%) (42562/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.3114) |  Loss2: (0.1613) | Acc: (94.00%) (43775/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.3107) |  Loss2: (0.1612) | Acc: (94.00%) (45007/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.3107) |  Loss2: (0.1612) | Acc: (94.00%) (46221/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.3102) |  Loss2: (0.1612) | Acc: (94.00%) (47398/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_155.pth.tar'
# TEST : Loss: (0.4352) | Acc: (87.00%) (8778/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1231,  0.0741,  0.1003],
          [ 0.1159, -0.3228, -0.1217],
          [ 0.2832, -0.0696,  0.0075]],

         [[-0.0236, -0.0807,  0.2770],
          [ 0.0123, -0.1239,  0.0464],
          [ 0.0740,  0.0089, -0.0726]],

         [[ 0.0009, -0.0271,  0.2147],
          [ 0.0778, -0.2138,  0.1616],
          [-0.1138,  0.0138, -0.1937]]],


        [[[ 0.0956, -0.0729, -0.0044],
          [ 0.1384,  0.1776,  0.1822],
          [ 0.0153, -0.0602,  0.1160]],

         [[-0.1496, -0.1755,  0.1484],
          [ 0.0364, -0.0429,  0.1011],
          [ 0.1239, -0.1977,  0.1293]],

         [[ 0.0990, -0.0493, -0.0154],
          [ 0.0108,  0.0190, -0.1204],
          [-0.0279, -0.1589, -0.0223]]],


        [[[-0.0732,  0.0861, -0.0432],
          [-0.1262,  0.0630,  0.1132],
          [ 0.1055,  0.1628,  0.0012]],

         [[ 0.0964,  0.1296,  0.1101],
          [ 0.0660, -0.0379, -0.0356],
          [-0.0482, -0.0295,  0.1355]],

         [[-0.0456,  0.1408,  0.0719],
          [-0.0433, -0.0455, -0.0601],
          [-0.0407, -0.0088,  0.0398]]],


        ...,


        [[[ 0.2227, -0.1133, -0.1220],
          [-0.0535, -0.1178, -0.0901],
          [-0.0980,  0.0213,  0.0730]],

         [[ 0.1641,  0.1925, -0.1187],
          [ 0.1997, -0.1415, -0.1123],
          [-0.0606, -0.0770, -0.0275]],

         [[ 0.1785,  0.1022, -0.1085],
          [-0.0629, -0.1531, -0.0757],
          [ 0.0737,  0.1778, -0.1273]]],


        [[[-0.0989,  0.0884, -0.1403],
          [-0.1431, -0.1491,  0.1179],
          [-0.1415,  0.1529, -0.1299]],

         [[ 0.1033, -0.1081,  0.0896],
          [ 0.1467,  0.0010, -0.0783],
          [-0.1308,  0.1213, -0.1873]],

         [[ 0.1359, -0.0121,  0.0043],
          [ 0.1431, -0.1720,  0.0157],
          [-0.0103,  0.0250, -0.1134]]],


        [[[ 0.0704,  0.1845,  0.1216],
          [-0.0845, -0.2093,  0.2234],
          [-0.1715, -0.0356, -0.0751]],

         [[-0.0118, -0.0533,  0.1604],
          [ 0.0074, -0.2327,  0.0696],
          [-0.0069,  0.0712,  0.0270]],

         [[-0.1769, -0.1961,  0.1106],
          [ 0.1480, -0.2121, -0.0032],
          [ 0.0177, -0.0059,  0.3052]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2308e-05,  7.4130e-06,  1.0031e-05],
          [ 1.1588e-05, -3.2283e-05, -1.2169e-05],
          [ 2.8321e-05, -6.9582e-06,  7.4877e-07]],

         [[-2.3565e-06, -8.0696e-06,  2.7701e-05],
          [ 1.2331e-06, -1.2387e-05,  4.6419e-06],
          [ 7.3956e-06,  8.8829e-07, -7.2551e-06]],

         [[ 9.1348e-08, -2.7138e-06,  2.1472e-05],
          [ 7.7801e-06, -2.1380e-05,  1.6162e-05],
          [-1.1380e-05,  1.3795e-06, -1.9374e-05]]],


        [[[ 9.5557e-06, -7.2914e-06, -4.3954e-07],
          [ 1.3842e-05,  1.7762e-05,  1.8218e-05],
          [ 1.5269e-06, -6.0191e-06,  1.1595e-05]],

         [[-1.4963e-05, -1.7549e-05,  1.4837e-05],
          [ 3.6358e-06, -4.2940e-06,  1.0110e-05],
          [ 1.2386e-05, -1.9767e-05,  1.2926e-05]],

         [[ 9.9031e-06, -4.9308e-06, -1.5379e-06],
          [ 1.0760e-06,  1.9040e-06, -1.2042e-05],
          [-2.7905e-06, -1.5891e-05, -2.2305e-06]]],


        [[[-7.3208e-06,  8.6068e-06, -4.3237e-06],
          [-1.2619e-05,  6.3019e-06,  1.1316e-05],
          [ 1.0549e-05,  1.6281e-05,  1.1519e-07]],

         [[ 9.6447e-06,  1.2957e-05,  1.1006e-05],
          [ 6.5955e-06, -3.7867e-06, -3.5585e-06],
          [-4.8221e-06, -2.9524e-06,  1.3550e-05]],

         [[-4.5564e-06,  1.4082e-05,  7.1918e-06],
          [-4.3281e-06, -4.5465e-06, -6.0089e-06],
          [-4.0708e-06, -8.8373e-07,  3.9838e-06]]],


        ...,


        [[[ 2.2270e-05, -1.1333e-05, -1.2199e-05],
          [-5.3503e-06, -1.1782e-05, -9.0141e-06],
          [-9.7986e-06,  2.1275e-06,  7.3033e-06]],

         [[ 1.6412e-05,  1.9254e-05, -1.1874e-05],
          [ 1.9974e-05, -1.4152e-05, -1.1231e-05],
          [-6.0556e-06, -7.7018e-06, -2.7452e-06]],

         [[ 1.7848e-05,  1.0224e-05, -1.0855e-05],
          [-6.2919e-06, -1.5314e-05, -7.5694e-06],
          [ 7.3747e-06,  1.7778e-05, -1.2734e-05]]],


        [[[-9.8929e-06,  8.8436e-06, -1.4032e-05],
          [-1.4308e-05, -1.4912e-05,  1.1792e-05],
          [-1.4149e-05,  1.5289e-05, -1.2986e-05]],

         [[ 1.0325e-05, -1.0813e-05,  8.9575e-06],
          [ 1.4673e-05,  9.9181e-08, -7.8290e-06],
          [-1.3076e-05,  1.2126e-05, -1.8732e-05]],

         [[ 1.3591e-05, -1.2074e-06,  4.2972e-07],
          [ 1.4306e-05, -1.7196e-05,  1.5709e-06],
          [-1.0261e-06,  2.5046e-06, -1.1336e-05]]],


        [[[ 7.0384e-06,  1.8451e-05,  1.2156e-05],
          [-8.4508e-06, -2.0934e-05,  2.2338e-05],
          [-1.7150e-05, -3.5598e-06, -7.5097e-06]],

         [[-1.1848e-06, -5.3315e-06,  1.6042e-05],
          [ 7.4306e-07, -2.3272e-05,  6.9582e-06],
          [-6.8700e-07,  7.1216e-06,  2.6994e-06]],

         [[-1.7693e-05, -1.9607e-05,  1.1059e-05],
          [ 1.4799e-05, -2.1209e-05, -3.2216e-07],
          [ 1.7700e-06, -5.9405e-07,  3.0517e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.6683]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0206]], device='cuda:0')

Epoch: 156 | Batch_idx: 0 |  Loss: (0.3420) |  Loss2: (0.1593) | Acc: (95.00%) (122/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.2874) |  Loss2: (0.1593) | Acc: (96.00%) (1357/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.2865) |  Loss2: (0.1592) | Acc: (96.00%) (2581/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.2974) |  Loss2: (0.1592) | Acc: (95.00%) (3784/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.2984) |  Loss2: (0.1592) | Acc: (95.00%) (5001/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.3031) |  Loss2: (0.1591) | Acc: (95.00%) (6210/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.3044) |  Loss2: (0.1591) | Acc: (95.00%) (7419/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.3037) |  Loss2: (0.1590) | Acc: (95.00%) (8635/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.3016) |  Loss2: (0.1590) | Acc: (95.00%) (9854/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.3003) |  Loss2: (0.1590) | Acc: (95.00%) (11072/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.2994) |  Loss2: (0.1589) | Acc: (95.00%) (12295/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.2985) |  Loss2: (0.1589) | Acc: (95.00%) (13513/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.2986) |  Loss2: (0.1588) | Acc: (95.00%) (14725/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.2984) |  Loss2: (0.1588) | Acc: (95.00%) (15943/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.2971) |  Loss2: (0.1587) | Acc: (95.00%) (17167/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.2974) |  Loss2: (0.1587) | Acc: (95.00%) (18386/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.2956) |  Loss2: (0.1586) | Acc: (95.00%) (19619/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.2946) |  Loss2: (0.1586) | Acc: (95.00%) (20846/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.2941) |  Loss2: (0.1585) | Acc: (95.00%) (22070/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.2932) |  Loss2: (0.1585) | Acc: (95.00%) (23297/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.2925) |  Loss2: (0.1584) | Acc: (95.00%) (24519/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.2932) |  Loss2: (0.1584) | Acc: (95.00%) (25733/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.2927) |  Loss2: (0.1583) | Acc: (95.00%) (26957/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.2922) |  Loss2: (0.1583) | Acc: (95.00%) (28187/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.2919) |  Loss2: (0.1582) | Acc: (95.00%) (29410/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.2904) |  Loss2: (0.1582) | Acc: (95.00%) (30646/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.2904) |  Loss2: (0.1582) | Acc: (95.00%) (31871/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.2903) |  Loss2: (0.1581) | Acc: (95.00%) (33092/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.2903) |  Loss2: (0.1581) | Acc: (95.00%) (34321/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.2907) |  Loss2: (0.1580) | Acc: (95.00%) (35540/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.2906) |  Loss2: (0.1580) | Acc: (95.00%) (36770/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.2903) |  Loss2: (0.1579) | Acc: (95.00%) (37996/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.2909) |  Loss2: (0.1579) | Acc: (95.00%) (39210/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.2904) |  Loss2: (0.1578) | Acc: (95.00%) (40436/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.2910) |  Loss2: (0.1578) | Acc: (95.00%) (41640/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.2910) |  Loss2: (0.1577) | Acc: (95.00%) (42860/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.2908) |  Loss2: (0.1577) | Acc: (95.00%) (44086/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.2906) |  Loss2: (0.1576) | Acc: (95.00%) (45310/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.2909) |  Loss2: (0.1576) | Acc: (95.00%) (46520/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.2906) |  Loss2: (0.1576) | Acc: (95.00%) (47696/50000)
# TEST : Loss: (0.4137) | Acc: (88.00%) (8826/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1230,  0.0741,  0.1003],
          [ 0.1158, -0.3227, -0.1216],
          [ 0.2831, -0.0696,  0.0075]],

         [[-0.0236, -0.0807,  0.2769],
          [ 0.0123, -0.1238,  0.0464],
          [ 0.0739,  0.0089, -0.0725]],

         [[ 0.0009, -0.0271,  0.2146],
          [ 0.0778, -0.2137,  0.1616],
          [-0.1138,  0.0138, -0.1937]]],


        [[[ 0.0955, -0.0729, -0.0044],
          [ 0.1384,  0.1776,  0.1821],
          [ 0.0153, -0.0602,  0.1159]],

         [[-0.1496, -0.1754,  0.1483],
          [ 0.0363, -0.0429,  0.1011],
          [ 0.1238, -0.1976,  0.1292]],

         [[ 0.0990, -0.0493, -0.0154],
          [ 0.0108,  0.0190, -0.1204],
          [-0.0279, -0.1588, -0.0223]]],


        [[[-0.0732,  0.0860, -0.0432],
          [-0.1261,  0.0630,  0.1131],
          [ 0.1054,  0.1627,  0.0012]],

         [[ 0.0964,  0.1295,  0.1100],
          [ 0.0659, -0.0379, -0.0356],
          [-0.0482, -0.0295,  0.1355]],

         [[-0.0455,  0.1408,  0.0719],
          [-0.0433, -0.0454, -0.0601],
          [-0.0407, -0.0088,  0.0398]]],


        ...,


        [[[ 0.2226, -0.1133, -0.1219],
          [-0.0535, -0.1178, -0.0901],
          [-0.0979,  0.0213,  0.0730]],

         [[ 0.1641,  0.1925, -0.1187],
          [ 0.1997, -0.1415, -0.1123],
          [-0.0605, -0.0770, -0.0274]],

         [[ 0.1784,  0.1022, -0.1085],
          [-0.0629, -0.1531, -0.0757],
          [ 0.0737,  0.1777, -0.1273]]],


        [[[-0.0989,  0.0884, -0.1403],
          [-0.1430, -0.1491,  0.1179],
          [-0.1414,  0.1528, -0.1298]],

         [[ 0.1032, -0.1081,  0.0895],
          [ 0.1467,  0.0010, -0.0783],
          [-0.1307,  0.1212, -0.1872]],

         [[ 0.1359, -0.0121,  0.0043],
          [ 0.1430, -0.1719,  0.0157],
          [-0.0103,  0.0250, -0.1133]]],


        [[[ 0.0704,  0.1844,  0.1215],
          [-0.0845, -0.2093,  0.2233],
          [-0.1714, -0.0356, -0.0751]],

         [[-0.0118, -0.0533,  0.1604],
          [ 0.0074, -0.2326,  0.0696],
          [-0.0069,  0.0712,  0.0270]],

         [[-0.1769, -0.1960,  0.1105],
          [ 0.1479, -0.2120, -0.0032],
          [ 0.0177, -0.0059,  0.3050]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2303e-05,  7.4101e-06,  1.0027e-05],
          [ 1.1584e-05, -3.2270e-05, -1.2164e-05],
          [ 2.8310e-05, -6.9556e-06,  7.4848e-07]],

         [[-2.3555e-06, -8.0664e-06,  2.7691e-05],
          [ 1.2326e-06, -1.2382e-05,  4.6402e-06],
          [ 7.3927e-06,  8.8793e-07, -7.2522e-06]],

         [[ 9.1312e-08, -2.7127e-06,  2.1464e-05],
          [ 7.7772e-06, -2.1372e-05,  1.6155e-05],
          [-1.1376e-05,  1.3790e-06, -1.9366e-05]]],


        [[[ 9.5519e-06, -7.2885e-06, -4.3938e-07],
          [ 1.3837e-05,  1.7755e-05,  1.8211e-05],
          [ 1.5263e-06, -6.0168e-06,  1.1591e-05]],

         [[-1.4957e-05, -1.7542e-05,  1.4831e-05],
          [ 3.6343e-06, -4.2922e-06,  1.0106e-05],
          [ 1.2381e-05, -1.9760e-05,  1.2921e-05]],

         [[ 9.8994e-06, -4.9289e-06, -1.5373e-06],
          [ 1.0756e-06,  1.9033e-06, -1.2037e-05],
          [-2.7894e-06, -1.5885e-05, -2.2297e-06]]],


        [[[-7.3179e-06,  8.6033e-06, -4.3219e-06],
          [-1.2615e-05,  6.2995e-06,  1.1311e-05],
          [ 1.0545e-05,  1.6274e-05,  1.1515e-07]],

         [[ 9.6409e-06,  1.2952e-05,  1.1002e-05],
          [ 6.5928e-06, -3.7853e-06, -3.5571e-06],
          [-4.8202e-06, -2.9512e-06,  1.3545e-05]],

         [[-4.5546e-06,  1.4076e-05,  7.1889e-06],
          [-4.3263e-06, -4.5447e-06, -6.0066e-06],
          [-4.0692e-06, -8.8340e-07,  3.9822e-06]]],


        ...,


        [[[ 2.2262e-05, -1.1329e-05, -1.2195e-05],
          [-5.3483e-06, -1.1777e-05, -9.0106e-06],
          [-9.7948e-06,  2.1267e-06,  7.3004e-06]],

         [[ 1.6406e-05,  1.9246e-05, -1.1869e-05],
          [ 1.9966e-05, -1.4147e-05, -1.1227e-05],
          [-6.0532e-06, -7.6989e-06, -2.7441e-06]],

         [[ 1.7841e-05,  1.0220e-05, -1.0851e-05],
          [-6.2896e-06, -1.5308e-05, -7.5665e-06],
          [ 7.3718e-06,  1.7771e-05, -1.2729e-05]]],


        [[[-9.8891e-06,  8.8401e-06, -1.4027e-05],
          [-1.4302e-05, -1.4906e-05,  1.1788e-05],
          [-1.4144e-05,  1.5283e-05, -1.2981e-05]],

         [[ 1.0321e-05, -1.0809e-05,  8.9540e-06],
          [ 1.4668e-05,  9.9141e-08, -7.8258e-06],
          [-1.3071e-05,  1.2122e-05, -1.8725e-05]],

         [[ 1.3585e-05, -1.2069e-06,  4.2956e-07],
          [ 1.4300e-05, -1.7189e-05,  1.5703e-06],
          [-1.0257e-06,  2.5036e-06, -1.1332e-05]]],


        [[[ 7.0358e-06,  1.8444e-05,  1.2152e-05],
          [-8.4476e-06, -2.0926e-05,  2.2330e-05],
          [-1.7143e-05, -3.5583e-06, -7.5068e-06]],

         [[-1.1843e-06, -5.3295e-06,  1.6036e-05],
          [ 7.4277e-07, -2.3263e-05,  6.9555e-06],
          [-6.8673e-07,  7.1186e-06,  2.6983e-06]],

         [[-1.7686e-05, -1.9599e-05,  1.1054e-05],
          [ 1.4793e-05, -2.1201e-05, -3.2204e-07],
          [ 1.7693e-06, -5.9381e-07,  3.0505e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.7159]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0287]], device='cuda:0')

Epoch: 157 | Batch_idx: 0 |  Loss: (0.2766) |  Loss2: (0.1557) | Acc: (96.00%) (124/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.2772) |  Loss2: (0.1557) | Acc: (95.00%) (1349/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.2905) |  Loss2: (0.1556) | Acc: (95.00%) (2562/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.2863) |  Loss2: (0.1556) | Acc: (95.00%) (3790/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.2859) |  Loss2: (0.1556) | Acc: (95.00%) (5015/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.2834) |  Loss2: (0.1555) | Acc: (95.00%) (6243/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.2855) |  Loss2: (0.1555) | Acc: (95.00%) (7462/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.2859) |  Loss2: (0.1554) | Acc: (95.00%) (8677/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.2847) |  Loss2: (0.1554) | Acc: (95.00%) (9910/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.2850) |  Loss2: (0.1554) | Acc: (95.00%) (11131/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.2836) |  Loss2: (0.1553) | Acc: (95.00%) (12353/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.2839) |  Loss2: (0.1553) | Acc: (95.00%) (13576/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.2839) |  Loss2: (0.1552) | Acc: (95.00%) (14811/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.2861) |  Loss2: (0.1552) | Acc: (95.00%) (16020/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.2846) |  Loss2: (0.1552) | Acc: (95.00%) (17251/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.2835) |  Loss2: (0.1551) | Acc: (95.00%) (18474/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.2822) |  Loss2: (0.1551) | Acc: (95.00%) (19713/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.2819) |  Loss2: (0.1550) | Acc: (95.00%) (20940/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.2816) |  Loss2: (0.1550) | Acc: (95.00%) (22165/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.2824) |  Loss2: (0.1549) | Acc: (95.00%) (23379/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.2828) |  Loss2: (0.1549) | Acc: (95.00%) (24596/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.2819) |  Loss2: (0.1549) | Acc: (95.00%) (25832/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.2814) |  Loss2: (0.1548) | Acc: (95.00%) (27059/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.2807) |  Loss2: (0.1548) | Acc: (95.00%) (28292/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.2808) |  Loss2: (0.1547) | Acc: (95.00%) (29512/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.2806) |  Loss2: (0.1547) | Acc: (95.00%) (30733/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.2808) |  Loss2: (0.1546) | Acc: (95.00%) (31950/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.2804) |  Loss2: (0.1546) | Acc: (95.00%) (33181/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.2798) |  Loss2: (0.1546) | Acc: (95.00%) (34414/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.2797) |  Loss2: (0.1545) | Acc: (95.00%) (35633/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.2797) |  Loss2: (0.1545) | Acc: (95.00%) (36863/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.2796) |  Loss2: (0.1544) | Acc: (95.00%) (38090/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.2790) |  Loss2: (0.1544) | Acc: (95.00%) (39324/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.2788) |  Loss2: (0.1543) | Acc: (95.00%) (40552/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.2791) |  Loss2: (0.1543) | Acc: (95.00%) (41771/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.2786) |  Loss2: (0.1543) | Acc: (95.00%) (43001/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.2786) |  Loss2: (0.1542) | Acc: (95.00%) (44222/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.2783) |  Loss2: (0.1542) | Acc: (95.00%) (45452/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.2785) |  Loss2: (0.1541) | Acc: (95.00%) (46675/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.2788) |  Loss2: (0.1541) | Acc: (95.00%) (47843/50000)
# TEST : Loss: (0.4005) | Acc: (88.00%) (8855/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1230,  0.0741,  0.1002],
          [ 0.1158, -0.3226, -0.1216],
          [ 0.2830, -0.0695,  0.0075]],

         [[-0.0235, -0.0806,  0.2768],
          [ 0.0123, -0.1238,  0.0464],
          [ 0.0739,  0.0089, -0.0725]],

         [[ 0.0009, -0.0271,  0.2146],
          [ 0.0777, -0.2136,  0.1615],
          [-0.1137,  0.0138, -0.1936]]],


        [[[ 0.0955, -0.0729, -0.0044],
          [ 0.1383,  0.1775,  0.1820],
          [ 0.0153, -0.0601,  0.1159]],

         [[-0.1495, -0.1754,  0.1482],
          [ 0.0363, -0.0429,  0.1010],
          [ 0.1238, -0.1975,  0.1292]],

         [[ 0.0990, -0.0493, -0.0154],
          [ 0.0108,  0.0190, -0.1203],
          [-0.0279, -0.1588, -0.0223]]],


        [[[-0.0731,  0.0860, -0.0432],
          [-0.1261,  0.0630,  0.1131],
          [ 0.1054,  0.1627,  0.0012]],

         [[ 0.0964,  0.1295,  0.1100],
          [ 0.0659, -0.0378, -0.0356],
          [-0.0482, -0.0295,  0.1354]],

         [[-0.0455,  0.1407,  0.0719],
          [-0.0432, -0.0454, -0.0600],
          [-0.0407, -0.0088,  0.0398]]],


        ...,


        [[[ 0.2225, -0.1132, -0.1219],
          [-0.0535, -0.1177, -0.0901],
          [-0.0979,  0.0213,  0.0730]],

         [[ 0.1640,  0.1924, -0.1186],
          [ 0.1996, -0.1414, -0.1122],
          [-0.0605, -0.0770, -0.0274]],

         [[ 0.1783,  0.1022, -0.1085],
          [-0.0629, -0.1530, -0.0756],
          [ 0.0737,  0.1776, -0.1272]]],


        [[[-0.0989,  0.0884, -0.1402],
          [-0.1430, -0.1490,  0.1178],
          [-0.1414,  0.1528, -0.1298]],

         [[ 0.1032, -0.1080,  0.0895],
          [ 0.1466,  0.0010, -0.0782],
          [-0.1307,  0.1212, -0.1872]],

         [[ 0.1358, -0.0121,  0.0043],
          [ 0.1429, -0.1718,  0.0157],
          [-0.0103,  0.0250, -0.1133]]],


        [[[ 0.0703,  0.1844,  0.1215],
          [-0.0844, -0.2092,  0.2232],
          [-0.1714, -0.0356, -0.0750]],

         [[-0.0118, -0.0533,  0.1603],
          [ 0.0074, -0.2325,  0.0695],
          [-0.0069,  0.0712,  0.0270]],

         [[-0.1768, -0.1959,  0.1105],
          [ 0.1479, -0.2119, -0.0032],
          [ 0.0177, -0.0059,  0.3049]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2298e-05,  7.4072e-06,  1.0023e-05],
          [ 1.1579e-05, -3.2257e-05, -1.2159e-05],
          [ 2.8300e-05, -6.9529e-06,  7.4819e-07]],

         [[-2.3546e-06, -8.0632e-06,  2.7680e-05],
          [ 1.2321e-06, -1.2378e-05,  4.6384e-06],
          [ 7.3898e-06,  8.8756e-07, -7.2492e-06]],

         [[ 9.1275e-08, -2.7116e-06,  2.1455e-05],
          [ 7.7743e-06, -2.1364e-05,  1.6149e-05],
          [-1.1371e-05,  1.3784e-06, -1.9358e-05]]],


        [[[ 9.5481e-06, -7.2856e-06, -4.3921e-07],
          [ 1.3831e-05,  1.7748e-05,  1.8204e-05],
          [ 1.5257e-06, -6.0145e-06,  1.1586e-05]],

         [[-1.4951e-05, -1.7535e-05,  1.4825e-05],
          [ 3.6329e-06, -4.2905e-06,  1.0102e-05],
          [ 1.2376e-05, -1.9752e-05,  1.2915e-05]],

         [[ 9.8956e-06, -4.9270e-06, -1.5367e-06],
          [ 1.0751e-06,  1.9025e-06, -1.2033e-05],
          [-2.7884e-06, -1.5879e-05, -2.2288e-06]]],


        [[[-7.3149e-06,  8.5998e-06, -4.3202e-06],
          [-1.2610e-05,  6.2972e-06,  1.1307e-05],
          [ 1.0541e-05,  1.6268e-05,  1.1510e-07]],

         [[ 9.6371e-06,  1.2946e-05,  1.0998e-05],
          [ 6.5902e-06, -3.7838e-06, -3.5556e-06],
          [-4.8183e-06, -2.9501e-06,  1.3540e-05]],

         [[-4.5529e-06,  1.4071e-05,  7.1860e-06],
          [-4.3246e-06, -4.5430e-06, -6.0043e-06],
          [-4.0676e-06, -8.8307e-07,  3.9805e-06]]],


        ...,


        [[[ 2.2253e-05, -1.1324e-05, -1.2190e-05],
          [-5.3463e-06, -1.1773e-05, -9.0072e-06],
          [-9.7910e-06,  2.1259e-06,  7.2975e-06]],

         [[ 1.6399e-05,  1.9239e-05, -1.1865e-05],
          [ 1.9959e-05, -1.4142e-05, -1.1222e-05],
          [-6.0509e-06, -7.6960e-06, -2.7430e-06]],

         [[ 1.7834e-05,  1.0216e-05, -1.0846e-05],
          [-6.2872e-06, -1.5302e-05, -7.5635e-06],
          [ 7.3689e-06,  1.7764e-05, -1.2723e-05]]],


        [[[-9.8853e-06,  8.8366e-06, -1.4022e-05],
          [-1.4296e-05, -1.4900e-05,  1.1783e-05],
          [-1.4139e-05,  1.5277e-05, -1.2975e-05]],

         [[ 1.0317e-05, -1.0805e-05,  8.9505e-06],
          [ 1.4662e-05,  9.9100e-08, -7.8226e-06],
          [-1.3065e-05,  1.2117e-05, -1.8717e-05]],

         [[ 1.3580e-05, -1.2065e-06,  4.2939e-07],
          [ 1.4294e-05, -1.7182e-05,  1.5698e-06],
          [-1.0253e-06,  2.5027e-06, -1.1327e-05]]],


        [[[ 7.0331e-06,  1.8437e-05,  1.2147e-05],
          [-8.4444e-06, -2.0918e-05,  2.2321e-05],
          [-1.7136e-05, -3.5568e-06, -7.5039e-06]],

         [[-1.1839e-06, -5.3275e-06,  1.6029e-05],
          [ 7.4248e-07, -2.3254e-05,  6.9529e-06],
          [-6.8646e-07,  7.1157e-06,  2.6973e-06]],

         [[-1.7679e-05, -1.9591e-05,  1.1050e-05],
          [ 1.4787e-05, -2.1193e-05, -3.2191e-07],
          [ 1.7687e-06, -5.9357e-07,  3.0493e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.7598]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0054]], device='cuda:0')

Epoch: 158 | Batch_idx: 0 |  Loss: (0.3673) |  Loss2: (0.1524) | Acc: (92.00%) (118/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.2709) |  Loss2: (0.1524) | Acc: (96.00%) (1354/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.2689) |  Loss2: (0.1523) | Acc: (96.00%) (2582/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.2762) |  Loss2: (0.1523) | Acc: (95.00%) (3802/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.2743) |  Loss2: (0.1522) | Acc: (95.00%) (5035/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.2728) |  Loss2: (0.1522) | Acc: (96.00%) (6267/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.2719) |  Loss2: (0.1522) | Acc: (96.00%) (7497/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.2712) |  Loss2: (0.1521) | Acc: (96.00%) (8728/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.2714) |  Loss2: (0.1521) | Acc: (96.00%) (9955/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.2699) |  Loss2: (0.1520) | Acc: (95.00%) (11182/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.2702) |  Loss2: (0.1520) | Acc: (96.00%) (12413/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.2708) |  Loss2: (0.1520) | Acc: (95.00%) (13634/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.2711) |  Loss2: (0.1519) | Acc: (95.00%) (14868/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.2714) |  Loss2: (0.1519) | Acc: (96.00%) (16098/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.2714) |  Loss2: (0.1518) | Acc: (95.00%) (17323/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.2715) |  Loss2: (0.1518) | Acc: (95.00%) (18550/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.2707) |  Loss2: (0.1517) | Acc: (96.00%) (19789/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.2702) |  Loss2: (0.1517) | Acc: (96.00%) (21023/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.2705) |  Loss2: (0.1517) | Acc: (96.00%) (22250/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.2696) |  Loss2: (0.1516) | Acc: (96.00%) (23483/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.2697) |  Loss2: (0.1516) | Acc: (96.00%) (24711/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.2702) |  Loss2: (0.1515) | Acc: (96.00%) (25942/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.2702) |  Loss2: (0.1515) | Acc: (96.00%) (27163/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.2701) |  Loss2: (0.1515) | Acc: (96.00%) (28396/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.2704) |  Loss2: (0.1514) | Acc: (95.00%) (29614/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.2705) |  Loss2: (0.1514) | Acc: (95.00%) (30842/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.2704) |  Loss2: (0.1513) | Acc: (95.00%) (32067/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.2716) |  Loss2: (0.1513) | Acc: (95.00%) (33287/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.2712) |  Loss2: (0.1513) | Acc: (95.00%) (34515/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.2715) |  Loss2: (0.1512) | Acc: (95.00%) (35736/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.2716) |  Loss2: (0.1512) | Acc: (95.00%) (36962/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.2708) |  Loss2: (0.1511) | Acc: (95.00%) (38204/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.2712) |  Loss2: (0.1511) | Acc: (95.00%) (39422/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.2708) |  Loss2: (0.1511) | Acc: (95.00%) (40658/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.2704) |  Loss2: (0.1510) | Acc: (95.00%) (41887/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.2701) |  Loss2: (0.1510) | Acc: (95.00%) (43119/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.2699) |  Loss2: (0.1509) | Acc: (95.00%) (44347/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.2698) |  Loss2: (0.1509) | Acc: (95.00%) (45575/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.2694) |  Loss2: (0.1509) | Acc: (95.00%) (46808/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.2695) |  Loss2: (0.1508) | Acc: (95.00%) (47995/50000)
# TEST : Loss: (0.3969) | Acc: (88.00%) (8854/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1229,  0.0740,  0.1002],
          [ 0.1157, -0.3224, -0.1215],
          [ 0.2829, -0.0695,  0.0075]],

         [[-0.0235, -0.0806,  0.2767],
          [ 0.0123, -0.1237,  0.0464],
          [ 0.0739,  0.0089, -0.0725]],

         [[ 0.0009, -0.0271,  0.2145],
          [ 0.0777, -0.2136,  0.1614],
          [-0.1137,  0.0138, -0.1935]]],


        [[[ 0.0954, -0.0728, -0.0044],
          [ 0.1383,  0.1774,  0.1820],
          [ 0.0153, -0.0601,  0.1158]],

         [[-0.1495, -0.1753,  0.1482],
          [ 0.0363, -0.0429,  0.1010],
          [ 0.1237, -0.1974,  0.1291]],

         [[ 0.0989, -0.0493, -0.0154],
          [ 0.0107,  0.0190, -0.1203],
          [-0.0279, -0.1587, -0.0223]]],


        [[[-0.0731,  0.0860, -0.0432],
          [-0.1261,  0.0629,  0.1130],
          [ 0.1054,  0.1626,  0.0012]],

         [[ 0.0963,  0.1294,  0.1099],
          [ 0.0659, -0.0378, -0.0355],
          [-0.0482, -0.0295,  0.1353]],

         [[-0.0455,  0.1407,  0.0718],
          [-0.0432, -0.0454, -0.0600],
          [-0.0407, -0.0088,  0.0398]]],


        ...,


        [[[ 0.2224, -0.1132, -0.1219],
          [-0.0534, -0.1177, -0.0900],
          [-0.0979,  0.0213,  0.0729]],

         [[ 0.1639,  0.1923, -0.1186],
          [ 0.1995, -0.1414, -0.1122],
          [-0.0605, -0.0769, -0.0274]],

         [[ 0.1783,  0.1021, -0.1084],
          [-0.0628, -0.1530, -0.0756],
          [ 0.0737,  0.1776, -0.1272]]],


        [[[-0.0988,  0.0883, -0.1402],
          [-0.1429, -0.1489,  0.1178],
          [-0.1413,  0.1527, -0.1297]],

         [[ 0.1031, -0.1080,  0.0895],
          [ 0.1466,  0.0010, -0.0782],
          [-0.1306,  0.1211, -0.1871]],

         [[ 0.1357, -0.0121,  0.0043],
          [ 0.1429, -0.1718,  0.0157],
          [-0.0102,  0.0250, -0.1132]]],


        [[[ 0.0703,  0.1843,  0.1214],
          [-0.0844, -0.2091,  0.2231],
          [-0.1713, -0.0356, -0.0750]],

         [[-0.0118, -0.0533,  0.1602],
          [ 0.0074, -0.2324,  0.0695],
          [-0.0069,  0.0711,  0.0270]],

         [[-0.1767, -0.1958,  0.1105],
          [ 0.1478, -0.2118, -0.0032],
          [ 0.0177, -0.0059,  0.3048]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2293e-05,  7.4043e-06,  1.0020e-05],
          [ 1.1574e-05, -3.2244e-05, -1.2155e-05],
          [ 2.8289e-05, -6.9503e-06,  7.4790e-07]],

         [[-2.3536e-06, -8.0600e-06,  2.7670e-05],
          [ 1.2317e-06, -1.2373e-05,  4.6367e-06],
          [ 7.3869e-06,  8.8720e-07, -7.2463e-06]],

         [[ 9.1239e-08, -2.7105e-06,  2.1447e-05],
          [ 7.7714e-06, -2.1356e-05,  1.6142e-05],
          [-1.1367e-05,  1.3779e-06, -1.9351e-05]]],


        [[[ 9.5443e-06, -7.2827e-06, -4.3905e-07],
          [ 1.3826e-05,  1.7741e-05,  1.8197e-05],
          [ 1.5251e-06, -6.0121e-06,  1.1581e-05]],

         [[-1.4945e-05, -1.7528e-05,  1.4819e-05],
          [ 3.6314e-06, -4.2887e-06,  1.0098e-05],
          [ 1.2371e-05, -1.9745e-05,  1.2910e-05]],

         [[ 9.8918e-06, -4.9251e-06, -1.5361e-06],
          [ 1.0747e-06,  1.9018e-06, -1.2028e-05],
          [-2.7873e-06, -1.5872e-05, -2.2279e-06]]],


        [[[-7.3120e-06,  8.5963e-06, -4.3184e-06],
          [-1.2605e-05,  6.2949e-06,  1.1302e-05],
          [ 1.0537e-05,  1.6262e-05,  1.1505e-07]],

         [[ 9.6334e-06,  1.2941e-05,  1.0993e-05],
          [ 6.5876e-06, -3.7824e-06, -3.5542e-06],
          [-4.8164e-06, -2.9489e-06,  1.3535e-05]],

         [[-4.5511e-06,  1.4066e-05,  7.1831e-06],
          [-4.3228e-06, -4.5412e-06, -6.0019e-06],
          [-4.0660e-06, -8.8274e-07,  3.9789e-06]]],


        ...,


        [[[ 2.2244e-05, -1.1320e-05, -1.2185e-05],
          [-5.3442e-06, -1.1768e-05, -9.0037e-06],
          [-9.7872e-06,  2.1251e-06,  7.2946e-06]],

         [[ 1.6393e-05,  1.9231e-05, -1.1860e-05],
          [ 1.9951e-05, -1.4137e-05, -1.1218e-05],
          [-6.0486e-06, -7.6931e-06, -2.7419e-06]],

         [[ 1.7827e-05,  1.0212e-05, -1.0842e-05],
          [-6.2849e-06, -1.5296e-05, -7.5606e-06],
          [ 7.3660e-06,  1.7757e-05, -1.2718e-05]]],


        [[[-9.8815e-06,  8.8331e-06, -1.4017e-05],
          [-1.4291e-05, -1.4895e-05,  1.1778e-05],
          [-1.4133e-05,  1.5271e-05, -1.2970e-05]],

         [[ 1.0313e-05, -1.0801e-05,  8.9470e-06],
          [ 1.4656e-05,  9.9059e-08, -7.8197e-06],
          [-1.3060e-05,  1.2112e-05, -1.8710e-05]],

         [[ 1.3575e-05, -1.2060e-06,  4.2923e-07],
          [ 1.4288e-05, -1.7175e-05,  1.5692e-06],
          [-1.0249e-06,  2.5018e-06, -1.1323e-05]]],


        [[[ 7.0305e-06,  1.8430e-05,  1.2142e-05],
          [-8.4412e-06, -2.0910e-05,  2.2312e-05],
          [-1.7130e-05, -3.5554e-06, -7.5010e-06]],

         [[-1.1834e-06, -5.3254e-06,  1.6023e-05],
          [ 7.4219e-07, -2.3244e-05,  6.9503e-06],
          [-6.8618e-07,  7.1128e-06,  2.6963e-06]],

         [[-1.7672e-05, -1.9584e-05,  1.1046e-05],
          [ 1.4781e-05, -2.1185e-05, -3.2178e-07],
          [ 1.7680e-06, -5.9334e-07,  3.0482e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8001]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0184]], device='cuda:0')

Epoch: 159 | Batch_idx: 0 |  Loss: (0.2078) |  Loss2: (0.1493) | Acc: (98.00%) (126/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.2767) |  Loss2: (0.1492) | Acc: (95.00%) (1345/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.2794) |  Loss2: (0.1492) | Acc: (95.00%) (2565/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.2855) |  Loss2: (0.1492) | Acc: (94.00%) (3769/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.2785) |  Loss2: (0.1491) | Acc: (95.00%) (4998/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.2755) |  Loss2: (0.1491) | Acc: (95.00%) (6225/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.2792) |  Loss2: (0.1490) | Acc: (95.00%) (7436/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.2759) |  Loss2: (0.1490) | Acc: (95.00%) (8670/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.2751) |  Loss2: (0.1490) | Acc: (95.00%) (9896/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.2746) |  Loss2: (0.1489) | Acc: (95.00%) (11127/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.2708) |  Loss2: (0.1489) | Acc: (95.00%) (12375/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.2694) |  Loss2: (0.1489) | Acc: (95.00%) (13607/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.2695) |  Loss2: (0.1488) | Acc: (95.00%) (14835/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.2703) |  Loss2: (0.1488) | Acc: (95.00%) (16061/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.2705) |  Loss2: (0.1487) | Acc: (95.00%) (17280/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.2696) |  Loss2: (0.1487) | Acc: (95.00%) (18514/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.2687) |  Loss2: (0.1487) | Acc: (95.00%) (19745/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.2685) |  Loss2: (0.1486) | Acc: (95.00%) (20979/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.2685) |  Loss2: (0.1486) | Acc: (95.00%) (22205/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.2677) |  Loss2: (0.1485) | Acc: (95.00%) (23441/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.2677) |  Loss2: (0.1485) | Acc: (95.00%) (24669/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.2682) |  Loss2: (0.1485) | Acc: (95.00%) (25893/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.2684) |  Loss2: (0.1484) | Acc: (95.00%) (27116/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.2684) |  Loss2: (0.1484) | Acc: (95.00%) (28347/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.2687) |  Loss2: (0.1484) | Acc: (95.00%) (29574/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.2689) |  Loss2: (0.1483) | Acc: (95.00%) (30800/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.2681) |  Loss2: (0.1483) | Acc: (95.00%) (32038/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.2678) |  Loss2: (0.1483) | Acc: (95.00%) (33267/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.2669) |  Loss2: (0.1482) | Acc: (95.00%) (34505/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.2667) |  Loss2: (0.1482) | Acc: (95.00%) (35737/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.2667) |  Loss2: (0.1481) | Acc: (95.00%) (36966/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.2662) |  Loss2: (0.1481) | Acc: (95.00%) (38204/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.2666) |  Loss2: (0.1481) | Acc: (95.00%) (39423/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.2666) |  Loss2: (0.1480) | Acc: (95.00%) (40660/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.2669) |  Loss2: (0.1480) | Acc: (95.00%) (41881/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.2669) |  Loss2: (0.1479) | Acc: (95.00%) (43108/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.2669) |  Loss2: (0.1479) | Acc: (95.00%) (44332/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.2667) |  Loss2: (0.1479) | Acc: (95.00%) (45571/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.2663) |  Loss2: (0.1478) | Acc: (95.00%) (46806/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.2663) |  Loss2: (0.1478) | Acc: (95.00%) (47995/50000)
# TEST : Loss: (0.3929) | Acc: (88.00%) (8842/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1229,  0.0740,  0.1002],
          [ 0.1157, -0.3223, -0.1215],
          [ 0.2828, -0.0695,  0.0075]],

         [[-0.0235, -0.0806,  0.2766],
          [ 0.0123, -0.1237,  0.0463],
          [ 0.0738,  0.0089, -0.0724]],

         [[ 0.0009, -0.0271,  0.2144],
          [ 0.0777, -0.2135,  0.1614],
          [-0.1136,  0.0138, -0.1934]]],


        [[[ 0.0954, -0.0728, -0.0044],
          [ 0.1382,  0.1773,  0.1819],
          [ 0.0152, -0.0601,  0.1158]],

         [[-0.1494, -0.1752,  0.1481],
          [ 0.0363, -0.0429,  0.1009],
          [ 0.1237, -0.1974,  0.1290]],

         [[ 0.0989, -0.0492, -0.0154],
          [ 0.0107,  0.0190, -0.1202],
          [-0.0279, -0.1587, -0.0223]]],


        [[[-0.0731,  0.0859, -0.0432],
          [-0.1260,  0.0629,  0.1130],
          [ 0.1053,  0.1626,  0.0012]],

         [[ 0.0963,  0.1294,  0.1099],
          [ 0.0658, -0.0378, -0.0355],
          [-0.0481, -0.0295,  0.1353]],

         [[-0.0455,  0.1406,  0.0718],
          [-0.0432, -0.0454, -0.0600],
          [-0.0406, -0.0088,  0.0398]]],


        ...,


        [[[ 0.2224, -0.1132, -0.1218],
          [-0.0534, -0.1176, -0.0900],
          [-0.0978,  0.0212,  0.0729]],

         [[ 0.1639,  0.1922, -0.1186],
          [ 0.1994, -0.1413, -0.1121],
          [-0.0605, -0.0769, -0.0274]],

         [[ 0.1782,  0.1021, -0.1084],
          [-0.0628, -0.1529, -0.0756],
          [ 0.0736,  0.1775, -0.1271]]],


        [[[-0.0988,  0.0883, -0.1401],
          [-0.1428, -0.1489,  0.1177],
          [-0.1413,  0.1527, -0.1296]],

         [[ 0.1031, -0.1080,  0.0894],
          [ 0.1465,  0.0010, -0.0782],
          [-0.1305,  0.1211, -0.1870]],

         [[ 0.1357, -0.0121,  0.0043],
          [ 0.1428, -0.1717,  0.0157],
          [-0.0102,  0.0250, -0.1132]]],


        [[[ 0.0703,  0.1842,  0.1214],
          [-0.0844, -0.2090,  0.2230],
          [-0.1712, -0.0355, -0.0750]],

         [[-0.0118, -0.0532,  0.1602],
          [ 0.0074, -0.2323,  0.0695],
          [-0.0069,  0.0711,  0.0270]],

         [[-0.1767, -0.1958,  0.1104],
          [ 0.1478, -0.2118, -0.0032],
          [ 0.0177, -0.0059,  0.3047]]]], device='cuda:0')

conv1_weight_grad tensor([[[[-1.2289e-05,  7.4013e-06,  1.0016e-05],
          [ 1.1570e-05, -3.2231e-05, -1.2150e-05],
          [ 2.8279e-05, -6.9477e-06,  7.4761e-07]],

         [[-2.3527e-06, -8.0568e-06,  2.7659e-05],
          [ 1.2312e-06, -1.2368e-05,  4.6349e-06],
          [ 7.3840e-06,  8.8683e-07, -7.2434e-06]],

         [[ 9.1203e-08, -2.7094e-06,  2.1439e-05],
          [ 7.7685e-06, -2.1347e-05,  1.6136e-05],
          [-1.1363e-05,  1.3773e-06, -1.9343e-05]]],


        [[[ 9.5405e-06, -7.2798e-06, -4.3889e-07],
          [ 1.3821e-05,  1.7734e-05,  1.8190e-05],
          [ 1.5245e-06, -6.0098e-06,  1.1577e-05]],

         [[-1.4939e-05, -1.7521e-05,  1.4813e-05],
          [ 3.6300e-06, -4.2870e-06,  1.0094e-05],
          [ 1.2366e-05, -1.9737e-05,  1.2905e-05]],

         [[ 9.8880e-06, -4.9232e-06, -1.5355e-06],
          [ 1.0742e-06,  1.9011e-06, -1.2023e-05],
          [-2.7862e-06, -1.5866e-05, -2.2270e-06]]],


        [[[-7.3091e-06,  8.5928e-06, -4.3167e-06],
          [-1.2601e-05,  6.2925e-06,  1.1298e-05],
          [ 1.0533e-05,  1.6255e-05,  1.1501e-07]],

         [[ 9.6296e-06,  1.2936e-05,  1.0989e-05],
          [ 6.5850e-06, -3.7809e-06, -3.5527e-06],
          [-4.8145e-06, -2.9477e-06,  1.3530e-05]],

         [[-4.5494e-06,  1.4061e-05,  7.1802e-06],
          [-4.3211e-06, -4.5395e-06, -5.9996e-06],
          [-4.0644e-06, -8.8242e-07,  3.9773e-06]]],


        ...,


        [[[ 2.2235e-05, -1.1316e-05, -1.2181e-05],
          [-5.3422e-06, -1.1763e-05, -9.0002e-06],
          [-9.7834e-06,  2.1243e-06,  7.2917e-06]],

         [[ 1.6387e-05,  1.9224e-05, -1.1855e-05],
          [ 1.9944e-05, -1.4131e-05, -1.1214e-05],
          [-6.0462e-06, -7.6902e-06, -2.7408e-06]],

         [[ 1.7820e-05,  1.0208e-05, -1.0837e-05],
          [-6.2826e-06, -1.5291e-05, -7.5577e-06],
          [ 7.3631e-06,  1.7750e-05, -1.2713e-05]]],


        [[[-9.8777e-06,  8.8296e-06, -1.4011e-05],
          [-1.4285e-05, -1.4889e-05,  1.1774e-05],
          [-1.4128e-05,  1.5265e-05, -1.2965e-05]],

         [[ 1.0309e-05, -1.0796e-05,  8.9435e-06],
          [ 1.4650e-05,  9.9018e-08, -7.8168e-06],
          [-1.3055e-05,  1.2108e-05, -1.8702e-05]],

         [[ 1.3570e-05, -1.2055e-06,  4.2907e-07],
          [ 1.4282e-05, -1.7168e-05,  1.5686e-06],
          [-1.0245e-06,  2.5008e-06, -1.1319e-05]]],


        [[[ 7.0279e-06,  1.8423e-05,  1.2138e-05],
          [-8.4380e-06, -2.0902e-05,  2.2303e-05],
          [-1.7124e-05, -3.5539e-06, -7.4981e-06]],

         [[-1.1829e-06, -5.3234e-06,  1.6016e-05],
          [ 7.4190e-07, -2.3235e-05,  6.9477e-06],
          [-6.8591e-07,  7.1099e-06,  2.6953e-06]],

         [[-1.7665e-05, -1.9576e-05,  1.1041e-05],
          [ 1.4775e-05, -2.1177e-05, -3.2165e-07],
          [ 1.7674e-06, -5.9310e-07,  3.0470e-05]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8372]], device='cuda:0', requires_grad=True)

percentage_weight_grad tensor([[-0.0196]], device='cuda:0')

True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (2572/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (3794/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (6245/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (7473/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (8688/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (9910/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (11134/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (12355/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (13592/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (14795/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (16005/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (17226/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (18458/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (19704/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (20923/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (22148/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (23365/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (24594/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (25823/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (27047/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (28277/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (29512/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (30744/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (31961/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (33166/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (34381/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (35600/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (36809/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (38021/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (39241/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (40470/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (41693/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (42927/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (44147/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (45366/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (46584/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (47757/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_160.pth.tar'
# TEST : Loss: (0.4507) | Acc: (87.00%) (8705/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1222,  0.0717,  0.0987],
          [ 0.1147, -0.3260, -0.1224],
          [ 0.2819, -0.0733,  0.0067]],

         [[-0.0216, -0.0817,  0.2757],
          [ 0.0122, -0.1266,  0.0458],
          [ 0.0739,  0.0057, -0.0729]],

         [[ 0.0019, -0.0281,  0.2131],
          [ 0.0763, -0.2163,  0.1612],
          [-0.1140,  0.0104, -0.1933]]],


        [[[ 0.0919, -0.0761, -0.0065],
          [ 0.1351,  0.1752,  0.1809],
          [ 0.0131, -0.0614,  0.1152]],

         [[-0.1533, -0.1793,  0.1450],
          [ 0.0327, -0.0457,  0.0990],
          [ 0.1208, -0.1993,  0.1279]],

         [[ 0.0955, -0.0533, -0.0185],
          [ 0.0076,  0.0160, -0.1223],
          [-0.0304, -0.1609, -0.0237]]],


        [[[-0.0733,  0.0857, -0.0435],
          [-0.1260,  0.0629,  0.1128],
          [ 0.1053,  0.1626,  0.0010]],

         [[ 0.0963,  0.1294,  0.1097],
          [ 0.0659, -0.0377, -0.0356],
          [-0.0481, -0.0293,  0.1352]],

         [[-0.0454,  0.1406,  0.0717],
          [-0.0432, -0.0454, -0.0602],
          [-0.0405, -0.0087,  0.0396]]],


        ...,


        [[[ 0.2207, -0.1150, -0.1237],
          [-0.0542, -0.1190, -0.0906],
          [-0.0979,  0.0206,  0.0730]],

         [[ 0.1620,  0.1900, -0.1210],
          [ 0.1988, -0.1426, -0.1135],
          [-0.0601, -0.0771, -0.0278]],

         [[ 0.1755,  0.0998, -0.1110],
          [-0.0642, -0.1542, -0.0769],
          [ 0.0732,  0.1770, -0.1277]]],


        [[[-0.1005,  0.0869, -0.1408],
          [-0.1445, -0.1500,  0.1176],
          [-0.1420,  0.1517, -0.1298]],

         [[ 0.1017, -0.1087,  0.0896],
          [ 0.1452,  0.0003, -0.0780],
          [-0.1306,  0.1208, -0.1868]],

         [[ 0.1340, -0.0131,  0.0039],
          [ 0.1409, -0.1728,  0.0151],
          [-0.0113,  0.0238, -0.1140]]],


        [[[ 0.0675,  0.1811,  0.1181],
          [-0.0851, -0.2123,  0.2213],
          [-0.1710, -0.0375, -0.0747]],

         [[-0.0154, -0.0579,  0.1551],
          [ 0.0060, -0.2360,  0.0668],
          [-0.0080,  0.0692,  0.0262]],

         [[-0.1806, -0.2019,  0.1027],
          [ 0.1467, -0.2154, -0.0067],
          [ 0.0168, -0.0074,  0.3041]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0669, -0.0588, -0.0631],
          [-0.0501, -0.0385, -0.0723],
          [-0.0288, -0.0279, -0.0660]],

         [[-0.0286, -0.0091, -0.0124],
          [-0.0111,  0.0070, -0.0240],
          [ 0.0107,  0.0143, -0.0203]],

         [[-0.0346, -0.0214, -0.0307],
          [-0.0173, -0.0072, -0.0427],
          [ 0.0023,  0.0003, -0.0398]]],


        [[[ 0.0157,  0.0273,  0.0303],
          [ 0.0233,  0.0363,  0.0353],
          [ 0.0296,  0.0403,  0.0388]],

         [[ 0.0207,  0.0333,  0.0347],
          [ 0.0301,  0.0445,  0.0434],
          [ 0.0380,  0.0502,  0.0489]],

         [[ 0.0273,  0.0409,  0.0450],
          [ 0.0371,  0.0522,  0.0539],
          [ 0.0433,  0.0556,  0.0567]]],


        [[[ 0.0004,  0.0000,  0.0010],
          [ 0.0022,  0.0012,  0.0015],
          [ 0.0040,  0.0033,  0.0019]],

         [[-0.0000, -0.0015, -0.0013],
          [ 0.0034,  0.0015,  0.0007],
          [ 0.0064,  0.0051,  0.0027]],

         [[ 0.0011, -0.0003, -0.0002],
          [ 0.0034,  0.0018,  0.0008],
          [ 0.0049,  0.0043,  0.0018]]],


        ...,


        [[[-0.0307, -0.0210, -0.0168],
          [-0.0266, -0.0162, -0.0172],
          [-0.0251, -0.0157, -0.0197]],

         [[-0.0278, -0.0217, -0.0204],
          [-0.0241, -0.0183, -0.0215],
          [-0.0231, -0.0193, -0.0271]],

         [[-0.0255, -0.0191, -0.0170],
          [-0.0229, -0.0155, -0.0178],
          [-0.0218, -0.0168, -0.0218]]],


        [[[-0.0170, -0.0066,  0.0060],
          [-0.0131, -0.0118, -0.0060],
          [-0.0043, -0.0100, -0.0087]],

         [[-0.0182, -0.0063,  0.0129],
          [-0.0132, -0.0104,  0.0004],
          [-0.0010, -0.0059, -0.0026]],

         [[-0.0163, -0.0062,  0.0084],
          [-0.0083, -0.0078, -0.0026],
          [ 0.0012, -0.0045, -0.0057]]],


        [[[ 0.0045,  0.0404,  0.0430],
          [ 0.0127,  0.0412,  0.0378],
          [ 0.0171,  0.0228,  0.0297]],

         [[ 0.0205,  0.0657,  0.0687],
          [ 0.0264,  0.0660,  0.0656],
          [ 0.0276,  0.0429,  0.0544]],

         [[-0.0071,  0.0385,  0.0485],
          [-0.0163,  0.0257,  0.0367],
          [-0.0233, -0.0040,  0.0173]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8372]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 161 | Batch_idx: 0 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (5049/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (6277/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (7506/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (8735/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (9963/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (11198/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (12421/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (96.00%) (13643/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (14864/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (16082/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (17319/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (18535/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (19776/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (20996/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (22229/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (23460/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (24682/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (25912/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (27128/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (28359/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (29590/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (30809/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (32046/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (33283/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (34511/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (35744/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (36963/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (38175/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (39397/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (40619/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (41839/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (43067/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (44283/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (45509/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (46742/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (47924/50000)
# TEST : Loss: (0.4186) | Acc: (88.00%) (8849/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1234,  0.0696,  0.1001],
          [ 0.1146, -0.3279, -0.1215],
          [ 0.2807, -0.0743,  0.0089]],

         [[-0.0218, -0.0827,  0.2773],
          [ 0.0137, -0.1275,  0.0468],
          [ 0.0737,  0.0045, -0.0714]],

         [[ 0.0013, -0.0293,  0.2139],
          [ 0.0768, -0.2173,  0.1617],
          [-0.1143,  0.0092, -0.1917]]],


        [[[ 0.0910, -0.0775, -0.0069],
          [ 0.1351,  0.1745,  0.1813],
          [ 0.0137, -0.0614,  0.1160]],

         [[-0.1527, -0.1797,  0.1454],
          [ 0.0342, -0.0454,  0.1003],
          [ 0.1228, -0.1984,  0.1295]],

         [[ 0.0967, -0.0530, -0.0173],
          [ 0.0098,  0.0169, -0.1201],
          [-0.0276, -0.1594, -0.0213]]],


        [[[-0.0734,  0.0856, -0.0434],
          [-0.1261,  0.0629,  0.1129],
          [ 0.1051,  0.1627,  0.0014]],

         [[ 0.0959,  0.1290,  0.1095],
          [ 0.0655, -0.0379, -0.0356],
          [-0.0483, -0.0294,  0.1353]],

         [[-0.0457,  0.1402,  0.0715],
          [-0.0435, -0.0457, -0.0603],
          [-0.0409, -0.0090,  0.0395]]],


        ...,


        [[[ 0.2180, -0.1169, -0.1251],
          [-0.0566, -0.1210, -0.0917],
          [-0.1001,  0.0193,  0.0723]],

         [[ 0.1614,  0.1899, -0.1206],
          [ 0.1981, -0.1430, -0.1133],
          [-0.0605, -0.0772, -0.0274]],

         [[ 0.1747,  0.0995, -0.1105],
          [-0.0646, -0.1544, -0.0765],
          [ 0.0734,  0.1774, -0.1269]]],


        [[[-0.1009,  0.0869, -0.1399],
          [-0.1442, -0.1497,  0.1189],
          [-0.1415,  0.1518, -0.1292]],

         [[ 0.1042, -0.1059,  0.0926],
          [ 0.1480,  0.0027, -0.0748],
          [-0.1275,  0.1229, -0.1843]],

         [[ 0.1380, -0.0089,  0.0085],
          [ 0.1447, -0.1695,  0.0192],
          [-0.0077,  0.0263, -0.1110]]],


        [[[ 0.0653,  0.1808,  0.1176],
          [-0.0863, -0.2141,  0.2194],
          [-0.1723, -0.0391, -0.0767]],

         [[-0.0162, -0.0569,  0.1556],
          [ 0.0058, -0.2371,  0.0662],
          [-0.0088,  0.0682,  0.0255]],

         [[-0.1809, -0.1999,  0.1042],
          [ 0.1478, -0.2147, -0.0059],
          [ 0.0185, -0.0056,  0.3057]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0062,  0.0159,  0.0153],
          [ 0.0027,  0.0178,  0.0122],
          [-0.0020,  0.0149,  0.0142]],

         [[-0.0122,  0.0106,  0.0144],
          [ 0.0002,  0.0121,  0.0103],
          [-0.0025,  0.0103,  0.0076]],

         [[-0.0186,  0.0085,  0.0203],
          [-0.0052,  0.0099,  0.0173],
          [-0.0077,  0.0068,  0.0091]]],


        [[[-0.0105, -0.0035,  0.0013],
          [-0.0045,  0.0060,  0.0076],
          [-0.0019,  0.0062,  0.0067]],

         [[-0.0070,  0.0017,  0.0070],
          [-0.0020,  0.0097,  0.0120],
          [ 0.0001,  0.0094,  0.0105]],

         [[-0.0050,  0.0036,  0.0092],
          [-0.0028,  0.0084,  0.0120],
          [-0.0026,  0.0073,  0.0102]]],


        [[[ 0.0023,  0.0012,  0.0003],
          [ 0.0005, -0.0001, -0.0008],
          [-0.0001, -0.0003, -0.0001]],

         [[ 0.0019,  0.0011,  0.0003],
          [ 0.0007,  0.0003, -0.0005],
          [ 0.0004,  0.0004,  0.0004]],

         [[ 0.0035,  0.0024,  0.0006],
          [ 0.0029,  0.0025,  0.0007],
          [ 0.0021,  0.0024,  0.0016]]],


        ...,


        [[[-0.0099, -0.0069, -0.0035],
          [-0.0130, -0.0098, -0.0058],
          [-0.0089, -0.0052, -0.0048]],

         [[-0.0042, -0.0019,  0.0006],
          [-0.0068, -0.0027,  0.0014],
          [-0.0043,  0.0016,  0.0022]],

         [[-0.0097, -0.0106, -0.0093],
          [-0.0117, -0.0108, -0.0076],
          [-0.0102, -0.0075, -0.0079]]],


        [[[ 0.0017, -0.0022,  0.0007],
          [ 0.0031, -0.0011, -0.0023],
          [ 0.0036,  0.0033,  0.0022]],

         [[ 0.0159,  0.0147,  0.0186],
          [ 0.0172,  0.0151,  0.0146],
          [ 0.0155,  0.0164,  0.0157]],

         [[ 0.0168,  0.0174,  0.0228],
          [ 0.0191,  0.0173,  0.0181],
          [ 0.0186,  0.0182,  0.0178]]],


        [[[-0.0595, -0.0659, -0.0653],
          [-0.0515, -0.0631, -0.0614],
          [-0.0344, -0.0403, -0.0407]],

         [[-0.0540, -0.0492, -0.0476],
          [-0.0483, -0.0503, -0.0454],
          [-0.0330, -0.0346, -0.0314]],

         [[-0.0571, -0.0451, -0.0407],
          [-0.0579, -0.0529, -0.0419],
          [-0.0474, -0.0449, -0.0371]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8365]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 162 | Batch_idx: 0 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (96.00%) (3818/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (5051/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (6293/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (7524/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (8750/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (9987/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (11221/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (12460/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (13687/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (14912/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (96.00%) (16139/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (17343/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (18567/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (19790/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (21013/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (95.00%) (22239/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (23465/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (24692/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (25892/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (27120/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (28341/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (29572/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (30816/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (32039/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (33259/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (34489/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (35711/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (36940/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (38164/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (39394/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (40617/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (41842/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (43068/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (44299/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (45525/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (46751/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (47931/50000)
# TEST : Loss: (0.4424) | Acc: (87.00%) (8792/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1215,  0.0698,  0.0960],
          [ 0.1158, -0.3292, -0.1247],
          [ 0.2816, -0.0748,  0.0071]],

         [[-0.0181, -0.0792,  0.2784],
          [ 0.0161, -0.1267,  0.0476],
          [ 0.0754,  0.0052, -0.0713]],

         [[ 0.0055, -0.0245,  0.2165],
          [ 0.0786, -0.2156,  0.1634],
          [-0.1138,  0.0091, -0.1915]]],


        [[[ 0.0924, -0.0757, -0.0061],
          [ 0.1365,  0.1765,  0.1820],
          [ 0.0156, -0.0594,  0.1165]],

         [[-0.1517, -0.1785,  0.1448],
          [ 0.0349, -0.0442,  0.0997],
          [ 0.1238, -0.1972,  0.1288]],

         [[ 0.0968, -0.0526, -0.0182],
          [ 0.0100,  0.0173, -0.1212],
          [-0.0271, -0.1589, -0.0227]]],


        [[[-0.0732,  0.0856, -0.0434],
          [-0.1262,  0.0626,  0.1126],
          [ 0.1049,  0.1623,  0.0010]],

         [[ 0.0963,  0.1292,  0.1097],
          [ 0.0656, -0.0380, -0.0357],
          [-0.0485, -0.0296,  0.1349]],

         [[-0.0454,  0.1403,  0.0716],
          [-0.0435, -0.0459, -0.0605],
          [-0.0411, -0.0094,  0.0389]]],


        ...,


        [[[ 0.2205, -0.1154, -0.1240],
          [-0.0531, -0.1190, -0.0903],
          [-0.0961,  0.0212,  0.0738]],

         [[ 0.1644,  0.1920, -0.1187],
          [ 0.2018, -0.1405, -0.1114],
          [-0.0565, -0.0751, -0.0258]],

         [[ 0.1764,  0.1002, -0.1101],
          [-0.0619, -0.1530, -0.0761],
          [ 0.0761,  0.1781, -0.1271]]],


        [[[-0.1015,  0.0858, -0.1412],
          [-0.1447, -0.1503,  0.1187],
          [-0.1414,  0.1518, -0.1286]],

         [[ 0.1014, -0.1089,  0.0895],
          [ 0.1448, -0.0006, -0.0772],
          [-0.1303,  0.1201, -0.1863]],

         [[ 0.1352, -0.0116,  0.0058],
          [ 0.1412, -0.1728,  0.0167],
          [-0.0114,  0.0227, -0.1138]]],


        [[[ 0.0646,  0.1811,  0.1181],
          [-0.0884, -0.2158,  0.2186],
          [-0.1760, -0.0416, -0.0789]],

         [[-0.0147, -0.0551,  0.1568],
          [ 0.0050, -0.2375,  0.0667],
          [-0.0112,  0.0670,  0.0243]],

         [[-0.1784, -0.1980,  0.1055],
          [ 0.1486, -0.2136, -0.0038],
          [ 0.0180, -0.0043,  0.3073]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0954, -0.1195, -0.1193],
          [-0.0511, -0.0793, -0.0680],
          [-0.0398, -0.0604, -0.0432]],

         [[-0.1132, -0.1391, -0.1406],
          [-0.0779, -0.1086, -0.0968],
          [-0.0750, -0.0973, -0.0784]],

         [[-0.1034, -0.1219, -0.1135],
          [-0.0829, -0.1060, -0.0887],
          [-0.0890, -0.1099, -0.0839]]],


        [[[ 0.0125,  0.0049, -0.0045],
          [ 0.0077, -0.0003, -0.0096],
          [ 0.0145,  0.0086, -0.0032]],

         [[ 0.0120,  0.0039, -0.0078],
          [ 0.0074, -0.0013, -0.0107],
          [ 0.0134,  0.0072, -0.0035]],

         [[-0.0005, -0.0086, -0.0203],
          [-0.0083, -0.0161, -0.0237],
          [-0.0053, -0.0091, -0.0163]]],


        [[[ 0.0020, -0.0010,  0.0008],
          [-0.0025, -0.0031, -0.0006],
          [-0.0057, -0.0028,  0.0002]],

         [[ 0.0007, -0.0017,  0.0010],
          [-0.0047, -0.0045, -0.0010],
          [-0.0085, -0.0049, -0.0006]],

         [[ 0.0032,  0.0010,  0.0034],
          [-0.0038, -0.0038, -0.0003],
          [-0.0086, -0.0056, -0.0014]]],


        ...,


        [[[ 0.0540,  0.0313,  0.0137],
          [ 0.0708,  0.0528,  0.0404],
          [ 0.0759,  0.0611,  0.0507]],

         [[ 0.0279,  0.0067, -0.0097],
          [ 0.0439,  0.0285,  0.0173],
          [ 0.0502,  0.0394,  0.0285]],

         [[ 0.0170, -0.0023, -0.0207],
          [ 0.0265,  0.0127, -0.0002],
          [ 0.0279,  0.0195,  0.0095]]],


        [[[-0.0076, -0.0107, -0.0189],
          [-0.0085, -0.0046, -0.0080],
          [ 0.0001,  0.0040,  0.0083]],

         [[ 0.0002, -0.0019, -0.0106],
          [-0.0010,  0.0025, -0.0011],
          [ 0.0055,  0.0084,  0.0112]],

         [[ 0.0036,  0.0026, -0.0061],
          [-0.0013,  0.0017, -0.0021],
          [-0.0010,  0.0008,  0.0029]]],


        [[[ 0.0690,  0.0588,  0.0596],
          [ 0.0639,  0.0457,  0.0495],
          [ 0.0547,  0.0154,  0.0042]],

         [[ 0.0274,  0.0051, -0.0058],
          [ 0.0429,  0.0093, -0.0011],
          [ 0.0498, -0.0001, -0.0294]],

         [[-0.0068, -0.0301, -0.0321],
          [ 0.0026, -0.0297, -0.0308],
          [ 0.0153, -0.0259, -0.0422]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8359]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 163 | Batch_idx: 0 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (1349/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (3815/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (5036/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (6267/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (7493/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (8724/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1156) |  Loss2: (0.0000) | Acc: (96.00%) (9959/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (11180/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (12400/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (13633/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (14855/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (16081/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (17309/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (18529/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (19742/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (20973/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (22191/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (23417/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (24639/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (25863/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (27083/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (28300/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (29519/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (30743/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (31967/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (33196/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (34428/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (35666/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (36889/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (38106/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (39341/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (40567/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (41782/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (43009/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (44235/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (45468/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (46684/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (47864/50000)
# TEST : Loss: (0.4123) | Acc: (87.00%) (8785/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1237,  0.0677,  0.0979],
          [ 0.1140, -0.3316, -0.1236],
          [ 0.2803, -0.0766,  0.0065]],

         [[-0.0192, -0.0805,  0.2808],
          [ 0.0162, -0.1270,  0.0503],
          [ 0.0752,  0.0051, -0.0702]],

         [[ 0.0072, -0.0229,  0.2208],
          [ 0.0819, -0.2126,  0.1688],
          [-0.1103,  0.0127, -0.1868]]],


        [[[ 0.0900, -0.0771, -0.0066],
          [ 0.1351,  0.1757,  0.1823],
          [ 0.0149, -0.0597,  0.1170]],

         [[-0.1539, -0.1801,  0.1442],
          [ 0.0329, -0.0457,  0.0995],
          [ 0.1220, -0.1986,  0.1285]],

         [[ 0.0958, -0.0528, -0.0176],
          [ 0.0093,  0.0172, -0.1203],
          [-0.0274, -0.1591, -0.0220]]],


        [[[-0.0732,  0.0856, -0.0433],
          [-0.1261,  0.0626,  0.1124],
          [ 0.1052,  0.1624,  0.0009]],

         [[ 0.0962,  0.1291,  0.1095],
          [ 0.0655, -0.0380, -0.0360],
          [-0.0482, -0.0295,  0.1347]],

         [[-0.0459,  0.1398,  0.0711],
          [-0.0440, -0.0464, -0.0612],
          [-0.0413, -0.0098,  0.0384]]],


        ...,


        [[[ 0.2178, -0.1183, -0.1263],
          [-0.0557, -0.1220, -0.0922],
          [-0.0984,  0.0192,  0.0725]],

         [[ 0.1613,  0.1887, -0.1210],
          [ 0.1991, -0.1436, -0.1134],
          [-0.0589, -0.0773, -0.0273]],

         [[ 0.1733,  0.0974, -0.1117],
          [-0.0640, -0.1554, -0.0771],
          [ 0.0742,  0.1766, -0.1273]]],


        [[[-0.0994,  0.0875, -0.1395],
          [-0.1422, -0.1486,  0.1197],
          [-0.1409,  0.1513, -0.1296]],

         [[ 0.1025, -0.1083,  0.0904],
          [ 0.1465,  0.0000, -0.0769],
          [-0.1303,  0.1190, -0.1877]],

         [[ 0.1352, -0.0119,  0.0058],
          [ 0.1422, -0.1728,  0.0163],
          [-0.0116,  0.0215, -0.1153]]],


        [[[ 0.0643,  0.1825,  0.1206],
          [-0.0873, -0.2147,  0.2217],
          [-0.1730, -0.0373, -0.0744]],

         [[-0.0164, -0.0551,  0.1580],
          [ 0.0038, -0.2391,  0.0672],
          [-0.0114,  0.0680,  0.0260]],

         [[-0.1806, -0.1978,  0.1059],
          [ 0.1463, -0.2157, -0.0045],
          [ 0.0167, -0.0045,  0.3072]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.0273, -0.0443, -0.0410],
          [-0.0218, -0.0352, -0.0353],
          [ 0.0089, -0.0058,  0.0075]],

         [[-0.0307, -0.0501, -0.0421],
          [-0.0224, -0.0383, -0.0375],
          [ 0.0117, -0.0046,  0.0046]],

         [[-0.0460, -0.0655, -0.0604],
          [-0.0367, -0.0539, -0.0558],
          [-0.0059, -0.0253, -0.0215]]],


        [[[ 0.0003,  0.0112,  0.0049],
          [-0.0071,  0.0015, -0.0049],
          [ 0.0050,  0.0111,  0.0026]],

         [[-0.0103,  0.0040, -0.0004],
          [-0.0173, -0.0067, -0.0127],
          [-0.0063,  0.0016, -0.0069]],

         [[-0.0270, -0.0144, -0.0167],
          [-0.0341, -0.0241, -0.0287],
          [-0.0262, -0.0187, -0.0264]]],


        [[[ 0.0025,  0.0008, -0.0013],
          [ 0.0037,  0.0022,  0.0017],
          [ 0.0024,  0.0016,  0.0024]],

         [[ 0.0026,  0.0001, -0.0021],
          [ 0.0042,  0.0018,  0.0010],
          [ 0.0038,  0.0019,  0.0026]],

         [[ 0.0003, -0.0015, -0.0024],
          [ 0.0026,  0.0002, -0.0002],
          [ 0.0029,  0.0007,  0.0010]]],


        ...,


        [[[ 0.0249,  0.0124,  0.0053],
          [ 0.0313,  0.0215,  0.0175],
          [ 0.0368,  0.0254,  0.0242]],

         [[ 0.0141,  0.0008, -0.0082],
          [ 0.0207,  0.0121,  0.0075],
          [ 0.0272,  0.0169,  0.0159]],

         [[ 0.0084, -0.0027, -0.0089],
          [ 0.0111,  0.0038,  0.0019],
          [ 0.0170,  0.0067,  0.0056]]],


        [[[-0.0314, -0.0319, -0.0318],
          [-0.0177, -0.0168, -0.0179],
          [-0.0056, -0.0084, -0.0031]],

         [[-0.0283, -0.0287, -0.0257],
          [-0.0170, -0.0156, -0.0149],
          [-0.0037, -0.0063,  0.0002]],

         [[-0.0217, -0.0187, -0.0150],
          [-0.0135, -0.0085, -0.0067],
          [-0.0049, -0.0039,  0.0033]]],


        [[[ 0.0080,  0.0149,  0.0154],
          [ 0.0211,  0.0229,  0.0213],
          [ 0.0554,  0.0464,  0.0383]],

         [[-0.0122, -0.0001,  0.0019],
          [ 0.0028,  0.0089,  0.0074],
          [ 0.0437,  0.0414,  0.0298]],

         [[-0.0211, -0.0095, -0.0060],
          [-0.0148, -0.0032, -0.0014],
          [ 0.0238,  0.0266,  0.0174]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8352]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

Epoch: 164 | Batch_idx: 0 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (6306/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (7543/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (10016/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (11236/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (12468/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (13693/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (14918/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (16150/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (17381/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (18633/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (19873/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (21112/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (22337/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (23573/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (24800/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (26031/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (27253/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (28476/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (29711/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (30948/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (32177/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (33403/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (34643/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (35859/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (37088/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (38312/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (39536/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (40761/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (41990/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (43209/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (44433/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (45649/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (46885/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (48078/50000)
# TEST : Loss: (0.4331) | Acc: (87.00%) (8775/10000)
conv1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

conv1_weight Parameter containing:
tensor([[[[-0.1220,  0.0701,  0.0965],
          [ 0.1162, -0.3301, -0.1261],
          [ 0.2811, -0.0759,  0.0018]],

         [[-0.0213, -0.0793,  0.2799],
          [ 0.0168, -0.1258,  0.0488],
          [ 0.0756,  0.0058, -0.0744]],

         [[ 0.0021, -0.0240,  0.2192],
          [ 0.0788, -0.2146,  0.1656],
          [-0.1134,  0.0093, -0.1937]]],


        [[[ 0.0896, -0.0782, -0.0078],
          [ 0.1344,  0.1750,  0.1820],
          [ 0.0143, -0.0611,  0.1161]],

         [[-0.1528, -0.1799,  0.1441],
          [ 0.0332, -0.0455,  0.1002],
          [ 0.1220, -0.1992,  0.1287]],

         [[ 0.0977, -0.0524, -0.0179],
          [ 0.0098,  0.0172, -0.1202],
          [-0.0272, -0.1597, -0.0223]]],


        [[[-0.0732,  0.0856, -0.0433],
          [-0.1261,  0.0627,  0.1125],
          [ 0.1050,  0.1624,  0.0008]],

         [[ 0.0963,  0.1292,  0.1096],
          [ 0.0656, -0.0378, -0.0357],
          [-0.0481, -0.0293,  0.1347]],

         [[-0.0455,  0.1400,  0.0712],
          [-0.0436, -0.0460, -0.0608],
          [-0.0410, -0.0094,  0.0386]]],


        ...,


        [[[ 0.2187, -0.1178, -0.1255],
          [-0.0541, -0.1216, -0.0921],
          [-0.0960,  0.0206,  0.0739]],

         [[ 0.1622,  0.1893, -0.1201],
          [ 0.2006, -0.1429, -0.1132],
          [-0.0567, -0.0756, -0.0258]],

         [[ 0.1749,  0.0982, -0.1107],
          [-0.0620, -0.1546, -0.0767],
          [ 0.0761,  0.1779, -0.1259]]],


        [[[-0.1005,  0.0869, -0.1388],
          [-0.1432, -0.1498,  0.1190],
          [-0.1400,  0.1515, -0.1294]],

         [[ 0.1012, -0.1086,  0.0913],
          [ 0.1451, -0.0014, -0.0776],
          [-0.1299,  0.1189, -0.1879]],

         [[ 0.1345, -0.0118,  0.0069],
          [ 0.1415, -0.1736,  0.0159],
          [-0.0111,  0.0212, -0.1158]]],


        [[[ 0.0646,  0.1840,  0.1223],
          [-0.0884, -0.2155,  0.2228],
          [-0.1730, -0.0368, -0.0722]],

         [[-0.0167, -0.0542,  0.1586],
          [ 0.0020, -0.2403,  0.0673],
          [-0.0121,  0.0679,  0.0258]],

         [[-0.1782, -0.1947,  0.1078],
          [ 0.1476, -0.2141, -0.0023],
          [ 0.0191, -0.0011,  0.3100]]]], device='cuda:0', requires_grad=True)

conv1_weight_grad tensor([[[[-0.1007, -0.1249, -0.1186],
          [-0.1315, -0.1356, -0.1217],
          [-0.1135, -0.1027, -0.0741]],

         [[-0.0593, -0.0751, -0.0742],
          [-0.0992, -0.0953, -0.0850],
          [-0.0867, -0.0736, -0.0450]],

         [[-0.0204, -0.0332, -0.0397],
          [-0.0640, -0.0528, -0.0492],
          [-0.0535, -0.0372, -0.0119]]],


        [[[ 0.0088,  0.0068,  0.0101],
          [ 0.0058,  0.0023,  0.0057],
          [-0.0011, -0.0047, -0.0009]],

         [[ 0.0172,  0.0141,  0.0187],
          [ 0.0150,  0.0103,  0.0141],
          [ 0.0093,  0.0053,  0.0088]],

         [[ 0.0150,  0.0122,  0.0171],
          [ 0.0146,  0.0109,  0.0153],
          [ 0.0100,  0.0076,  0.0119]]],


        [[[ 0.0004, -0.0008,  0.0013],
          [ 0.0037,  0.0020,  0.0035],
          [ 0.0026,  0.0017,  0.0038]],

         [[-0.0004, -0.0018, -0.0001],
          [ 0.0032,  0.0012,  0.0025],
          [ 0.0020,  0.0011,  0.0031]],

         [[-0.0001, -0.0017, -0.0003],
          [ 0.0028,  0.0010,  0.0021],
          [ 0.0017,  0.0009,  0.0027]]],


        ...,


        [[[ 0.0198,  0.0040,  0.0010],
          [ 0.0316,  0.0215,  0.0144],
          [ 0.0467,  0.0309,  0.0224]],

         [[ 0.0245,  0.0078,  0.0039],
          [ 0.0387,  0.0281,  0.0187],
          [ 0.0583,  0.0424,  0.0302]],

         [[ 0.0112, -0.0054, -0.0075],
          [ 0.0287,  0.0176,  0.0090],
          [ 0.0526,  0.0377,  0.0239]]],


        [[[-0.0301, -0.0294, -0.0247],
          [-0.0264, -0.0246, -0.0204],
          [-0.0065, -0.0102, -0.0059]],

         [[-0.0136, -0.0105, -0.0053],
          [-0.0122, -0.0083, -0.0028],
          [ 0.0097,  0.0059,  0.0116]],

         [[-0.0178, -0.0148, -0.0094],
          [-0.0159, -0.0092, -0.0046],
          [ 0.0033,  0.0044,  0.0095]]],


        [[[ 0.0956,  0.0719,  0.0658],
          [ 0.0703,  0.0515,  0.0696],
          [ 0.0513,  0.0350,  0.0556]],

         [[ 0.0413,  0.0244,  0.0146],
          [ 0.0100,  0.0016,  0.0162],
          [-0.0171, -0.0252, -0.0028]],

         [[ 0.0268,  0.0112,  0.0040],
          [-0.0064, -0.0102,  0.0033],
          [-0.0436, -0.0425, -0.0155]]]], device='cuda:0')

percentage_weight Parameter containing:
tensor([[1.8345]], device='cuda:0')

percentage_weight_grad tensor([[0.0002]], device='cuda:0')

False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
2 hours 38 mins 45 secs for training

Linear(in_features=1, out_features=1, bias=False) 0.8626230172149623
Linear(in_features=1, out_features=1, bias=False) 0.857600638569636
Linear(in_features=1, out_features=1, bias=False) 0.6130708845693615
Linear(in_features=1, out_features=1, bias=False) 0.9278481890342967
Linear(in_features=1, out_features=1, bias=False) 0.8853119322477961
Linear(in_features=1, out_features=1, bias=False) 0.96660225624035
Linear(in_features=1, out_features=1, bias=False) 0.9647398212812033
Linear(in_features=1, out_features=1, bias=False) 0.9808225622288337