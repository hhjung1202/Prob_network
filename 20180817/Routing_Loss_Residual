Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_models/checkpoint_040.pth.tar'
Epoch: 41 | Batch_idx: 0 |  Loss: (0.7364) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.6115) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.5860) |  Loss2: (0.0000) | Acc: (79.00%) (2146/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.5795) |  Loss2: (0.0000) | Acc: (79.00%) (3166/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.5897) |  Loss2: (0.0000) | Acc: (79.00%) (4172/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.5797) |  Loss2: (0.0000) | Acc: (79.00%) (5214/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.5753) |  Loss2: (0.0000) | Acc: (80.00%) (6254/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (7296/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.5681) |  Loss2: (0.0000) | Acc: (80.00%) (8331/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.5678) |  Loss2: (0.0000) | Acc: (80.00%) (9371/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (80.00%) (10396/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.5707) |  Loss2: (0.0000) | Acc: (80.00%) (11418/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.5701) |  Loss2: (0.0000) | Acc: (80.00%) (12456/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.5712) |  Loss2: (0.0000) | Acc: (80.00%) (13489/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.5711) |  Loss2: (0.0000) | Acc: (80.00%) (14503/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.5724) |  Loss2: (0.0000) | Acc: (80.00%) (15527/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.5731) |  Loss2: (0.0000) | Acc: (80.00%) (16550/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (80.00%) (17586/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.5702) |  Loss2: (0.0000) | Acc: (80.00%) (18616/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.5699) |  Loss2: (0.0000) | Acc: (80.00%) (19631/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.5719) |  Loss2: (0.0000) | Acc: (80.00%) (20638/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.5722) |  Loss2: (0.0000) | Acc: (80.00%) (21672/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.5724) |  Loss2: (0.0000) | Acc: (80.00%) (22697/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (23719/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (24757/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.5717) |  Loss2: (0.0000) | Acc: (80.00%) (25772/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.5727) |  Loss2: (0.0000) | Acc: (80.00%) (26783/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (80.00%) (27806/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.5748) |  Loss2: (0.0000) | Acc: (80.00%) (28809/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.5745) |  Loss2: (0.0000) | Acc: (80.00%) (29838/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.5731) |  Loss2: (0.0000) | Acc: (80.00%) (30880/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.5726) |  Loss2: (0.0000) | Acc: (80.00%) (31913/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.5725) |  Loss2: (0.0000) | Acc: (80.00%) (32941/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.5740) |  Loss2: (0.0000) | Acc: (80.00%) (33951/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.5739) |  Loss2: (0.0000) | Acc: (80.00%) (34977/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.5724) |  Loss2: (0.0000) | Acc: (80.00%) (36030/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.5718) |  Loss2: (0.0000) | Acc: (80.00%) (37080/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.5721) |  Loss2: (0.0000) | Acc: (80.00%) (38110/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.5706) |  Loss2: (0.0000) | Acc: (80.00%) (39150/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.5706) |  Loss2: (0.0000) | Acc: (80.00%) (40155/50000)
# TEST : Loss: (0.6791) | Acc: (77.00%) (7738/10000)
percent tensor([0.6099], device='cuda:0')
percent tensor([0.6062], device='cuda:0')
percent tensor([0.6482], device='cuda:0')
percent tensor([0.6124], device='cuda:0')
percent tensor([0.6206], device='cuda:0')
percent tensor([0.6610], device='cuda:0')
percent tensor([0.6975], device='cuda:0')
percent tensor([0.2567], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5643) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (1140/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.5856) |  Loss2: (0.0000) | Acc: (80.00%) (2154/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.5672) |  Loss2: (0.0000) | Acc: (80.00%) (3202/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.5613) |  Loss2: (0.0000) | Acc: (80.00%) (4229/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.5604) |  Loss2: (0.0000) | Acc: (80.00%) (5264/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.5564) |  Loss2: (0.0000) | Acc: (80.00%) (6313/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.5557) |  Loss2: (0.0000) | Acc: (80.00%) (7349/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.5567) |  Loss2: (0.0000) | Acc: (80.00%) (8378/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.5542) |  Loss2: (0.0000) | Acc: (80.00%) (9429/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.5557) |  Loss2: (0.0000) | Acc: (80.00%) (10467/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (11486/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.5584) |  Loss2: (0.0000) | Acc: (80.00%) (12527/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.5567) |  Loss2: (0.0000) | Acc: (80.00%) (13561/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (14591/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (81.00%) (15657/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.5547) |  Loss2: (0.0000) | Acc: (80.00%) (16681/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.5520) |  Loss2: (0.0000) | Acc: (80.00%) (17729/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (80.00%) (18753/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (80.00%) (19802/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.5521) |  Loss2: (0.0000) | Acc: (81.00%) (20854/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (81.00%) (21893/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.5529) |  Loss2: (0.0000) | Acc: (81.00%) (22930/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (81.00%) (23964/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.5544) |  Loss2: (0.0000) | Acc: (81.00%) (24989/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (26009/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.5545) |  Loss2: (0.0000) | Acc: (80.00%) (27056/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.5542) |  Loss2: (0.0000) | Acc: (80.00%) (28078/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.5542) |  Loss2: (0.0000) | Acc: (80.00%) (29121/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.5527) |  Loss2: (0.0000) | Acc: (80.00%) (30167/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.5516) |  Loss2: (0.0000) | Acc: (81.00%) (31224/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.5518) |  Loss2: (0.0000) | Acc: (81.00%) (32246/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.5502) |  Loss2: (0.0000) | Acc: (81.00%) (33319/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.5500) |  Loss2: (0.0000) | Acc: (81.00%) (34351/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.5509) |  Loss2: (0.0000) | Acc: (81.00%) (35380/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.5509) |  Loss2: (0.0000) | Acc: (81.00%) (36420/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (81.00%) (37471/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.5503) |  Loss2: (0.0000) | Acc: (81.00%) (38504/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.5502) |  Loss2: (0.0000) | Acc: (81.00%) (39541/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (81.00%) (40543/50000)
# TEST : Loss: (0.6361) | Acc: (78.00%) (7844/10000)
percent tensor([0.6099], device='cuda:0')
percent tensor([0.6062], device='cuda:0')
percent tensor([0.6482], device='cuda:0')
percent tensor([0.6124], device='cuda:0')
percent tensor([0.6206], device='cuda:0')
percent tensor([0.6610], device='cuda:0')
percent tensor([0.6975], device='cuda:0')
percent tensor([0.2567], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4508) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.5247) |  Loss2: (0.0000) | Acc: (80.00%) (1134/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.5416) |  Loss2: (0.0000) | Acc: (80.00%) (2167/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (3216/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.5303) |  Loss2: (0.0000) | Acc: (81.00%) (4275/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.5303) |  Loss2: (0.0000) | Acc: (81.00%) (5335/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.5236) |  Loss2: (0.0000) | Acc: (82.00%) (6414/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.5297) |  Loss2: (0.0000) | Acc: (81.00%) (7432/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.5284) |  Loss2: (0.0000) | Acc: (81.00%) (8486/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (81.00%) (9546/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.5270) |  Loss2: (0.0000) | Acc: (81.00%) (10590/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (82.00%) (11671/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.5295) |  Loss2: (0.0000) | Acc: (81.00%) (12690/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.5283) |  Loss2: (0.0000) | Acc: (81.00%) (13731/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (14770/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.5281) |  Loss2: (0.0000) | Acc: (81.00%) (15822/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.5315) |  Loss2: (0.0000) | Acc: (81.00%) (16848/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.5335) |  Loss2: (0.0000) | Acc: (81.00%) (17890/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.5340) |  Loss2: (0.0000) | Acc: (81.00%) (18923/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.5340) |  Loss2: (0.0000) | Acc: (81.00%) (19977/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.5346) |  Loss2: (0.0000) | Acc: (81.00%) (21007/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.5337) |  Loss2: (0.0000) | Acc: (81.00%) (22059/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (23096/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.5340) |  Loss2: (0.0000) | Acc: (81.00%) (24127/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (25175/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (26230/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (27254/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.5351) |  Loss2: (0.0000) | Acc: (81.00%) (28298/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (29347/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.5338) |  Loss2: (0.0000) | Acc: (81.00%) (30394/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.5345) |  Loss2: (0.0000) | Acc: (81.00%) (31413/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.5338) |  Loss2: (0.0000) | Acc: (81.00%) (32471/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.5349) |  Loss2: (0.0000) | Acc: (81.00%) (33501/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (34530/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.5351) |  Loss2: (0.0000) | Acc: (81.00%) (35574/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (36625/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.5349) |  Loss2: (0.0000) | Acc: (81.00%) (37660/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (38703/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.5364) |  Loss2: (0.0000) | Acc: (81.00%) (39729/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.5360) |  Loss2: (0.0000) | Acc: (81.00%) (40740/50000)
# TEST : Loss: (0.7259) | Acc: (75.00%) (7593/10000)
percent tensor([0.6099], device='cuda:0')
percent tensor([0.6062], device='cuda:0')
percent tensor([0.6482], device='cuda:0')
percent tensor([0.6124], device='cuda:0')
percent tensor([0.6206], device='cuda:0')
percent tensor([0.6610], device='cuda:0')
percent tensor([0.6975], device='cuda:0')
percent tensor([0.2567], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.5246) |  Loss2: (0.0000) | Acc: (81.00%) (1154/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (2206/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (3265/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (4327/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.5134) |  Loss2: (0.0000) | Acc: (82.00%) (5376/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.5169) |  Loss2: (0.0000) | Acc: (82.00%) (6424/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (7489/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (8534/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.5168) |  Loss2: (0.0000) | Acc: (82.00%) (9570/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (10619/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (11692/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.5125) |  Loss2: (0.0000) | Acc: (82.00%) (12774/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.5141) |  Loss2: (0.0000) | Acc: (82.00%) (13808/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.5154) |  Loss2: (0.0000) | Acc: (82.00%) (14857/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (15895/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.5155) |  Loss2: (0.0000) | Acc: (82.00%) (16962/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (82.00%) (18014/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.5129) |  Loss2: (0.0000) | Acc: (82.00%) (19092/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.5136) |  Loss2: (0.0000) | Acc: (82.00%) (20142/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (21172/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (22216/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (23247/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (24295/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (25354/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (82.00%) (26391/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (27470/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (28537/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (29590/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (30628/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (31664/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (82.00%) (32700/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (82.00%) (33754/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (34799/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (35856/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (36920/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (82.00%) (37984/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (82.00%) (39036/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (40108/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (41130/50000)
# TEST : Loss: (0.6034) | Acc: (79.00%) (7990/10000)
percent tensor([0.6099], device='cuda:0')
percent tensor([0.6062], device='cuda:0')
percent tensor([0.6482], device='cuda:0')
percent tensor([0.6124], device='cuda:0')
percent tensor([0.6206], device='cuda:0')
percent tensor([0.6610], device='cuda:0')
percent tensor([0.6975], device='cuda:0')
percent tensor([0.2567], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.9349) |  Loss2: (0.5091) | Acc: (89.00%) (114/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (1.0584) |  Loss2: (0.5090) | Acc: (80.00%) (1134/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (1.0983) |  Loss2: (0.5088) | Acc: (79.00%) (2128/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (1.1387) |  Loss2: (0.5086) | Acc: (78.00%) (3103/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (1.1313) |  Loss2: (0.5085) | Acc: (78.00%) (4114/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (1.1239) |  Loss2: (0.5083) | Acc: (78.00%) (5140/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (1.1266) |  Loss2: (0.5081) | Acc: (78.00%) (6136/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (1.1298) |  Loss2: (0.5079) | Acc: (78.00%) (7129/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (1.1318) |  Loss2: (0.5077) | Acc: (78.00%) (8141/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (1.1259) |  Loss2: (0.5075) | Acc: (78.00%) (9188/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (1.1219) |  Loss2: (0.5073) | Acc: (78.00%) (10199/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (1.1167) |  Loss2: (0.5071) | Acc: (79.00%) (11228/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (1.1097) |  Loss2: (0.5069) | Acc: (79.00%) (12271/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (1.1069) |  Loss2: (0.5067) | Acc: (79.00%) (13313/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (1.1038) |  Loss2: (0.5065) | Acc: (79.00%) (14346/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (1.1001) |  Loss2: (0.5063) | Acc: (79.00%) (15387/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (1.0984) |  Loss2: (0.5061) | Acc: (79.00%) (16406/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (1.0945) |  Loss2: (0.5059) | Acc: (79.00%) (17456/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (1.0937) |  Loss2: (0.5057) | Acc: (79.00%) (18479/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (1.0907) |  Loss2: (0.5056) | Acc: (79.00%) (19521/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (1.0908) |  Loss2: (0.5054) | Acc: (79.00%) (20530/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (1.0882) |  Loss2: (0.5052) | Acc: (79.00%) (21580/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (1.0868) |  Loss2: (0.5050) | Acc: (79.00%) (22606/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (1.0831) |  Loss2: (0.5049) | Acc: (80.00%) (23667/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (1.0820) |  Loss2: (0.5047) | Acc: (80.00%) (24692/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (1.0815) |  Loss2: (0.5046) | Acc: (80.00%) (25716/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (1.0786) |  Loss2: (0.5044) | Acc: (80.00%) (26765/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (1.0784) |  Loss2: (0.5043) | Acc: (80.00%) (27804/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (1.0781) |  Loss2: (0.5041) | Acc: (80.00%) (28829/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (1.0777) |  Loss2: (0.5040) | Acc: (80.00%) (29870/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (1.0777) |  Loss2: (0.5038) | Acc: (80.00%) (30881/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (1.0780) |  Loss2: (0.5037) | Acc: (80.00%) (31916/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (1.0786) |  Loss2: (0.5035) | Acc: (80.00%) (32929/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (1.0778) |  Loss2: (0.5034) | Acc: (80.00%) (33955/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (1.0774) |  Loss2: (0.5032) | Acc: (80.00%) (34982/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (1.0759) |  Loss2: (0.5031) | Acc: (80.00%) (36029/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (1.0748) |  Loss2: (0.5029) | Acc: (80.00%) (37067/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (1.0739) |  Loss2: (0.5028) | Acc: (80.00%) (38118/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (1.0730) |  Loss2: (0.5026) | Acc: (80.00%) (39168/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (1.0716) |  Loss2: (0.5025) | Acc: (80.00%) (40171/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_045.pth.tar'
# TEST : Loss: (0.5905) | Acc: (79.00%) (7950/10000)
percent tensor([0.6273], device='cuda:0')
percent tensor([0.6243], device='cuda:0')
percent tensor([0.6567], device='cuda:0')
percent tensor([0.6044], device='cuda:0')
percent tensor([0.6458], device='cuda:0')
percent tensor([0.6819], device='cuda:0')
percent tensor([0.7093], device='cuda:0')
percent tensor([0.2570], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (1.0258) |  Loss2: (0.4969) | Acc: (82.00%) (105/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (1.0314) |  Loss2: (0.4969) | Acc: (81.00%) (1149/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (1.0321) |  Loss2: (0.4969) | Acc: (81.00%) (2195/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (1.0208) |  Loss2: (0.4968) | Acc: (81.00%) (3252/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (1.0290) |  Loss2: (0.4967) | Acc: (81.00%) (4277/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (1.0306) |  Loss2: (0.4967) | Acc: (81.00%) (5322/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (1.0406) |  Loss2: (0.4966) | Acc: (81.00%) (6344/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (1.0438) |  Loss2: (0.4965) | Acc: (81.00%) (7366/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (1.0465) |  Loss2: (0.4965) | Acc: (81.00%) (8401/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (1.0442) |  Loss2: (0.4964) | Acc: (81.00%) (9435/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (1.0419) |  Loss2: (0.4963) | Acc: (81.00%) (10476/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (1.0372) |  Loss2: (0.4962) | Acc: (81.00%) (11540/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (1.0358) |  Loss2: (0.4961) | Acc: (81.00%) (12593/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (1.0329) |  Loss2: (0.4961) | Acc: (81.00%) (13652/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (1.0342) |  Loss2: (0.4960) | Acc: (81.00%) (14675/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (1.0314) |  Loss2: (0.4959) | Acc: (81.00%) (15746/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (1.0304) |  Loss2: (0.4958) | Acc: (81.00%) (16792/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (1.0297) |  Loss2: (0.4958) | Acc: (81.00%) (17841/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (1.0284) |  Loss2: (0.4957) | Acc: (81.00%) (18886/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (1.0266) |  Loss2: (0.4956) | Acc: (81.00%) (19943/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (1.0264) |  Loss2: (0.4956) | Acc: (81.00%) (20988/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (1.0252) |  Loss2: (0.4955) | Acc: (81.00%) (22052/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (1.0264) |  Loss2: (0.4954) | Acc: (81.00%) (23087/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (1.0258) |  Loss2: (0.4954) | Acc: (81.00%) (24142/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (1.0258) |  Loss2: (0.4953) | Acc: (81.00%) (25168/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (1.0259) |  Loss2: (0.4953) | Acc: (81.00%) (26207/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (1.0254) |  Loss2: (0.4952) | Acc: (81.00%) (27263/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (1.0254) |  Loss2: (0.4951) | Acc: (81.00%) (28303/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (1.0246) |  Loss2: (0.4951) | Acc: (81.00%) (29374/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (1.0250) |  Loss2: (0.4950) | Acc: (81.00%) (30414/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (1.0249) |  Loss2: (0.4949) | Acc: (81.00%) (31475/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (1.0254) |  Loss2: (0.4949) | Acc: (81.00%) (32508/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (1.0235) |  Loss2: (0.4948) | Acc: (81.00%) (33592/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (1.0230) |  Loss2: (0.4947) | Acc: (81.00%) (34643/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (1.0227) |  Loss2: (0.4947) | Acc: (81.00%) (35688/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (1.0243) |  Loss2: (0.4946) | Acc: (81.00%) (36713/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (1.0253) |  Loss2: (0.4945) | Acc: (81.00%) (37732/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (1.0257) |  Loss2: (0.4945) | Acc: (81.00%) (38773/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (1.0251) |  Loss2: (0.4944) | Acc: (81.00%) (39825/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (1.0257) |  Loss2: (0.4943) | Acc: (81.00%) (40803/50000)
# TEST : Loss: (0.5719) | Acc: (80.00%) (8019/10000)
percent tensor([0.6329], device='cuda:0')
percent tensor([0.6284], device='cuda:0')
percent tensor([0.6621], device='cuda:0')
percent tensor([0.6089], device='cuda:0')
percent tensor([0.6496], device='cuda:0')
percent tensor([0.6922], device='cuda:0')
percent tensor([0.7167], device='cuda:0')
percent tensor([0.2521], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.9477) |  Loss2: (0.4917) | Acc: (85.00%) (110/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (1.0268) |  Loss2: (0.4917) | Acc: (81.00%) (1147/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (1.0166) |  Loss2: (0.4916) | Acc: (81.00%) (2202/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (1.0148) |  Loss2: (0.4915) | Acc: (81.00%) (3247/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (1.0219) |  Loss2: (0.4914) | Acc: (81.00%) (4291/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (1.0148) |  Loss2: (0.4913) | Acc: (82.00%) (5361/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (1.0172) |  Loss2: (0.4912) | Acc: (81.00%) (6396/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (1.0152) |  Loss2: (0.4912) | Acc: (82.00%) (7456/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (1.0139) |  Loss2: (0.4911) | Acc: (82.00%) (8507/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (1.0111) |  Loss2: (0.4911) | Acc: (82.00%) (9573/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (1.0105) |  Loss2: (0.4910) | Acc: (82.00%) (10629/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (1.0134) |  Loss2: (0.4910) | Acc: (82.00%) (11659/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (1.0134) |  Loss2: (0.4909) | Acc: (82.00%) (12715/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (1.0145) |  Loss2: (0.4909) | Acc: (82.00%) (13769/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (1.0144) |  Loss2: (0.4908) | Acc: (82.00%) (14830/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (1.0124) |  Loss2: (0.4908) | Acc: (82.00%) (15907/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (1.0118) |  Loss2: (0.4908) | Acc: (82.00%) (16954/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (1.0121) |  Loss2: (0.4908) | Acc: (82.00%) (18006/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (1.0128) |  Loss2: (0.4907) | Acc: (82.00%) (19058/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (1.0132) |  Loss2: (0.4907) | Acc: (82.00%) (20103/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (1.0146) |  Loss2: (0.4907) | Acc: (82.00%) (21143/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (1.0169) |  Loss2: (0.4907) | Acc: (82.00%) (22164/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (1.0162) |  Loss2: (0.4907) | Acc: (82.00%) (23229/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (1.0158) |  Loss2: (0.4906) | Acc: (82.00%) (24285/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (1.0153) |  Loss2: (0.4906) | Acc: (82.00%) (25339/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (1.0186) |  Loss2: (0.4906) | Acc: (82.00%) (26347/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (1.0171) |  Loss2: (0.4906) | Acc: (82.00%) (27400/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (1.0178) |  Loss2: (0.4906) | Acc: (81.00%) (28434/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (1.0171) |  Loss2: (0.4905) | Acc: (81.00%) (29492/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (1.0173) |  Loss2: (0.4905) | Acc: (81.00%) (30532/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (1.0173) |  Loss2: (0.4905) | Acc: (81.00%) (31588/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (1.0172) |  Loss2: (0.4905) | Acc: (81.00%) (32639/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (1.0159) |  Loss2: (0.4905) | Acc: (82.00%) (33705/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (1.0153) |  Loss2: (0.4905) | Acc: (82.00%) (34767/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (1.0166) |  Loss2: (0.4904) | Acc: (82.00%) (35805/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (1.0150) |  Loss2: (0.4904) | Acc: (82.00%) (36884/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (1.0149) |  Loss2: (0.4904) | Acc: (82.00%) (37945/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (1.0140) |  Loss2: (0.4904) | Acc: (82.00%) (39024/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (1.0133) |  Loss2: (0.4903) | Acc: (82.00%) (40095/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (1.0124) |  Loss2: (0.4903) | Acc: (82.00%) (41112/50000)
# TEST : Loss: (0.5627) | Acc: (80.00%) (8054/10000)
percent tensor([0.6355], device='cuda:0')
percent tensor([0.6299], device='cuda:0')
percent tensor([0.6637], device='cuda:0')
percent tensor([0.6145], device='cuda:0')
percent tensor([0.6507], device='cuda:0')
percent tensor([0.6964], device='cuda:0')
percent tensor([0.7215], device='cuda:0')
percent tensor([0.2458], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (1.0414) |  Loss2: (0.4892) | Acc: (81.00%) (104/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (1.0069) |  Loss2: (0.4892) | Acc: (82.00%) (1157/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (1.0001) |  Loss2: (0.4891) | Acc: (82.00%) (2223/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.9966) |  Loss2: (0.4891) | Acc: (83.00%) (3294/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.9998) |  Loss2: (0.4891) | Acc: (82.00%) (4341/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (1.0069) |  Loss2: (0.4891) | Acc: (82.00%) (5386/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (1.0051) |  Loss2: (0.4891) | Acc: (82.00%) (6440/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (1.0056) |  Loss2: (0.4891) | Acc: (82.00%) (7496/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (1.0082) |  Loss2: (0.4891) | Acc: (82.00%) (8538/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (1.0077) |  Loss2: (0.4891) | Acc: (82.00%) (9579/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (1.0123) |  Loss2: (0.4891) | Acc: (81.00%) (10593/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (1.0135) |  Loss2: (0.4891) | Acc: (82.00%) (11652/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (1.0107) |  Loss2: (0.4891) | Acc: (82.00%) (12727/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (1.0099) |  Loss2: (0.4891) | Acc: (82.00%) (13792/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (1.0089) |  Loss2: (0.4891) | Acc: (82.00%) (14850/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (1.0102) |  Loss2: (0.4891) | Acc: (82.00%) (15885/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (1.0079) |  Loss2: (0.4891) | Acc: (82.00%) (16956/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (1.0091) |  Loss2: (0.4891) | Acc: (82.00%) (17994/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (1.0088) |  Loss2: (0.4891) | Acc: (82.00%) (19047/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (1.0093) |  Loss2: (0.4891) | Acc: (82.00%) (20079/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (1.0091) |  Loss2: (0.4891) | Acc: (82.00%) (21136/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (1.0102) |  Loss2: (0.4891) | Acc: (82.00%) (22180/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (1.0110) |  Loss2: (0.4891) | Acc: (82.00%) (23218/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (1.0103) |  Loss2: (0.4890) | Acc: (82.00%) (24272/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (1.0103) |  Loss2: (0.4890) | Acc: (82.00%) (25320/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (1.0098) |  Loss2: (0.4890) | Acc: (82.00%) (26377/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (1.0106) |  Loss2: (0.4890) | Acc: (82.00%) (27416/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (1.0096) |  Loss2: (0.4890) | Acc: (82.00%) (28477/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (1.0101) |  Loss2: (0.4890) | Acc: (82.00%) (29538/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (1.0093) |  Loss2: (0.4890) | Acc: (82.00%) (30599/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (1.0091) |  Loss2: (0.4890) | Acc: (82.00%) (31648/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (1.0087) |  Loss2: (0.4890) | Acc: (82.00%) (32710/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (1.0081) |  Loss2: (0.4890) | Acc: (82.00%) (33766/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (1.0068) |  Loss2: (0.4890) | Acc: (82.00%) (34839/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (1.0066) |  Loss2: (0.4889) | Acc: (82.00%) (35904/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (1.0066) |  Loss2: (0.4889) | Acc: (82.00%) (36950/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (1.0071) |  Loss2: (0.4889) | Acc: (82.00%) (38001/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (1.0061) |  Loss2: (0.4889) | Acc: (82.00%) (39073/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (1.0062) |  Loss2: (0.4889) | Acc: (82.00%) (40116/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (1.0050) |  Loss2: (0.4889) | Acc: (82.00%) (41148/50000)
# TEST : Loss: (0.5596) | Acc: (80.00%) (8069/10000)
percent tensor([0.6360], device='cuda:0')
percent tensor([0.6298], device='cuda:0')
percent tensor([0.6675], device='cuda:0')
percent tensor([0.6142], device='cuda:0')
percent tensor([0.6505], device='cuda:0')
percent tensor([0.6980], device='cuda:0')
percent tensor([0.7249], device='cuda:0')
percent tensor([0.2386], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.9436) |  Loss2: (0.4883) | Acc: (81.00%) (104/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.9936) |  Loss2: (0.4883) | Acc: (81.00%) (1150/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (1.0095) |  Loss2: (0.4883) | Acc: (82.00%) (2208/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (1.0206) |  Loss2: (0.4883) | Acc: (82.00%) (3255/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (1.0160) |  Loss2: (0.4882) | Acc: (82.00%) (4317/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (1.0084) |  Loss2: (0.4882) | Acc: (82.00%) (5407/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (1.0022) |  Loss2: (0.4882) | Acc: (82.00%) (6476/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (1.0030) |  Loss2: (0.4882) | Acc: (82.00%) (7517/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (1.0020) |  Loss2: (0.4882) | Acc: (82.00%) (8582/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (1.0005) |  Loss2: (0.4882) | Acc: (82.00%) (9632/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.9997) |  Loss2: (0.4882) | Acc: (82.00%) (10694/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (1.0042) |  Loss2: (0.4882) | Acc: (82.00%) (11716/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (1.0002) |  Loss2: (0.4882) | Acc: (82.00%) (12810/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (1.0014) |  Loss2: (0.4882) | Acc: (82.00%) (13846/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (1.0020) |  Loss2: (0.4882) | Acc: (82.00%) (14898/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (1.0050) |  Loss2: (0.4882) | Acc: (82.00%) (15927/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (1.0041) |  Loss2: (0.4882) | Acc: (82.00%) (16991/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (1.0029) |  Loss2: (0.4882) | Acc: (82.00%) (18048/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (1.0028) |  Loss2: (0.4882) | Acc: (82.00%) (19097/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (1.0018) |  Loss2: (0.4882) | Acc: (82.00%) (20163/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (1.0006) |  Loss2: (0.4882) | Acc: (82.00%) (21225/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.9998) |  Loss2: (0.4881) | Acc: (82.00%) (22290/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.9994) |  Loss2: (0.4881) | Acc: (82.00%) (23351/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (1.0005) |  Loss2: (0.4881) | Acc: (82.00%) (24393/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (1.0011) |  Loss2: (0.4881) | Acc: (82.00%) (25440/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (1.0005) |  Loss2: (0.4881) | Acc: (82.00%) (26502/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (1.0007) |  Loss2: (0.4880) | Acc: (82.00%) (27567/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (1.0011) |  Loss2: (0.4880) | Acc: (82.00%) (28617/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (1.0002) |  Loss2: (0.4880) | Acc: (82.00%) (29684/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (1.0017) |  Loss2: (0.4879) | Acc: (82.00%) (30729/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (1.0015) |  Loss2: (0.4879) | Acc: (82.00%) (31772/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (1.0010) |  Loss2: (0.4879) | Acc: (82.00%) (32829/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.9997) |  Loss2: (0.4879) | Acc: (82.00%) (33914/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (1.0003) |  Loss2: (0.4879) | Acc: (82.00%) (34959/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (1.0002) |  Loss2: (0.4878) | Acc: (82.00%) (36020/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.9993) |  Loss2: (0.4878) | Acc: (82.00%) (37071/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.9985) |  Loss2: (0.4878) | Acc: (82.00%) (38140/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.9997) |  Loss2: (0.4878) | Acc: (82.00%) (39154/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.9994) |  Loss2: (0.4878) | Acc: (82.00%) (40217/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.9991) |  Loss2: (0.4877) | Acc: (82.00%) (41250/50000)
# TEST : Loss: (0.5562) | Acc: (80.00%) (8067/10000)
percent tensor([0.6355], device='cuda:0')
percent tensor([0.6296], device='cuda:0')
percent tensor([0.6703], device='cuda:0')
percent tensor([0.6175], device='cuda:0')
percent tensor([0.6511], device='cuda:0')
percent tensor([0.7009], device='cuda:0')
percent tensor([0.7273], device='cuda:0')
percent tensor([0.2319], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.6027) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.5675) |  Loss2: (0.0000) | Acc: (80.00%) (1139/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.5411) |  Loss2: (0.0000) | Acc: (81.00%) (2197/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.5248) |  Loss2: (0.0000) | Acc: (82.00%) (3261/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (4301/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (82.00%) (5361/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (82.00%) (6416/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.5241) |  Loss2: (0.0000) | Acc: (82.00%) (7463/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.5237) |  Loss2: (0.0000) | Acc: (82.00%) (8507/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (82.00%) (9568/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.5278) |  Loss2: (0.0000) | Acc: (82.00%) (10604/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.5289) |  Loss2: (0.0000) | Acc: (81.00%) (11644/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (82.00%) (12710/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.5252) |  Loss2: (0.0000) | Acc: (82.00%) (13770/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.5251) |  Loss2: (0.0000) | Acc: (82.00%) (14829/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.5247) |  Loss2: (0.0000) | Acc: (82.00%) (15872/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (82.00%) (16957/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.5211) |  Loss2: (0.0000) | Acc: (82.00%) (18012/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.5206) |  Loss2: (0.0000) | Acc: (82.00%) (19069/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (82.00%) (20098/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (21156/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (82.00%) (22203/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (23282/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.5207) |  Loss2: (0.0000) | Acc: (82.00%) (24321/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.5214) |  Loss2: (0.0000) | Acc: (82.00%) (25364/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (82.00%) (26394/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (82.00%) (27442/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (28481/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.5227) |  Loss2: (0.0000) | Acc: (82.00%) (29519/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (82.00%) (30574/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (82.00%) (31630/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (32679/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (82.00%) (33732/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (34802/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (82.00%) (35834/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (82.00%) (36884/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (37938/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (39001/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (40044/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (41066/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_050.pth.tar'
# TEST : Loss: (0.5816) | Acc: (80.00%) (8002/10000)
percent tensor([0.6352], device='cuda:0')
percent tensor([0.6291], device='cuda:0')
percent tensor([0.6699], device='cuda:0')
percent tensor([0.6177], device='cuda:0')
percent tensor([0.6511], device='cuda:0')
percent tensor([0.7007], device='cuda:0')
percent tensor([0.7273], device='cuda:0')
percent tensor([0.2318], device='cuda:0')
Epoch: 51 | Batch_idx: 0 |  Loss: (0.5786) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (1156/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4721) |  Loss2: (0.0000) | Acc: (83.00%) (2254/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (3322/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (4378/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (83.00%) (5430/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4860) |  Loss2: (0.0000) | Acc: (83.00%) (6509/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4896) |  Loss2: (0.0000) | Acc: (83.00%) (7561/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (83.00%) (8615/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4950) |  Loss2: (0.0000) | Acc: (82.00%) (9660/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4968) |  Loss2: (0.0000) | Acc: (82.00%) (10707/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4955) |  Loss2: (0.0000) | Acc: (82.00%) (11776/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4962) |  Loss2: (0.0000) | Acc: (82.00%) (12843/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (13890/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (14955/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4970) |  Loss2: (0.0000) | Acc: (82.00%) (16022/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (17088/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (18171/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (82.00%) (19227/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (82.00%) (20275/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4953) |  Loss2: (0.0000) | Acc: (82.00%) (21351/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (22390/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (23443/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4992) |  Loss2: (0.0000) | Acc: (82.00%) (24509/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4985) |  Loss2: (0.0000) | Acc: (82.00%) (25578/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4996) |  Loss2: (0.0000) | Acc: (82.00%) (26624/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (27652/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (28725/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (29782/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (30821/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (31848/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (32894/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (33919/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (34962/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.5085) |  Loss2: (0.0000) | Acc: (82.00%) (36004/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.5075) |  Loss2: (0.0000) | Acc: (82.00%) (37071/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.5070) |  Loss2: (0.0000) | Acc: (82.00%) (38147/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.5057) |  Loss2: (0.0000) | Acc: (82.00%) (39223/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.5052) |  Loss2: (0.0000) | Acc: (82.00%) (40284/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.5063) |  Loss2: (0.0000) | Acc: (82.00%) (41285/50000)
# TEST : Loss: (0.5898) | Acc: (80.00%) (8012/10000)
percent tensor([0.6351], device='cuda:0')
percent tensor([0.6290], device='cuda:0')
percent tensor([0.6698], device='cuda:0')
percent tensor([0.6176], device='cuda:0')
percent tensor([0.6511], device='cuda:0')
percent tensor([0.7006], device='cuda:0')
percent tensor([0.7272], device='cuda:0')
percent tensor([0.2319], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.4693) |  Loss2: (0.0000) | Acc: (83.00%) (1171/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.4843) |  Loss2: (0.0000) | Acc: (82.00%) (2227/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (82.00%) (3281/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4816) |  Loss2: (0.0000) | Acc: (82.00%) (4352/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4811) |  Loss2: (0.0000) | Acc: (83.00%) (5426/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4798) |  Loss2: (0.0000) | Acc: (83.00%) (6491/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4788) |  Loss2: (0.0000) | Acc: (83.00%) (7557/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4775) |  Loss2: (0.0000) | Acc: (83.00%) (8629/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (9673/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4875) |  Loss2: (0.0000) | Acc: (82.00%) (10721/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4832) |  Loss2: (0.0000) | Acc: (83.00%) (11804/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4827) |  Loss2: (0.0000) | Acc: (83.00%) (12873/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4861) |  Loss2: (0.0000) | Acc: (83.00%) (13919/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4874) |  Loss2: (0.0000) | Acc: (83.00%) (14985/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4865) |  Loss2: (0.0000) | Acc: (83.00%) (16060/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4874) |  Loss2: (0.0000) | Acc: (83.00%) (17109/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (82.00%) (18162/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4909) |  Loss2: (0.0000) | Acc: (82.00%) (19211/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4914) |  Loss2: (0.0000) | Acc: (82.00%) (20256/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4922) |  Loss2: (0.0000) | Acc: (82.00%) (21317/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4917) |  Loss2: (0.0000) | Acc: (82.00%) (22377/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4920) |  Loss2: (0.0000) | Acc: (82.00%) (23444/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4913) |  Loss2: (0.0000) | Acc: (82.00%) (24519/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (82.00%) (25579/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (82.00%) (26648/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (27724/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (82.00%) (28782/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4897) |  Loss2: (0.0000) | Acc: (83.00%) (29870/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (30934/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4894) |  Loss2: (0.0000) | Acc: (83.00%) (32006/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4878) |  Loss2: (0.0000) | Acc: (83.00%) (33086/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4891) |  Loss2: (0.0000) | Acc: (83.00%) (34134/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (83.00%) (35206/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (83.00%) (36282/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4886) |  Loss2: (0.0000) | Acc: (83.00%) (37362/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (83.00%) (38402/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4905) |  Loss2: (0.0000) | Acc: (83.00%) (39461/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (40514/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (83.00%) (41519/50000)
# TEST : Loss: (0.5519) | Acc: (80.00%) (8067/10000)
percent tensor([0.6351], device='cuda:0')
percent tensor([0.6290], device='cuda:0')
percent tensor([0.6697], device='cuda:0')
percent tensor([0.6176], device='cuda:0')
percent tensor([0.6510], device='cuda:0')
percent tensor([0.7006], device='cuda:0')
percent tensor([0.7271], device='cuda:0')
percent tensor([0.2320], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (1185/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (2251/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.4706) |  Loss2: (0.0000) | Acc: (83.00%) (3332/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (4401/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.4786) |  Loss2: (0.0000) | Acc: (83.00%) (5469/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.4738) |  Loss2: (0.0000) | Acc: (84.00%) (6562/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (7624/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.4726) |  Loss2: (0.0000) | Acc: (83.00%) (8705/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (9771/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.4711) |  Loss2: (0.0000) | Acc: (83.00%) (10850/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.4737) |  Loss2: (0.0000) | Acc: (83.00%) (11913/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.4726) |  Loss2: (0.0000) | Acc: (83.00%) (12989/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (14071/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.4727) |  Loss2: (0.0000) | Acc: (83.00%) (15150/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (16235/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (17306/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.4703) |  Loss2: (0.0000) | Acc: (83.00%) (18375/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.4698) |  Loss2: (0.0000) | Acc: (83.00%) (19459/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.4709) |  Loss2: (0.0000) | Acc: (83.00%) (20523/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.4713) |  Loss2: (0.0000) | Acc: (83.00%) (21581/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.4712) |  Loss2: (0.0000) | Acc: (83.00%) (22638/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.4726) |  Loss2: (0.0000) | Acc: (83.00%) (23699/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (24762/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (25826/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.4747) |  Loss2: (0.0000) | Acc: (83.00%) (26889/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.4753) |  Loss2: (0.0000) | Acc: (83.00%) (27956/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (83.00%) (28996/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (30085/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.4765) |  Loss2: (0.0000) | Acc: (83.00%) (31151/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.4780) |  Loss2: (0.0000) | Acc: (83.00%) (32195/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (33270/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.4787) |  Loss2: (0.0000) | Acc: (83.00%) (34327/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (35415/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.4803) |  Loss2: (0.0000) | Acc: (83.00%) (36436/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.4802) |  Loss2: (0.0000) | Acc: (83.00%) (37506/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (38557/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.4803) |  Loss2: (0.0000) | Acc: (83.00%) (39625/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.4801) |  Loss2: (0.0000) | Acc: (83.00%) (40695/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.4801) |  Loss2: (0.0000) | Acc: (83.00%) (41724/50000)
# TEST : Loss: (0.5985) | Acc: (80.00%) (8016/10000)
percent tensor([0.6350], device='cuda:0')
percent tensor([0.6289], device='cuda:0')
percent tensor([0.6697], device='cuda:0')
percent tensor([0.6175], device='cuda:0')
percent tensor([0.6510], device='cuda:0')
percent tensor([0.7005], device='cuda:0')
percent tensor([0.7271], device='cuda:0')
percent tensor([0.2320], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (83.00%) (1178/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (82.00%) (2209/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.4674) |  Loss2: (0.0000) | Acc: (83.00%) (3303/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.4720) |  Loss2: (0.0000) | Acc: (83.00%) (4374/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.4752) |  Loss2: (0.0000) | Acc: (83.00%) (5447/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.4685) |  Loss2: (0.0000) | Acc: (83.00%) (6522/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.4658) |  Loss2: (0.0000) | Acc: (83.00%) (7594/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.4665) |  Loss2: (0.0000) | Acc: (83.00%) (8668/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.4617) |  Loss2: (0.0000) | Acc: (83.00%) (9767/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.4615) |  Loss2: (0.0000) | Acc: (83.00%) (10834/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (83.00%) (11910/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (83.00%) (12977/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (14037/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.4638) |  Loss2: (0.0000) | Acc: (83.00%) (15109/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.4644) |  Loss2: (0.0000) | Acc: (83.00%) (16203/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (83.00%) (17296/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.4637) |  Loss2: (0.0000) | Acc: (83.00%) (18374/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.4620) |  Loss2: (0.0000) | Acc: (84.00%) (19463/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (84.00%) (20542/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.4633) |  Loss2: (0.0000) | Acc: (84.00%) (21620/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.4613) |  Loss2: (0.0000) | Acc: (84.00%) (22710/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (84.00%) (23794/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.4617) |  Loss2: (0.0000) | Acc: (84.00%) (24862/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.4614) |  Loss2: (0.0000) | Acc: (84.00%) (25936/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.4611) |  Loss2: (0.0000) | Acc: (84.00%) (27010/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.4611) |  Loss2: (0.0000) | Acc: (84.00%) (28087/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.4615) |  Loss2: (0.0000) | Acc: (84.00%) (29156/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.4613) |  Loss2: (0.0000) | Acc: (84.00%) (30240/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.4608) |  Loss2: (0.0000) | Acc: (84.00%) (31326/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.4619) |  Loss2: (0.0000) | Acc: (84.00%) (32381/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.4620) |  Loss2: (0.0000) | Acc: (84.00%) (33450/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (84.00%) (34518/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.4620) |  Loss2: (0.0000) | Acc: (84.00%) (35607/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (36676/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (84.00%) (37762/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.4633) |  Loss2: (0.0000) | Acc: (84.00%) (38816/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (39917/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.4613) |  Loss2: (0.0000) | Acc: (84.00%) (41000/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (42035/50000)
# TEST : Loss: (0.5884) | Acc: (80.00%) (8023/10000)
percent tensor([0.6350], device='cuda:0')
percent tensor([0.6289], device='cuda:0')
percent tensor([0.6696], device='cuda:0')
percent tensor([0.6175], device='cuda:0')
percent tensor([0.6509], device='cuda:0')
percent tensor([0.7004], device='cuda:0')
percent tensor([0.7270], device='cuda:0')
percent tensor([0.2321], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (1.0745) |  Loss2: (0.4875) | Acc: (78.00%) (100/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.9499) |  Loss2: (0.4874) | Acc: (84.00%) (1188/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.9963) |  Loss2: (0.4873) | Acc: (83.00%) (2236/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (1.0372) |  Loss2: (0.4871) | Acc: (81.00%) (3219/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (1.0513) |  Loss2: (0.4869) | Acc: (80.00%) (4219/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (1.0542) |  Loss2: (0.4867) | Acc: (80.00%) (5237/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (1.0555) |  Loss2: (0.4865) | Acc: (80.00%) (6284/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (1.0563) |  Loss2: (0.4863) | Acc: (80.00%) (7319/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (1.0528) |  Loss2: (0.4860) | Acc: (80.00%) (8350/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (1.0548) |  Loss2: (0.4858) | Acc: (80.00%) (9379/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (1.0497) |  Loss2: (0.4857) | Acc: (80.00%) (10422/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (1.0467) |  Loss2: (0.4855) | Acc: (80.00%) (11462/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (1.0476) |  Loss2: (0.4853) | Acc: (80.00%) (12492/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (1.0449) |  Loss2: (0.4852) | Acc: (80.00%) (13536/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (1.0415) |  Loss2: (0.4850) | Acc: (80.00%) (14597/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (1.0409) |  Loss2: (0.4849) | Acc: (80.00%) (15647/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (1.0401) |  Loss2: (0.4847) | Acc: (80.00%) (16685/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (1.0360) |  Loss2: (0.4846) | Acc: (81.00%) (17760/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (1.0334) |  Loss2: (0.4844) | Acc: (81.00%) (18829/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (1.0329) |  Loss2: (0.4843) | Acc: (81.00%) (19865/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (1.0309) |  Loss2: (0.4842) | Acc: (81.00%) (20918/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (1.0304) |  Loss2: (0.4840) | Acc: (81.00%) (21957/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (1.0271) |  Loss2: (0.4839) | Acc: (81.00%) (23034/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (1.0261) |  Loss2: (0.4838) | Acc: (81.00%) (24091/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (1.0234) |  Loss2: (0.4837) | Acc: (81.00%) (25163/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (1.0224) |  Loss2: (0.4835) | Acc: (81.00%) (26213/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (1.0218) |  Loss2: (0.4834) | Acc: (81.00%) (27268/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (1.0196) |  Loss2: (0.4833) | Acc: (81.00%) (28334/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (1.0181) |  Loss2: (0.4832) | Acc: (81.00%) (29392/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (1.0168) |  Loss2: (0.4831) | Acc: (81.00%) (30444/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (1.0157) |  Loss2: (0.4829) | Acc: (81.00%) (31504/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (1.0133) |  Loss2: (0.4828) | Acc: (81.00%) (32581/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (1.0126) |  Loss2: (0.4827) | Acc: (81.00%) (33649/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (1.0112) |  Loss2: (0.4826) | Acc: (81.00%) (34716/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (1.0106) |  Loss2: (0.4825) | Acc: (81.00%) (35757/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (1.0093) |  Loss2: (0.4823) | Acc: (81.00%) (36812/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (1.0082) |  Loss2: (0.4822) | Acc: (81.00%) (37875/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (1.0072) |  Loss2: (0.4821) | Acc: (81.00%) (38934/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (1.0073) |  Loss2: (0.4820) | Acc: (81.00%) (39982/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (1.0055) |  Loss2: (0.4819) | Acc: (82.00%) (41022/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_055.pth.tar'
# TEST : Loss: (0.5797) | Acc: (80.00%) (8075/10000)
percent tensor([0.6435], device='cuda:0')
percent tensor([0.6451], device='cuda:0')
percent tensor([0.6600], device='cuda:0')
percent tensor([0.6125], device='cuda:0')
percent tensor([0.6697], device='cuda:0')
percent tensor([0.7239], device='cuda:0')
percent tensor([0.7456], device='cuda:0')
percent tensor([0.2357], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.9856) |  Loss2: (0.4775) | Acc: (79.00%) (102/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.9775) |  Loss2: (0.4774) | Acc: (82.00%) (1164/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.9878) |  Loss2: (0.4773) | Acc: (82.00%) (2219/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.9715) |  Loss2: (0.4772) | Acc: (82.00%) (3287/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.9707) |  Loss2: (0.4771) | Acc: (82.00%) (4343/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.9708) |  Loss2: (0.4770) | Acc: (82.00%) (5410/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.9704) |  Loss2: (0.4769) | Acc: (82.00%) (6477/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.9753) |  Loss2: (0.4768) | Acc: (82.00%) (7526/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.9687) |  Loss2: (0.4767) | Acc: (83.00%) (8612/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.9687) |  Loss2: (0.4766) | Acc: (82.00%) (9665/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.9702) |  Loss2: (0.4765) | Acc: (82.00%) (10720/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.9729) |  Loss2: (0.4764) | Acc: (82.00%) (11778/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.9755) |  Loss2: (0.4764) | Acc: (82.00%) (12823/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.9709) |  Loss2: (0.4763) | Acc: (82.00%) (13916/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.9691) |  Loss2: (0.4762) | Acc: (83.00%) (14987/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.9683) |  Loss2: (0.4762) | Acc: (82.00%) (16036/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.9678) |  Loss2: (0.4761) | Acc: (83.00%) (17105/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.9677) |  Loss2: (0.4760) | Acc: (83.00%) (18171/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.9661) |  Loss2: (0.4760) | Acc: (83.00%) (19247/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.9649) |  Loss2: (0.4759) | Acc: (83.00%) (20319/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.9634) |  Loss2: (0.4758) | Acc: (83.00%) (21392/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.9633) |  Loss2: (0.4758) | Acc: (83.00%) (22466/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.9616) |  Loss2: (0.4757) | Acc: (83.00%) (23550/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.9625) |  Loss2: (0.4756) | Acc: (83.00%) (24616/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.9619) |  Loss2: (0.4756) | Acc: (83.00%) (25685/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.9616) |  Loss2: (0.4755) | Acc: (83.00%) (26755/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.9618) |  Loss2: (0.4754) | Acc: (83.00%) (27834/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.9616) |  Loss2: (0.4754) | Acc: (83.00%) (28889/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.9615) |  Loss2: (0.4753) | Acc: (83.00%) (29960/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.9613) |  Loss2: (0.4753) | Acc: (83.00%) (31018/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.9620) |  Loss2: (0.4752) | Acc: (83.00%) (32081/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.9615) |  Loss2: (0.4751) | Acc: (83.00%) (33159/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.9605) |  Loss2: (0.4751) | Acc: (83.00%) (34235/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.9585) |  Loss2: (0.4750) | Acc: (83.00%) (35342/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.9578) |  Loss2: (0.4750) | Acc: (83.00%) (36420/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.9585) |  Loss2: (0.4749) | Acc: (83.00%) (37483/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.9581) |  Loss2: (0.4749) | Acc: (83.00%) (38536/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.9581) |  Loss2: (0.4748) | Acc: (83.00%) (39590/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.9569) |  Loss2: (0.4747) | Acc: (83.00%) (40697/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.9570) |  Loss2: (0.4747) | Acc: (83.00%) (41718/50000)
# TEST : Loss: (0.5439) | Acc: (81.00%) (8185/10000)
percent tensor([0.6460], device='cuda:0')
percent tensor([0.6532], device='cuda:0')
percent tensor([0.6638], device='cuda:0')
percent tensor([0.6138], device='cuda:0')
percent tensor([0.6726], device='cuda:0')
percent tensor([0.7324], device='cuda:0')
percent tensor([0.7526], device='cuda:0')
percent tensor([0.2322], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.9726) |  Loss2: (0.4729) | Acc: (80.00%) (103/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.9437) |  Loss2: (0.4729) | Acc: (83.00%) (1174/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.9522) |  Loss2: (0.4728) | Acc: (83.00%) (2255/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.9569) |  Loss2: (0.4728) | Acc: (83.00%) (3323/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.9583) |  Loss2: (0.4727) | Acc: (83.00%) (4395/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.9636) |  Loss2: (0.4727) | Acc: (83.00%) (5458/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.9564) |  Loss2: (0.4726) | Acc: (83.00%) (6543/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.9571) |  Loss2: (0.4726) | Acc: (83.00%) (7612/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.9554) |  Loss2: (0.4726) | Acc: (83.00%) (8692/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.9536) |  Loss2: (0.4725) | Acc: (83.00%) (9760/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.9507) |  Loss2: (0.4725) | Acc: (83.00%) (10856/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.9466) |  Loss2: (0.4724) | Acc: (84.00%) (11942/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.9467) |  Loss2: (0.4724) | Acc: (83.00%) (13006/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.9445) |  Loss2: (0.4724) | Acc: (84.00%) (14088/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.9461) |  Loss2: (0.4723) | Acc: (83.00%) (15152/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.9473) |  Loss2: (0.4723) | Acc: (83.00%) (16209/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.9476) |  Loss2: (0.4722) | Acc: (83.00%) (17283/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.9454) |  Loss2: (0.4722) | Acc: (83.00%) (18382/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.9475) |  Loss2: (0.4721) | Acc: (83.00%) (19436/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.9451) |  Loss2: (0.4721) | Acc: (83.00%) (20517/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.9475) |  Loss2: (0.4720) | Acc: (83.00%) (21575/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.9455) |  Loss2: (0.4720) | Acc: (83.00%) (22670/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.9462) |  Loss2: (0.4719) | Acc: (83.00%) (23734/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.9460) |  Loss2: (0.4719) | Acc: (83.00%) (24802/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.9464) |  Loss2: (0.4718) | Acc: (83.00%) (25865/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.9456) |  Loss2: (0.4718) | Acc: (83.00%) (26949/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.9441) |  Loss2: (0.4717) | Acc: (83.00%) (28049/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.9415) |  Loss2: (0.4717) | Acc: (84.00%) (29164/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.9399) |  Loss2: (0.4716) | Acc: (84.00%) (30266/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.9405) |  Loss2: (0.4716) | Acc: (84.00%) (31341/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.9406) |  Loss2: (0.4715) | Acc: (84.00%) (32422/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.9399) |  Loss2: (0.4715) | Acc: (84.00%) (33508/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.9399) |  Loss2: (0.4715) | Acc: (84.00%) (34584/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.9392) |  Loss2: (0.4714) | Acc: (84.00%) (35671/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.9389) |  Loss2: (0.4714) | Acc: (84.00%) (36755/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.9382) |  Loss2: (0.4713) | Acc: (84.00%) (37837/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.9381) |  Loss2: (0.4713) | Acc: (84.00%) (38919/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.9380) |  Loss2: (0.4712) | Acc: (84.00%) (39992/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.9369) |  Loss2: (0.4712) | Acc: (84.00%) (41088/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.9362) |  Loss2: (0.4712) | Acc: (84.00%) (42140/50000)
# TEST : Loss: (0.5342) | Acc: (82.00%) (8218/10000)
percent tensor([0.6520], device='cuda:0')
percent tensor([0.6579], device='cuda:0')
percent tensor([0.6685], device='cuda:0')
percent tensor([0.6176], device='cuda:0')
percent tensor([0.6742], device='cuda:0')
percent tensor([0.7357], device='cuda:0')
percent tensor([0.7550], device='cuda:0')
percent tensor([0.2263], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (1.0224) |  Loss2: (0.4697) | Acc: (85.00%) (109/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.9615) |  Loss2: (0.4697) | Acc: (82.00%) (1167/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.9421) |  Loss2: (0.4697) | Acc: (83.00%) (2251/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.9346) |  Loss2: (0.4698) | Acc: (83.00%) (3316/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.9322) |  Loss2: (0.4698) | Acc: (83.00%) (4390/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.9329) |  Loss2: (0.4698) | Acc: (83.00%) (5465/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.9310) |  Loss2: (0.4698) | Acc: (83.00%) (6552/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.9313) |  Loss2: (0.4697) | Acc: (84.00%) (7634/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.9322) |  Loss2: (0.4697) | Acc: (84.00%) (8717/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.9321) |  Loss2: (0.4697) | Acc: (84.00%) (9801/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.9316) |  Loss2: (0.4697) | Acc: (84.00%) (10885/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.9300) |  Loss2: (0.4697) | Acc: (84.00%) (11976/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.9264) |  Loss2: (0.4697) | Acc: (84.00%) (13085/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.9265) |  Loss2: (0.4697) | Acc: (84.00%) (14176/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.9255) |  Loss2: (0.4697) | Acc: (84.00%) (15265/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.9301) |  Loss2: (0.4697) | Acc: (84.00%) (16315/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.9287) |  Loss2: (0.4697) | Acc: (84.00%) (17405/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.9281) |  Loss2: (0.4696) | Acc: (84.00%) (18490/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.9300) |  Loss2: (0.4696) | Acc: (84.00%) (19545/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.9301) |  Loss2: (0.4696) | Acc: (84.00%) (20625/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.9296) |  Loss2: (0.4696) | Acc: (84.00%) (21704/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.9306) |  Loss2: (0.4695) | Acc: (84.00%) (22775/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.9286) |  Loss2: (0.4695) | Acc: (84.00%) (23866/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.9284) |  Loss2: (0.4695) | Acc: (84.00%) (24948/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.9279) |  Loss2: (0.4695) | Acc: (84.00%) (26035/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.9288) |  Loss2: (0.4695) | Acc: (84.00%) (27103/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.9293) |  Loss2: (0.4694) | Acc: (84.00%) (28166/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.9307) |  Loss2: (0.4694) | Acc: (84.00%) (29225/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.9311) |  Loss2: (0.4694) | Acc: (84.00%) (30300/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.9306) |  Loss2: (0.4694) | Acc: (84.00%) (31380/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.9310) |  Loss2: (0.4694) | Acc: (84.00%) (32462/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.9314) |  Loss2: (0.4694) | Acc: (84.00%) (33537/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.9318) |  Loss2: (0.4693) | Acc: (84.00%) (34613/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.9321) |  Loss2: (0.4693) | Acc: (84.00%) (35690/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.9328) |  Loss2: (0.4693) | Acc: (84.00%) (36770/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.9328) |  Loss2: (0.4693) | Acc: (84.00%) (37840/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.9322) |  Loss2: (0.4693) | Acc: (84.00%) (38926/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.9316) |  Loss2: (0.4693) | Acc: (84.00%) (40012/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.9328) |  Loss2: (0.4692) | Acc: (84.00%) (41071/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.9321) |  Loss2: (0.4692) | Acc: (84.00%) (42122/50000)
# TEST : Loss: (0.5253) | Acc: (82.00%) (8258/10000)
percent tensor([0.6518], device='cuda:0')
percent tensor([0.6586], device='cuda:0')
percent tensor([0.6728], device='cuda:0')
percent tensor([0.6210], device='cuda:0')
percent tensor([0.6747], device='cuda:0')
percent tensor([0.7356], device='cuda:0')
percent tensor([0.7563], device='cuda:0')
percent tensor([0.2202], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.8277) |  Loss2: (0.4686) | Acc: (85.00%) (110/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.9140) |  Loss2: (0.4687) | Acc: (84.00%) (1189/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.8944) |  Loss2: (0.4687) | Acc: (85.00%) (2297/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.9110) |  Loss2: (0.4688) | Acc: (85.00%) (3377/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.9093) |  Loss2: (0.4688) | Acc: (85.00%) (4462/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.9066) |  Loss2: (0.4689) | Acc: (85.00%) (5567/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.9147) |  Loss2: (0.4688) | Acc: (85.00%) (6645/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.9177) |  Loss2: (0.4688) | Acc: (85.00%) (7726/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.9205) |  Loss2: (0.4688) | Acc: (84.00%) (8799/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.9200) |  Loss2: (0.4688) | Acc: (84.00%) (9879/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.9203) |  Loss2: (0.4688) | Acc: (84.00%) (10957/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.9212) |  Loss2: (0.4688) | Acc: (84.00%) (12033/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.9207) |  Loss2: (0.4687) | Acc: (84.00%) (13121/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.9273) |  Loss2: (0.4687) | Acc: (84.00%) (14158/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.9259) |  Loss2: (0.4687) | Acc: (84.00%) (15242/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.9253) |  Loss2: (0.4687) | Acc: (84.00%) (16323/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.9263) |  Loss2: (0.4686) | Acc: (84.00%) (17407/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.9279) |  Loss2: (0.4686) | Acc: (84.00%) (18470/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.9272) |  Loss2: (0.4686) | Acc: (84.00%) (19558/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.9262) |  Loss2: (0.4685) | Acc: (84.00%) (20626/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.9263) |  Loss2: (0.4685) | Acc: (84.00%) (21718/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.9252) |  Loss2: (0.4685) | Acc: (84.00%) (22803/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.9257) |  Loss2: (0.4685) | Acc: (84.00%) (23869/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.9271) |  Loss2: (0.4684) | Acc: (84.00%) (24946/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.9267) |  Loss2: (0.4684) | Acc: (84.00%) (26030/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.9264) |  Loss2: (0.4684) | Acc: (84.00%) (27108/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.9265) |  Loss2: (0.4684) | Acc: (84.00%) (28178/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.9264) |  Loss2: (0.4683) | Acc: (84.00%) (29266/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.9262) |  Loss2: (0.4683) | Acc: (84.00%) (30361/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.9263) |  Loss2: (0.4683) | Acc: (84.00%) (31437/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.9267) |  Loss2: (0.4682) | Acc: (84.00%) (32523/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.9271) |  Loss2: (0.4682) | Acc: (84.00%) (33592/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.9271) |  Loss2: (0.4682) | Acc: (84.00%) (34677/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.9266) |  Loss2: (0.4682) | Acc: (84.00%) (35762/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.9264) |  Loss2: (0.4681) | Acc: (84.00%) (36849/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.9256) |  Loss2: (0.4681) | Acc: (84.00%) (37930/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.9256) |  Loss2: (0.4681) | Acc: (84.00%) (39006/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.9259) |  Loss2: (0.4681) | Acc: (84.00%) (40079/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.9264) |  Loss2: (0.4681) | Acc: (84.00%) (41147/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.9257) |  Loss2: (0.4681) | Acc: (84.00%) (42192/50000)
# TEST : Loss: (0.5212) | Acc: (82.00%) (8274/10000)
percent tensor([0.6522], device='cuda:0')
percent tensor([0.6595], device='cuda:0')
percent tensor([0.6759], device='cuda:0')
percent tensor([0.6239], device='cuda:0')
percent tensor([0.6744], device='cuda:0')
percent tensor([0.7364], device='cuda:0')
percent tensor([0.7571], device='cuda:0')
percent tensor([0.2142], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.5441) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.4457) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.4545) |  Loss2: (0.0000) | Acc: (83.00%) (2249/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (3338/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.4509) |  Loss2: (0.0000) | Acc: (84.00%) (4412/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (5491/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (6568/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (84.00%) (7649/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (8743/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (9793/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (84.00%) (10865/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.4637) |  Loss2: (0.0000) | Acc: (84.00%) (11942/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.4630) |  Loss2: (0.0000) | Acc: (83.00%) (13008/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.4670) |  Loss2: (0.0000) | Acc: (83.00%) (14059/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.4663) |  Loss2: (0.0000) | Acc: (83.00%) (15135/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.4659) |  Loss2: (0.0000) | Acc: (83.00%) (16211/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.4658) |  Loss2: (0.0000) | Acc: (83.00%) (17294/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.4648) |  Loss2: (0.0000) | Acc: (83.00%) (18373/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.4665) |  Loss2: (0.0000) | Acc: (83.00%) (19443/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.4664) |  Loss2: (0.0000) | Acc: (83.00%) (20515/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.4664) |  Loss2: (0.0000) | Acc: (83.00%) (21606/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.4669) |  Loss2: (0.0000) | Acc: (83.00%) (22680/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.4661) |  Loss2: (0.0000) | Acc: (84.00%) (23771/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.4673) |  Loss2: (0.0000) | Acc: (83.00%) (24833/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.4658) |  Loss2: (0.0000) | Acc: (83.00%) (25911/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (84.00%) (27002/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (84.00%) (28080/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.4658) |  Loss2: (0.0000) | Acc: (83.00%) (29136/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.4658) |  Loss2: (0.0000) | Acc: (84.00%) (30214/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.4655) |  Loss2: (0.0000) | Acc: (83.00%) (31284/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (32358/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (33413/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (34501/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.4660) |  Loss2: (0.0000) | Acc: (83.00%) (35548/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.4662) |  Loss2: (0.0000) | Acc: (83.00%) (36615/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.4657) |  Loss2: (0.0000) | Acc: (83.00%) (37692/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.4654) |  Loss2: (0.0000) | Acc: (83.00%) (38770/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (39853/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (40920/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.4661) |  Loss2: (0.0000) | Acc: (83.00%) (41942/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_060.pth.tar'
# TEST : Loss: (0.6015) | Acc: (79.00%) (7971/10000)
percent tensor([0.6523], device='cuda:0')
percent tensor([0.6591], device='cuda:0')
percent tensor([0.6757], device='cuda:0')
percent tensor([0.6240], device='cuda:0')
percent tensor([0.6741], device='cuda:0')
percent tensor([0.7362], device='cuda:0')
percent tensor([0.7571], device='cuda:0')
percent tensor([0.2142], device='cuda:0')
Epoch: 61 | Batch_idx: 0 |  Loss: (0.4912) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.4562) |  Loss2: (0.0000) | Acc: (84.00%) (2259/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (84.00%) (3343/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.4481) |  Loss2: (0.0000) | Acc: (84.00%) (4420/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (83.00%) (5481/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.4549) |  Loss2: (0.0000) | Acc: (84.00%) (6562/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (7644/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (8746/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.4508) |  Loss2: (0.0000) | Acc: (84.00%) (9815/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (10909/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (84.00%) (11991/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (13076/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (14150/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.4463) |  Loss2: (0.0000) | Acc: (84.00%) (15245/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.4454) |  Loss2: (0.0000) | Acc: (84.00%) (16339/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (17431/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (18510/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (19571/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.4490) |  Loss2: (0.0000) | Acc: (84.00%) (20652/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (21728/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.4499) |  Loss2: (0.0000) | Acc: (84.00%) (22805/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (23906/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.4484) |  Loss2: (0.0000) | Acc: (84.00%) (24988/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (26061/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (84.00%) (27125/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.4515) |  Loss2: (0.0000) | Acc: (84.00%) (28209/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (29305/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.4505) |  Loss2: (0.0000) | Acc: (84.00%) (30401/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.4510) |  Loss2: (0.0000) | Acc: (84.00%) (31482/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.4507) |  Loss2: (0.0000) | Acc: (84.00%) (32574/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (33656/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (34723/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (35803/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (36887/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (84.00%) (37972/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (39056/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (40128/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.4520) |  Loss2: (0.0000) | Acc: (84.00%) (41205/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.4511) |  Loss2: (0.0000) | Acc: (84.00%) (42259/50000)
# TEST : Loss: (0.5189) | Acc: (82.00%) (8289/10000)
percent tensor([0.6522], device='cuda:0')
percent tensor([0.6590], device='cuda:0')
percent tensor([0.6756], device='cuda:0')
percent tensor([0.6240], device='cuda:0')
percent tensor([0.6740], device='cuda:0')
percent tensor([0.7362], device='cuda:0')
percent tensor([0.7570], device='cuda:0')
percent tensor([0.2143], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.4876) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (86.00%) (1217/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (2301/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (86.00%) (3416/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.4225) |  Loss2: (0.0000) | Acc: (86.00%) (4518/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (86.00%) (5619/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.4270) |  Loss2: (0.0000) | Acc: (85.00%) (6696/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.4265) |  Loss2: (0.0000) | Acc: (85.00%) (7800/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (85.00%) (8871/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (85.00%) (9941/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.4343) |  Loss2: (0.0000) | Acc: (85.00%) (11039/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (85.00%) (12127/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (85.00%) (13209/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (85.00%) (14299/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (85.00%) (15375/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (85.00%) (16441/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (85.00%) (17537/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (85.00%) (18608/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (85.00%) (19710/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.4391) |  Loss2: (0.0000) | Acc: (85.00%) (20791/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (21852/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (22946/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (24021/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (25097/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (26195/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (27295/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (28353/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (29445/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (30536/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (31629/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (32717/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (33798/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.4433) |  Loss2: (0.0000) | Acc: (84.00%) (34861/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (35936/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (37020/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.4438) |  Loss2: (0.0000) | Acc: (84.00%) (38081/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (39164/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (40246/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (41348/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (42431/50000)
# TEST : Loss: (0.5211) | Acc: (82.00%) (8246/10000)
percent tensor([0.6521], device='cuda:0')
percent tensor([0.6590], device='cuda:0')
percent tensor([0.6756], device='cuda:0')
percent tensor([0.6239], device='cuda:0')
percent tensor([0.6740], device='cuda:0')
percent tensor([0.7361], device='cuda:0')
percent tensor([0.7569], device='cuda:0')
percent tensor([0.2144], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3940) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.4509) |  Loss2: (0.0000) | Acc: (85.00%) (1201/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (85.00%) (2299/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (85.00%) (3393/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (4497/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.4273) |  Loss2: (0.0000) | Acc: (85.00%) (5599/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.4266) |  Loss2: (0.0000) | Acc: (85.00%) (6696/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (85.00%) (7784/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (8852/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (9948/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (11032/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (85.00%) (12108/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (85.00%) (13185/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (85.00%) (14280/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (85.00%) (15372/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.4355) |  Loss2: (0.0000) | Acc: (85.00%) (16475/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (85.00%) (17576/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (18673/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (19755/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (85.00%) (20845/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (21919/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (85.00%) (22999/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (24095/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.4349) |  Loss2: (0.0000) | Acc: (85.00%) (25186/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.4333) |  Loss2: (0.0000) | Acc: (85.00%) (26295/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (85.00%) (27396/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.4324) |  Loss2: (0.0000) | Acc: (85.00%) (28484/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (29574/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (30658/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (31759/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (85.00%) (32828/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (33929/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (35033/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (36132/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.4290) |  Loss2: (0.0000) | Acc: (85.00%) (37224/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (38302/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.4291) |  Loss2: (0.0000) | Acc: (85.00%) (39409/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (40527/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (41602/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.4287) |  Loss2: (0.0000) | Acc: (85.00%) (42664/50000)
# TEST : Loss: (0.5261) | Acc: (82.00%) (8211/10000)
percent tensor([0.6521], device='cuda:0')
percent tensor([0.6589], device='cuda:0')
percent tensor([0.6755], device='cuda:0')
percent tensor([0.6239], device='cuda:0')
percent tensor([0.6739], device='cuda:0')
percent tensor([0.7360], device='cuda:0')
percent tensor([0.7568], device='cuda:0')
percent tensor([0.2144], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (2306/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (86.00%) (3417/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.4063) |  Loss2: (0.0000) | Acc: (86.00%) (4523/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (5608/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (6698/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (7778/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (8885/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (9976/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.4150) |  Loss2: (0.0000) | Acc: (85.00%) (11061/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (12160/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (13257/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (14360/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (15473/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (16574/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (17666/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.4129) |  Loss2: (0.0000) | Acc: (85.00%) (18779/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (19868/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (20972/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (22053/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (23136/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (24210/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (25328/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (26425/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (27511/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.4172) |  Loss2: (0.0000) | Acc: (85.00%) (28595/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (29673/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.4181) |  Loss2: (0.0000) | Acc: (85.00%) (30775/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (31878/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.4180) |  Loss2: (0.0000) | Acc: (85.00%) (32983/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.4171) |  Loss2: (0.0000) | Acc: (85.00%) (34099/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (35170/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (36278/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (37379/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (38461/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (39554/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.4198) |  Loss2: (0.0000) | Acc: (85.00%) (40639/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (41741/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (42805/50000)
# TEST : Loss: (0.6037) | Acc: (79.00%) (7977/10000)
percent tensor([0.6520], device='cuda:0')
percent tensor([0.6589], device='cuda:0')
percent tensor([0.6754], device='cuda:0')
percent tensor([0.6238], device='cuda:0')
percent tensor([0.6738], device='cuda:0')
percent tensor([0.7359], device='cuda:0')
percent tensor([0.7567], device='cuda:0')
percent tensor([0.2145], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.9736) |  Loss2: (0.4681) | Acc: (82.00%) (105/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.9592) |  Loss2: (0.4681) | Acc: (83.00%) (1170/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.9835) |  Loss2: (0.4680) | Acc: (82.00%) (2209/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (1.0108) |  Loss2: (0.4680) | Acc: (81.00%) (3221/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (1.0202) |  Loss2: (0.4679) | Acc: (80.00%) (4228/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (1.0226) |  Loss2: (0.4678) | Acc: (80.00%) (5249/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (1.0263) |  Loss2: (0.4678) | Acc: (80.00%) (6261/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (1.0224) |  Loss2: (0.4677) | Acc: (80.00%) (7300/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (1.0206) |  Loss2: (0.4676) | Acc: (80.00%) (8343/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (1.0235) |  Loss2: (0.4674) | Acc: (80.00%) (9345/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (1.0225) |  Loss2: (0.4673) | Acc: (80.00%) (10387/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (1.0192) |  Loss2: (0.4671) | Acc: (80.00%) (11440/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (1.0145) |  Loss2: (0.4670) | Acc: (80.00%) (12505/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (1.0102) |  Loss2: (0.4668) | Acc: (80.00%) (13543/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (1.0067) |  Loss2: (0.4667) | Acc: (80.00%) (14600/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (1.0030) |  Loss2: (0.4666) | Acc: (81.00%) (15660/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (1.0018) |  Loss2: (0.4664) | Acc: (81.00%) (16708/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.9979) |  Loss2: (0.4663) | Acc: (81.00%) (17769/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.9971) |  Loss2: (0.4661) | Acc: (81.00%) (18820/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.9951) |  Loss2: (0.4660) | Acc: (81.00%) (19880/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.9935) |  Loss2: (0.4658) | Acc: (81.00%) (20937/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.9911) |  Loss2: (0.4657) | Acc: (81.00%) (22020/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.9881) |  Loss2: (0.4656) | Acc: (81.00%) (23087/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.9851) |  Loss2: (0.4654) | Acc: (81.00%) (24168/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.9828) |  Loss2: (0.4653) | Acc: (81.00%) (25229/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.9794) |  Loss2: (0.4651) | Acc: (81.00%) (26328/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.9784) |  Loss2: (0.4650) | Acc: (81.00%) (27382/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.9761) |  Loss2: (0.4649) | Acc: (82.00%) (28465/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.9755) |  Loss2: (0.4648) | Acc: (82.00%) (29520/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.9751) |  Loss2: (0.4646) | Acc: (82.00%) (30582/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.9741) |  Loss2: (0.4645) | Acc: (82.00%) (31639/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.9727) |  Loss2: (0.4644) | Acc: (82.00%) (32718/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.9720) |  Loss2: (0.4643) | Acc: (82.00%) (33787/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.9700) |  Loss2: (0.4642) | Acc: (82.00%) (34862/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.9691) |  Loss2: (0.4640) | Acc: (82.00%) (35927/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.9685) |  Loss2: (0.4639) | Acc: (82.00%) (36981/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.9678) |  Loss2: (0.4638) | Acc: (82.00%) (38040/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.9671) |  Loss2: (0.4637) | Acc: (82.00%) (39112/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.9649) |  Loss2: (0.4636) | Acc: (82.00%) (40193/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.9643) |  Loss2: (0.4635) | Acc: (82.00%) (41232/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_065.pth.tar'
# TEST : Loss: (0.5340) | Acc: (82.00%) (8200/10000)
percent tensor([0.6706], device='cuda:0')
percent tensor([0.6734], device='cuda:0')
percent tensor([0.6931], device='cuda:0')
percent tensor([0.6077], device='cuda:0')
percent tensor([0.6858], device='cuda:0')
percent tensor([0.7379], device='cuda:0')
percent tensor([0.7724], device='cuda:0')
percent tensor([0.2178], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.8839) |  Loss2: (0.4589) | Acc: (84.00%) (108/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.9379) |  Loss2: (0.4587) | Acc: (83.00%) (1181/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.9149) |  Loss2: (0.4586) | Acc: (84.00%) (2279/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.9054) |  Loss2: (0.4585) | Acc: (84.00%) (3366/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.9092) |  Loss2: (0.4584) | Acc: (84.00%) (4446/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.9127) |  Loss2: (0.4584) | Acc: (84.00%) (5531/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.9138) |  Loss2: (0.4583) | Acc: (84.00%) (6600/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.9133) |  Loss2: (0.4582) | Acc: (84.00%) (7674/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.9147) |  Loss2: (0.4581) | Acc: (84.00%) (8751/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.9143) |  Loss2: (0.4580) | Acc: (84.00%) (9827/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.9116) |  Loss2: (0.4579) | Acc: (84.00%) (10929/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.9087) |  Loss2: (0.4579) | Acc: (84.00%) (12032/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.9077) |  Loss2: (0.4578) | Acc: (84.00%) (13115/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.9087) |  Loss2: (0.4577) | Acc: (84.00%) (14189/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.9084) |  Loss2: (0.4576) | Acc: (84.00%) (15273/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.9123) |  Loss2: (0.4575) | Acc: (84.00%) (16323/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.9124) |  Loss2: (0.4574) | Acc: (84.00%) (17389/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.9123) |  Loss2: (0.4574) | Acc: (84.00%) (18474/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.9126) |  Loss2: (0.4573) | Acc: (84.00%) (19563/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.9134) |  Loss2: (0.4572) | Acc: (84.00%) (20624/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.9122) |  Loss2: (0.4571) | Acc: (84.00%) (21715/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.9121) |  Loss2: (0.4571) | Acc: (84.00%) (22791/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.9117) |  Loss2: (0.4570) | Acc: (84.00%) (23869/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.9120) |  Loss2: (0.4570) | Acc: (84.00%) (24940/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.9135) |  Loss2: (0.4569) | Acc: (84.00%) (26001/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.9145) |  Loss2: (0.4568) | Acc: (84.00%) (27079/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.9147) |  Loss2: (0.4568) | Acc: (84.00%) (28158/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.9140) |  Loss2: (0.4567) | Acc: (84.00%) (29238/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.9127) |  Loss2: (0.4567) | Acc: (84.00%) (30335/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.9132) |  Loss2: (0.4566) | Acc: (84.00%) (31408/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.9135) |  Loss2: (0.4566) | Acc: (84.00%) (32480/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.9127) |  Loss2: (0.4565) | Acc: (84.00%) (33575/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.9123) |  Loss2: (0.4564) | Acc: (84.00%) (34657/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.9112) |  Loss2: (0.4564) | Acc: (84.00%) (35769/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.9110) |  Loss2: (0.4563) | Acc: (84.00%) (36845/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.9101) |  Loss2: (0.4563) | Acc: (84.00%) (37933/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.9094) |  Loss2: (0.4562) | Acc: (84.00%) (39027/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.9098) |  Loss2: (0.4562) | Acc: (84.00%) (40093/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.9096) |  Loss2: (0.4561) | Acc: (84.00%) (41178/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.9091) |  Loss2: (0.4561) | Acc: (84.00%) (42224/50000)
# TEST : Loss: (0.5145) | Acc: (82.00%) (8280/10000)
percent tensor([0.6755], device='cuda:0')
percent tensor([0.6745], device='cuda:0')
percent tensor([0.6987], device='cuda:0')
percent tensor([0.6126], device='cuda:0')
percent tensor([0.6932], device='cuda:0')
percent tensor([0.7422], device='cuda:0')
percent tensor([0.7775], device='cuda:0')
percent tensor([0.2152], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.8793) |  Loss2: (0.4543) | Acc: (85.00%) (110/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.9576) |  Loss2: (0.4542) | Acc: (82.00%) (1166/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.9309) |  Loss2: (0.4541) | Acc: (83.00%) (2238/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.9273) |  Loss2: (0.4540) | Acc: (83.00%) (3310/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.9228) |  Loss2: (0.4540) | Acc: (83.00%) (4369/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.9232) |  Loss2: (0.4539) | Acc: (83.00%) (5440/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.9229) |  Loss2: (0.4538) | Acc: (83.00%) (6517/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.9211) |  Loss2: (0.4538) | Acc: (83.00%) (7591/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.9143) |  Loss2: (0.4537) | Acc: (83.00%) (8695/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.9116) |  Loss2: (0.4537) | Acc: (84.00%) (9787/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.9119) |  Loss2: (0.4536) | Acc: (83.00%) (10852/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.9132) |  Loss2: (0.4536) | Acc: (83.00%) (11923/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.9113) |  Loss2: (0.4535) | Acc: (84.00%) (13021/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.9107) |  Loss2: (0.4535) | Acc: (84.00%) (14098/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.9092) |  Loss2: (0.4534) | Acc: (84.00%) (15172/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.9078) |  Loss2: (0.4534) | Acc: (84.00%) (16257/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.9061) |  Loss2: (0.4533) | Acc: (84.00%) (17345/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.9055) |  Loss2: (0.4533) | Acc: (84.00%) (18422/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.9046) |  Loss2: (0.4533) | Acc: (84.00%) (19500/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.9045) |  Loss2: (0.4533) | Acc: (84.00%) (20578/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.9013) |  Loss2: (0.4532) | Acc: (84.00%) (21693/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.8992) |  Loss2: (0.4532) | Acc: (84.00%) (22793/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.8997) |  Loss2: (0.4532) | Acc: (84.00%) (23867/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.8998) |  Loss2: (0.4531) | Acc: (84.00%) (24960/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.9007) |  Loss2: (0.4531) | Acc: (84.00%) (26044/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.9019) |  Loss2: (0.4530) | Acc: (84.00%) (27101/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.9009) |  Loss2: (0.4530) | Acc: (84.00%) (28183/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.9007) |  Loss2: (0.4530) | Acc: (84.00%) (29259/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.8996) |  Loss2: (0.4529) | Acc: (84.00%) (30360/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.9006) |  Loss2: (0.4529) | Acc: (84.00%) (31417/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.9010) |  Loss2: (0.4528) | Acc: (84.00%) (32499/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.9018) |  Loss2: (0.4528) | Acc: (84.00%) (33568/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.9004) |  Loss2: (0.4528) | Acc: (84.00%) (34670/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.9001) |  Loss2: (0.4527) | Acc: (84.00%) (35758/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.9008) |  Loss2: (0.4527) | Acc: (84.00%) (36819/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.9001) |  Loss2: (0.4527) | Acc: (84.00%) (37914/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.8992) |  Loss2: (0.4526) | Acc: (84.00%) (39010/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.8990) |  Loss2: (0.4526) | Acc: (84.00%) (40093/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.8988) |  Loss2: (0.4526) | Acc: (84.00%) (41180/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.8987) |  Loss2: (0.4525) | Acc: (84.00%) (42223/50000)
# TEST : Loss: (0.5021) | Acc: (83.00%) (8300/10000)
percent tensor([0.6779], device='cuda:0')
percent tensor([0.6758], device='cuda:0')
percent tensor([0.6998], device='cuda:0')
percent tensor([0.6188], device='cuda:0')
percent tensor([0.6998], device='cuda:0')
percent tensor([0.7464], device='cuda:0')
percent tensor([0.7792], device='cuda:0')
percent tensor([0.2120], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.8649) |  Loss2: (0.4512) | Acc: (84.00%) (108/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.9285) |  Loss2: (0.4512) | Acc: (84.00%) (1195/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.9079) |  Loss2: (0.4512) | Acc: (84.00%) (2277/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.8943) |  Loss2: (0.4511) | Acc: (85.00%) (3385/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.8950) |  Loss2: (0.4511) | Acc: (84.00%) (4460/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.8904) |  Loss2: (0.4511) | Acc: (85.00%) (5558/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.8935) |  Loss2: (0.4511) | Acc: (84.00%) (6631/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.8955) |  Loss2: (0.4510) | Acc: (84.00%) (7719/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.8936) |  Loss2: (0.4510) | Acc: (84.00%) (8809/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.8955) |  Loss2: (0.4510) | Acc: (84.00%) (9894/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.8917) |  Loss2: (0.4510) | Acc: (85.00%) (11009/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.8916) |  Loss2: (0.4510) | Acc: (85.00%) (12099/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.8897) |  Loss2: (0.4509) | Acc: (85.00%) (13189/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.8898) |  Loss2: (0.4509) | Acc: (85.00%) (14271/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.8882) |  Loss2: (0.4509) | Acc: (85.00%) (15362/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.8894) |  Loss2: (0.4508) | Acc: (85.00%) (16432/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.8916) |  Loss2: (0.4508) | Acc: (84.00%) (17490/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.8930) |  Loss2: (0.4508) | Acc: (84.00%) (18558/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.8924) |  Loss2: (0.4507) | Acc: (84.00%) (19649/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.8925) |  Loss2: (0.4507) | Acc: (84.00%) (20727/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.8926) |  Loss2: (0.4507) | Acc: (84.00%) (21808/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.8932) |  Loss2: (0.4507) | Acc: (84.00%) (22892/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.8928) |  Loss2: (0.4507) | Acc: (84.00%) (23992/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.8922) |  Loss2: (0.4506) | Acc: (84.00%) (25084/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.8917) |  Loss2: (0.4506) | Acc: (84.00%) (26171/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.8928) |  Loss2: (0.4506) | Acc: (84.00%) (27236/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.8926) |  Loss2: (0.4506) | Acc: (84.00%) (28335/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.8926) |  Loss2: (0.4506) | Acc: (84.00%) (29439/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.8937) |  Loss2: (0.4506) | Acc: (84.00%) (30503/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.8927) |  Loss2: (0.4506) | Acc: (84.00%) (31607/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.8924) |  Loss2: (0.4506) | Acc: (84.00%) (32702/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.8909) |  Loss2: (0.4506) | Acc: (84.00%) (33813/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.8905) |  Loss2: (0.4505) | Acc: (84.00%) (34903/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.8904) |  Loss2: (0.4505) | Acc: (85.00%) (36013/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.8899) |  Loss2: (0.4505) | Acc: (85.00%) (37107/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.8906) |  Loss2: (0.4505) | Acc: (84.00%) (38180/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.8902) |  Loss2: (0.4505) | Acc: (84.00%) (39270/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.8897) |  Loss2: (0.4505) | Acc: (85.00%) (40373/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.8889) |  Loss2: (0.4505) | Acc: (85.00%) (41473/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.8880) |  Loss2: (0.4505) | Acc: (85.00%) (42541/50000)
# TEST : Loss: (0.4984) | Acc: (83.00%) (8324/10000)
percent tensor([0.6776], device='cuda:0')
percent tensor([0.6765], device='cuda:0')
percent tensor([0.7012], device='cuda:0')
percent tensor([0.6243], device='cuda:0')
percent tensor([0.7029], device='cuda:0')
percent tensor([0.7482], device='cuda:0')
percent tensor([0.7788], device='cuda:0')
percent tensor([0.2078], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (1.0081) |  Loss2: (0.4498) | Acc: (81.00%) (104/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.9080) |  Loss2: (0.4498) | Acc: (85.00%) (1198/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.9176) |  Loss2: (0.4497) | Acc: (84.00%) (2268/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.9059) |  Loss2: (0.4497) | Acc: (84.00%) (3366/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.8993) |  Loss2: (0.4497) | Acc: (85.00%) (4461/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.8917) |  Loss2: (0.4496) | Acc: (85.00%) (5558/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.8896) |  Loss2: (0.4496) | Acc: (85.00%) (6649/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.8872) |  Loss2: (0.4496) | Acc: (85.00%) (7729/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.8878) |  Loss2: (0.4495) | Acc: (85.00%) (8816/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.8855) |  Loss2: (0.4495) | Acc: (85.00%) (9905/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.8843) |  Loss2: (0.4495) | Acc: (85.00%) (11003/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.8858) |  Loss2: (0.4495) | Acc: (85.00%) (12081/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.8858) |  Loss2: (0.4495) | Acc: (85.00%) (13170/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.8830) |  Loss2: (0.4494) | Acc: (85.00%) (14275/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.8826) |  Loss2: (0.4494) | Acc: (85.00%) (15362/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.8824) |  Loss2: (0.4494) | Acc: (85.00%) (16451/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.8861) |  Loss2: (0.4493) | Acc: (85.00%) (17527/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.8857) |  Loss2: (0.4493) | Acc: (85.00%) (18620/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.8853) |  Loss2: (0.4493) | Acc: (85.00%) (19706/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.8835) |  Loss2: (0.4493) | Acc: (85.00%) (20794/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.8810) |  Loss2: (0.4492) | Acc: (85.00%) (21904/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.8800) |  Loss2: (0.4492) | Acc: (85.00%) (23004/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.8814) |  Loss2: (0.4492) | Acc: (85.00%) (24076/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.8823) |  Loss2: (0.4492) | Acc: (85.00%) (25160/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.8838) |  Loss2: (0.4492) | Acc: (85.00%) (26225/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.8842) |  Loss2: (0.4492) | Acc: (84.00%) (27304/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.8841) |  Loss2: (0.4491) | Acc: (84.00%) (28381/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.8824) |  Loss2: (0.4491) | Acc: (85.00%) (29492/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.8818) |  Loss2: (0.4491) | Acc: (85.00%) (30585/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.8818) |  Loss2: (0.4491) | Acc: (85.00%) (31678/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.8815) |  Loss2: (0.4491) | Acc: (85.00%) (32768/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.8819) |  Loss2: (0.4491) | Acc: (85.00%) (33851/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.8809) |  Loss2: (0.4491) | Acc: (85.00%) (34961/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.8809) |  Loss2: (0.4491) | Acc: (85.00%) (36052/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.8796) |  Loss2: (0.4491) | Acc: (85.00%) (37170/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.8798) |  Loss2: (0.4490) | Acc: (85.00%) (38249/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.8797) |  Loss2: (0.4490) | Acc: (85.00%) (39344/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.8801) |  Loss2: (0.4490) | Acc: (85.00%) (40428/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.8799) |  Loss2: (0.4490) | Acc: (85.00%) (41519/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.8801) |  Loss2: (0.4490) | Acc: (85.00%) (42557/50000)
# TEST : Loss: (0.4993) | Acc: (83.00%) (8333/10000)
percent tensor([0.6787], device='cuda:0')
percent tensor([0.6789], device='cuda:0')
percent tensor([0.7020], device='cuda:0')
percent tensor([0.6282], device='cuda:0')
percent tensor([0.7033], device='cuda:0')
percent tensor([0.7499], device='cuda:0')
percent tensor([0.7786], device='cuda:0')
percent tensor([0.2029], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.4234) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.4050) |  Loss2: (0.0000) | Acc: (85.00%) (1203/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.4065) |  Loss2: (0.0000) | Acc: (86.00%) (2312/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (85.00%) (3390/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.4281) |  Loss2: (0.0000) | Acc: (85.00%) (4476/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.4262) |  Loss2: (0.0000) | Acc: (85.00%) (5570/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (84.00%) (6635/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.4341) |  Loss2: (0.0000) | Acc: (84.00%) (7711/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (84.00%) (8787/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (9872/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (10953/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (84.00%) (12049/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (84.00%) (13135/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (84.00%) (14237/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (84.00%) (15331/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (16431/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (17521/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (18616/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (19711/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.4330) |  Loss2: (0.0000) | Acc: (84.00%) (20767/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.4327) |  Loss2: (0.0000) | Acc: (85.00%) (21869/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (22954/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.4322) |  Loss2: (0.0000) | Acc: (85.00%) (24050/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (25145/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (26253/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (27345/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (28435/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.4304) |  Loss2: (0.0000) | Acc: (85.00%) (29521/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.4313) |  Loss2: (0.0000) | Acc: (85.00%) (30600/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (85.00%) (31695/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (32793/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.4302) |  Loss2: (0.0000) | Acc: (85.00%) (33874/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (34965/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (36064/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (37170/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.4278) |  Loss2: (0.0000) | Acc: (85.00%) (38276/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.4276) |  Loss2: (0.0000) | Acc: (85.00%) (39363/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.4272) |  Loss2: (0.0000) | Acc: (85.00%) (40450/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.4270) |  Loss2: (0.0000) | Acc: (85.00%) (41543/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.4271) |  Loss2: (0.0000) | Acc: (85.00%) (42603/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_070.pth.tar'
# TEST : Loss: (0.6394) | Acc: (78.00%) (7875/10000)
percent tensor([0.6787], device='cuda:0')
percent tensor([0.6787], device='cuda:0')
percent tensor([0.7018], device='cuda:0')
percent tensor([0.6280], device='cuda:0')
percent tensor([0.7032], device='cuda:0')
percent tensor([0.7499], device='cuda:0')
percent tensor([0.7786], device='cuda:0')
percent tensor([0.2029], device='cuda:0')
Epoch: 71 | Batch_idx: 0 |  Loss: (0.4467) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3879) |  Loss2: (0.0000) | Acc: (86.00%) (1217/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (2335/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3930) |  Loss2: (0.0000) | Acc: (86.00%) (3448/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (4546/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (5654/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (6745/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (7834/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (8919/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (10039/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (11132/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (12235/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.4049) |  Loss2: (0.0000) | Acc: (86.00%) (13320/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (14427/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.4083) |  Loss2: (0.0000) | Acc: (85.00%) (15508/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (16601/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.4072) |  Loss2: (0.0000) | Acc: (85.00%) (17696/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.4071) |  Loss2: (0.0000) | Acc: (85.00%) (18808/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.4081) |  Loss2: (0.0000) | Acc: (85.00%) (19904/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (20986/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.4096) |  Loss2: (0.0000) | Acc: (85.00%) (22094/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (23171/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (24289/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (25387/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (26490/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (27582/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (28680/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (29779/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (30885/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (31958/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.4125) |  Loss2: (0.0000) | Acc: (85.00%) (33079/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (34195/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (35278/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.4117) |  Loss2: (0.0000) | Acc: (85.00%) (36374/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (37464/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (38561/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (39669/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (40751/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (41835/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.4128) |  Loss2: (0.0000) | Acc: (85.00%) (42881/50000)
# TEST : Loss: (0.4919) | Acc: (83.00%) (8350/10000)
percent tensor([0.6786], device='cuda:0')
percent tensor([0.6786], device='cuda:0')
percent tensor([0.7018], device='cuda:0')
percent tensor([0.6280], device='cuda:0')
percent tensor([0.7031], device='cuda:0')
percent tensor([0.7498], device='cuda:0')
percent tensor([0.7785], device='cuda:0')
percent tensor([0.2030], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.4057) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.3869) |  Loss2: (0.0000) | Acc: (87.00%) (2350/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3943) |  Loss2: (0.0000) | Acc: (87.00%) (3457/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (86.00%) (4549/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (5666/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.4042) |  Loss2: (0.0000) | Acc: (86.00%) (6752/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (7868/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (86.00%) (8963/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (10064/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (11176/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (12273/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (13376/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (14475/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3985) |  Loss2: (0.0000) | Acc: (86.00%) (15601/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (16700/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (17781/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (18891/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.4002) |  Loss2: (0.0000) | Acc: (86.00%) (19995/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (86.00%) (21104/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (22219/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (23338/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (24442/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (25542/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3999) |  Loss2: (0.0000) | Acc: (86.00%) (26635/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3998) |  Loss2: (0.0000) | Acc: (86.00%) (27732/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.4004) |  Loss2: (0.0000) | Acc: (86.00%) (28835/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3997) |  Loss2: (0.0000) | Acc: (86.00%) (29945/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3998) |  Loss2: (0.0000) | Acc: (86.00%) (31044/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.4002) |  Loss2: (0.0000) | Acc: (86.00%) (32148/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (33239/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (34350/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (35452/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (36549/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (37654/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.4031) |  Loss2: (0.0000) | Acc: (86.00%) (38762/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (86.00%) (39846/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.4029) |  Loss2: (0.0000) | Acc: (86.00%) (40960/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (42068/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (43125/50000)
# TEST : Loss: (0.5716) | Acc: (81.00%) (8149/10000)
percent tensor([0.6786], device='cuda:0')
percent tensor([0.6786], device='cuda:0')
percent tensor([0.7017], device='cuda:0')
percent tensor([0.6279], device='cuda:0')
percent tensor([0.7030], device='cuda:0')
percent tensor([0.7497], device='cuda:0')
percent tensor([0.7785], device='cuda:0')
percent tensor([0.2030], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (1223/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.4062) |  Loss2: (0.0000) | Acc: (86.00%) (2323/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.4099) |  Loss2: (0.0000) | Acc: (85.00%) (3409/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (4530/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (5664/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.3891) |  Loss2: (0.0000) | Acc: (86.00%) (6766/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (7869/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (8971/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.3918) |  Loss2: (0.0000) | Acc: (86.00%) (10077/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (11176/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (12284/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3943) |  Loss2: (0.0000) | Acc: (86.00%) (13393/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3940) |  Loss2: (0.0000) | Acc: (86.00%) (14495/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (15596/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (16717/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (17810/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (18915/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (20013/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3971) |  Loss2: (0.0000) | Acc: (86.00%) (21096/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (22214/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (23311/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (24430/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3948) |  Loss2: (0.0000) | Acc: (86.00%) (25538/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3940) |  Loss2: (0.0000) | Acc: (86.00%) (26641/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3949) |  Loss2: (0.0000) | Acc: (86.00%) (27746/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (28845/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (29943/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (31029/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3965) |  Loss2: (0.0000) | Acc: (86.00%) (32127/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (33242/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3970) |  Loss2: (0.0000) | Acc: (86.00%) (34342/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (35450/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (36548/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (37651/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (38772/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (39870/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (40982/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3979) |  Loss2: (0.0000) | Acc: (86.00%) (42092/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3971) |  Loss2: (0.0000) | Acc: (86.00%) (43166/50000)
# TEST : Loss: (0.5522) | Acc: (81.00%) (8180/10000)
percent tensor([0.6785], device='cuda:0')
percent tensor([0.6785], device='cuda:0')
percent tensor([0.7016], device='cuda:0')
percent tensor([0.6279], device='cuda:0')
percent tensor([0.7030], device='cuda:0')
percent tensor([0.7497], device='cuda:0')
percent tensor([0.7784], device='cuda:0')
percent tensor([0.2031], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.4088) |  Loss2: (0.0000) | Acc: (85.00%) (1204/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.3871) |  Loss2: (0.0000) | Acc: (86.00%) (2331/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (87.00%) (3458/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (87.00%) (4573/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (5687/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.3773) |  Loss2: (0.0000) | Acc: (86.00%) (6789/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.3767) |  Loss2: (0.0000) | Acc: (86.00%) (7898/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (86.00%) (9002/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (86.00%) (10120/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (86.00%) (11228/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (86.00%) (12335/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (13436/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (14538/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (15644/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (16751/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (17850/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (18961/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (20075/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (21198/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (22323/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (23443/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (24549/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (25641/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (26765/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (27877/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (29006/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (30134/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (31216/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (32326/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.3854) |  Loss2: (0.0000) | Acc: (86.00%) (33438/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.3850) |  Loss2: (0.0000) | Acc: (86.00%) (34558/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (35691/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (36794/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (37898/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3831) |  Loss2: (0.0000) | Acc: (86.00%) (39017/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (40128/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (41230/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (42341/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (43426/50000)
# TEST : Loss: (0.5176) | Acc: (83.00%) (8343/10000)
percent tensor([0.6784], device='cuda:0')
percent tensor([0.6784], device='cuda:0')
percent tensor([0.7016], device='cuda:0')
percent tensor([0.6278], device='cuda:0')
percent tensor([0.7029], device='cuda:0')
percent tensor([0.7496], device='cuda:0')
percent tensor([0.7783], device='cuda:0')
percent tensor([0.2032], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.9002) |  Loss2: (0.4490) | Acc: (85.00%) (109/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.8287) |  Loss2: (0.4489) | Acc: (86.00%) (1224/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.8622) |  Loss2: (0.4488) | Acc: (85.00%) (2307/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.8811) |  Loss2: (0.4486) | Acc: (85.00%) (3374/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.9014) |  Loss2: (0.4484) | Acc: (84.00%) (4428/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.9012) |  Loss2: (0.4483) | Acc: (84.00%) (5506/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.9022) |  Loss2: (0.4481) | Acc: (84.00%) (6584/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.9018) |  Loss2: (0.4479) | Acc: (84.00%) (7662/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.8979) |  Loss2: (0.4477) | Acc: (84.00%) (8759/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.9016) |  Loss2: (0.4476) | Acc: (84.00%) (9823/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.8988) |  Loss2: (0.4474) | Acc: (84.00%) (10910/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.8968) |  Loss2: (0.4473) | Acc: (84.00%) (12002/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.8946) |  Loss2: (0.4472) | Acc: (84.00%) (13094/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.8954) |  Loss2: (0.4470) | Acc: (84.00%) (14163/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.8942) |  Loss2: (0.4469) | Acc: (84.00%) (15239/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.8909) |  Loss2: (0.4468) | Acc: (84.00%) (16340/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.8925) |  Loss2: (0.4466) | Acc: (84.00%) (17420/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.8902) |  Loss2: (0.4465) | Acc: (84.00%) (18525/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.8905) |  Loss2: (0.4464) | Acc: (84.00%) (19603/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.8897) |  Loss2: (0.4462) | Acc: (84.00%) (20686/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.8866) |  Loss2: (0.4461) | Acc: (84.00%) (21807/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.8859) |  Loss2: (0.4459) | Acc: (84.00%) (22879/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.8848) |  Loss2: (0.4458) | Acc: (84.00%) (23963/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.8847) |  Loss2: (0.4457) | Acc: (84.00%) (25054/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.8833) |  Loss2: (0.4456) | Acc: (84.00%) (26159/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.8825) |  Loss2: (0.4454) | Acc: (84.00%) (27244/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.8822) |  Loss2: (0.4453) | Acc: (84.00%) (28326/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.8814) |  Loss2: (0.4452) | Acc: (84.00%) (29428/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.8793) |  Loss2: (0.4451) | Acc: (84.00%) (30551/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.8789) |  Loss2: (0.4450) | Acc: (84.00%) (31645/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.8775) |  Loss2: (0.4449) | Acc: (84.00%) (32746/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.8765) |  Loss2: (0.4447) | Acc: (85.00%) (33853/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.8750) |  Loss2: (0.4446) | Acc: (85.00%) (34955/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.8745) |  Loss2: (0.4445) | Acc: (85.00%) (36054/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.8736) |  Loss2: (0.4444) | Acc: (85.00%) (37157/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.8730) |  Loss2: (0.4443) | Acc: (85.00%) (38254/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.8720) |  Loss2: (0.4442) | Acc: (85.00%) (39367/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.8718) |  Loss2: (0.4441) | Acc: (85.00%) (40454/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.8711) |  Loss2: (0.4441) | Acc: (85.00%) (41565/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.8701) |  Loss2: (0.4440) | Acc: (85.00%) (42633/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_075.pth.tar'
# TEST : Loss: (0.5090) | Acc: (83.00%) (8346/10000)
percent tensor([0.6801], device='cuda:0')
percent tensor([0.6866], device='cuda:0')
percent tensor([0.7165], device='cuda:0')
percent tensor([0.6468], device='cuda:0')
percent tensor([0.7008], device='cuda:0')
percent tensor([0.7542], device='cuda:0')
percent tensor([0.7916], device='cuda:0')
percent tensor([0.2057], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.7076) |  Loss2: (0.4406) | Acc: (90.00%) (116/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.8206) |  Loss2: (0.4405) | Acc: (87.00%) (1228/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.8093) |  Loss2: (0.4405) | Acc: (87.00%) (2349/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.8137) |  Loss2: (0.4404) | Acc: (87.00%) (3471/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.8190) |  Loss2: (0.4403) | Acc: (87.00%) (4569/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.8232) |  Loss2: (0.4402) | Acc: (87.00%) (5683/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.8266) |  Loss2: (0.4402) | Acc: (86.00%) (6779/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.8250) |  Loss2: (0.4401) | Acc: (86.00%) (7883/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.8228) |  Loss2: (0.4401) | Acc: (86.00%) (9001/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.8241) |  Loss2: (0.4400) | Acc: (86.00%) (10106/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.8254) |  Loss2: (0.4400) | Acc: (86.00%) (11202/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.8295) |  Loss2: (0.4400) | Acc: (86.00%) (12287/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.8268) |  Loss2: (0.4399) | Acc: (86.00%) (13420/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.8302) |  Loss2: (0.4399) | Acc: (86.00%) (14506/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.8322) |  Loss2: (0.4398) | Acc: (86.00%) (15611/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.8308) |  Loss2: (0.4398) | Acc: (86.00%) (16727/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.8331) |  Loss2: (0.4397) | Acc: (86.00%) (17816/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.8349) |  Loss2: (0.4397) | Acc: (86.00%) (18915/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.8356) |  Loss2: (0.4396) | Acc: (86.00%) (20013/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.8358) |  Loss2: (0.4396) | Acc: (86.00%) (21109/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.8359) |  Loss2: (0.4395) | Acc: (86.00%) (22204/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.8360) |  Loss2: (0.4395) | Acc: (86.00%) (23313/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.8363) |  Loss2: (0.4394) | Acc: (86.00%) (24412/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.8356) |  Loss2: (0.4394) | Acc: (86.00%) (25520/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.8355) |  Loss2: (0.4393) | Acc: (86.00%) (26631/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.8364) |  Loss2: (0.4392) | Acc: (86.00%) (27722/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.8363) |  Loss2: (0.4392) | Acc: (86.00%) (28833/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.8364) |  Loss2: (0.4391) | Acc: (86.00%) (29932/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.8356) |  Loss2: (0.4390) | Acc: (86.00%) (31043/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.8349) |  Loss2: (0.4389) | Acc: (86.00%) (32146/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.8369) |  Loss2: (0.4389) | Acc: (86.00%) (33221/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.8371) |  Loss2: (0.4388) | Acc: (86.00%) (34322/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.8370) |  Loss2: (0.4388) | Acc: (86.00%) (35436/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.8375) |  Loss2: (0.4387) | Acc: (86.00%) (36528/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.8373) |  Loss2: (0.4386) | Acc: (86.00%) (37635/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.8370) |  Loss2: (0.4386) | Acc: (86.00%) (38734/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.8374) |  Loss2: (0.4385) | Acc: (86.00%) (39828/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.8362) |  Loss2: (0.4385) | Acc: (86.00%) (40944/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.8360) |  Loss2: (0.4384) | Acc: (86.00%) (42060/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.8355) |  Loss2: (0.4384) | Acc: (86.00%) (43129/50000)
# TEST : Loss: (0.4885) | Acc: (84.00%) (8425/10000)
percent tensor([0.6840], device='cuda:0')
percent tensor([0.6879], device='cuda:0')
percent tensor([0.7222], device='cuda:0')
percent tensor([0.6495], device='cuda:0')
percent tensor([0.7058], device='cuda:0')
percent tensor([0.7606], device='cuda:0')
percent tensor([0.7967], device='cuda:0')
percent tensor([0.2045], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.7556) |  Loss2: (0.4362) | Acc: (88.00%) (113/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.8546) |  Loss2: (0.4362) | Acc: (85.00%) (1206/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.8465) |  Loss2: (0.4361) | Acc: (86.00%) (2314/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.8373) |  Loss2: (0.4361) | Acc: (86.00%) (3424/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.8290) |  Loss2: (0.4361) | Acc: (86.00%) (4534/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.8311) |  Loss2: (0.4361) | Acc: (86.00%) (5638/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.8318) |  Loss2: (0.4361) | Acc: (86.00%) (6743/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.8287) |  Loss2: (0.4361) | Acc: (86.00%) (7859/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.8259) |  Loss2: (0.4361) | Acc: (86.00%) (8975/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.8275) |  Loss2: (0.4360) | Acc: (86.00%) (10077/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.8293) |  Loss2: (0.4360) | Acc: (86.00%) (11190/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.8279) |  Loss2: (0.4360) | Acc: (86.00%) (12298/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.8275) |  Loss2: (0.4359) | Acc: (86.00%) (13409/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.8245) |  Loss2: (0.4359) | Acc: (86.00%) (14539/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.8226) |  Loss2: (0.4359) | Acc: (86.00%) (15658/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.8238) |  Loss2: (0.4358) | Acc: (86.00%) (16774/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.8241) |  Loss2: (0.4357) | Acc: (86.00%) (17887/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.8243) |  Loss2: (0.4357) | Acc: (86.00%) (18998/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.8258) |  Loss2: (0.4356) | Acc: (86.00%) (20082/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.8241) |  Loss2: (0.4356) | Acc: (86.00%) (21205/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.8238) |  Loss2: (0.4355) | Acc: (86.00%) (22323/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.8243) |  Loss2: (0.4355) | Acc: (86.00%) (23422/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.8237) |  Loss2: (0.4354) | Acc: (86.00%) (24533/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.8230) |  Loss2: (0.4354) | Acc: (86.00%) (25644/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.8236) |  Loss2: (0.4353) | Acc: (86.00%) (26747/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.8218) |  Loss2: (0.4353) | Acc: (86.00%) (27872/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.8219) |  Loss2: (0.4352) | Acc: (86.00%) (28983/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.8218) |  Loss2: (0.4352) | Acc: (86.00%) (30102/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.8222) |  Loss2: (0.4352) | Acc: (86.00%) (31205/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.8218) |  Loss2: (0.4351) | Acc: (86.00%) (32328/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.8209) |  Loss2: (0.4351) | Acc: (86.00%) (33458/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.8196) |  Loss2: (0.4350) | Acc: (86.00%) (34587/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.8196) |  Loss2: (0.4350) | Acc: (86.00%) (35703/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.8207) |  Loss2: (0.4349) | Acc: (86.00%) (36800/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.8209) |  Loss2: (0.4349) | Acc: (86.00%) (37902/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.8201) |  Loss2: (0.4349) | Acc: (86.00%) (39030/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.8198) |  Loss2: (0.4348) | Acc: (86.00%) (40158/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.8203) |  Loss2: (0.4348) | Acc: (86.00%) (41264/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.8206) |  Loss2: (0.4348) | Acc: (86.00%) (42374/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.8198) |  Loss2: (0.4347) | Acc: (86.00%) (43463/50000)
# TEST : Loss: (0.4796) | Acc: (84.00%) (8435/10000)
percent tensor([0.6883], device='cuda:0')
percent tensor([0.6886], device='cuda:0')
percent tensor([0.7249], device='cuda:0')
percent tensor([0.6504], device='cuda:0')
percent tensor([0.7112], device='cuda:0')
percent tensor([0.7644], device='cuda:0')
percent tensor([0.7991], device='cuda:0')
percent tensor([0.2015], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.7780) |  Loss2: (0.4334) | Acc: (89.00%) (114/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.8462) |  Loss2: (0.4335) | Acc: (86.00%) (1218/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.8465) |  Loss2: (0.4335) | Acc: (86.00%) (2316/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.8285) |  Loss2: (0.4335) | Acc: (86.00%) (3432/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.8276) |  Loss2: (0.4335) | Acc: (86.00%) (4542/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.8282) |  Loss2: (0.4335) | Acc: (86.00%) (5652/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.8245) |  Loss2: (0.4335) | Acc: (86.00%) (6771/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.8240) |  Loss2: (0.4335) | Acc: (86.00%) (7868/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.8238) |  Loss2: (0.4334) | Acc: (86.00%) (8971/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.8202) |  Loss2: (0.4334) | Acc: (86.00%) (10093/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.8196) |  Loss2: (0.4334) | Acc: (86.00%) (11208/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.8186) |  Loss2: (0.4333) | Acc: (86.00%) (12325/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.8183) |  Loss2: (0.4333) | Acc: (86.00%) (13437/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.8170) |  Loss2: (0.4333) | Acc: (86.00%) (14561/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.8184) |  Loss2: (0.4333) | Acc: (86.00%) (15666/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.8173) |  Loss2: (0.4333) | Acc: (86.00%) (16789/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.8182) |  Loss2: (0.4332) | Acc: (86.00%) (17898/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.8166) |  Loss2: (0.4332) | Acc: (86.00%) (19026/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.8172) |  Loss2: (0.4332) | Acc: (86.00%) (20116/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.8187) |  Loss2: (0.4332) | Acc: (86.00%) (21215/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.8193) |  Loss2: (0.4331) | Acc: (86.00%) (22330/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.8180) |  Loss2: (0.4331) | Acc: (86.00%) (23457/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.8167) |  Loss2: (0.4331) | Acc: (86.00%) (24575/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.8171) |  Loss2: (0.4330) | Acc: (86.00%) (25686/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.8171) |  Loss2: (0.4330) | Acc: (86.00%) (26794/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.8187) |  Loss2: (0.4330) | Acc: (86.00%) (27895/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.8176) |  Loss2: (0.4329) | Acc: (86.00%) (29023/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.8180) |  Loss2: (0.4329) | Acc: (86.00%) (30133/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.8178) |  Loss2: (0.4329) | Acc: (86.00%) (31239/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.8173) |  Loss2: (0.4328) | Acc: (86.00%) (32345/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.8161) |  Loss2: (0.4328) | Acc: (86.00%) (33473/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.8158) |  Loss2: (0.4328) | Acc: (86.00%) (34602/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.8141) |  Loss2: (0.4327) | Acc: (87.00%) (35748/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.8136) |  Loss2: (0.4327) | Acc: (87.00%) (36867/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.8134) |  Loss2: (0.4327) | Acc: (87.00%) (37980/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.8140) |  Loss2: (0.4327) | Acc: (86.00%) (39073/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.8135) |  Loss2: (0.4326) | Acc: (86.00%) (40199/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.8139) |  Loss2: (0.4326) | Acc: (86.00%) (41312/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.8140) |  Loss2: (0.4326) | Acc: (86.00%) (42428/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.8138) |  Loss2: (0.4326) | Acc: (87.00%) (43500/50000)
# TEST : Loss: (0.4681) | Acc: (84.00%) (8468/10000)
percent tensor([0.6899], device='cuda:0')
percent tensor([0.6886], device='cuda:0')
percent tensor([0.7266], device='cuda:0')
percent tensor([0.6512], device='cuda:0')
percent tensor([0.7160], device='cuda:0')
percent tensor([0.7669], device='cuda:0')
percent tensor([0.8008], device='cuda:0')
percent tensor([0.1979], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.7695) |  Loss2: (0.4317) | Acc: (89.00%) (114/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.7955) |  Loss2: (0.4317) | Acc: (87.00%) (1239/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.7866) |  Loss2: (0.4316) | Acc: (88.00%) (2375/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.7968) |  Loss2: (0.4316) | Acc: (87.00%) (3484/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.8083) |  Loss2: (0.4316) | Acc: (87.00%) (4582/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.8046) |  Loss2: (0.4315) | Acc: (87.00%) (5717/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.8014) |  Loss2: (0.4315) | Acc: (87.00%) (6851/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.8014) |  Loss2: (0.4315) | Acc: (87.00%) (7979/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.8020) |  Loss2: (0.4315) | Acc: (87.00%) (9099/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.8030) |  Loss2: (0.4315) | Acc: (87.00%) (10220/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.8049) |  Loss2: (0.4315) | Acc: (87.00%) (11330/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.8063) |  Loss2: (0.4315) | Acc: (87.00%) (12438/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.8057) |  Loss2: (0.4315) | Acc: (87.00%) (13556/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.8064) |  Loss2: (0.4315) | Acc: (87.00%) (14664/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.8063) |  Loss2: (0.4315) | Acc: (87.00%) (15768/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.8060) |  Loss2: (0.4315) | Acc: (87.00%) (16890/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.8060) |  Loss2: (0.4315) | Acc: (87.00%) (18006/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.8038) |  Loss2: (0.4315) | Acc: (87.00%) (19130/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.8034) |  Loss2: (0.4315) | Acc: (87.00%) (20255/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.8022) |  Loss2: (0.4315) | Acc: (87.00%) (21381/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.8032) |  Loss2: (0.4315) | Acc: (87.00%) (22507/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.8061) |  Loss2: (0.4315) | Acc: (87.00%) (23603/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.8067) |  Loss2: (0.4315) | Acc: (87.00%) (24700/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.8098) |  Loss2: (0.4315) | Acc: (87.00%) (25777/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.8098) |  Loss2: (0.4315) | Acc: (87.00%) (26895/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.8080) |  Loss2: (0.4315) | Acc: (87.00%) (28039/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.8084) |  Loss2: (0.4315) | Acc: (87.00%) (29147/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.8078) |  Loss2: (0.4315) | Acc: (87.00%) (30265/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.8081) |  Loss2: (0.4315) | Acc: (87.00%) (31381/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.8074) |  Loss2: (0.4315) | Acc: (87.00%) (32506/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.8070) |  Loss2: (0.4315) | Acc: (87.00%) (33630/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.8062) |  Loss2: (0.4314) | Acc: (87.00%) (34755/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.8054) |  Loss2: (0.4314) | Acc: (87.00%) (35881/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.8050) |  Loss2: (0.4314) | Acc: (87.00%) (37002/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.8052) |  Loss2: (0.4314) | Acc: (87.00%) (38115/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.8052) |  Loss2: (0.4314) | Acc: (87.00%) (39225/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.8046) |  Loss2: (0.4314) | Acc: (87.00%) (40351/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.8056) |  Loss2: (0.4314) | Acc: (87.00%) (41457/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.8053) |  Loss2: (0.4314) | Acc: (87.00%) (42577/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.8057) |  Loss2: (0.4314) | Acc: (87.00%) (43649/50000)
# TEST : Loss: (0.4641) | Acc: (84.00%) (8494/10000)
percent tensor([0.6895], device='cuda:0')
percent tensor([0.6877], device='cuda:0')
percent tensor([0.7290], device='cuda:0')
percent tensor([0.6511], device='cuda:0')
percent tensor([0.7183], device='cuda:0')
percent tensor([0.7681], device='cuda:0')
percent tensor([0.8014], device='cuda:0')
percent tensor([0.1937], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.4045) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (1236/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (2341/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.3751) |  Loss2: (0.0000) | Acc: (87.00%) (3457/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (87.00%) (4568/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (6753/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (7872/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (8971/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (10082/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (11189/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (12304/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (13434/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.3834) |  Loss2: (0.0000) | Acc: (86.00%) (14552/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (15658/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (16768/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.3820) |  Loss2: (0.0000) | Acc: (86.00%) (17881/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (18971/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (20063/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (21176/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (22308/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.3839) |  Loss2: (0.0000) | Acc: (86.00%) (23434/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (24554/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.3838) |  Loss2: (0.0000) | Acc: (86.00%) (25661/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (26758/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (27864/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (28964/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (30083/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (31178/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (32315/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (33407/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (34525/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (35619/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (36743/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (37847/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (38954/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (40076/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (41170/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.3888) |  Loss2: (0.0000) | Acc: (86.00%) (42270/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (43357/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_080.pth.tar'
# TEST : Loss: (0.5632) | Acc: (81.00%) (8168/10000)
percent tensor([0.6897], device='cuda:0')
percent tensor([0.6879], device='cuda:0')
percent tensor([0.7290], device='cuda:0')
percent tensor([0.6511], device='cuda:0')
percent tensor([0.7180], device='cuda:0')
percent tensor([0.7678], device='cuda:0')
percent tensor([0.8013], device='cuda:0')
percent tensor([0.1938], device='cuda:0')
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (87.00%) (2347/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (87.00%) (3463/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (86.00%) (4548/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.3818) |  Loss2: (0.0000) | Acc: (86.00%) (5666/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (6748/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (7881/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (8997/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.3851) |  Loss2: (0.0000) | Acc: (86.00%) (10101/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (11228/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (12347/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (13468/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (14586/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (87.00%) (15717/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (87.00%) (16822/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (87.00%) (17934/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.3790) |  Loss2: (0.0000) | Acc: (87.00%) (19057/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.3798) |  Loss2: (0.0000) | Acc: (87.00%) (20159/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (21255/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (22377/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.3775) |  Loss2: (0.0000) | Acc: (87.00%) (23499/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (24580/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (25677/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (86.00%) (26796/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (27904/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (29024/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (30132/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (31265/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (32373/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.3828) |  Loss2: (0.0000) | Acc: (86.00%) (33474/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (34581/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (35696/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.3830) |  Loss2: (0.0000) | Acc: (86.00%) (36809/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.3825) |  Loss2: (0.0000) | Acc: (86.00%) (37927/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.3813) |  Loss2: (0.0000) | Acc: (86.00%) (39056/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (40163/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (41289/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (42408/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (86.00%) (43475/50000)
# TEST : Loss: (0.4987) | Acc: (83.00%) (8341/10000)
percent tensor([0.6896], device='cuda:0')
percent tensor([0.6879], device='cuda:0')
percent tensor([0.7289], device='cuda:0')
percent tensor([0.6510], device='cuda:0')
percent tensor([0.7179], device='cuda:0')
percent tensor([0.7677], device='cuda:0')
percent tensor([0.8012], device='cuda:0')
percent tensor([0.1938], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (1223/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (2345/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.3844) |  Loss2: (0.0000) | Acc: (87.00%) (3464/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (5706/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (6827/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (7964/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (9067/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (10176/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.3685) |  Loss2: (0.0000) | Acc: (87.00%) (11291/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (12416/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (13550/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.3679) |  Loss2: (0.0000) | Acc: (87.00%) (14661/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (15789/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (16895/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (17995/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (87.00%) (19110/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (20240/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (21368/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (22492/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (23603/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.3693) |  Loss2: (0.0000) | Acc: (87.00%) (24725/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (25831/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (26942/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (28062/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (29190/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (30323/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (31439/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (32567/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (33685/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (34805/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (35903/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (36997/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (38122/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (39239/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (40363/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (41480/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (87.00%) (42575/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (43653/50000)
# TEST : Loss: (0.5107) | Acc: (83.00%) (8347/10000)
percent tensor([0.6896], device='cuda:0')
percent tensor([0.6878], device='cuda:0')
percent tensor([0.7288], device='cuda:0')
percent tensor([0.6510], device='cuda:0')
percent tensor([0.7178], device='cuda:0')
percent tensor([0.7676], device='cuda:0')
percent tensor([0.8011], device='cuda:0')
percent tensor([0.1939], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.4882) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (2355/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (86.00%) (3451/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (5697/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (6828/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (7948/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (9059/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (10185/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.3659) |  Loss2: (0.0000) | Acc: (87.00%) (11301/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.3671) |  Loss2: (0.0000) | Acc: (87.00%) (12406/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (13524/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (14646/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (15764/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (16865/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.3664) |  Loss2: (0.0000) | Acc: (87.00%) (17979/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (19098/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (20227/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (21324/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (22442/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (23570/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (24689/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (25816/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (26941/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (28068/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (29193/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (30317/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (31436/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (32541/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (33663/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (34775/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (35882/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (37019/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (38150/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.3661) |  Loss2: (0.0000) | Acc: (87.00%) (39279/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (40399/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (41514/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (42615/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (43699/50000)
# TEST : Loss: (0.5070) | Acc: (82.00%) (8274/10000)
percent tensor([0.6895], device='cuda:0')
percent tensor([0.6877], device='cuda:0')
percent tensor([0.7288], device='cuda:0')
percent tensor([0.6509], device='cuda:0')
percent tensor([0.7178], device='cuda:0')
percent tensor([0.7676], device='cuda:0')
percent tensor([0.8011], device='cuda:0')
percent tensor([0.1940], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (2376/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (4651/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (5773/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (6900/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (8027/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (88.00%) (9130/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (10236/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (11353/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (12474/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (13588/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (14696/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (15850/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.3569) |  Loss2: (0.0000) | Acc: (87.00%) (16975/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (18094/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (19200/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (20300/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (21412/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (22534/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (23678/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (24803/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (25939/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.3576) |  Loss2: (0.0000) | Acc: (87.00%) (27067/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (28200/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (87.00%) (29329/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.3567) |  Loss2: (0.0000) | Acc: (87.00%) (30446/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (31578/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (87.00%) (32704/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (33824/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (34942/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.3571) |  Loss2: (0.0000) | Acc: (87.00%) (36059/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (37179/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (38327/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (39459/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (40574/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (41700/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (42828/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (43917/50000)
# TEST : Loss: (0.5236) | Acc: (82.00%) (8292/10000)
percent tensor([0.6894], device='cuda:0')
percent tensor([0.6877], device='cuda:0')
percent tensor([0.7287], device='cuda:0')
percent tensor([0.6509], device='cuda:0')
percent tensor([0.7177], device='cuda:0')
percent tensor([0.7675], device='cuda:0')
percent tensor([0.8010], device='cuda:0')
percent tensor([0.1941], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.8673) |  Loss2: (0.4314) | Acc: (85.00%) (110/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.8261) |  Loss2: (0.4313) | Acc: (86.00%) (1211/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.8430) |  Loss2: (0.4312) | Acc: (85.00%) (2297/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.8553) |  Loss2: (0.4311) | Acc: (84.00%) (3365/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.8685) |  Loss2: (0.4309) | Acc: (84.00%) (4424/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.8650) |  Loss2: (0.4307) | Acc: (84.00%) (5519/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.8707) |  Loss2: (0.4304) | Acc: (84.00%) (6586/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.8696) |  Loss2: (0.4302) | Acc: (84.00%) (7676/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.8704) |  Loss2: (0.4300) | Acc: (84.00%) (8768/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.8679) |  Loss2: (0.4298) | Acc: (84.00%) (9867/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.8665) |  Loss2: (0.4296) | Acc: (84.00%) (10956/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.8687) |  Loss2: (0.4294) | Acc: (84.00%) (12028/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.8663) |  Loss2: (0.4291) | Acc: (84.00%) (13120/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.8644) |  Loss2: (0.4289) | Acc: (84.00%) (14230/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.8625) |  Loss2: (0.4287) | Acc: (84.00%) (15340/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.8590) |  Loss2: (0.4285) | Acc: (85.00%) (16446/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.8574) |  Loss2: (0.4283) | Acc: (85.00%) (17541/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.8552) |  Loss2: (0.4282) | Acc: (85.00%) (18647/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.8521) |  Loss2: (0.4280) | Acc: (85.00%) (19765/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.8519) |  Loss2: (0.4278) | Acc: (85.00%) (20849/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.8490) |  Loss2: (0.4276) | Acc: (85.00%) (21961/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.8481) |  Loss2: (0.4274) | Acc: (85.00%) (23061/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.8468) |  Loss2: (0.4273) | Acc: (85.00%) (24163/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.8450) |  Loss2: (0.4271) | Acc: (85.00%) (25283/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.8427) |  Loss2: (0.4269) | Acc: (85.00%) (26407/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.8414) |  Loss2: (0.4268) | Acc: (85.00%) (27509/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.8397) |  Loss2: (0.4266) | Acc: (85.00%) (28620/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.8399) |  Loss2: (0.4264) | Acc: (85.00%) (29717/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.8392) |  Loss2: (0.4263) | Acc: (85.00%) (30827/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.8396) |  Loss2: (0.4261) | Acc: (85.00%) (31924/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.8371) |  Loss2: (0.4260) | Acc: (85.00%) (33036/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.8363) |  Loss2: (0.4258) | Acc: (85.00%) (34145/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.8348) |  Loss2: (0.4257) | Acc: (85.00%) (35252/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.8333) |  Loss2: (0.4255) | Acc: (85.00%) (36374/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.8329) |  Loss2: (0.4254) | Acc: (85.00%) (37470/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.8326) |  Loss2: (0.4252) | Acc: (85.00%) (38567/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.8317) |  Loss2: (0.4251) | Acc: (85.00%) (39681/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.8313) |  Loss2: (0.4250) | Acc: (85.00%) (40776/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.8304) |  Loss2: (0.4248) | Acc: (85.00%) (41878/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.8294) |  Loss2: (0.4247) | Acc: (85.00%) (42950/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_085.pth.tar'
# TEST : Loss: (0.4834) | Acc: (83.00%) (8391/10000)
percent tensor([0.7032], device='cuda:0')
percent tensor([0.7044], device='cuda:0')
percent tensor([0.7427], device='cuda:0')
percent tensor([0.6552], device='cuda:0')
percent tensor([0.7286], device='cuda:0')
percent tensor([0.7742], device='cuda:0')
percent tensor([0.8138], device='cuda:0')
percent tensor([0.1961], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.8082) |  Loss2: (0.4197) | Acc: (85.00%) (110/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.7485) |  Loss2: (0.4196) | Acc: (89.00%) (1257/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.7702) |  Loss2: (0.4195) | Acc: (87.00%) (2364/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.7836) |  Loss2: (0.4194) | Acc: (87.00%) (3466/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.7872) |  Loss2: (0.4193) | Acc: (87.00%) (4585/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.7946) |  Loss2: (0.4193) | Acc: (86.00%) (5674/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.7982) |  Loss2: (0.4192) | Acc: (86.00%) (6778/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.7957) |  Loss2: (0.4191) | Acc: (86.00%) (7899/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.7936) |  Loss2: (0.4191) | Acc: (86.00%) (9017/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.7945) |  Loss2: (0.4190) | Acc: (86.00%) (10123/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.7936) |  Loss2: (0.4189) | Acc: (86.00%) (11246/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.7912) |  Loss2: (0.4188) | Acc: (87.00%) (12381/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.7921) |  Loss2: (0.4188) | Acc: (87.00%) (13488/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.7899) |  Loss2: (0.4187) | Acc: (87.00%) (14608/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.7920) |  Loss2: (0.4186) | Acc: (86.00%) (15696/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.7939) |  Loss2: (0.4186) | Acc: (86.00%) (16789/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.7952) |  Loss2: (0.4185) | Acc: (86.00%) (17898/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.7935) |  Loss2: (0.4185) | Acc: (86.00%) (19036/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.7938) |  Loss2: (0.4184) | Acc: (86.00%) (20148/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.7936) |  Loss2: (0.4184) | Acc: (87.00%) (21273/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.7922) |  Loss2: (0.4183) | Acc: (87.00%) (22384/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.7908) |  Loss2: (0.4182) | Acc: (87.00%) (23516/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.7913) |  Loss2: (0.4182) | Acc: (87.00%) (24623/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.7907) |  Loss2: (0.4181) | Acc: (87.00%) (25741/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.7901) |  Loss2: (0.4181) | Acc: (87.00%) (26856/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.7902) |  Loss2: (0.4180) | Acc: (87.00%) (27979/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.7885) |  Loss2: (0.4180) | Acc: (87.00%) (29121/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.7871) |  Loss2: (0.4179) | Acc: (87.00%) (30258/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.7875) |  Loss2: (0.4179) | Acc: (87.00%) (31371/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.7880) |  Loss2: (0.4178) | Acc: (87.00%) (32474/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.7880) |  Loss2: (0.4178) | Acc: (87.00%) (33588/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.7891) |  Loss2: (0.4177) | Acc: (87.00%) (34682/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.7892) |  Loss2: (0.4177) | Acc: (87.00%) (35795/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.7880) |  Loss2: (0.4176) | Acc: (87.00%) (36927/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.7899) |  Loss2: (0.4176) | Acc: (87.00%) (38017/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.7892) |  Loss2: (0.4175) | Acc: (87.00%) (39145/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.7893) |  Loss2: (0.4175) | Acc: (87.00%) (40253/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.7893) |  Loss2: (0.4174) | Acc: (87.00%) (41367/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.7889) |  Loss2: (0.4173) | Acc: (87.00%) (42502/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.7886) |  Loss2: (0.4173) | Acc: (87.00%) (43575/50000)
# TEST : Loss: (0.4677) | Acc: (84.00%) (8443/10000)
percent tensor([0.7091], device='cuda:0')
percent tensor([0.7095], device='cuda:0')
percent tensor([0.7498], device='cuda:0')
percent tensor([0.6588], device='cuda:0')
percent tensor([0.7341], device='cuda:0')
percent tensor([0.7758], device='cuda:0')
percent tensor([0.8188], device='cuda:0')
percent tensor([0.1943], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.8609) |  Loss2: (0.4149) | Acc: (82.00%) (105/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.8074) |  Loss2: (0.4148) | Acc: (86.00%) (1217/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.7989) |  Loss2: (0.4148) | Acc: (86.00%) (2337/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.7939) |  Loss2: (0.4148) | Acc: (86.00%) (3449/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.7814) |  Loss2: (0.4148) | Acc: (87.00%) (4580/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.7835) |  Loss2: (0.4148) | Acc: (87.00%) (5703/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.7824) |  Loss2: (0.4148) | Acc: (87.00%) (6834/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.7839) |  Loss2: (0.4148) | Acc: (87.00%) (7945/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.7798) |  Loss2: (0.4148) | Acc: (87.00%) (9081/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.7834) |  Loss2: (0.4148) | Acc: (87.00%) (10180/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.7850) |  Loss2: (0.4147) | Acc: (87.00%) (11284/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.7809) |  Loss2: (0.4147) | Acc: (87.00%) (12415/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.7801) |  Loss2: (0.4147) | Acc: (87.00%) (13539/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.7788) |  Loss2: (0.4147) | Acc: (87.00%) (14661/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.7786) |  Loss2: (0.4146) | Acc: (87.00%) (15782/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.7795) |  Loss2: (0.4146) | Acc: (87.00%) (16893/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.7780) |  Loss2: (0.4146) | Acc: (87.00%) (18030/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.7773) |  Loss2: (0.4145) | Acc: (87.00%) (19161/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.7773) |  Loss2: (0.4145) | Acc: (87.00%) (20277/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.7781) |  Loss2: (0.4145) | Acc: (87.00%) (21385/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.7788) |  Loss2: (0.4145) | Acc: (87.00%) (22499/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.7791) |  Loss2: (0.4144) | Acc: (87.00%) (23612/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.7788) |  Loss2: (0.4144) | Acc: (87.00%) (24742/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.7778) |  Loss2: (0.4144) | Acc: (87.00%) (25872/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.7774) |  Loss2: (0.4143) | Acc: (87.00%) (26996/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.7770) |  Loss2: (0.4143) | Acc: (87.00%) (28119/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.7773) |  Loss2: (0.4143) | Acc: (87.00%) (29238/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.7782) |  Loss2: (0.4142) | Acc: (87.00%) (30348/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.7786) |  Loss2: (0.4142) | Acc: (87.00%) (31465/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.7788) |  Loss2: (0.4141) | Acc: (87.00%) (32584/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.7793) |  Loss2: (0.4141) | Acc: (87.00%) (33692/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.7792) |  Loss2: (0.4141) | Acc: (87.00%) (34805/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.7790) |  Loss2: (0.4141) | Acc: (87.00%) (35928/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.7789) |  Loss2: (0.4140) | Acc: (87.00%) (37048/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.7777) |  Loss2: (0.4140) | Acc: (87.00%) (38201/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.7782) |  Loss2: (0.4140) | Acc: (87.00%) (39319/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.7778) |  Loss2: (0.4140) | Acc: (87.00%) (40446/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.7780) |  Loss2: (0.4139) | Acc: (87.00%) (41560/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.7778) |  Loss2: (0.4139) | Acc: (87.00%) (42681/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.7777) |  Loss2: (0.4139) | Acc: (87.00%) (43774/50000)
# TEST : Loss: (0.4603) | Acc: (84.00%) (8465/10000)
percent tensor([0.7108], device='cuda:0')
percent tensor([0.7098], device='cuda:0')
percent tensor([0.7523], device='cuda:0')
percent tensor([0.6592], device='cuda:0')
percent tensor([0.7384], device='cuda:0')
percent tensor([0.7787], device='cuda:0')
percent tensor([0.8214], device='cuda:0')
percent tensor([0.1914], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.9634) |  Loss2: (0.4127) | Acc: (85.00%) (109/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.8075) |  Loss2: (0.4127) | Acc: (86.00%) (1215/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.8127) |  Loss2: (0.4127) | Acc: (85.00%) (2309/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.8002) |  Loss2: (0.4127) | Acc: (86.00%) (3431/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.7992) |  Loss2: (0.4127) | Acc: (86.00%) (4544/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.7958) |  Loss2: (0.4127) | Acc: (86.00%) (5659/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.7864) |  Loss2: (0.4127) | Acc: (87.00%) (6808/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.7885) |  Loss2: (0.4127) | Acc: (87.00%) (7916/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.7849) |  Loss2: (0.4127) | Acc: (87.00%) (9045/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.7788) |  Loss2: (0.4127) | Acc: (87.00%) (10183/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.7777) |  Loss2: (0.4126) | Acc: (87.00%) (11312/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.7737) |  Loss2: (0.4126) | Acc: (87.00%) (12460/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.7742) |  Loss2: (0.4126) | Acc: (87.00%) (13583/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.7737) |  Loss2: (0.4126) | Acc: (87.00%) (14712/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.7762) |  Loss2: (0.4125) | Acc: (87.00%) (15811/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.7763) |  Loss2: (0.4125) | Acc: (87.00%) (16934/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.7768) |  Loss2: (0.4125) | Acc: (87.00%) (18054/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.7767) |  Loss2: (0.4125) | Acc: (87.00%) (19176/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.7754) |  Loss2: (0.4124) | Acc: (87.00%) (20308/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.7745) |  Loss2: (0.4124) | Acc: (87.00%) (21438/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.7736) |  Loss2: (0.4124) | Acc: (87.00%) (22579/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.7736) |  Loss2: (0.4124) | Acc: (87.00%) (23700/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.7732) |  Loss2: (0.4124) | Acc: (87.00%) (24830/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.7713) |  Loss2: (0.4123) | Acc: (87.00%) (25973/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.7712) |  Loss2: (0.4123) | Acc: (87.00%) (27112/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.7703) |  Loss2: (0.4123) | Acc: (87.00%) (28246/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.7695) |  Loss2: (0.4123) | Acc: (87.00%) (29373/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.7699) |  Loss2: (0.4122) | Acc: (87.00%) (30486/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.7698) |  Loss2: (0.4122) | Acc: (87.00%) (31615/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.7688) |  Loss2: (0.4122) | Acc: (87.00%) (32761/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.7696) |  Loss2: (0.4122) | Acc: (87.00%) (33883/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.7689) |  Loss2: (0.4121) | Acc: (87.00%) (35016/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.7688) |  Loss2: (0.4121) | Acc: (87.00%) (36129/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.7681) |  Loss2: (0.4121) | Acc: (87.00%) (37255/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.7681) |  Loss2: (0.4120) | Acc: (87.00%) (38384/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.7687) |  Loss2: (0.4120) | Acc: (87.00%) (39488/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.7679) |  Loss2: (0.4120) | Acc: (87.00%) (40635/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.7685) |  Loss2: (0.4120) | Acc: (87.00%) (41749/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.7687) |  Loss2: (0.4120) | Acc: (87.00%) (42867/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.7689) |  Loss2: (0.4119) | Acc: (87.00%) (43951/50000)
# TEST : Loss: (0.4526) | Acc: (84.00%) (8472/10000)
percent tensor([0.7116], device='cuda:0')
percent tensor([0.7126], device='cuda:0')
percent tensor([0.7546], device='cuda:0')
percent tensor([0.6616], device='cuda:0')
percent tensor([0.7408], device='cuda:0')
percent tensor([0.7796], device='cuda:0')
percent tensor([0.8221], device='cuda:0')
percent tensor([0.1877], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.7646) |  Loss2: (0.4111) | Acc: (89.00%) (114/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.7602) |  Loss2: (0.4111) | Acc: (89.00%) (1254/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.7425) |  Loss2: (0.4112) | Acc: (89.00%) (2397/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.7544) |  Loss2: (0.4112) | Acc: (88.00%) (3518/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.7526) |  Loss2: (0.4112) | Acc: (88.00%) (4650/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.7506) |  Loss2: (0.4111) | Acc: (88.00%) (5786/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.7560) |  Loss2: (0.4111) | Acc: (88.00%) (6907/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.7570) |  Loss2: (0.4112) | Acc: (88.00%) (8037/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.7599) |  Loss2: (0.4112) | Acc: (88.00%) (9150/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.7594) |  Loss2: (0.4112) | Acc: (88.00%) (10286/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.7583) |  Loss2: (0.4112) | Acc: (88.00%) (11419/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.7597) |  Loss2: (0.4112) | Acc: (88.00%) (12531/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.7609) |  Loss2: (0.4112) | Acc: (88.00%) (13650/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.7610) |  Loss2: (0.4112) | Acc: (88.00%) (14780/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.7605) |  Loss2: (0.4112) | Acc: (88.00%) (15899/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.7610) |  Loss2: (0.4112) | Acc: (88.00%) (17018/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.7647) |  Loss2: (0.4112) | Acc: (87.00%) (18125/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.7638) |  Loss2: (0.4112) | Acc: (87.00%) (19260/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.7609) |  Loss2: (0.4112) | Acc: (88.00%) (20411/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.7621) |  Loss2: (0.4112) | Acc: (88.00%) (21523/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.7631) |  Loss2: (0.4112) | Acc: (88.00%) (22641/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.7635) |  Loss2: (0.4112) | Acc: (87.00%) (23767/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.7633) |  Loss2: (0.4112) | Acc: (88.00%) (24897/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.7643) |  Loss2: (0.4112) | Acc: (87.00%) (26010/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.7662) |  Loss2: (0.4112) | Acc: (87.00%) (27128/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.7680) |  Loss2: (0.4112) | Acc: (87.00%) (28231/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.7680) |  Loss2: (0.4111) | Acc: (87.00%) (29352/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.7686) |  Loss2: (0.4111) | Acc: (87.00%) (30474/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.7688) |  Loss2: (0.4111) | Acc: (87.00%) (31598/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.7677) |  Loss2: (0.4111) | Acc: (87.00%) (32729/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.7676) |  Loss2: (0.4110) | Acc: (87.00%) (33855/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.7684) |  Loss2: (0.4110) | Acc: (87.00%) (34963/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.7691) |  Loss2: (0.4110) | Acc: (87.00%) (36083/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.7691) |  Loss2: (0.4110) | Acc: (87.00%) (37203/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.7692) |  Loss2: (0.4110) | Acc: (87.00%) (38326/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.7699) |  Loss2: (0.4110) | Acc: (87.00%) (39441/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.7702) |  Loss2: (0.4110) | Acc: (87.00%) (40561/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.7695) |  Loss2: (0.4109) | Acc: (87.00%) (41694/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.7698) |  Loss2: (0.4109) | Acc: (87.00%) (42809/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.7683) |  Loss2: (0.4109) | Acc: (87.00%) (43907/50000)
# TEST : Loss: (0.4485) | Acc: (85.00%) (8506/10000)
percent tensor([0.7111], device='cuda:0')
percent tensor([0.7113], device='cuda:0')
percent tensor([0.7563], device='cuda:0')
percent tensor([0.6629], device='cuda:0')
percent tensor([0.7434], device='cuda:0')
percent tensor([0.7805], device='cuda:0')
percent tensor([0.8226], device='cuda:0')
percent tensor([0.1842], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (3477/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (4586/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (5687/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (6801/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (7916/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (9029/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (10142/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (11272/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (12399/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (13507/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (14610/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (15720/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (16850/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (17956/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.3700) |  Loss2: (0.0000) | Acc: (87.00%) (19081/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (20200/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (21327/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (22437/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (23564/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (24710/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (25851/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (26949/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (28080/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (29192/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (30300/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (87.00%) (31401/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (32522/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (33641/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (34758/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (35892/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (37019/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (38155/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (39279/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (40384/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (41515/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (42652/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (43709/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_090.pth.tar'
# TEST : Loss: (0.4939) | Acc: (83.00%) (8365/10000)
percent tensor([0.7108], device='cuda:0')
percent tensor([0.7114], device='cuda:0')
percent tensor([0.7562], device='cuda:0')
percent tensor([0.6630], device='cuda:0')
percent tensor([0.7434], device='cuda:0')
percent tensor([0.7803], device='cuda:0')
percent tensor([0.8226], device='cuda:0')
percent tensor([0.1841], device='cuda:0')
Epoch: 91 | Batch_idx: 0 |  Loss: (0.3562) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (86.00%) (1221/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.3626) |  Loss2: (0.0000) | Acc: (87.00%) (2346/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.3601) |  Loss2: (0.0000) | Acc: (87.00%) (3465/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (5732/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (6859/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (8008/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (9128/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (10233/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (11374/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (88.00%) (12508/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (88.00%) (13632/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (14759/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.3489) |  Loss2: (0.0000) | Acc: (87.00%) (15880/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (16985/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (18111/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (19239/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (20375/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (21480/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (22601/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (23738/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (24859/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (25985/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (27119/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (28243/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (29374/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (30508/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.3502) |  Loss2: (0.0000) | Acc: (87.00%) (31627/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (32754/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.3496) |  Loss2: (0.0000) | Acc: (87.00%) (33893/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (35011/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (36127/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (37246/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (38373/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (87.00%) (39524/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (40657/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (41779/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (42883/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (87.00%) (43951/50000)
# TEST : Loss: (0.5169) | Acc: (83.00%) (8334/10000)
percent tensor([0.7107], device='cuda:0')
percent tensor([0.7113], device='cuda:0')
percent tensor([0.7561], device='cuda:0')
percent tensor([0.6630], device='cuda:0')
percent tensor([0.7434], device='cuda:0')
percent tensor([0.7802], device='cuda:0')
percent tensor([0.8225], device='cuda:0')
percent tensor([0.1842], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (1243/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (88.00%) (2383/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (3498/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (88.00%) (4622/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (5745/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (88.00%) (6877/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (8025/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (9162/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (10278/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (11419/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.3438) |  Loss2: (0.0000) | Acc: (88.00%) (12552/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (13675/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (88.00%) (14816/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (15943/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (17090/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (18218/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (19339/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (20486/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (21622/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (22735/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (23860/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (25005/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (26130/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (27245/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (28371/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (29517/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (30628/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (31758/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.3439) |  Loss2: (0.0000) | Acc: (88.00%) (32860/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.3439) |  Loss2: (0.0000) | Acc: (88.00%) (33991/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (35131/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (88.00%) (36247/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (37386/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (38503/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (39627/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (40747/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (41881/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (43011/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (44086/50000)
# TEST : Loss: (0.5218) | Acc: (82.00%) (8297/10000)
percent tensor([0.7106], device='cuda:0')
percent tensor([0.7113], device='cuda:0')
percent tensor([0.7560], device='cuda:0')
percent tensor([0.6629], device='cuda:0')
percent tensor([0.7433], device='cuda:0')
percent tensor([0.7801], device='cuda:0')
percent tensor([0.8224], device='cuda:0')
percent tensor([0.1843], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (2375/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (88.00%) (3505/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (4650/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (5783/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (6913/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (8062/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (9195/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (10325/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (11466/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (12610/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (13728/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (14852/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (15978/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (17120/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (18268/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (19399/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (20545/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (21681/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (22805/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (23951/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (25080/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (26212/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (27355/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (28465/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (29597/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (30724/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.3333) |  Loss2: (0.0000) | Acc: (88.00%) (31840/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.3335) |  Loss2: (0.0000) | Acc: (88.00%) (32984/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (34107/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35228/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (36341/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (37470/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (38600/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (39746/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (40881/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (41998/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.3378) |  Loss2: (0.0000) | Acc: (88.00%) (43107/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (44200/50000)
# TEST : Loss: (0.4434) | Acc: (85.00%) (8508/10000)
percent tensor([0.7106], device='cuda:0')
percent tensor([0.7112], device='cuda:0')
percent tensor([0.7559], device='cuda:0')
percent tensor([0.6628], device='cuda:0')
percent tensor([0.7432], device='cuda:0')
percent tensor([0.7800], device='cuda:0')
percent tensor([0.8224], device='cuda:0')
percent tensor([0.1844], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (3522/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (4658/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (5794/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (88.00%) (6944/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (8073/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (9212/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (10335/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (11486/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.3267) |  Loss2: (0.0000) | Acc: (88.00%) (12615/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (13746/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.3278) |  Loss2: (0.0000) | Acc: (88.00%) (14886/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (16028/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (17165/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (18293/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (19403/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (20558/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (21704/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (22845/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (23957/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (25076/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (26206/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (27358/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (28487/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (29616/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (30756/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (31857/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (32980/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (34122/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (35253/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (36381/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (37506/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (38626/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (39749/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (40897/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (42037/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (43172/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.3321) |  Loss2: (0.0000) | Acc: (88.00%) (44266/50000)
# TEST : Loss: (0.4921) | Acc: (83.00%) (8381/10000)
percent tensor([0.7105], device='cuda:0')
percent tensor([0.7111], device='cuda:0')
percent tensor([0.7559], device='cuda:0')
percent tensor([0.6628], device='cuda:0')
percent tensor([0.7431], device='cuda:0')
percent tensor([0.7800], device='cuda:0')
percent tensor([0.8223], device='cuda:0')
percent tensor([0.1845], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.7634) |  Loss2: (0.4108) | Acc: (89.00%) (115/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.7597) |  Loss2: (0.4108) | Acc: (87.00%) (1232/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.7884) |  Loss2: (0.4107) | Acc: (86.00%) (2321/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.7961) |  Loss2: (0.4106) | Acc: (86.00%) (3421/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.7973) |  Loss2: (0.4105) | Acc: (86.00%) (4534/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.7974) |  Loss2: (0.4104) | Acc: (86.00%) (5623/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.7961) |  Loss2: (0.4103) | Acc: (86.00%) (6719/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.7922) |  Loss2: (0.4102) | Acc: (86.00%) (7847/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.7942) |  Loss2: (0.4101) | Acc: (86.00%) (8947/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.7944) |  Loss2: (0.4100) | Acc: (86.00%) (10059/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.7941) |  Loss2: (0.4099) | Acc: (86.00%) (11158/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.7962) |  Loss2: (0.4098) | Acc: (86.00%) (12252/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.7941) |  Loss2: (0.4097) | Acc: (86.00%) (13381/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.7948) |  Loss2: (0.4096) | Acc: (86.00%) (14483/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.7958) |  Loss2: (0.4095) | Acc: (86.00%) (15579/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.7940) |  Loss2: (0.4094) | Acc: (86.00%) (16695/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.7919) |  Loss2: (0.4093) | Acc: (86.00%) (17812/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.7894) |  Loss2: (0.4092) | Acc: (86.00%) (18941/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.7882) |  Loss2: (0.4091) | Acc: (86.00%) (20063/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.7853) |  Loss2: (0.4090) | Acc: (86.00%) (21194/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.7839) |  Loss2: (0.4089) | Acc: (86.00%) (22313/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.7837) |  Loss2: (0.4089) | Acc: (86.00%) (23424/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.7842) |  Loss2: (0.4088) | Acc: (86.00%) (24533/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.7823) |  Loss2: (0.4087) | Acc: (86.00%) (25658/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.7819) |  Loss2: (0.4086) | Acc: (86.00%) (26780/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.7822) |  Loss2: (0.4085) | Acc: (86.00%) (27897/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.7818) |  Loss2: (0.4084) | Acc: (86.00%) (29022/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.7826) |  Loss2: (0.4083) | Acc: (86.00%) (30119/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.7830) |  Loss2: (0.4082) | Acc: (86.00%) (31224/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.7836) |  Loss2: (0.4081) | Acc: (86.00%) (32330/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.7822) |  Loss2: (0.4081) | Acc: (86.00%) (33473/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.7811) |  Loss2: (0.4080) | Acc: (86.00%) (34598/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.7806) |  Loss2: (0.4079) | Acc: (86.00%) (35725/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.7792) |  Loss2: (0.4078) | Acc: (87.00%) (36866/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.7782) |  Loss2: (0.4077) | Acc: (87.00%) (37988/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.7769) |  Loss2: (0.4077) | Acc: (87.00%) (39129/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.7755) |  Loss2: (0.4076) | Acc: (87.00%) (40267/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.7749) |  Loss2: (0.4075) | Acc: (87.00%) (41405/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.7739) |  Loss2: (0.4074) | Acc: (87.00%) (42540/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.7729) |  Loss2: (0.4073) | Acc: (87.00%) (43641/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_095.pth.tar'
# TEST : Loss: (0.4465) | Acc: (85.00%) (8517/10000)
percent tensor([0.7148], device='cuda:0')
percent tensor([0.7263], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.6694], device='cuda:0')
percent tensor([0.7444], device='cuda:0')
percent tensor([0.7803], device='cuda:0')
percent tensor([0.8267], device='cuda:0')
percent tensor([0.1841], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.6741) |  Loss2: (0.4041) | Acc: (91.00%) (117/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.7672) |  Loss2: (0.4040) | Acc: (87.00%) (1234/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.7536) |  Loss2: (0.4039) | Acc: (87.00%) (2360/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.7549) |  Loss2: (0.4038) | Acc: (87.00%) (3485/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.7552) |  Loss2: (0.4037) | Acc: (87.00%) (4608/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.7555) |  Loss2: (0.4036) | Acc: (88.00%) (5746/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.7507) |  Loss2: (0.4036) | Acc: (88.00%) (6886/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.7525) |  Loss2: (0.4035) | Acc: (88.00%) (8006/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.7492) |  Loss2: (0.4035) | Acc: (88.00%) (9142/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.7520) |  Loss2: (0.4034) | Acc: (88.00%) (10255/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.7540) |  Loss2: (0.4033) | Acc: (88.00%) (11381/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.7515) |  Loss2: (0.4033) | Acc: (88.00%) (12518/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.7526) |  Loss2: (0.4032) | Acc: (88.00%) (13638/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.7512) |  Loss2: (0.4032) | Acc: (88.00%) (14761/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.7487) |  Loss2: (0.4031) | Acc: (88.00%) (15913/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.7494) |  Loss2: (0.4031) | Acc: (88.00%) (17036/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.7466) |  Loss2: (0.4030) | Acc: (88.00%) (18176/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.7456) |  Loss2: (0.4029) | Acc: (88.00%) (19315/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.7442) |  Loss2: (0.4029) | Acc: (88.00%) (20463/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.7438) |  Loss2: (0.4028) | Acc: (88.00%) (21604/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.7445) |  Loss2: (0.4028) | Acc: (88.00%) (22720/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.7442) |  Loss2: (0.4027) | Acc: (88.00%) (23853/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.7430) |  Loss2: (0.4027) | Acc: (88.00%) (24998/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.7403) |  Loss2: (0.4026) | Acc: (88.00%) (26152/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.7418) |  Loss2: (0.4026) | Acc: (88.00%) (27262/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.7410) |  Loss2: (0.4025) | Acc: (88.00%) (28408/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.7398) |  Loss2: (0.4025) | Acc: (88.00%) (29535/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.7399) |  Loss2: (0.4024) | Acc: (88.00%) (30661/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.7392) |  Loss2: (0.4024) | Acc: (88.00%) (31799/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.7391) |  Loss2: (0.4023) | Acc: (88.00%) (32930/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.7385) |  Loss2: (0.4023) | Acc: (88.00%) (34074/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.7379) |  Loss2: (0.4022) | Acc: (88.00%) (35219/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.7388) |  Loss2: (0.4022) | Acc: (88.00%) (36334/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.7386) |  Loss2: (0.4021) | Acc: (88.00%) (37459/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.7380) |  Loss2: (0.4021) | Acc: (88.00%) (38606/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.7385) |  Loss2: (0.4021) | Acc: (88.00%) (39731/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.7400) |  Loss2: (0.4020) | Acc: (88.00%) (40836/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.7399) |  Loss2: (0.4020) | Acc: (88.00%) (41976/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.7395) |  Loss2: (0.4020) | Acc: (88.00%) (43122/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.7401) |  Loss2: (0.4019) | Acc: (88.00%) (44191/50000)
# TEST : Loss: (0.4396) | Acc: (85.00%) (8536/10000)
percent tensor([0.7166], device='cuda:0')
percent tensor([0.7284], device='cuda:0')
percent tensor([0.7712], device='cuda:0')
percent tensor([0.6774], device='cuda:0')
percent tensor([0.7484], device='cuda:0')
percent tensor([0.7851], device='cuda:0')
percent tensor([0.8278], device='cuda:0')
percent tensor([0.1816], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.8103) |  Loss2: (0.4008) | Acc: (84.00%) (108/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.7443) |  Loss2: (0.4007) | Acc: (88.00%) (1242/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.7361) |  Loss2: (0.4007) | Acc: (88.00%) (2379/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.7301) |  Loss2: (0.4006) | Acc: (88.00%) (3527/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.7351) |  Loss2: (0.4005) | Acc: (88.00%) (4651/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.7417) |  Loss2: (0.4004) | Acc: (88.00%) (5768/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.7326) |  Loss2: (0.4004) | Acc: (88.00%) (6928/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.7342) |  Loss2: (0.4003) | Acc: (88.00%) (8065/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.7331) |  Loss2: (0.4003) | Acc: (88.00%) (9201/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.7330) |  Loss2: (0.4002) | Acc: (88.00%) (10336/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.7323) |  Loss2: (0.4002) | Acc: (88.00%) (11462/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.7356) |  Loss2: (0.4002) | Acc: (88.00%) (12579/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.7355) |  Loss2: (0.4001) | Acc: (88.00%) (13726/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.7356) |  Loss2: (0.4001) | Acc: (88.00%) (14856/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.7337) |  Loss2: (0.4001) | Acc: (88.00%) (16003/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.7341) |  Loss2: (0.4000) | Acc: (88.00%) (17126/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.7366) |  Loss2: (0.4000) | Acc: (88.00%) (18242/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.7346) |  Loss2: (0.4000) | Acc: (88.00%) (19385/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.7336) |  Loss2: (0.3999) | Acc: (88.00%) (20519/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.7310) |  Loss2: (0.3999) | Acc: (88.00%) (21675/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.7316) |  Loss2: (0.3999) | Acc: (88.00%) (22798/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.7302) |  Loss2: (0.3998) | Acc: (88.00%) (23950/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.7302) |  Loss2: (0.3998) | Acc: (88.00%) (25095/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.7299) |  Loss2: (0.3998) | Acc: (88.00%) (26228/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.7285) |  Loss2: (0.3997) | Acc: (88.00%) (27382/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.7301) |  Loss2: (0.3997) | Acc: (88.00%) (28500/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.7321) |  Loss2: (0.3997) | Acc: (88.00%) (29610/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.7315) |  Loss2: (0.3996) | Acc: (88.00%) (30744/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.7320) |  Loss2: (0.3996) | Acc: (88.00%) (31869/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.7306) |  Loss2: (0.3996) | Acc: (88.00%) (33034/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.7307) |  Loss2: (0.3996) | Acc: (88.00%) (34178/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.7314) |  Loss2: (0.3995) | Acc: (88.00%) (35313/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.7313) |  Loss2: (0.3995) | Acc: (88.00%) (36450/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.7309) |  Loss2: (0.3995) | Acc: (88.00%) (37586/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.7313) |  Loss2: (0.3995) | Acc: (88.00%) (38723/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.7309) |  Loss2: (0.3994) | Acc: (88.00%) (39874/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.7303) |  Loss2: (0.3994) | Acc: (88.00%) (41023/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.7307) |  Loss2: (0.3994) | Acc: (88.00%) (42140/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.7311) |  Loss2: (0.3993) | Acc: (88.00%) (43275/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.7312) |  Loss2: (0.3993) | Acc: (88.00%) (44360/50000)
# TEST : Loss: (0.4320) | Acc: (85.00%) (8564/10000)
percent tensor([0.7189], device='cuda:0')
percent tensor([0.7313], device='cuda:0')
percent tensor([0.7749], device='cuda:0')
percent tensor([0.6806], device='cuda:0')
percent tensor([0.7499], device='cuda:0')
percent tensor([0.7893], device='cuda:0')
percent tensor([0.8286], device='cuda:0')
percent tensor([0.1790], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.7839) |  Loss2: (0.3981) | Acc: (88.00%) (113/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.7254) |  Loss2: (0.3981) | Acc: (89.00%) (1255/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.7189) |  Loss2: (0.3981) | Acc: (89.00%) (2402/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.7273) |  Loss2: (0.3981) | Acc: (89.00%) (3537/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.7210) |  Loss2: (0.3981) | Acc: (89.00%) (4694/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.7266) |  Loss2: (0.3980) | Acc: (89.00%) (5825/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.7263) |  Loss2: (0.3980) | Acc: (89.00%) (6963/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.7281) |  Loss2: (0.3980) | Acc: (89.00%) (8092/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.7304) |  Loss2: (0.3980) | Acc: (88.00%) (9225/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.7297) |  Loss2: (0.3980) | Acc: (88.00%) (10361/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.7273) |  Loss2: (0.3980) | Acc: (89.00%) (11509/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.7280) |  Loss2: (0.3980) | Acc: (89.00%) (12647/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.7321) |  Loss2: (0.3980) | Acc: (88.00%) (13772/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.7316) |  Loss2: (0.3980) | Acc: (88.00%) (14916/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.7323) |  Loss2: (0.3979) | Acc: (88.00%) (16050/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.7311) |  Loss2: (0.3979) | Acc: (88.00%) (17182/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.7288) |  Loss2: (0.3979) | Acc: (88.00%) (18341/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.7265) |  Loss2: (0.3979) | Acc: (89.00%) (19500/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.7269) |  Loss2: (0.3979) | Acc: (88.00%) (20617/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.7267) |  Loss2: (0.3979) | Acc: (88.00%) (21751/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.7273) |  Loss2: (0.3979) | Acc: (88.00%) (22884/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.7267) |  Loss2: (0.3978) | Acc: (88.00%) (24033/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.7272) |  Loss2: (0.3978) | Acc: (88.00%) (25167/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.7255) |  Loss2: (0.3978) | Acc: (89.00%) (26326/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.7251) |  Loss2: (0.3978) | Acc: (89.00%) (27467/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.7254) |  Loss2: (0.3978) | Acc: (89.00%) (28599/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.7247) |  Loss2: (0.3978) | Acc: (89.00%) (29742/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.7247) |  Loss2: (0.3977) | Acc: (89.00%) (30885/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.7259) |  Loss2: (0.3977) | Acc: (88.00%) (32004/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.7261) |  Loss2: (0.3977) | Acc: (88.00%) (33147/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.7249) |  Loss2: (0.3976) | Acc: (89.00%) (34298/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.7251) |  Loss2: (0.3976) | Acc: (88.00%) (35425/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.7240) |  Loss2: (0.3976) | Acc: (89.00%) (36574/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.7236) |  Loss2: (0.3976) | Acc: (88.00%) (37700/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.7242) |  Loss2: (0.3975) | Acc: (88.00%) (38821/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.7239) |  Loss2: (0.3975) | Acc: (88.00%) (39961/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.7232) |  Loss2: (0.3975) | Acc: (88.00%) (41122/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.7231) |  Loss2: (0.3975) | Acc: (89.00%) (42269/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.7228) |  Loss2: (0.3974) | Acc: (89.00%) (43415/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.7228) |  Loss2: (0.3974) | Acc: (89.00%) (44502/50000)
# TEST : Loss: (0.4311) | Acc: (85.00%) (8558/10000)
percent tensor([0.7211], device='cuda:0')
percent tensor([0.7296], device='cuda:0')
percent tensor([0.7748], device='cuda:0')
percent tensor([0.6811], device='cuda:0')
percent tensor([0.7550], device='cuda:0')
percent tensor([0.7915], device='cuda:0')
percent tensor([0.8296], device='cuda:0')
percent tensor([0.1759], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.6779) |  Loss2: (0.3968) | Acc: (90.00%) (116/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.7058) |  Loss2: (0.3967) | Acc: (89.00%) (1261/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.7316) |  Loss2: (0.3967) | Acc: (87.00%) (2364/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.7194) |  Loss2: (0.3967) | Acc: (88.00%) (3510/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.7227) |  Loss2: (0.3967) | Acc: (88.00%) (4633/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.7248) |  Loss2: (0.3966) | Acc: (88.00%) (5764/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.7211) |  Loss2: (0.3966) | Acc: (88.00%) (6920/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.7143) |  Loss2: (0.3966) | Acc: (88.00%) (8088/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.7167) |  Loss2: (0.3966) | Acc: (88.00%) (9215/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.7141) |  Loss2: (0.3966) | Acc: (89.00%) (10375/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.7166) |  Loss2: (0.3966) | Acc: (88.00%) (11496/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.7180) |  Loss2: (0.3966) | Acc: (88.00%) (12625/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.7189) |  Loss2: (0.3966) | Acc: (88.00%) (13760/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.7184) |  Loss2: (0.3966) | Acc: (88.00%) (14897/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.7220) |  Loss2: (0.3966) | Acc: (88.00%) (15991/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.7201) |  Loss2: (0.3966) | Acc: (88.00%) (17140/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.7189) |  Loss2: (0.3966) | Acc: (88.00%) (18297/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.7178) |  Loss2: (0.3966) | Acc: (88.00%) (19439/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.7179) |  Loss2: (0.3966) | Acc: (88.00%) (20573/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.7186) |  Loss2: (0.3966) | Acc: (88.00%) (21705/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.7198) |  Loss2: (0.3966) | Acc: (88.00%) (22835/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.7205) |  Loss2: (0.3966) | Acc: (88.00%) (23965/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.7212) |  Loss2: (0.3965) | Acc: (88.00%) (25091/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.7227) |  Loss2: (0.3965) | Acc: (88.00%) (26209/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.7233) |  Loss2: (0.3965) | Acc: (88.00%) (27346/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.7232) |  Loss2: (0.3965) | Acc: (88.00%) (28487/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.7229) |  Loss2: (0.3965) | Acc: (88.00%) (29625/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.7226) |  Loss2: (0.3965) | Acc: (88.00%) (30761/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.7227) |  Loss2: (0.3965) | Acc: (88.00%) (31892/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.7234) |  Loss2: (0.3964) | Acc: (88.00%) (33022/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.7226) |  Loss2: (0.3964) | Acc: (88.00%) (34163/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.7227) |  Loss2: (0.3964) | Acc: (88.00%) (35290/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.7220) |  Loss2: (0.3964) | Acc: (88.00%) (36444/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.7214) |  Loss2: (0.3964) | Acc: (88.00%) (37591/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.7217) |  Loss2: (0.3964) | Acc: (88.00%) (38720/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.7219) |  Loss2: (0.3964) | Acc: (88.00%) (39850/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.7216) |  Loss2: (0.3963) | Acc: (88.00%) (41003/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.7213) |  Loss2: (0.3963) | Acc: (88.00%) (42150/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.7206) |  Loss2: (0.3963) | Acc: (88.00%) (43304/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.7206) |  Loss2: (0.3963) | Acc: (88.00%) (44397/50000)
# TEST : Loss: (0.4280) | Acc: (85.00%) (8564/10000)
percent tensor([0.7211], device='cuda:0')
percent tensor([0.7303], device='cuda:0')
percent tensor([0.7750], device='cuda:0')
percent tensor([0.6841], device='cuda:0')
percent tensor([0.7551], device='cuda:0')
percent tensor([0.7930], device='cuda:0')
percent tensor([0.8314], device='cuda:0')
percent tensor([0.1729], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (2374/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (4636/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (5772/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (6903/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (8030/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (9187/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (10319/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (11459/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (12606/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (13753/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (14891/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (16021/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (17156/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (18286/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (19396/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.3333) |  Loss2: (0.0000) | Acc: (88.00%) (20533/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (21647/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (22767/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (23908/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (25011/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (26156/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.3368) |  Loss2: (0.0000) | Acc: (88.00%) (27276/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (28419/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (29573/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (30699/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (31824/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (32967/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (34076/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.3392) |  Loss2: (0.0000) | Acc: (88.00%) (35196/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (36323/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (37449/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (38559/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (39700/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (40821/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (88.00%) (41938/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (43089/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.3426) |  Loss2: (0.0000) | Acc: (88.00%) (44153/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_100.pth.tar'
# TEST : Loss: (0.4585) | Acc: (84.00%) (8489/10000)
percent tensor([0.7212], device='cuda:0')
percent tensor([0.7304], device='cuda:0')
percent tensor([0.7751], device='cuda:0')
percent tensor([0.6836], device='cuda:0')
percent tensor([0.7549], device='cuda:0')
percent tensor([0.7928], device='cuda:0')
percent tensor([0.8313], device='cuda:0')
percent tensor([0.1730], device='cuda:0')
Epoch: 101 | Batch_idx: 0 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (1249/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (89.00%) (2393/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (3529/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (88.00%) (4665/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (5804/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (6937/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.3253) |  Loss2: (0.0000) | Acc: (88.00%) (8066/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (9204/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (10343/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (11474/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (12595/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (13716/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (14853/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (15989/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (17115/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (18248/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (19388/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (20537/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (21670/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (22798/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (23941/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (25090/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (26231/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (27343/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (28491/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (29630/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (30771/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (31903/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (33045/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (34162/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (35307/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (36437/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (37551/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (38702/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (39823/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (40951/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.3311) |  Loss2: (0.0000) | Acc: (88.00%) (42079/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (43216/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (44313/50000)
# TEST : Loss: (0.5427) | Acc: (82.00%) (8285/10000)
percent tensor([0.7211], device='cuda:0')
percent tensor([0.7303], device='cuda:0')
percent tensor([0.7751], device='cuda:0')
percent tensor([0.6836], device='cuda:0')
percent tensor([0.7549], device='cuda:0')
percent tensor([0.7927], device='cuda:0')
percent tensor([0.8312], device='cuda:0')
percent tensor([0.1731], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (2393/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (4676/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (5822/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (6961/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (8101/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (9258/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (10405/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (11532/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (12670/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (13818/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (14951/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (89.00%) (16089/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (17216/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (18374/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (89.00%) (19513/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (89.00%) (20655/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (21785/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (22927/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (24061/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (25200/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (26345/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (27469/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (28589/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (29723/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (30865/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (89.00%) (32020/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (89.00%) (33152/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (34276/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (35409/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (36538/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (37672/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (38817/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (39944/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (41090/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (42215/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.3240) |  Loss2: (0.0000) | Acc: (88.00%) (43329/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (44434/50000)
# TEST : Loss: (0.4761) | Acc: (84.00%) (8467/10000)
percent tensor([0.7210], device='cuda:0')
percent tensor([0.7302], device='cuda:0')
percent tensor([0.7750], device='cuda:0')
percent tensor([0.6835], device='cuda:0')
percent tensor([0.7548], device='cuda:0')
percent tensor([0.7927], device='cuda:0')
percent tensor([0.8311], device='cuda:0')
percent tensor([0.1731], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (1262/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (2408/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (3569/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (90.00%) (4727/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (5865/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (6993/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (8137/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (9278/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (10408/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (11543/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (12689/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (13839/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.3079) |  Loss2: (0.0000) | Acc: (89.00%) (14983/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (16127/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (17274/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.3075) |  Loss2: (0.0000) | Acc: (89.00%) (18416/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (19570/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (20706/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (21838/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (22963/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (24100/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (25240/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (26373/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (27527/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (28674/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (29808/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (30968/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (32110/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.3139) |  Loss2: (0.0000) | Acc: (89.00%) (33255/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (34389/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (35531/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (36654/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (37800/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (38931/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (40062/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (41207/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (42347/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (43474/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (89.00%) (44580/50000)
# TEST : Loss: (0.4696) | Acc: (84.00%) (8468/10000)
percent tensor([0.7210], device='cuda:0')
percent tensor([0.7301], device='cuda:0')
percent tensor([0.7749], device='cuda:0')
percent tensor([0.6834], device='cuda:0')
percent tensor([0.7547], device='cuda:0')
percent tensor([0.7926], device='cuda:0')
percent tensor([0.8311], device='cuda:0')
percent tensor([0.1732], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (3556/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (4697/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (5833/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (6982/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (8146/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (9283/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (10452/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (11604/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (12738/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (13874/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (15015/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (16151/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (17300/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (18431/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (19576/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (20729/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (21869/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (23012/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.3085) |  Loss2: (0.0000) | Acc: (89.00%) (24135/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (25288/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (26407/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (27543/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (28703/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (29842/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (30996/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (32149/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (33288/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (34412/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (35545/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (36670/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (37823/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (38968/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (40112/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (41261/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (42403/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (43561/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (44666/50000)
# TEST : Loss: (0.5250) | Acc: (84.00%) (8404/10000)
percent tensor([0.7209], device='cuda:0')
percent tensor([0.7301], device='cuda:0')
percent tensor([0.7748], device='cuda:0')
percent tensor([0.6834], device='cuda:0')
percent tensor([0.7546], device='cuda:0')
percent tensor([0.7925], device='cuda:0')
percent tensor([0.8310], device='cuda:0')
percent tensor([0.1733], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.6581) |  Loss2: (0.3962) | Acc: (88.00%) (113/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.7217) |  Loss2: (0.3961) | Acc: (88.00%) (1242/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.7369) |  Loss2: (0.3959) | Acc: (87.00%) (2356/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.7547) |  Loss2: (0.3958) | Acc: (87.00%) (3469/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.7661) |  Loss2: (0.3957) | Acc: (87.00%) (4566/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.7636) |  Loss2: (0.3956) | Acc: (87.00%) (5683/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.7626) |  Loss2: (0.3955) | Acc: (87.00%) (6805/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.7552) |  Loss2: (0.3954) | Acc: (87.00%) (7925/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.7514) |  Loss2: (0.3953) | Acc: (87.00%) (9066/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.7509) |  Loss2: (0.3952) | Acc: (87.00%) (10185/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.7487) |  Loss2: (0.3951) | Acc: (87.00%) (11294/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.7469) |  Loss2: (0.3949) | Acc: (87.00%) (12418/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.7478) |  Loss2: (0.3948) | Acc: (87.00%) (13540/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.7454) |  Loss2: (0.3947) | Acc: (87.00%) (14676/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.7437) |  Loss2: (0.3946) | Acc: (87.00%) (15814/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.7419) |  Loss2: (0.3945) | Acc: (87.00%) (16946/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.7409) |  Loss2: (0.3944) | Acc: (87.00%) (18078/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.7393) |  Loss2: (0.3943) | Acc: (87.00%) (19226/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.7412) |  Loss2: (0.3942) | Acc: (87.00%) (20335/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.7409) |  Loss2: (0.3941) | Acc: (87.00%) (21455/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.7411) |  Loss2: (0.3940) | Acc: (87.00%) (22576/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.7411) |  Loss2: (0.3939) | Acc: (87.00%) (23714/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.7390) |  Loss2: (0.3938) | Acc: (87.00%) (24867/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.7392) |  Loss2: (0.3937) | Acc: (87.00%) (25983/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.7382) |  Loss2: (0.3936) | Acc: (87.00%) (27119/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.7382) |  Loss2: (0.3935) | Acc: (87.00%) (28248/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.7366) |  Loss2: (0.3934) | Acc: (88.00%) (29400/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.7361) |  Loss2: (0.3933) | Acc: (87.00%) (30523/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.7348) |  Loss2: (0.3932) | Acc: (88.00%) (31671/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.7337) |  Loss2: (0.3931) | Acc: (88.00%) (32822/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.7335) |  Loss2: (0.3930) | Acc: (88.00%) (33955/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.7325) |  Loss2: (0.3929) | Acc: (88.00%) (35111/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.7319) |  Loss2: (0.3928) | Acc: (88.00%) (36241/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.7308) |  Loss2: (0.3927) | Acc: (88.00%) (37381/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.7303) |  Loss2: (0.3926) | Acc: (88.00%) (38515/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.7304) |  Loss2: (0.3926) | Acc: (88.00%) (39649/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.7297) |  Loss2: (0.3925) | Acc: (88.00%) (40782/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.7301) |  Loss2: (0.3924) | Acc: (88.00%) (41904/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.7292) |  Loss2: (0.3923) | Acc: (88.00%) (43058/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.7282) |  Loss2: (0.3922) | Acc: (88.00%) (44161/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_105.pth.tar'
# TEST : Loss: (0.4494) | Acc: (85.00%) (8547/10000)
percent tensor([0.7229], device='cuda:0')
percent tensor([0.7406], device='cuda:0')
percent tensor([0.7864], device='cuda:0')
percent tensor([0.6960], device='cuda:0')
percent tensor([0.7598], device='cuda:0')
percent tensor([0.7983], device='cuda:0')
percent tensor([0.8346], device='cuda:0')
percent tensor([0.1736], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.5984) |  Loss2: (0.3885) | Acc: (93.00%) (120/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.6888) |  Loss2: (0.3884) | Acc: (89.00%) (1264/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.7038) |  Loss2: (0.3883) | Acc: (88.00%) (2388/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.6939) |  Loss2: (0.3882) | Acc: (89.00%) (3540/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.6926) |  Loss2: (0.3882) | Acc: (89.00%) (4679/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.6901) |  Loss2: (0.3881) | Acc: (89.00%) (5837/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.6935) |  Loss2: (0.3880) | Acc: (89.00%) (6974/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.6931) |  Loss2: (0.3880) | Acc: (89.00%) (8120/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.6933) |  Loss2: (0.3880) | Acc: (89.00%) (9261/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.6981) |  Loss2: (0.3879) | Acc: (89.00%) (10390/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.6981) |  Loss2: (0.3879) | Acc: (89.00%) (11526/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.6978) |  Loss2: (0.3878) | Acc: (89.00%) (12670/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.6988) |  Loss2: (0.3878) | Acc: (89.00%) (13824/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.6988) |  Loss2: (0.3877) | Acc: (89.00%) (14976/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.7006) |  Loss2: (0.3877) | Acc: (89.00%) (16111/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.7003) |  Loss2: (0.3876) | Acc: (89.00%) (17261/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.7017) |  Loss2: (0.3876) | Acc: (89.00%) (18386/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.7005) |  Loss2: (0.3875) | Acc: (89.00%) (19535/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.6989) |  Loss2: (0.3874) | Acc: (89.00%) (20688/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.6990) |  Loss2: (0.3874) | Acc: (89.00%) (21835/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.6990) |  Loss2: (0.3873) | Acc: (89.00%) (22972/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.6997) |  Loss2: (0.3873) | Acc: (89.00%) (24093/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.7001) |  Loss2: (0.3872) | Acc: (89.00%) (25233/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.6991) |  Loss2: (0.3871) | Acc: (89.00%) (26386/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.6983) |  Loss2: (0.3871) | Acc: (89.00%) (27528/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.6975) |  Loss2: (0.3870) | Acc: (89.00%) (28686/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.6978) |  Loss2: (0.3869) | Acc: (89.00%) (29825/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.6979) |  Loss2: (0.3869) | Acc: (89.00%) (30959/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.6982) |  Loss2: (0.3868) | Acc: (89.00%) (32095/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.6990) |  Loss2: (0.3868) | Acc: (89.00%) (33240/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.6982) |  Loss2: (0.3867) | Acc: (89.00%) (34391/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.6977) |  Loss2: (0.3867) | Acc: (89.00%) (35528/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.6969) |  Loss2: (0.3866) | Acc: (89.00%) (36678/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.6959) |  Loss2: (0.3866) | Acc: (89.00%) (37841/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.6946) |  Loss2: (0.3865) | Acc: (89.00%) (38984/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.6952) |  Loss2: (0.3864) | Acc: (89.00%) (40110/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.6949) |  Loss2: (0.3864) | Acc: (89.00%) (41267/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.6961) |  Loss2: (0.3863) | Acc: (89.00%) (42392/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.6947) |  Loss2: (0.3863) | Acc: (89.00%) (43560/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.6949) |  Loss2: (0.3862) | Acc: (89.00%) (44657/50000)
# TEST : Loss: (0.4387) | Acc: (85.00%) (8574/10000)
percent tensor([0.7308], device='cuda:0')
percent tensor([0.7423], device='cuda:0')
percent tensor([0.7898], device='cuda:0')
percent tensor([0.6998], device='cuda:0')
percent tensor([0.7632], device='cuda:0')
percent tensor([0.8045], device='cuda:0')
percent tensor([0.8363], device='cuda:0')
percent tensor([0.1718], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.7200) |  Loss2: (0.3843) | Acc: (85.00%) (109/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.6824) |  Loss2: (0.3842) | Acc: (89.00%) (1260/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.6890) |  Loss2: (0.3842) | Acc: (89.00%) (2398/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.6835) |  Loss2: (0.3841) | Acc: (89.00%) (3560/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.6907) |  Loss2: (0.3841) | Acc: (89.00%) (4688/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.6896) |  Loss2: (0.3841) | Acc: (89.00%) (5837/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.6882) |  Loss2: (0.3841) | Acc: (89.00%) (6988/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.6875) |  Loss2: (0.3841) | Acc: (89.00%) (8139/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.6933) |  Loss2: (0.3841) | Acc: (89.00%) (9268/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.6924) |  Loss2: (0.3841) | Acc: (89.00%) (10420/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.6910) |  Loss2: (0.3841) | Acc: (89.00%) (11573/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.6902) |  Loss2: (0.3841) | Acc: (89.00%) (12726/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.6905) |  Loss2: (0.3841) | Acc: (89.00%) (13873/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.6917) |  Loss2: (0.3841) | Acc: (89.00%) (15018/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.6918) |  Loss2: (0.3840) | Acc: (89.00%) (16158/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.6921) |  Loss2: (0.3840) | Acc: (89.00%) (17297/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.6910) |  Loss2: (0.3840) | Acc: (89.00%) (18450/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.6918) |  Loss2: (0.3840) | Acc: (89.00%) (19589/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.6912) |  Loss2: (0.3839) | Acc: (89.00%) (20734/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.6895) |  Loss2: (0.3839) | Acc: (89.00%) (21905/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.6887) |  Loss2: (0.3839) | Acc: (89.00%) (23064/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.6883) |  Loss2: (0.3838) | Acc: (89.00%) (24211/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.6883) |  Loss2: (0.3838) | Acc: (89.00%) (25368/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.6885) |  Loss2: (0.3838) | Acc: (89.00%) (26504/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.6887) |  Loss2: (0.3837) | Acc: (89.00%) (27652/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.6867) |  Loss2: (0.3837) | Acc: (89.00%) (28821/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.6864) |  Loss2: (0.3837) | Acc: (89.00%) (29979/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.6876) |  Loss2: (0.3837) | Acc: (89.00%) (31112/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.6880) |  Loss2: (0.3836) | Acc: (89.00%) (32264/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.6863) |  Loss2: (0.3836) | Acc: (89.00%) (33427/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.6855) |  Loss2: (0.3836) | Acc: (89.00%) (34585/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.6862) |  Loss2: (0.3835) | Acc: (89.00%) (35734/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.6862) |  Loss2: (0.3835) | Acc: (89.00%) (36884/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.6862) |  Loss2: (0.3835) | Acc: (89.00%) (38024/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.6865) |  Loss2: (0.3834) | Acc: (89.00%) (39173/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.6864) |  Loss2: (0.3834) | Acc: (89.00%) (40322/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.6865) |  Loss2: (0.3834) | Acc: (89.00%) (41467/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.6869) |  Loss2: (0.3833) | Acc: (89.00%) (42607/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.6860) |  Loss2: (0.3833) | Acc: (89.00%) (43775/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.6864) |  Loss2: (0.3833) | Acc: (89.00%) (44879/50000)
# TEST : Loss: (0.4321) | Acc: (85.00%) (8589/10000)
percent tensor([0.7319], device='cuda:0')
percent tensor([0.7439], device='cuda:0')
percent tensor([0.7927], device='cuda:0')
percent tensor([0.7015], device='cuda:0')
percent tensor([0.7664], device='cuda:0')
percent tensor([0.8078], device='cuda:0')
percent tensor([0.8376], device='cuda:0')
percent tensor([0.1692], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.6610) |  Loss2: (0.3820) | Acc: (92.00%) (118/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.6701) |  Loss2: (0.3820) | Acc: (90.00%) (1270/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.6633) |  Loss2: (0.3820) | Acc: (90.00%) (2444/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.6584) |  Loss2: (0.3820) | Acc: (90.00%) (3601/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.6626) |  Loss2: (0.3819) | Acc: (90.00%) (4749/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.6615) |  Loss2: (0.3819) | Acc: (90.00%) (5904/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.6707) |  Loss2: (0.3819) | Acc: (90.00%) (7034/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.6725) |  Loss2: (0.3819) | Acc: (89.00%) (8173/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.6741) |  Loss2: (0.3819) | Acc: (89.00%) (9316/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.6750) |  Loss2: (0.3819) | Acc: (89.00%) (10472/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.6749) |  Loss2: (0.3819) | Acc: (89.00%) (11618/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.6751) |  Loss2: (0.3818) | Acc: (89.00%) (12769/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.6759) |  Loss2: (0.3818) | Acc: (89.00%) (13909/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.6761) |  Loss2: (0.3818) | Acc: (89.00%) (15059/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.6765) |  Loss2: (0.3818) | Acc: (89.00%) (16204/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.6779) |  Loss2: (0.3817) | Acc: (89.00%) (17330/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.6794) |  Loss2: (0.3817) | Acc: (89.00%) (18473/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.6788) |  Loss2: (0.3816) | Acc: (89.00%) (19630/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.6775) |  Loss2: (0.3816) | Acc: (89.00%) (20790/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.6799) |  Loss2: (0.3816) | Acc: (89.00%) (21924/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.6816) |  Loss2: (0.3816) | Acc: (89.00%) (23064/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.6819) |  Loss2: (0.3815) | Acc: (89.00%) (24202/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.6822) |  Loss2: (0.3815) | Acc: (89.00%) (25331/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.6832) |  Loss2: (0.3815) | Acc: (89.00%) (26467/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.6847) |  Loss2: (0.3815) | Acc: (89.00%) (27606/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.6863) |  Loss2: (0.3815) | Acc: (89.00%) (28729/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.6865) |  Loss2: (0.3815) | Acc: (89.00%) (29878/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.6862) |  Loss2: (0.3814) | Acc: (89.00%) (31018/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.6867) |  Loss2: (0.3814) | Acc: (89.00%) (32150/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.6856) |  Loss2: (0.3814) | Acc: (89.00%) (33291/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.6856) |  Loss2: (0.3814) | Acc: (89.00%) (34445/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.6853) |  Loss2: (0.3814) | Acc: (89.00%) (35594/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.6851) |  Loss2: (0.3814) | Acc: (89.00%) (36742/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.6860) |  Loss2: (0.3814) | Acc: (89.00%) (37881/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.6865) |  Loss2: (0.3814) | Acc: (89.00%) (39022/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.6856) |  Loss2: (0.3814) | Acc: (89.00%) (40183/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.6858) |  Loss2: (0.3813) | Acc: (89.00%) (41320/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.6854) |  Loss2: (0.3813) | Acc: (89.00%) (42479/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.6852) |  Loss2: (0.3813) | Acc: (89.00%) (43623/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.6845) |  Loss2: (0.3813) | Acc: (89.00%) (44737/50000)
# TEST : Loss: (0.4279) | Acc: (85.00%) (8596/10000)
percent tensor([0.7327], device='cuda:0')
percent tensor([0.7440], device='cuda:0')
percent tensor([0.7944], device='cuda:0')
percent tensor([0.7025], device='cuda:0')
percent tensor([0.7680], device='cuda:0')
percent tensor([0.8103], device='cuda:0')
percent tensor([0.8383], device='cuda:0')
percent tensor([0.1667], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.7266) |  Loss2: (0.3808) | Acc: (89.00%) (114/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.6803) |  Loss2: (0.3808) | Acc: (90.00%) (1273/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.6753) |  Loss2: (0.3807) | Acc: (90.00%) (2425/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.6860) |  Loss2: (0.3807) | Acc: (89.00%) (3560/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.6936) |  Loss2: (0.3807) | Acc: (89.00%) (4684/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.6873) |  Loss2: (0.3806) | Acc: (89.00%) (5845/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.6876) |  Loss2: (0.3806) | Acc: (89.00%) (7000/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.6874) |  Loss2: (0.3806) | Acc: (89.00%) (8148/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.6911) |  Loss2: (0.3806) | Acc: (89.00%) (9279/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.6877) |  Loss2: (0.3806) | Acc: (89.00%) (10430/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.6869) |  Loss2: (0.3805) | Acc: (89.00%) (11576/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.6881) |  Loss2: (0.3805) | Acc: (89.00%) (12716/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.6898) |  Loss2: (0.3805) | Acc: (89.00%) (13850/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.6920) |  Loss2: (0.3805) | Acc: (89.00%) (14985/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.6927) |  Loss2: (0.3805) | Acc: (89.00%) (16119/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.6915) |  Loss2: (0.3805) | Acc: (89.00%) (17273/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.6905) |  Loss2: (0.3804) | Acc: (89.00%) (18423/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.6909) |  Loss2: (0.3804) | Acc: (89.00%) (19566/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.6893) |  Loss2: (0.3804) | Acc: (89.00%) (20722/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.6890) |  Loss2: (0.3803) | Acc: (89.00%) (21873/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.6884) |  Loss2: (0.3803) | Acc: (89.00%) (23036/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.6866) |  Loss2: (0.3802) | Acc: (89.00%) (24197/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.6863) |  Loss2: (0.3802) | Acc: (89.00%) (25330/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.6856) |  Loss2: (0.3802) | Acc: (89.00%) (26483/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.6845) |  Loss2: (0.3801) | Acc: (89.00%) (27632/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.6839) |  Loss2: (0.3801) | Acc: (89.00%) (28784/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.6832) |  Loss2: (0.3801) | Acc: (89.00%) (29934/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.6838) |  Loss2: (0.3800) | Acc: (89.00%) (31066/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.6842) |  Loss2: (0.3800) | Acc: (89.00%) (32202/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.6840) |  Loss2: (0.3800) | Acc: (89.00%) (33358/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.6845) |  Loss2: (0.3800) | Acc: (89.00%) (34506/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.6842) |  Loss2: (0.3800) | Acc: (89.00%) (35670/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.6842) |  Loss2: (0.3799) | Acc: (89.00%) (36818/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.6831) |  Loss2: (0.3799) | Acc: (89.00%) (37982/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.6832) |  Loss2: (0.3799) | Acc: (89.00%) (39116/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.6834) |  Loss2: (0.3799) | Acc: (89.00%) (40250/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.6832) |  Loss2: (0.3799) | Acc: (89.00%) (41404/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.6838) |  Loss2: (0.3798) | Acc: (89.00%) (42552/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.6829) |  Loss2: (0.3798) | Acc: (89.00%) (43719/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.6830) |  Loss2: (0.3798) | Acc: (89.00%) (44823/50000)
# TEST : Loss: (0.4266) | Acc: (86.00%) (8606/10000)
percent tensor([0.7353], device='cuda:0')
percent tensor([0.7442], device='cuda:0')
percent tensor([0.7958], device='cuda:0')
percent tensor([0.7048], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8400], device='cuda:0')
percent tensor([0.1640], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (1241/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (2385/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (3525/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (4662/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (5789/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (6922/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (8069/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (9198/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.3211) |  Loss2: (0.0000) | Acc: (88.00%) (10350/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (11471/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (12618/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (13754/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (14907/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (88.00%) (16044/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (17181/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (18325/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (88.00%) (19466/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (20597/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (21737/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (22883/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (88.00%) (24008/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (25129/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (26263/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (88.00%) (27403/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (28562/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (88.00%) (29714/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.3175) |  Loss2: (0.0000) | Acc: (88.00%) (30862/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (88.00%) (32005/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (88.00%) (33140/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (34294/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.3165) |  Loss2: (0.0000) | Acc: (89.00%) (35438/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (36574/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (37710/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (89.00%) (38867/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (89.00%) (39994/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (41146/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (42289/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (43409/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (88.00%) (44498/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
# TEST : Loss: (0.4546) | Acc: (85.00%) (8541/10000)

Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
Epoch: 111 | Batch_idx: 0 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (88.00%) (1248/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (88.00%) (2390/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (3547/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (4701/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (5822/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (6958/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (8098/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (9233/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (88.00%) (10363/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.3067) |  Loss2: (0.0000) | Acc: (88.00%) (11501/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.3071) |  Loss2: (0.0000) | Acc: (88.00%) (12643/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (13794/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.3094) |  Loss2: (0.0000) | Acc: (89.00%) (14933/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (16067/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (17209/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (18360/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (19505/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (20643/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (21788/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (22930/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (24074/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (25211/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (26333/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (27479/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (28605/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (88.00%) (29727/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (30893/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (32018/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (33157/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (34318/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (35476/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (36619/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (37761/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (38900/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (40053/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (41191/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (42325/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (43460/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (44559/50000)
# TEST : Loss: (0.4548) | Acc: (85.00%) (8543/10000)
percent tensor([0.7354], device='cuda:0')
percent tensor([0.7442], device='cuda:0')
percent tensor([0.7955], device='cuda:0')
percent tensor([0.7044], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.1641], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (4768/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (90.00%) (5911/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (7056/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (90.00%) (8213/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (9357/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (90.00%) (10484/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (11651/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (12802/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (13938/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (15081/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (16241/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (89.00%) (17376/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (18503/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (19611/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (20760/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (21921/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (23053/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (24204/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (25355/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (26510/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (27640/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (28787/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (29955/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (31095/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (32234/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (33384/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (34525/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (35677/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (36819/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (37943/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (39084/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (40204/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (41330/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (42489/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (43647/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (44738/50000)
# TEST : Loss: (0.4602) | Acc: (84.00%) (8473/10000)
percent tensor([0.7354], device='cuda:0')
percent tensor([0.7442], device='cuda:0')
percent tensor([0.7955], device='cuda:0')
percent tensor([0.7044], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.1641], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (2412/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (3560/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (4724/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (5869/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (89.00%) (7011/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (8154/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (9310/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (10451/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (11588/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (12716/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (13867/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (15016/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (16159/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (17317/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (18469/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.3009) |  Loss2: (0.0000) | Acc: (89.00%) (19601/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (20736/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (21870/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (23039/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (24183/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (25340/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (26487/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (27632/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (28786/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (29940/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (31085/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (32250/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (33401/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (34559/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (35721/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (36868/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (37996/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (39131/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (40279/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (41446/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (42605/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (43735/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (44830/50000)
# TEST : Loss: (0.4883) | Acc: (85.00%) (8505/10000)
percent tensor([0.7354], device='cuda:0')
percent tensor([0.7442], device='cuda:0')
percent tensor([0.7955], device='cuda:0')
percent tensor([0.7044], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.1641], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (2430/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (3578/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (4747/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (5916/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (7077/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (8232/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (9381/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (10554/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (11719/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (12866/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (14012/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (15165/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (16321/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (17467/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (18617/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (19769/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (20924/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (22074/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (23224/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (24379/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (25543/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (26674/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (27832/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (28996/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (90.00%) (30137/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (90.00%) (31271/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (32412/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (33563/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (34731/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (35896/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (37037/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (38193/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (39313/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (40466/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (90.00%) (41618/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (90.00%) (42761/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (90.00%) (43930/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (90.00%) (45043/50000)
# TEST : Loss: (0.4631) | Acc: (85.00%) (8532/10000)
percent tensor([0.7354], device='cuda:0')
percent tensor([0.7442], device='cuda:0')
percent tensor([0.7955], device='cuda:0')
percent tensor([0.7044], device='cuda:0')
percent tensor([0.7693], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.1641], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.7041) |  Loss2: (0.3791) | Acc: (91.00%) (117/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.6854) |  Loss2: (0.3790) | Acc: (89.00%) (1266/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.6911) |  Loss2: (0.3789) | Acc: (89.00%) (2403/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.7198) |  Loss2: (0.3788) | Acc: (88.00%) (3510/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.7297) |  Loss2: (0.3787) | Acc: (87.00%) (4614/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.7306) |  Loss2: (0.3786) | Acc: (87.00%) (5733/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.7313) |  Loss2: (0.3784) | Acc: (87.00%) (6853/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.7314) |  Loss2: (0.3782) | Acc: (87.00%) (7954/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.7316) |  Loss2: (0.3781) | Acc: (87.00%) (9078/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.7322) |  Loss2: (0.3779) | Acc: (87.00%) (10188/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.7284) |  Loss2: (0.3778) | Acc: (87.00%) (11329/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.7247) |  Loss2: (0.3776) | Acc: (87.00%) (12464/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.7254) |  Loss2: (0.3775) | Acc: (87.00%) (13577/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.7215) |  Loss2: (0.3774) | Acc: (87.00%) (14738/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.7193) |  Loss2: (0.3773) | Acc: (88.00%) (15888/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.7169) |  Loss2: (0.3772) | Acc: (88.00%) (17042/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.7176) |  Loss2: (0.3770) | Acc: (88.00%) (18167/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.7177) |  Loss2: (0.3769) | Acc: (88.00%) (19285/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.7177) |  Loss2: (0.3768) | Acc: (88.00%) (20407/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.7170) |  Loss2: (0.3767) | Acc: (88.00%) (21538/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.7153) |  Loss2: (0.3766) | Acc: (88.00%) (22678/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.7131) |  Loss2: (0.3765) | Acc: (88.00%) (23825/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.7141) |  Loss2: (0.3764) | Acc: (88.00%) (24946/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.7137) |  Loss2: (0.3763) | Acc: (88.00%) (26086/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.7124) |  Loss2: (0.3762) | Acc: (88.00%) (27225/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.7130) |  Loss2: (0.3761) | Acc: (88.00%) (28349/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.7108) |  Loss2: (0.3760) | Acc: (88.00%) (29488/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.7113) |  Loss2: (0.3759) | Acc: (88.00%) (30615/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.7105) |  Loss2: (0.3759) | Acc: (88.00%) (31761/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.7094) |  Loss2: (0.3758) | Acc: (88.00%) (32918/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.7085) |  Loss2: (0.3757) | Acc: (88.00%) (34055/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.7086) |  Loss2: (0.3756) | Acc: (88.00%) (35183/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.7070) |  Loss2: (0.3755) | Acc: (88.00%) (36343/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.7057) |  Loss2: (0.3754) | Acc: (88.00%) (37498/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.7047) |  Loss2: (0.3754) | Acc: (88.00%) (38641/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.7040) |  Loss2: (0.3753) | Acc: (88.00%) (39790/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.7036) |  Loss2: (0.3752) | Acc: (88.00%) (40930/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.7033) |  Loss2: (0.3752) | Acc: (88.00%) (42076/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.7031) |  Loss2: (0.3751) | Acc: (88.00%) (43212/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.7024) |  Loss2: (0.3750) | Acc: (88.00%) (44313/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_115.pth.tar'
# TEST : Loss: (0.4308) | Acc: (85.00%) (8599/10000)
percent tensor([0.7512], device='cuda:0')
percent tensor([0.7595], device='cuda:0')
percent tensor([0.8007], device='cuda:0')
percent tensor([0.7096], device='cuda:0')
percent tensor([0.7743], device='cuda:0')
percent tensor([0.8077], device='cuda:0')
percent tensor([0.8442], device='cuda:0')
percent tensor([0.1637], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.6972) |  Loss2: (0.3723) | Acc: (89.00%) (115/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.6924) |  Loss2: (0.3722) | Acc: (89.00%) (1261/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.6772) |  Loss2: (0.3721) | Acc: (89.00%) (2407/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.6845) |  Loss2: (0.3720) | Acc: (89.00%) (3549/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.6801) |  Loss2: (0.3719) | Acc: (89.00%) (4702/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.6775) |  Loss2: (0.3719) | Acc: (89.00%) (5851/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.6807) |  Loss2: (0.3718) | Acc: (89.00%) (6979/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.6807) |  Loss2: (0.3717) | Acc: (89.00%) (8141/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.6829) |  Loss2: (0.3717) | Acc: (89.00%) (9274/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.6818) |  Loss2: (0.3716) | Acc: (89.00%) (10426/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.6812) |  Loss2: (0.3716) | Acc: (89.00%) (11563/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.6787) |  Loss2: (0.3716) | Acc: (89.00%) (12714/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.6769) |  Loss2: (0.3715) | Acc: (89.00%) (13870/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.6775) |  Loss2: (0.3715) | Acc: (89.00%) (15008/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.6762) |  Loss2: (0.3715) | Acc: (89.00%) (16169/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.6751) |  Loss2: (0.3715) | Acc: (89.00%) (17313/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.6768) |  Loss2: (0.3714) | Acc: (89.00%) (18443/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.6779) |  Loss2: (0.3714) | Acc: (89.00%) (19576/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.6771) |  Loss2: (0.3714) | Acc: (89.00%) (20732/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.6770) |  Loss2: (0.3713) | Acc: (89.00%) (21871/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.6776) |  Loss2: (0.3713) | Acc: (89.00%) (23000/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.6779) |  Loss2: (0.3713) | Acc: (89.00%) (24138/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.6784) |  Loss2: (0.3712) | Acc: (89.00%) (25280/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.6782) |  Loss2: (0.3712) | Acc: (89.00%) (26426/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.6793) |  Loss2: (0.3712) | Acc: (89.00%) (27548/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.6791) |  Loss2: (0.3711) | Acc: (89.00%) (28705/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.6783) |  Loss2: (0.3711) | Acc: (89.00%) (29845/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.6781) |  Loss2: (0.3711) | Acc: (89.00%) (31001/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.6779) |  Loss2: (0.3710) | Acc: (89.00%) (32135/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.6781) |  Loss2: (0.3710) | Acc: (89.00%) (33278/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.6779) |  Loss2: (0.3709) | Acc: (89.00%) (34426/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.6780) |  Loss2: (0.3709) | Acc: (89.00%) (35566/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.6772) |  Loss2: (0.3708) | Acc: (89.00%) (36722/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.6761) |  Loss2: (0.3708) | Acc: (89.00%) (37879/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.6771) |  Loss2: (0.3707) | Acc: (89.00%) (39018/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.6761) |  Loss2: (0.3707) | Acc: (89.00%) (40180/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.6764) |  Loss2: (0.3706) | Acc: (89.00%) (41322/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.6758) |  Loss2: (0.3706) | Acc: (89.00%) (42479/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.6745) |  Loss2: (0.3705) | Acc: (89.00%) (43648/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.6740) |  Loss2: (0.3705) | Acc: (89.00%) (44751/50000)
# TEST : Loss: (0.4224) | Acc: (86.00%) (8613/10000)
percent tensor([0.7547], device='cuda:0')
percent tensor([0.7604], device='cuda:0')
percent tensor([0.8044], device='cuda:0')
percent tensor([0.7154], device='cuda:0')
percent tensor([0.7790], device='cuda:0')
percent tensor([0.8103], device='cuda:0')
percent tensor([0.8462], device='cuda:0')
percent tensor([0.1620], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.6423) |  Loss2: (0.3688) | Acc: (86.00%) (111/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.6261) |  Loss2: (0.3688) | Acc: (91.00%) (1288/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.6411) |  Loss2: (0.3687) | Acc: (90.00%) (2437/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.6567) |  Loss2: (0.3687) | Acc: (90.00%) (3574/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.6693) |  Loss2: (0.3687) | Acc: (89.00%) (4695/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.6682) |  Loss2: (0.3687) | Acc: (89.00%) (5841/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.6689) |  Loss2: (0.3687) | Acc: (89.00%) (6992/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.6693) |  Loss2: (0.3687) | Acc: (89.00%) (8140/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.6647) |  Loss2: (0.3687) | Acc: (89.00%) (9295/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.6638) |  Loss2: (0.3687) | Acc: (89.00%) (10447/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.6620) |  Loss2: (0.3687) | Acc: (89.00%) (11604/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.6599) |  Loss2: (0.3687) | Acc: (89.00%) (12760/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.6596) |  Loss2: (0.3686) | Acc: (89.00%) (13915/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.6600) |  Loss2: (0.3686) | Acc: (89.00%) (15062/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.6593) |  Loss2: (0.3686) | Acc: (89.00%) (16218/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.6597) |  Loss2: (0.3686) | Acc: (89.00%) (17367/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.6613) |  Loss2: (0.3686) | Acc: (89.00%) (18506/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.6605) |  Loss2: (0.3685) | Acc: (89.00%) (19676/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.6599) |  Loss2: (0.3685) | Acc: (89.00%) (20848/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.6613) |  Loss2: (0.3685) | Acc: (89.00%) (21993/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.6596) |  Loss2: (0.3684) | Acc: (90.00%) (23167/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.6611) |  Loss2: (0.3684) | Acc: (89.00%) (24291/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.6619) |  Loss2: (0.3684) | Acc: (89.00%) (25433/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.6627) |  Loss2: (0.3684) | Acc: (89.00%) (26570/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.6621) |  Loss2: (0.3683) | Acc: (89.00%) (27732/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.6627) |  Loss2: (0.3683) | Acc: (89.00%) (28886/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.6638) |  Loss2: (0.3683) | Acc: (89.00%) (30034/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.6637) |  Loss2: (0.3683) | Acc: (89.00%) (31186/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.6638) |  Loss2: (0.3682) | Acc: (89.00%) (32336/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.6630) |  Loss2: (0.3682) | Acc: (89.00%) (33505/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.6635) |  Loss2: (0.3682) | Acc: (89.00%) (34648/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.6646) |  Loss2: (0.3682) | Acc: (89.00%) (35770/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.6637) |  Loss2: (0.3681) | Acc: (89.00%) (36933/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.6638) |  Loss2: (0.3681) | Acc: (89.00%) (38089/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.6643) |  Loss2: (0.3681) | Acc: (89.00%) (39248/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.6633) |  Loss2: (0.3680) | Acc: (89.00%) (40420/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.6634) |  Loss2: (0.3680) | Acc: (89.00%) (41571/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.6641) |  Loss2: (0.3680) | Acc: (89.00%) (42699/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.6638) |  Loss2: (0.3679) | Acc: (89.00%) (43848/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.6630) |  Loss2: (0.3679) | Acc: (89.00%) (44963/50000)
# TEST : Loss: (0.4132) | Acc: (86.00%) (8645/10000)
percent tensor([0.7557], device='cuda:0')
percent tensor([0.7609], device='cuda:0')
percent tensor([0.8061], device='cuda:0')
percent tensor([0.7184], device='cuda:0')
percent tensor([0.7806], device='cuda:0')
percent tensor([0.8129], device='cuda:0')
percent tensor([0.8478], device='cuda:0')
percent tensor([0.1597], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.6596) |  Loss2: (0.3670) | Acc: (88.00%) (113/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.6545) |  Loss2: (0.3669) | Acc: (90.00%) (1270/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.6480) |  Loss2: (0.3669) | Acc: (90.00%) (2437/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.6500) |  Loss2: (0.3669) | Acc: (90.00%) (3596/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.6623) |  Loss2: (0.3669) | Acc: (89.00%) (4709/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.6618) |  Loss2: (0.3668) | Acc: (89.00%) (5857/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.6673) |  Loss2: (0.3668) | Acc: (89.00%) (6990/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.6629) |  Loss2: (0.3667) | Acc: (89.00%) (8153/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.6603) |  Loss2: (0.3667) | Acc: (89.00%) (9313/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.6620) |  Loss2: (0.3666) | Acc: (89.00%) (10454/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.6602) |  Loss2: (0.3666) | Acc: (89.00%) (11616/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.6595) |  Loss2: (0.3666) | Acc: (89.00%) (12767/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.6615) |  Loss2: (0.3665) | Acc: (89.00%) (13900/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.6618) |  Loss2: (0.3665) | Acc: (89.00%) (15037/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.6617) |  Loss2: (0.3665) | Acc: (89.00%) (16190/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.6613) |  Loss2: (0.3664) | Acc: (89.00%) (17348/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.6597) |  Loss2: (0.3664) | Acc: (89.00%) (18509/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.6616) |  Loss2: (0.3664) | Acc: (89.00%) (19650/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.6618) |  Loss2: (0.3664) | Acc: (89.00%) (20799/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.6622) |  Loss2: (0.3663) | Acc: (89.00%) (21945/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.6619) |  Loss2: (0.3663) | Acc: (89.00%) (23097/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.6625) |  Loss2: (0.3663) | Acc: (89.00%) (24240/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.6619) |  Loss2: (0.3663) | Acc: (89.00%) (25397/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.6611) |  Loss2: (0.3663) | Acc: (89.00%) (26544/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.6611) |  Loss2: (0.3663) | Acc: (89.00%) (27697/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.6606) |  Loss2: (0.3663) | Acc: (89.00%) (28849/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.6598) |  Loss2: (0.3662) | Acc: (89.00%) (30013/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.6606) |  Loss2: (0.3662) | Acc: (89.00%) (31152/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.6604) |  Loss2: (0.3662) | Acc: (89.00%) (32306/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.6603) |  Loss2: (0.3662) | Acc: (89.00%) (33470/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.6616) |  Loss2: (0.3662) | Acc: (89.00%) (34605/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.6619) |  Loss2: (0.3661) | Acc: (89.00%) (35758/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.6619) |  Loss2: (0.3661) | Acc: (89.00%) (36903/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.6623) |  Loss2: (0.3661) | Acc: (89.00%) (38046/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.6622) |  Loss2: (0.3661) | Acc: (89.00%) (39192/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.6625) |  Loss2: (0.3660) | Acc: (89.00%) (40344/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.6621) |  Loss2: (0.3660) | Acc: (89.00%) (41502/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.6618) |  Loss2: (0.3660) | Acc: (89.00%) (42662/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.6612) |  Loss2: (0.3660) | Acc: (89.00%) (43829/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.6616) |  Loss2: (0.3660) | Acc: (89.00%) (44932/50000)
# TEST : Loss: (0.4122) | Acc: (86.00%) (8648/10000)
percent tensor([0.7570], device='cuda:0')
percent tensor([0.7616], device='cuda:0')
percent tensor([0.8081], device='cuda:0')
percent tensor([0.7209], device='cuda:0')
percent tensor([0.7813], device='cuda:0')
percent tensor([0.8157], device='cuda:0')
percent tensor([0.8491], device='cuda:0')
percent tensor([0.1574], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.7257) |  Loss2: (0.3653) | Acc: (89.00%) (115/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.6872) |  Loss2: (0.3653) | Acc: (89.00%) (1257/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.6629) |  Loss2: (0.3653) | Acc: (90.00%) (2422/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.6718) |  Loss2: (0.3653) | Acc: (89.00%) (3562/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.6629) |  Loss2: (0.3652) | Acc: (90.00%) (4725/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.6630) |  Loss2: (0.3652) | Acc: (90.00%) (5885/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.6602) |  Loss2: (0.3652) | Acc: (90.00%) (7049/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.6577) |  Loss2: (0.3651) | Acc: (90.00%) (8205/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.6569) |  Loss2: (0.3651) | Acc: (90.00%) (9358/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.6579) |  Loss2: (0.3651) | Acc: (90.00%) (10499/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.6565) |  Loss2: (0.3651) | Acc: (90.00%) (11658/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.6559) |  Loss2: (0.3650) | Acc: (90.00%) (12819/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.6553) |  Loss2: (0.3650) | Acc: (90.00%) (13974/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.6550) |  Loss2: (0.3650) | Acc: (90.00%) (15122/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.6559) |  Loss2: (0.3650) | Acc: (90.00%) (16259/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.6561) |  Loss2: (0.3649) | Acc: (90.00%) (17398/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.6572) |  Loss2: (0.3649) | Acc: (89.00%) (18532/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.6572) |  Loss2: (0.3649) | Acc: (89.00%) (19688/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.6579) |  Loss2: (0.3649) | Acc: (89.00%) (20828/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.6571) |  Loss2: (0.3648) | Acc: (89.00%) (21983/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.6584) |  Loss2: (0.3648) | Acc: (89.00%) (23120/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.6586) |  Loss2: (0.3648) | Acc: (89.00%) (24262/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.6573) |  Loss2: (0.3648) | Acc: (89.00%) (25422/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.6570) |  Loss2: (0.3647) | Acc: (89.00%) (26586/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.6570) |  Loss2: (0.3647) | Acc: (89.00%) (27745/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.6568) |  Loss2: (0.3647) | Acc: (89.00%) (28899/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.6568) |  Loss2: (0.3647) | Acc: (89.00%) (30053/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.6565) |  Loss2: (0.3647) | Acc: (89.00%) (31218/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.6564) |  Loss2: (0.3646) | Acc: (90.00%) (32372/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.6558) |  Loss2: (0.3646) | Acc: (90.00%) (33527/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.6566) |  Loss2: (0.3646) | Acc: (89.00%) (34673/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.6568) |  Loss2: (0.3646) | Acc: (89.00%) (35812/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.6555) |  Loss2: (0.3646) | Acc: (89.00%) (36978/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.6564) |  Loss2: (0.3646) | Acc: (89.00%) (38119/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.6566) |  Loss2: (0.3646) | Acc: (89.00%) (39276/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.6561) |  Loss2: (0.3646) | Acc: (90.00%) (40439/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.6555) |  Loss2: (0.3646) | Acc: (90.00%) (41604/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.6558) |  Loss2: (0.3646) | Acc: (89.00%) (42737/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.6558) |  Loss2: (0.3646) | Acc: (89.00%) (43880/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.6567) |  Loss2: (0.3646) | Acc: (89.00%) (44974/50000)
# TEST : Loss: (0.4096) | Acc: (86.00%) (8671/10000)
percent tensor([0.7573], device='cuda:0')
percent tensor([0.7607], device='cuda:0')
percent tensor([0.8080], device='cuda:0')
percent tensor([0.7226], device='cuda:0')
percent tensor([0.7832], device='cuda:0')
percent tensor([0.8174], device='cuda:0')
percent tensor([0.8496], device='cuda:0')
percent tensor([0.1550], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.4335) |  Loss2: (0.1823) | Acc: (93.00%) (120/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.4544) |  Loss2: (0.1823) | Acc: (91.00%) (1284/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.4797) |  Loss2: (0.1823) | Acc: (90.00%) (2420/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.4847) |  Loss2: (0.1824) | Acc: (89.00%) (3570/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.4840) |  Loss2: (0.1824) | Acc: (89.00%) (4714/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.4901) |  Loss2: (0.1825) | Acc: (89.00%) (5862/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.4927) |  Loss2: (0.1825) | Acc: (89.00%) (7005/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.4976) |  Loss2: (0.1825) | Acc: (89.00%) (8139/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.4973) |  Loss2: (0.1826) | Acc: (89.00%) (9276/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.4941) |  Loss2: (0.1826) | Acc: (89.00%) (10437/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.4900) |  Loss2: (0.1826) | Acc: (89.00%) (11587/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.4916) |  Loss2: (0.1827) | Acc: (89.00%) (12719/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.4909) |  Loss2: (0.1827) | Acc: (89.00%) (13860/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.4885) |  Loss2: (0.1828) | Acc: (89.00%) (15026/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.4873) |  Loss2: (0.1828) | Acc: (89.00%) (16166/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.4881) |  Loss2: (0.1828) | Acc: (89.00%) (17315/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.4873) |  Loss2: (0.1829) | Acc: (89.00%) (18467/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.4872) |  Loss2: (0.1829) | Acc: (89.00%) (19611/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.4868) |  Loss2: (0.1829) | Acc: (89.00%) (20761/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.4879) |  Loss2: (0.1830) | Acc: (89.00%) (21903/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.4883) |  Loss2: (0.1830) | Acc: (89.00%) (23032/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.4898) |  Loss2: (0.1830) | Acc: (89.00%) (24143/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.4883) |  Loss2: (0.1830) | Acc: (89.00%) (25299/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.4883) |  Loss2: (0.1831) | Acc: (89.00%) (26446/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.4879) |  Loss2: (0.1831) | Acc: (89.00%) (27608/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.4878) |  Loss2: (0.1831) | Acc: (89.00%) (28749/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.4882) |  Loss2: (0.1831) | Acc: (89.00%) (29892/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.4876) |  Loss2: (0.1832) | Acc: (89.00%) (31042/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.4870) |  Loss2: (0.1832) | Acc: (89.00%) (32192/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.4873) |  Loss2: (0.1832) | Acc: (89.00%) (33342/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.4860) |  Loss2: (0.1832) | Acc: (89.00%) (34513/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.4852) |  Loss2: (0.1833) | Acc: (89.00%) (35665/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.4861) |  Loss2: (0.1833) | Acc: (89.00%) (36786/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.4862) |  Loss2: (0.1833) | Acc: (89.00%) (37945/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.4857) |  Loss2: (0.1833) | Acc: (89.00%) (39082/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.4851) |  Loss2: (0.1833) | Acc: (89.00%) (40234/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.4847) |  Loss2: (0.1834) | Acc: (89.00%) (41384/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.4848) |  Loss2: (0.1834) | Acc: (89.00%) (42531/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.4852) |  Loss2: (0.1834) | Acc: (89.00%) (43679/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.4844) |  Loss2: (0.1834) | Acc: (89.00%) (44799/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_120.pth.tar'
# TEST : Loss: (0.4635) | Acc: (85.00%) (8511/10000)
percent tensor([0.7530], device='cuda:0')
percent tensor([0.7577], device='cuda:0')
percent tensor([0.8041], device='cuda:0')
percent tensor([0.7191], device='cuda:0')
percent tensor([0.7781], device='cuda:0')
percent tensor([0.8158], device='cuda:0')
percent tensor([0.8477], device='cuda:0')
percent tensor([0.1532], device='cuda:0')
Epoch: 121 | Batch_idx: 0 |  Loss: (0.4669) |  Loss2: (0.1841) | Acc: (89.00%) (114/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.4517) |  Loss2: (0.1841) | Acc: (89.00%) (1266/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.4746) |  Loss2: (0.1842) | Acc: (89.00%) (2405/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.4638) |  Loss2: (0.1842) | Acc: (90.00%) (3573/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.4733) |  Loss2: (0.1842) | Acc: (89.00%) (4705/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.4774) |  Loss2: (0.1842) | Acc: (89.00%) (5862/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.4748) |  Loss2: (0.1842) | Acc: (89.00%) (7016/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.4747) |  Loss2: (0.1843) | Acc: (89.00%) (8176/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.4731) |  Loss2: (0.1843) | Acc: (89.00%) (9324/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.4762) |  Loss2: (0.1843) | Acc: (89.00%) (10453/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.4734) |  Loss2: (0.1843) | Acc: (89.00%) (11615/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.4760) |  Loss2: (0.1843) | Acc: (89.00%) (12757/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.4742) |  Loss2: (0.1843) | Acc: (89.00%) (13919/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.4743) |  Loss2: (0.1843) | Acc: (89.00%) (15065/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.4740) |  Loss2: (0.1844) | Acc: (89.00%) (16224/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.4751) |  Loss2: (0.1844) | Acc: (89.00%) (17377/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.4751) |  Loss2: (0.1844) | Acc: (89.00%) (18542/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.4755) |  Loss2: (0.1844) | Acc: (89.00%) (19686/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.4736) |  Loss2: (0.1844) | Acc: (89.00%) (20846/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.4714) |  Loss2: (0.1844) | Acc: (90.00%) (22018/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.4710) |  Loss2: (0.1844) | Acc: (90.00%) (23171/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.4712) |  Loss2: (0.1844) | Acc: (90.00%) (24320/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.4719) |  Loss2: (0.1844) | Acc: (90.00%) (25475/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.4717) |  Loss2: (0.1844) | Acc: (90.00%) (26626/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.4723) |  Loss2: (0.1844) | Acc: (89.00%) (27762/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.4735) |  Loss2: (0.1845) | Acc: (89.00%) (28901/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.4732) |  Loss2: (0.1845) | Acc: (89.00%) (30051/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.4741) |  Loss2: (0.1845) | Acc: (89.00%) (31190/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.4746) |  Loss2: (0.1845) | Acc: (89.00%) (32341/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.4744) |  Loss2: (0.1845) | Acc: (89.00%) (33502/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.4747) |  Loss2: (0.1845) | Acc: (89.00%) (34663/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.4749) |  Loss2: (0.1845) | Acc: (89.00%) (35804/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.4752) |  Loss2: (0.1845) | Acc: (89.00%) (36951/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.4747) |  Loss2: (0.1845) | Acc: (89.00%) (38124/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.4747) |  Loss2: (0.1845) | Acc: (89.00%) (39282/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.4752) |  Loss2: (0.1845) | Acc: (89.00%) (40423/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.4746) |  Loss2: (0.1845) | Acc: (90.00%) (41588/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.4736) |  Loss2: (0.1845) | Acc: (90.00%) (42756/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.4737) |  Loss2: (0.1845) | Acc: (90.00%) (43901/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.4747) |  Loss2: (0.1845) | Acc: (89.00%) (44993/50000)
# TEST : Loss: (0.4725) | Acc: (85.00%) (8526/10000)
percent tensor([0.7531], device='cuda:0')
percent tensor([0.7576], device='cuda:0')
percent tensor([0.8033], device='cuda:0')
percent tensor([0.7177], device='cuda:0')
percent tensor([0.7758], device='cuda:0')
percent tensor([0.8143], device='cuda:0')
percent tensor([0.8459], device='cuda:0')
percent tensor([0.1515], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.4463) |  Loss2: (0.1847) | Acc: (90.00%) (116/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.4545) |  Loss2: (0.1848) | Acc: (90.00%) (1281/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.4597) |  Loss2: (0.1848) | Acc: (90.00%) (2435/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.4587) |  Loss2: (0.1848) | Acc: (90.00%) (3591/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.4618) |  Loss2: (0.1848) | Acc: (90.00%) (4741/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.4598) |  Loss2: (0.1848) | Acc: (90.00%) (5902/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.4648) |  Loss2: (0.1848) | Acc: (90.00%) (7040/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.4624) |  Loss2: (0.1848) | Acc: (90.00%) (8205/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.4693) |  Loss2: (0.1848) | Acc: (90.00%) (9341/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.4688) |  Loss2: (0.1849) | Acc: (90.00%) (10503/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.4671) |  Loss2: (0.1849) | Acc: (90.00%) (11665/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.4645) |  Loss2: (0.1849) | Acc: (90.00%) (12838/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.4627) |  Loss2: (0.1849) | Acc: (90.00%) (14006/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.4627) |  Loss2: (0.1849) | Acc: (90.00%) (15150/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.4636) |  Loss2: (0.1849) | Acc: (90.00%) (16293/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.4633) |  Loss2: (0.1849) | Acc: (90.00%) (17448/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.4643) |  Loss2: (0.1849) | Acc: (90.00%) (18595/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.4654) |  Loss2: (0.1849) | Acc: (90.00%) (19743/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.4648) |  Loss2: (0.1849) | Acc: (90.00%) (20902/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.4656) |  Loss2: (0.1849) | Acc: (90.00%) (22040/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.4658) |  Loss2: (0.1850) | Acc: (90.00%) (23203/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.4657) |  Loss2: (0.1850) | Acc: (90.00%) (24358/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.4657) |  Loss2: (0.1850) | Acc: (90.00%) (25506/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.4666) |  Loss2: (0.1850) | Acc: (90.00%) (26654/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.4676) |  Loss2: (0.1850) | Acc: (90.00%) (27794/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.4679) |  Loss2: (0.1850) | Acc: (90.00%) (28944/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.4683) |  Loss2: (0.1850) | Acc: (90.00%) (30099/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.4677) |  Loss2: (0.1850) | Acc: (90.00%) (31265/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.4676) |  Loss2: (0.1850) | Acc: (90.00%) (32427/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.4679) |  Loss2: (0.1850) | Acc: (90.00%) (33588/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.4662) |  Loss2: (0.1850) | Acc: (90.00%) (34770/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.4664) |  Loss2: (0.1850) | Acc: (90.00%) (35925/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.4671) |  Loss2: (0.1850) | Acc: (90.00%) (37067/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.4677) |  Loss2: (0.1850) | Acc: (90.00%) (38210/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.4686) |  Loss2: (0.1850) | Acc: (90.00%) (39350/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.4680) |  Loss2: (0.1850) | Acc: (90.00%) (40504/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.4674) |  Loss2: (0.1850) | Acc: (90.00%) (41680/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.4683) |  Loss2: (0.1850) | Acc: (90.00%) (42813/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.4687) |  Loss2: (0.1850) | Acc: (90.00%) (43959/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.4695) |  Loss2: (0.1850) | Acc: (90.00%) (45064/50000)
# TEST : Loss: (0.5585) | Acc: (82.00%) (8293/10000)
percent tensor([0.7523], device='cuda:0')
percent tensor([0.7589], device='cuda:0')
percent tensor([0.8022], device='cuda:0')
percent tensor([0.7175], device='cuda:0')
percent tensor([0.7752], device='cuda:0')
percent tensor([0.8131], device='cuda:0')
percent tensor([0.8454], device='cuda:0')
percent tensor([0.1500], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.5784) |  Loss2: (0.1850) | Acc: (85.00%) (110/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.4815) |  Loss2: (0.1850) | Acc: (89.00%) (1255/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.4794) |  Loss2: (0.1850) | Acc: (89.00%) (2396/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.4573) |  Loss2: (0.1850) | Acc: (90.00%) (3572/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.4654) |  Loss2: (0.1850) | Acc: (89.00%) (4714/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.4680) |  Loss2: (0.1850) | Acc: (89.00%) (5863/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.4626) |  Loss2: (0.1850) | Acc: (90.00%) (7040/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.4601) |  Loss2: (0.1850) | Acc: (90.00%) (8206/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.4620) |  Loss2: (0.1850) | Acc: (90.00%) (9351/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.4612) |  Loss2: (0.1850) | Acc: (90.00%) (10513/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.4589) |  Loss2: (0.1850) | Acc: (90.00%) (11676/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.4559) |  Loss2: (0.1850) | Acc: (90.00%) (12843/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.4571) |  Loss2: (0.1850) | Acc: (90.00%) (13998/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.4557) |  Loss2: (0.1850) | Acc: (90.00%) (15159/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.4562) |  Loss2: (0.1850) | Acc: (90.00%) (16310/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.4577) |  Loss2: (0.1850) | Acc: (90.00%) (17454/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.4592) |  Loss2: (0.1850) | Acc: (90.00%) (18602/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.4585) |  Loss2: (0.1850) | Acc: (90.00%) (19774/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.4581) |  Loss2: (0.1850) | Acc: (90.00%) (20942/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.4599) |  Loss2: (0.1850) | Acc: (90.00%) (22085/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.4600) |  Loss2: (0.1850) | Acc: (90.00%) (23248/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.4622) |  Loss2: (0.1850) | Acc: (90.00%) (24381/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.4609) |  Loss2: (0.1850) | Acc: (90.00%) (25547/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.4616) |  Loss2: (0.1850) | Acc: (90.00%) (26713/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.4614) |  Loss2: (0.1850) | Acc: (90.00%) (27879/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.4620) |  Loss2: (0.1850) | Acc: (90.00%) (29022/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.4618) |  Loss2: (0.1850) | Acc: (90.00%) (30175/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.4616) |  Loss2: (0.1850) | Acc: (90.00%) (31340/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.4613) |  Loss2: (0.1850) | Acc: (90.00%) (32505/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.4606) |  Loss2: (0.1850) | Acc: (90.00%) (33675/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.4608) |  Loss2: (0.1850) | Acc: (90.00%) (34840/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.4608) |  Loss2: (0.1850) | Acc: (90.00%) (35998/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.4597) |  Loss2: (0.1850) | Acc: (90.00%) (37174/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.4589) |  Loss2: (0.1850) | Acc: (90.00%) (38344/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.4606) |  Loss2: (0.1850) | Acc: (90.00%) (39474/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.4616) |  Loss2: (0.1850) | Acc: (90.00%) (40610/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.4618) |  Loss2: (0.1850) | Acc: (90.00%) (41763/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.4608) |  Loss2: (0.1850) | Acc: (90.00%) (42935/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.4612) |  Loss2: (0.1850) | Acc: (90.00%) (44088/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.4615) |  Loss2: (0.1850) | Acc: (90.00%) (45186/50000)
# TEST : Loss: (0.4796) | Acc: (85.00%) (8532/10000)
percent tensor([0.7547], device='cuda:0')
percent tensor([0.7605], device='cuda:0')
percent tensor([0.8028], device='cuda:0')
percent tensor([0.7174], device='cuda:0')
percent tensor([0.7759], device='cuda:0')
percent tensor([0.8113], device='cuda:0')
percent tensor([0.8443], device='cuda:0')
percent tensor([0.1484], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.4792) |  Loss2: (0.1849) | Acc: (92.00%) (118/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.4595) |  Loss2: (0.1849) | Acc: (90.00%) (1279/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.4588) |  Loss2: (0.1849) | Acc: (90.00%) (2444/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.4563) |  Loss2: (0.1849) | Acc: (91.00%) (3616/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.4533) |  Loss2: (0.1849) | Acc: (91.00%) (4778/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.4625) |  Loss2: (0.1849) | Acc: (90.00%) (5922/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.4625) |  Loss2: (0.1849) | Acc: (90.00%) (7082/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.4587) |  Loss2: (0.1849) | Acc: (90.00%) (8247/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.4601) |  Loss2: (0.1849) | Acc: (90.00%) (9404/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.4589) |  Loss2: (0.1849) | Acc: (90.00%) (10574/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.4550) |  Loss2: (0.1849) | Acc: (90.00%) (11763/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.4562) |  Loss2: (0.1849) | Acc: (90.00%) (12921/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.4549) |  Loss2: (0.1849) | Acc: (91.00%) (14103/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.4543) |  Loss2: (0.1850) | Acc: (91.00%) (15266/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.4532) |  Loss2: (0.1850) | Acc: (91.00%) (16432/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.4551) |  Loss2: (0.1850) | Acc: (90.00%) (17578/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.4534) |  Loss2: (0.1850) | Acc: (90.00%) (18743/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.4546) |  Loss2: (0.1850) | Acc: (90.00%) (19896/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.4545) |  Loss2: (0.1850) | Acc: (90.00%) (21063/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.4560) |  Loss2: (0.1850) | Acc: (90.00%) (22213/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.4561) |  Loss2: (0.1849) | Acc: (90.00%) (23363/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.4572) |  Loss2: (0.1849) | Acc: (90.00%) (24501/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.4561) |  Loss2: (0.1849) | Acc: (90.00%) (25668/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.4560) |  Loss2: (0.1849) | Acc: (90.00%) (26824/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.4551) |  Loss2: (0.1849) | Acc: (90.00%) (27993/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.4573) |  Loss2: (0.1849) | Acc: (90.00%) (29137/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.4562) |  Loss2: (0.1849) | Acc: (90.00%) (30316/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.4568) |  Loss2: (0.1849) | Acc: (90.00%) (31465/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.4574) |  Loss2: (0.1849) | Acc: (90.00%) (32614/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.4584) |  Loss2: (0.1849) | Acc: (90.00%) (33761/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.4578) |  Loss2: (0.1849) | Acc: (90.00%) (34924/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.4580) |  Loss2: (0.1849) | Acc: (90.00%) (36084/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.4581) |  Loss2: (0.1849) | Acc: (90.00%) (37243/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.4575) |  Loss2: (0.1849) | Acc: (90.00%) (38402/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.4579) |  Loss2: (0.1849) | Acc: (90.00%) (39558/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.4581) |  Loss2: (0.1849) | Acc: (90.00%) (40712/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.4588) |  Loss2: (0.1849) | Acc: (90.00%) (41860/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.4585) |  Loss2: (0.1849) | Acc: (90.00%) (43026/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.4581) |  Loss2: (0.1849) | Acc: (90.00%) (44198/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.4580) |  Loss2: (0.1849) | Acc: (90.00%) (45318/50000)
# TEST : Loss: (0.4452) | Acc: (85.00%) (8581/10000)
percent tensor([0.7556], device='cuda:0')
percent tensor([0.7609], device='cuda:0')
percent tensor([0.8032], device='cuda:0')
percent tensor([0.7188], device='cuda:0')
percent tensor([0.7761], device='cuda:0')
percent tensor([0.8112], device='cuda:0')
percent tensor([0.8434], device='cuda:0')
percent tensor([0.1470], device='cuda:0')
Epoch: 125 | Batch_idx: 0 |  Loss: (0.4468) |  Loss2: (0.1847) | Acc: (89.00%) (114/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.4629) |  Loss2: (0.1848) | Acc: (89.00%) (1266/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.4450) |  Loss2: (0.1848) | Acc: (90.00%) (2438/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.4565) |  Loss2: (0.1848) | Acc: (90.00%) (3587/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.4594) |  Loss2: (0.1848) | Acc: (90.00%) (4739/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.4561) |  Loss2: (0.1847) | Acc: (90.00%) (5922/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.4588) |  Loss2: (0.1847) | Acc: (90.00%) (7072/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.4557) |  Loss2: (0.1847) | Acc: (90.00%) (8247/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.4515) |  Loss2: (0.1848) | Acc: (90.00%) (9423/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.4505) |  Loss2: (0.1848) | Acc: (90.00%) (10585/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.4475) |  Loss2: (0.1848) | Acc: (90.00%) (11760/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.4466) |  Loss2: (0.1848) | Acc: (90.00%) (12921/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.4485) |  Loss2: (0.1848) | Acc: (90.00%) (14086/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.4491) |  Loss2: (0.1848) | Acc: (90.00%) (15252/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.4514) |  Loss2: (0.1848) | Acc: (90.00%) (16410/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.4544) |  Loss2: (0.1848) | Acc: (90.00%) (17553/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.4537) |  Loss2: (0.1848) | Acc: (90.00%) (18730/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.4553) |  Loss2: (0.1848) | Acc: (90.00%) (19883/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.4553) |  Loss2: (0.1848) | Acc: (90.00%) (21038/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.4552) |  Loss2: (0.1848) | Acc: (90.00%) (22200/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.4551) |  Loss2: (0.1847) | Acc: (90.00%) (23362/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.4549) |  Loss2: (0.1847) | Acc: (90.00%) (24524/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.4555) |  Loss2: (0.1847) | Acc: (90.00%) (25677/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.4558) |  Loss2: (0.1847) | Acc: (90.00%) (26819/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.4564) |  Loss2: (0.1847) | Acc: (90.00%) (27961/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.4578) |  Loss2: (0.1847) | Acc: (90.00%) (29107/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.4565) |  Loss2: (0.1847) | Acc: (90.00%) (30284/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.4560) |  Loss2: (0.1847) | Acc: (90.00%) (31461/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.4560) |  Loss2: (0.1847) | Acc: (90.00%) (32627/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.4548) |  Loss2: (0.1847) | Acc: (90.00%) (33804/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.4547) |  Loss2: (0.1847) | Acc: (90.00%) (34957/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.4543) |  Loss2: (0.1847) | Acc: (90.00%) (36116/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.4541) |  Loss2: (0.1847) | Acc: (90.00%) (37281/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.4537) |  Loss2: (0.1847) | Acc: (90.00%) (38450/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.4532) |  Loss2: (0.1847) | Acc: (90.00%) (39609/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.4528) |  Loss2: (0.1847) | Acc: (90.00%) (40780/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.4522) |  Loss2: (0.1847) | Acc: (90.00%) (41950/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.4526) |  Loss2: (0.1847) | Acc: (90.00%) (43101/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.4526) |  Loss2: (0.1847) | Acc: (90.00%) (44258/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.4522) |  Loss2: (0.1847) | Acc: (90.00%) (45379/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_125.pth.tar'
# TEST : Loss: (0.4473) | Acc: (85.00%) (8558/10000)
percent tensor([0.7586], device='cuda:0')
percent tensor([0.7618], device='cuda:0')
percent tensor([0.8033], device='cuda:0')
percent tensor([0.7183], device='cuda:0')
percent tensor([0.7767], device='cuda:0')
percent tensor([0.8107], device='cuda:0')
percent tensor([0.8434], device='cuda:0')
percent tensor([0.1452], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.3944) |  Loss2: (0.1845) | Acc: (91.00%) (117/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.4593) |  Loss2: (0.1845) | Acc: (89.00%) (1261/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.4485) |  Loss2: (0.1845) | Acc: (90.00%) (2438/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.4472) |  Loss2: (0.1845) | Acc: (90.00%) (3593/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.4459) |  Loss2: (0.1845) | Acc: (90.00%) (4762/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.4467) |  Loss2: (0.1845) | Acc: (90.00%) (5906/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.4508) |  Loss2: (0.1845) | Acc: (90.00%) (7059/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.4502) |  Loss2: (0.1846) | Acc: (90.00%) (8233/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.4522) |  Loss2: (0.1846) | Acc: (90.00%) (9384/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.4476) |  Loss2: (0.1846) | Acc: (90.00%) (10561/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.4466) |  Loss2: (0.1845) | Acc: (90.00%) (11725/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.4459) |  Loss2: (0.1845) | Acc: (90.00%) (12887/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.4440) |  Loss2: (0.1845) | Acc: (90.00%) (14073/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.4430) |  Loss2: (0.1845) | Acc: (90.00%) (15248/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.4425) |  Loss2: (0.1845) | Acc: (91.00%) (16424/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.4445) |  Loss2: (0.1845) | Acc: (90.00%) (17583/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.4452) |  Loss2: (0.1845) | Acc: (90.00%) (18747/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.4463) |  Loss2: (0.1845) | Acc: (90.00%) (19917/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.4452) |  Loss2: (0.1845) | Acc: (91.00%) (21090/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.4456) |  Loss2: (0.1845) | Acc: (90.00%) (22245/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.4463) |  Loss2: (0.1845) | Acc: (90.00%) (23407/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.4462) |  Loss2: (0.1845) | Acc: (90.00%) (24562/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.4467) |  Loss2: (0.1845) | Acc: (90.00%) (25726/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.4459) |  Loss2: (0.1844) | Acc: (91.00%) (26907/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.4470) |  Loss2: (0.1844) | Acc: (90.00%) (28055/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.4481) |  Loss2: (0.1844) | Acc: (90.00%) (29206/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.4470) |  Loss2: (0.1844) | Acc: (90.00%) (30385/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.4459) |  Loss2: (0.1844) | Acc: (90.00%) (31554/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.4470) |  Loss2: (0.1844) | Acc: (90.00%) (32703/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.4471) |  Loss2: (0.1844) | Acc: (90.00%) (33863/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.4473) |  Loss2: (0.1844) | Acc: (90.00%) (35033/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.4478) |  Loss2: (0.1844) | Acc: (90.00%) (36183/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.4472) |  Loss2: (0.1844) | Acc: (90.00%) (37349/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.4471) |  Loss2: (0.1844) | Acc: (90.00%) (38521/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.4474) |  Loss2: (0.1844) | Acc: (90.00%) (39687/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.4482) |  Loss2: (0.1844) | Acc: (90.00%) (40839/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.4484) |  Loss2: (0.1844) | Acc: (90.00%) (41991/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.4487) |  Loss2: (0.1844) | Acc: (90.00%) (43146/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.4499) |  Loss2: (0.1844) | Acc: (90.00%) (44292/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.4497) |  Loss2: (0.1844) | Acc: (90.00%) (45420/50000)
# TEST : Loss: (0.4873) | Acc: (85.00%) (8504/10000)
percent tensor([0.7586], device='cuda:0')
percent tensor([0.7647], device='cuda:0')
percent tensor([0.8042], device='cuda:0')
percent tensor([0.7195], device='cuda:0')
percent tensor([0.7766], device='cuda:0')
percent tensor([0.8102], device='cuda:0')
percent tensor([0.8433], device='cuda:0')
percent tensor([0.1438], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.4386) |  Loss2: (0.1842) | Acc: (92.00%) (118/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.3977) |  Loss2: (0.1842) | Acc: (92.00%) (1304/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.4331) |  Loss2: (0.1842) | Acc: (90.00%) (2446/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.4388) |  Loss2: (0.1841) | Acc: (91.00%) (3611/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.4431) |  Loss2: (0.1841) | Acc: (90.00%) (4768/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.4451) |  Loss2: (0.1841) | Acc: (90.00%) (5927/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.4406) |  Loss2: (0.1841) | Acc: (90.00%) (7104/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.4397) |  Loss2: (0.1841) | Acc: (91.00%) (8275/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.4373) |  Loss2: (0.1841) | Acc: (91.00%) (9443/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.4393) |  Loss2: (0.1841) | Acc: (90.00%) (10599/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.4424) |  Loss2: (0.1840) | Acc: (90.00%) (11757/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.4422) |  Loss2: (0.1840) | Acc: (90.00%) (12927/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.4426) |  Loss2: (0.1840) | Acc: (90.00%) (14094/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.4430) |  Loss2: (0.1840) | Acc: (90.00%) (15251/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.4443) |  Loss2: (0.1840) | Acc: (90.00%) (16405/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.4473) |  Loss2: (0.1840) | Acc: (90.00%) (17553/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.4469) |  Loss2: (0.1840) | Acc: (90.00%) (18719/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.4465) |  Loss2: (0.1840) | Acc: (90.00%) (19881/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.4446) |  Loss2: (0.1840) | Acc: (90.00%) (21056/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.4449) |  Loss2: (0.1840) | Acc: (90.00%) (22221/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.4437) |  Loss2: (0.1840) | Acc: (90.00%) (23393/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.4431) |  Loss2: (0.1840) | Acc: (90.00%) (24568/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.4424) |  Loss2: (0.1840) | Acc: (90.00%) (25737/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.4423) |  Loss2: (0.1840) | Acc: (90.00%) (26894/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.4442) |  Loss2: (0.1840) | Acc: (90.00%) (28039/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.4435) |  Loss2: (0.1840) | Acc: (90.00%) (29212/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.4429) |  Loss2: (0.1840) | Acc: (90.00%) (30381/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.4423) |  Loss2: (0.1840) | Acc: (90.00%) (31553/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.4413) |  Loss2: (0.1840) | Acc: (90.00%) (32727/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.4407) |  Loss2: (0.1840) | Acc: (90.00%) (33891/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.4408) |  Loss2: (0.1840) | Acc: (90.00%) (35057/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.4415) |  Loss2: (0.1840) | Acc: (90.00%) (36213/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.4430) |  Loss2: (0.1840) | Acc: (90.00%) (37357/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.4434) |  Loss2: (0.1840) | Acc: (90.00%) (38508/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.4441) |  Loss2: (0.1840) | Acc: (90.00%) (39664/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.4434) |  Loss2: (0.1840) | Acc: (90.00%) (40848/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.4437) |  Loss2: (0.1840) | Acc: (90.00%) (42007/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.4434) |  Loss2: (0.1839) | Acc: (90.00%) (43178/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.4438) |  Loss2: (0.1839) | Acc: (90.00%) (44329/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.4447) |  Loss2: (0.1839) | Acc: (90.00%) (45440/50000)
# TEST : Loss: (0.5176) | Acc: (84.00%) (8424/10000)
percent tensor([0.7606], device='cuda:0')
percent tensor([0.7656], device='cuda:0')
percent tensor([0.8058], device='cuda:0')
percent tensor([0.7209], device='cuda:0')
percent tensor([0.7761], device='cuda:0')
percent tensor([0.8104], device='cuda:0')
percent tensor([0.8430], device='cuda:0')
percent tensor([0.1423], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.3744) |  Loss2: (0.1838) | Acc: (94.00%) (121/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.4283) |  Loss2: (0.1838) | Acc: (92.00%) (1297/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.4312) |  Loss2: (0.1838) | Acc: (91.00%) (2461/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.4278) |  Loss2: (0.1838) | Acc: (91.00%) (3645/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.4262) |  Loss2: (0.1838) | Acc: (91.00%) (4815/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.4234) |  Loss2: (0.1838) | Acc: (91.00%) (6005/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.4241) |  Loss2: (0.1838) | Acc: (91.00%) (7171/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.4226) |  Loss2: (0.1838) | Acc: (91.00%) (8349/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.4241) |  Loss2: (0.1838) | Acc: (91.00%) (9532/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.4241) |  Loss2: (0.1838) | Acc: (91.00%) (10715/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.4260) |  Loss2: (0.1838) | Acc: (91.00%) (11879/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.4252) |  Loss2: (0.1838) | Acc: (91.00%) (13053/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.4269) |  Loss2: (0.1838) | Acc: (91.00%) (14220/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.4284) |  Loss2: (0.1838) | Acc: (91.00%) (15389/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.4296) |  Loss2: (0.1838) | Acc: (91.00%) (16552/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.4303) |  Loss2: (0.1837) | Acc: (91.00%) (17729/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.4304) |  Loss2: (0.1837) | Acc: (91.00%) (18888/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.4323) |  Loss2: (0.1837) | Acc: (91.00%) (20038/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.4329) |  Loss2: (0.1837) | Acc: (91.00%) (21206/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.4323) |  Loss2: (0.1837) | Acc: (91.00%) (22386/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.4316) |  Loss2: (0.1837) | Acc: (91.00%) (23564/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.4326) |  Loss2: (0.1837) | Acc: (91.00%) (24726/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.4326) |  Loss2: (0.1837) | Acc: (91.00%) (25889/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.4333) |  Loss2: (0.1837) | Acc: (91.00%) (27054/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.4329) |  Loss2: (0.1837) | Acc: (91.00%) (28223/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.4326) |  Loss2: (0.1837) | Acc: (91.00%) (29393/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.4327) |  Loss2: (0.1837) | Acc: (91.00%) (30560/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.4332) |  Loss2: (0.1837) | Acc: (91.00%) (31727/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.4329) |  Loss2: (0.1837) | Acc: (91.00%) (32889/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.4341) |  Loss2: (0.1837) | Acc: (91.00%) (34050/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.4355) |  Loss2: (0.1837) | Acc: (91.00%) (35206/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.4350) |  Loss2: (0.1837) | Acc: (91.00%) (36381/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.4354) |  Loss2: (0.1837) | Acc: (91.00%) (37542/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.4358) |  Loss2: (0.1837) | Acc: (91.00%) (38697/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.4352) |  Loss2: (0.1837) | Acc: (91.00%) (39877/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.4351) |  Loss2: (0.1837) | Acc: (91.00%) (41045/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.4351) |  Loss2: (0.1837) | Acc: (91.00%) (42215/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.4362) |  Loss2: (0.1837) | Acc: (91.00%) (43364/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.4360) |  Loss2: (0.1837) | Acc: (91.00%) (44530/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.4358) |  Loss2: (0.1837) | Acc: (91.00%) (45655/50000)
# TEST : Loss: (0.5273) | Acc: (84.00%) (8440/10000)
percent tensor([0.7605], device='cuda:0')
percent tensor([0.7670], device='cuda:0')
percent tensor([0.8061], device='cuda:0')
percent tensor([0.7214], device='cuda:0')
percent tensor([0.7764], device='cuda:0')
percent tensor([0.8105], device='cuda:0')
percent tensor([0.8428], device='cuda:0')
percent tensor([0.1408], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.5003) |  Loss2: (0.1837) | Acc: (89.00%) (115/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.4234) |  Loss2: (0.1837) | Acc: (91.00%) (1289/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.4344) |  Loss2: (0.1837) | Acc: (91.00%) (2452/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.4271) |  Loss2: (0.1837) | Acc: (91.00%) (3637/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.4267) |  Loss2: (0.1837) | Acc: (91.00%) (4811/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.4285) |  Loss2: (0.1837) | Acc: (91.00%) (5981/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.4306) |  Loss2: (0.1837) | Acc: (91.00%) (7143/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.4306) |  Loss2: (0.1836) | Acc: (91.00%) (8306/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.4306) |  Loss2: (0.1836) | Acc: (91.00%) (9479/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.4314) |  Loss2: (0.1836) | Acc: (91.00%) (10640/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.4300) |  Loss2: (0.1836) | Acc: (91.00%) (11815/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.4291) |  Loss2: (0.1836) | Acc: (91.00%) (12998/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.4313) |  Loss2: (0.1836) | Acc: (91.00%) (14155/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.4321) |  Loss2: (0.1836) | Acc: (91.00%) (15317/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.4321) |  Loss2: (0.1836) | Acc: (91.00%) (16482/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.4316) |  Loss2: (0.1836) | Acc: (91.00%) (17654/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.4330) |  Loss2: (0.1836) | Acc: (91.00%) (18806/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.4328) |  Loss2: (0.1836) | Acc: (91.00%) (19981/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.4327) |  Loss2: (0.1836) | Acc: (91.00%) (21151/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.4305) |  Loss2: (0.1836) | Acc: (91.00%) (22335/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.4298) |  Loss2: (0.1836) | Acc: (91.00%) (23512/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.4286) |  Loss2: (0.1836) | Acc: (91.00%) (24698/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.4291) |  Loss2: (0.1836) | Acc: (91.00%) (25863/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.4293) |  Loss2: (0.1835) | Acc: (91.00%) (27030/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.4298) |  Loss2: (0.1835) | Acc: (91.00%) (28204/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.4311) |  Loss2: (0.1835) | Acc: (91.00%) (29365/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.4308) |  Loss2: (0.1835) | Acc: (91.00%) (30535/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.4297) |  Loss2: (0.1835) | Acc: (91.00%) (31720/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.4292) |  Loss2: (0.1835) | Acc: (91.00%) (32895/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.4305) |  Loss2: (0.1835) | Acc: (91.00%) (34043/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.4301) |  Loss2: (0.1835) | Acc: (91.00%) (35214/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.4307) |  Loss2: (0.1835) | Acc: (91.00%) (36370/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.4308) |  Loss2: (0.1834) | Acc: (91.00%) (37543/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.4299) |  Loss2: (0.1834) | Acc: (91.00%) (38732/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.4301) |  Loss2: (0.1834) | Acc: (91.00%) (39902/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.4295) |  Loss2: (0.1834) | Acc: (91.00%) (41089/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.4295) |  Loss2: (0.1834) | Acc: (91.00%) (42257/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.4299) |  Loss2: (0.1834) | Acc: (91.00%) (43423/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.4307) |  Loss2: (0.1834) | Acc: (91.00%) (44584/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.4319) |  Loss2: (0.1834) | Acc: (91.00%) (45693/50000)
# TEST : Loss: (0.4789) | Acc: (85.00%) (8516/10000)
percent tensor([0.7642], device='cuda:0')
percent tensor([0.7694], device='cuda:0')
percent tensor([0.8077], device='cuda:0')
percent tensor([0.7235], device='cuda:0')
percent tensor([0.7769], device='cuda:0')
percent tensor([0.8106], device='cuda:0')
percent tensor([0.8427], device='cuda:0')
percent tensor([0.1392], device='cuda:0')
Epoch: 130 | Batch_idx: 0 |  Loss: (0.4730) |  Loss2: (0.1829) | Acc: (89.00%) (115/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.4252) |  Loss2: (0.1829) | Acc: (91.00%) (1294/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.4447) |  Loss2: (0.1829) | Acc: (91.00%) (2455/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.4382) |  Loss2: (0.1829) | Acc: (91.00%) (3623/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.4270) |  Loss2: (0.1829) | Acc: (91.00%) (4819/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.4265) |  Loss2: (0.1829) | Acc: (91.00%) (5990/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.4294) |  Loss2: (0.1829) | Acc: (91.00%) (7153/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.4308) |  Loss2: (0.1829) | Acc: (91.00%) (8322/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.4288) |  Loss2: (0.1829) | Acc: (91.00%) (9504/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.4259) |  Loss2: (0.1829) | Acc: (91.00%) (10691/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.4247) |  Loss2: (0.1829) | Acc: (91.00%) (11865/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.4234) |  Loss2: (0.1829) | Acc: (91.00%) (13046/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.4260) |  Loss2: (0.1828) | Acc: (91.00%) (14210/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.4228) |  Loss2: (0.1828) | Acc: (91.00%) (15407/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.4220) |  Loss2: (0.1828) | Acc: (91.00%) (16588/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.4229) |  Loss2: (0.1828) | Acc: (91.00%) (17758/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.4255) |  Loss2: (0.1828) | Acc: (91.00%) (18922/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.4263) |  Loss2: (0.1828) | Acc: (91.00%) (20081/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.4257) |  Loss2: (0.1828) | Acc: (91.00%) (21252/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.4260) |  Loss2: (0.1828) | Acc: (91.00%) (22426/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.4262) |  Loss2: (0.1828) | Acc: (91.00%) (23601/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.4247) |  Loss2: (0.1828) | Acc: (91.00%) (24789/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.4255) |  Loss2: (0.1828) | Acc: (91.00%) (25960/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.4258) |  Loss2: (0.1828) | Acc: (91.00%) (27127/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.4262) |  Loss2: (0.1828) | Acc: (91.00%) (28294/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.4268) |  Loss2: (0.1828) | Acc: (91.00%) (29465/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.4266) |  Loss2: (0.1828) | Acc: (91.00%) (30642/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.4263) |  Loss2: (0.1828) | Acc: (91.00%) (31807/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.4272) |  Loss2: (0.1828) | Acc: (91.00%) (32972/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.4269) |  Loss2: (0.1828) | Acc: (91.00%) (34151/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.4271) |  Loss2: (0.1828) | Acc: (91.00%) (35319/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.4274) |  Loss2: (0.1828) | Acc: (91.00%) (36485/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.4279) |  Loss2: (0.1828) | Acc: (91.00%) (37644/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.4280) |  Loss2: (0.1828) | Acc: (91.00%) (38808/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.4276) |  Loss2: (0.1828) | Acc: (91.00%) (39982/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.4275) |  Loss2: (0.1828) | Acc: (91.00%) (41155/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.4281) |  Loss2: (0.1828) | Acc: (91.00%) (42325/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.4280) |  Loss2: (0.1828) | Acc: (91.00%) (43504/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.4286) |  Loss2: (0.1828) | Acc: (91.00%) (44662/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.4286) |  Loss2: (0.1828) | Acc: (91.00%) (45785/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_130.pth.tar'
# TEST : Loss: (0.4513) | Acc: (85.00%) (8555/10000)
percent tensor([0.7654], device='cuda:0')
percent tensor([0.7708], device='cuda:0')
percent tensor([0.8087], device='cuda:0')
percent tensor([0.7252], device='cuda:0')
percent tensor([0.7760], device='cuda:0')
percent tensor([0.8105], device='cuda:0')
percent tensor([0.8427], device='cuda:0')
percent tensor([0.1378], device='cuda:0')
Epoch: 131 | Batch_idx: 0 |  Loss: (0.3824) |  Loss2: (0.1826) | Acc: (91.00%) (117/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.4155) |  Loss2: (0.1826) | Acc: (92.00%) (1296/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.4147) |  Loss2: (0.1826) | Acc: (92.00%) (2477/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.4153) |  Loss2: (0.1826) | Acc: (92.00%) (3656/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.4113) |  Loss2: (0.1826) | Acc: (92.00%) (4837/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.4144) |  Loss2: (0.1826) | Acc: (92.00%) (6014/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.4088) |  Loss2: (0.1826) | Acc: (92.00%) (7212/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.4062) |  Loss2: (0.1826) | Acc: (92.00%) (8406/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.4075) |  Loss2: (0.1826) | Acc: (92.00%) (9582/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.4091) |  Loss2: (0.1826) | Acc: (92.00%) (10759/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.4126) |  Loss2: (0.1826) | Acc: (92.00%) (11917/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.4126) |  Loss2: (0.1826) | Acc: (92.00%) (13092/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.4174) |  Loss2: (0.1826) | Acc: (92.00%) (14252/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.4191) |  Loss2: (0.1826) | Acc: (91.00%) (15412/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.4161) |  Loss2: (0.1826) | Acc: (91.00%) (16603/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.4154) |  Loss2: (0.1826) | Acc: (92.00%) (17785/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.4164) |  Loss2: (0.1826) | Acc: (92.00%) (18965/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.4177) |  Loss2: (0.1826) | Acc: (92.00%) (20138/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.4172) |  Loss2: (0.1826) | Acc: (92.00%) (21326/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.4171) |  Loss2: (0.1825) | Acc: (92.00%) (22511/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.4166) |  Loss2: (0.1825) | Acc: (92.00%) (23697/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.4184) |  Loss2: (0.1825) | Acc: (92.00%) (24852/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.4182) |  Loss2: (0.1825) | Acc: (92.00%) (26031/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.4189) |  Loss2: (0.1825) | Acc: (92.00%) (27206/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.4184) |  Loss2: (0.1825) | Acc: (92.00%) (28389/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.4183) |  Loss2: (0.1825) | Acc: (92.00%) (29562/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.4192) |  Loss2: (0.1825) | Acc: (91.00%) (30725/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.4200) |  Loss2: (0.1825) | Acc: (91.00%) (31901/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.4212) |  Loss2: (0.1824) | Acc: (91.00%) (33059/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.4216) |  Loss2: (0.1824) | Acc: (91.00%) (34237/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.4225) |  Loss2: (0.1824) | Acc: (91.00%) (35405/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.4223) |  Loss2: (0.1824) | Acc: (91.00%) (36577/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.4230) |  Loss2: (0.1824) | Acc: (91.00%) (37744/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.4230) |  Loss2: (0.1824) | Acc: (91.00%) (38916/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.4232) |  Loss2: (0.1824) | Acc: (91.00%) (40084/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.4239) |  Loss2: (0.1824) | Acc: (91.00%) (41255/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.4235) |  Loss2: (0.1824) | Acc: (91.00%) (42447/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.4234) |  Loss2: (0.1824) | Acc: (91.00%) (43615/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.4236) |  Loss2: (0.1824) | Acc: (91.00%) (44785/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.4237) |  Loss2: (0.1824) | Acc: (91.00%) (45915/50000)
# TEST : Loss: (0.5663) | Acc: (83.00%) (8377/10000)
percent tensor([0.7661], device='cuda:0')
percent tensor([0.7726], device='cuda:0')
percent tensor([0.8099], device='cuda:0')
percent tensor([0.7265], device='cuda:0')
percent tensor([0.7777], device='cuda:0')
percent tensor([0.8103], device='cuda:0')
percent tensor([0.8428], device='cuda:0')
percent tensor([0.1364], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.3521) |  Loss2: (0.1821) | Acc: (94.00%) (121/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.3883) |  Loss2: (0.1821) | Acc: (93.00%) (1313/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.3974) |  Loss2: (0.1821) | Acc: (93.00%) (2500/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.4065) |  Loss2: (0.1821) | Acc: (92.00%) (3679/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.4064) |  Loss2: (0.1821) | Acc: (92.00%) (4857/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.4066) |  Loss2: (0.1821) | Acc: (92.00%) (6038/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.4068) |  Loss2: (0.1821) | Acc: (92.00%) (7222/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.4059) |  Loss2: (0.1821) | Acc: (92.00%) (8404/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.4049) |  Loss2: (0.1821) | Acc: (92.00%) (9582/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.4060) |  Loss2: (0.1821) | Acc: (92.00%) (10759/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.4065) |  Loss2: (0.1821) | Acc: (92.00%) (11934/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.4069) |  Loss2: (0.1821) | Acc: (92.00%) (13122/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.4084) |  Loss2: (0.1821) | Acc: (92.00%) (14291/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.4084) |  Loss2: (0.1821) | Acc: (92.00%) (15470/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.4112) |  Loss2: (0.1820) | Acc: (92.00%) (16643/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.4135) |  Loss2: (0.1820) | Acc: (92.00%) (17811/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.4129) |  Loss2: (0.1820) | Acc: (92.00%) (18998/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.4131) |  Loss2: (0.1820) | Acc: (92.00%) (20169/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.4138) |  Loss2: (0.1820) | Acc: (92.00%) (21338/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.4148) |  Loss2: (0.1820) | Acc: (92.00%) (22514/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.4161) |  Loss2: (0.1820) | Acc: (92.00%) (23682/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.4160) |  Loss2: (0.1820) | Acc: (92.00%) (24857/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.4163) |  Loss2: (0.1820) | Acc: (92.00%) (26027/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.4165) |  Loss2: (0.1820) | Acc: (92.00%) (27204/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.4165) |  Loss2: (0.1819) | Acc: (91.00%) (28373/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.4167) |  Loss2: (0.1819) | Acc: (91.00%) (29537/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.4173) |  Loss2: (0.1819) | Acc: (91.00%) (30697/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.4180) |  Loss2: (0.1819) | Acc: (91.00%) (31859/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.4192) |  Loss2: (0.1819) | Acc: (91.00%) (33024/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.4192) |  Loss2: (0.1819) | Acc: (91.00%) (34196/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.4194) |  Loss2: (0.1819) | Acc: (91.00%) (35370/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.4197) |  Loss2: (0.1819) | Acc: (91.00%) (36538/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.4203) |  Loss2: (0.1819) | Acc: (91.00%) (37707/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.4198) |  Loss2: (0.1818) | Acc: (91.00%) (38896/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.4204) |  Loss2: (0.1818) | Acc: (91.00%) (40060/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.4197) |  Loss2: (0.1818) | Acc: (91.00%) (41253/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.4194) |  Loss2: (0.1818) | Acc: (91.00%) (42429/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.4208) |  Loss2: (0.1818) | Acc: (91.00%) (43589/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.4207) |  Loss2: (0.1818) | Acc: (91.00%) (44768/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.4210) |  Loss2: (0.1818) | Acc: (91.00%) (45900/50000)
# TEST : Loss: (0.4837) | Acc: (85.00%) (8546/10000)
percent tensor([0.7699], device='cuda:0')
percent tensor([0.7749], device='cuda:0')
percent tensor([0.8110], device='cuda:0')
percent tensor([0.7276], device='cuda:0')
percent tensor([0.7794], device='cuda:0')
percent tensor([0.8108], device='cuda:0')
percent tensor([0.8432], device='cuda:0')
percent tensor([0.1351], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.4085) |  Loss2: (0.1813) | Acc: (90.00%) (116/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.4007) |  Loss2: (0.1813) | Acc: (92.00%) (1306/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.3841) |  Loss2: (0.1813) | Acc: (93.00%) (2507/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.3940) |  Loss2: (0.1812) | Acc: (92.00%) (3681/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.3975) |  Loss2: (0.1812) | Acc: (92.00%) (4854/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.3978) |  Loss2: (0.1812) | Acc: (92.00%) (6042/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.4014) |  Loss2: (0.1812) | Acc: (92.00%) (7226/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.4039) |  Loss2: (0.1812) | Acc: (92.00%) (8407/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.4046) |  Loss2: (0.1812) | Acc: (92.00%) (9585/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.4041) |  Loss2: (0.1812) | Acc: (92.00%) (10765/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.4030) |  Loss2: (0.1812) | Acc: (92.00%) (11947/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.4018) |  Loss2: (0.1812) | Acc: (92.00%) (13138/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.4043) |  Loss2: (0.1812) | Acc: (92.00%) (14309/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.4073) |  Loss2: (0.1812) | Acc: (92.00%) (15469/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.4062) |  Loss2: (0.1812) | Acc: (92.00%) (16660/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.4071) |  Loss2: (0.1812) | Acc: (92.00%) (17833/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.4080) |  Loss2: (0.1812) | Acc: (92.00%) (18996/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.4093) |  Loss2: (0.1812) | Acc: (92.00%) (20161/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.4080) |  Loss2: (0.1812) | Acc: (92.00%) (21350/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.4082) |  Loss2: (0.1812) | Acc: (92.00%) (22534/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.4091) |  Loss2: (0.1812) | Acc: (92.00%) (23705/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.4087) |  Loss2: (0.1812) | Acc: (92.00%) (24891/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.4078) |  Loss2: (0.1812) | Acc: (92.00%) (26079/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.4084) |  Loss2: (0.1812) | Acc: (92.00%) (27258/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.4096) |  Loss2: (0.1812) | Acc: (92.00%) (28427/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.4097) |  Loss2: (0.1811) | Acc: (92.00%) (29603/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.4098) |  Loss2: (0.1811) | Acc: (92.00%) (30774/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.4096) |  Loss2: (0.1811) | Acc: (92.00%) (31960/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.4094) |  Loss2: (0.1811) | Acc: (92.00%) (33135/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.4107) |  Loss2: (0.1811) | Acc: (92.00%) (34284/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.4110) |  Loss2: (0.1811) | Acc: (92.00%) (35462/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.4113) |  Loss2: (0.1811) | Acc: (92.00%) (36636/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.4117) |  Loss2: (0.1811) | Acc: (92.00%) (37804/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.4128) |  Loss2: (0.1811) | Acc: (91.00%) (38974/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.4134) |  Loss2: (0.1811) | Acc: (91.00%) (40142/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.4135) |  Loss2: (0.1811) | Acc: (91.00%) (41321/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.4138) |  Loss2: (0.1811) | Acc: (91.00%) (42484/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.4148) |  Loss2: (0.1811) | Acc: (91.00%) (43629/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.4152) |  Loss2: (0.1811) | Acc: (91.00%) (44802/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.4153) |  Loss2: (0.1811) | Acc: (91.00%) (45924/50000)
# TEST : Loss: (0.4088) | Acc: (86.00%) (8692/10000)
percent tensor([0.7720], device='cuda:0')
percent tensor([0.7763], device='cuda:0')
percent tensor([0.8120], device='cuda:0')
percent tensor([0.7290], device='cuda:0')
percent tensor([0.7800], device='cuda:0')
percent tensor([0.8104], device='cuda:0')
percent tensor([0.8436], device='cuda:0')
percent tensor([0.1337], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.4334) |  Loss2: (0.1808) | Acc: (93.00%) (120/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.3844) |  Loss2: (0.1808) | Acc: (92.00%) (1304/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.3944) |  Loss2: (0.1808) | Acc: (92.00%) (2489/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.3916) |  Loss2: (0.1808) | Acc: (92.00%) (3676/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.3980) |  Loss2: (0.1808) | Acc: (92.00%) (4846/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.3983) |  Loss2: (0.1807) | Acc: (92.00%) (6022/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.4026) |  Loss2: (0.1807) | Acc: (92.00%) (7195/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.3987) |  Loss2: (0.1807) | Acc: (92.00%) (8394/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.4023) |  Loss2: (0.1807) | Acc: (92.00%) (9569/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.4054) |  Loss2: (0.1807) | Acc: (92.00%) (10738/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.4055) |  Loss2: (0.1807) | Acc: (92.00%) (11918/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.4072) |  Loss2: (0.1807) | Acc: (92.00%) (13083/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.4066) |  Loss2: (0.1807) | Acc: (92.00%) (14266/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.4068) |  Loss2: (0.1807) | Acc: (92.00%) (15440/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.4081) |  Loss2: (0.1807) | Acc: (92.00%) (16612/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.4079) |  Loss2: (0.1807) | Acc: (92.00%) (17788/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.4070) |  Loss2: (0.1807) | Acc: (92.00%) (18974/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.4076) |  Loss2: (0.1807) | Acc: (92.00%) (20143/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.4082) |  Loss2: (0.1806) | Acc: (92.00%) (21323/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.4095) |  Loss2: (0.1806) | Acc: (91.00%) (22480/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.4109) |  Loss2: (0.1806) | Acc: (91.00%) (23641/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.4128) |  Loss2: (0.1806) | Acc: (91.00%) (24806/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.4121) |  Loss2: (0.1806) | Acc: (91.00%) (25991/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.4115) |  Loss2: (0.1806) | Acc: (91.00%) (27177/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.4114) |  Loss2: (0.1806) | Acc: (91.00%) (28351/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.4103) |  Loss2: (0.1806) | Acc: (91.00%) (29541/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.4104) |  Loss2: (0.1806) | Acc: (91.00%) (30712/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.4098) |  Loss2: (0.1806) | Acc: (91.00%) (31895/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.4103) |  Loss2: (0.1806) | Acc: (91.00%) (33070/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.4116) |  Loss2: (0.1806) | Acc: (91.00%) (34241/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.4112) |  Loss2: (0.1806) | Acc: (91.00%) (35425/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.4123) |  Loss2: (0.1806) | Acc: (91.00%) (36585/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.4126) |  Loss2: (0.1806) | Acc: (91.00%) (37754/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.4121) |  Loss2: (0.1806) | Acc: (91.00%) (38945/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.4119) |  Loss2: (0.1805) | Acc: (91.00%) (40122/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.4120) |  Loss2: (0.1805) | Acc: (91.00%) (41300/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.4124) |  Loss2: (0.1805) | Acc: (91.00%) (42480/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.4134) |  Loss2: (0.1805) | Acc: (91.00%) (43642/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.4131) |  Loss2: (0.1805) | Acc: (91.00%) (44829/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.4133) |  Loss2: (0.1805) | Acc: (91.00%) (45953/50000)
# TEST : Loss: (0.4280) | Acc: (86.00%) (8693/10000)
percent tensor([0.7732], device='cuda:0')
percent tensor([0.7767], device='cuda:0')
percent tensor([0.8129], device='cuda:0')
percent tensor([0.7308], device='cuda:0')
percent tensor([0.7798], device='cuda:0')
percent tensor([0.8111], device='cuda:0')
percent tensor([0.8441], device='cuda:0')
percent tensor([0.1325], device='cuda:0')
Epoch: 135 | Batch_idx: 0 |  Loss: (0.3737) |  Loss2: (0.1804) | Acc: (91.00%) (117/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.4118) |  Loss2: (0.1804) | Acc: (90.00%) (1280/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.4032) |  Loss2: (0.1804) | Acc: (91.00%) (2467/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.3974) |  Loss2: (0.1804) | Acc: (92.00%) (3655/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.3959) |  Loss2: (0.1804) | Acc: (92.00%) (4832/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.3921) |  Loss2: (0.1804) | Acc: (92.00%) (6023/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.3985) |  Loss2: (0.1803) | Acc: (92.00%) (7187/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.4012) |  Loss2: (0.1803) | Acc: (92.00%) (8363/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.4036) |  Loss2: (0.1803) | Acc: (92.00%) (9544/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.4065) |  Loss2: (0.1803) | Acc: (91.00%) (10713/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.4072) |  Loss2: (0.1803) | Acc: (92.00%) (11896/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.4080) |  Loss2: (0.1803) | Acc: (92.00%) (13075/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.4074) |  Loss2: (0.1803) | Acc: (92.00%) (14263/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.4091) |  Loss2: (0.1803) | Acc: (92.00%) (15430/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.4105) |  Loss2: (0.1803) | Acc: (91.00%) (16603/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.4108) |  Loss2: (0.1803) | Acc: (91.00%) (17774/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.4097) |  Loss2: (0.1803) | Acc: (91.00%) (18956/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.4095) |  Loss2: (0.1803) | Acc: (91.00%) (20120/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.4106) |  Loss2: (0.1802) | Acc: (91.00%) (21296/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.4091) |  Loss2: (0.1802) | Acc: (92.00%) (22497/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.4089) |  Loss2: (0.1802) | Acc: (92.00%) (23677/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.4074) |  Loss2: (0.1802) | Acc: (92.00%) (24860/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.4081) |  Loss2: (0.1802) | Acc: (92.00%) (26030/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.4086) |  Loss2: (0.1802) | Acc: (91.00%) (27195/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.4093) |  Loss2: (0.1802) | Acc: (91.00%) (28367/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.4090) |  Loss2: (0.1802) | Acc: (91.00%) (29539/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.4097) |  Loss2: (0.1802) | Acc: (91.00%) (30710/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.4096) |  Loss2: (0.1802) | Acc: (91.00%) (31897/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.4096) |  Loss2: (0.1801) | Acc: (91.00%) (33076/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.4103) |  Loss2: (0.1801) | Acc: (91.00%) (34245/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.4108) |  Loss2: (0.1801) | Acc: (91.00%) (35417/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.4089) |  Loss2: (0.1801) | Acc: (92.00%) (36627/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.4086) |  Loss2: (0.1801) | Acc: (92.00%) (37802/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.4094) |  Loss2: (0.1801) | Acc: (92.00%) (38980/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.4094) |  Loss2: (0.1801) | Acc: (91.00%) (40153/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.4097) |  Loss2: (0.1801) | Acc: (91.00%) (41323/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.4096) |  Loss2: (0.1801) | Acc: (91.00%) (42507/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.4092) |  Loss2: (0.1801) | Acc: (92.00%) (43693/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.4101) |  Loss2: (0.1801) | Acc: (91.00%) (44855/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.4100) |  Loss2: (0.1801) | Acc: (91.00%) (45991/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_135.pth.tar'
# TEST : Loss: (0.4316) | Acc: (86.00%) (8630/10000)
percent tensor([0.7754], device='cuda:0')
percent tensor([0.7802], device='cuda:0')
percent tensor([0.8142], device='cuda:0')
percent tensor([0.7319], device='cuda:0')
percent tensor([0.7800], device='cuda:0')
percent tensor([0.8113], device='cuda:0')
percent tensor([0.8441], device='cuda:0')
percent tensor([0.1312], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.3908) |  Loss2: (0.1798) | Acc: (90.00%) (116/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.4118) |  Loss2: (0.1797) | Acc: (91.00%) (1294/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.4286) |  Loss2: (0.1797) | Acc: (91.00%) (2451/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.4201) |  Loss2: (0.1797) | Acc: (91.00%) (3626/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.4090) |  Loss2: (0.1797) | Acc: (91.00%) (4817/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.4103) |  Loss2: (0.1797) | Acc: (91.00%) (5990/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.4121) |  Loss2: (0.1797) | Acc: (91.00%) (7156/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.4165) |  Loss2: (0.1797) | Acc: (91.00%) (8332/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.4119) |  Loss2: (0.1797) | Acc: (91.00%) (9529/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.4093) |  Loss2: (0.1797) | Acc: (92.00%) (10719/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.4120) |  Loss2: (0.1797) | Acc: (92.00%) (11895/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.4104) |  Loss2: (0.1797) | Acc: (91.00%) (13069/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.4108) |  Loss2: (0.1797) | Acc: (92.00%) (14258/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.4094) |  Loss2: (0.1797) | Acc: (92.00%) (15439/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.4078) |  Loss2: (0.1797) | Acc: (92.00%) (16629/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.4081) |  Loss2: (0.1797) | Acc: (92.00%) (17810/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.4069) |  Loss2: (0.1797) | Acc: (92.00%) (18996/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.4077) |  Loss2: (0.1797) | Acc: (92.00%) (20163/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.4089) |  Loss2: (0.1796) | Acc: (92.00%) (21340/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.4058) |  Loss2: (0.1796) | Acc: (92.00%) (22549/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.4051) |  Loss2: (0.1796) | Acc: (92.00%) (23742/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.4055) |  Loss2: (0.1796) | Acc: (92.00%) (24921/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.4056) |  Loss2: (0.1796) | Acc: (92.00%) (26108/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.4042) |  Loss2: (0.1796) | Acc: (92.00%) (27304/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.4047) |  Loss2: (0.1796) | Acc: (92.00%) (28480/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.4035) |  Loss2: (0.1796) | Acc: (92.00%) (29680/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.4043) |  Loss2: (0.1796) | Acc: (92.00%) (30847/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.4048) |  Loss2: (0.1795) | Acc: (92.00%) (32019/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.4062) |  Loss2: (0.1795) | Acc: (92.00%) (33185/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.4075) |  Loss2: (0.1795) | Acc: (92.00%) (34340/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.4074) |  Loss2: (0.1795) | Acc: (92.00%) (35518/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.4074) |  Loss2: (0.1795) | Acc: (92.00%) (36702/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.4079) |  Loss2: (0.1795) | Acc: (92.00%) (37868/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.4066) |  Loss2: (0.1795) | Acc: (92.00%) (39064/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.4063) |  Loss2: (0.1795) | Acc: (92.00%) (40250/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.4052) |  Loss2: (0.1795) | Acc: (92.00%) (41447/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.4064) |  Loss2: (0.1795) | Acc: (92.00%) (42605/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.4064) |  Loss2: (0.1795) | Acc: (92.00%) (43785/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.4061) |  Loss2: (0.1794) | Acc: (92.00%) (44972/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.4054) |  Loss2: (0.1794) | Acc: (92.00%) (46113/50000)
# TEST : Loss: (0.4112) | Acc: (87.00%) (8720/10000)
percent tensor([0.7775], device='cuda:0')
percent tensor([0.7818], device='cuda:0')
percent tensor([0.8155], device='cuda:0')
percent tensor([0.7346], device='cuda:0')
percent tensor([0.7810], device='cuda:0')
percent tensor([0.8120], device='cuda:0')
percent tensor([0.8438], device='cuda:0')
percent tensor([0.1300], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.3542) |  Loss2: (0.1791) | Acc: (94.00%) (121/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.3830) |  Loss2: (0.1791) | Acc: (93.00%) (1312/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.3770) |  Loss2: (0.1791) | Acc: (93.00%) (2502/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.3797) |  Loss2: (0.1791) | Acc: (93.00%) (3694/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.3833) |  Loss2: (0.1791) | Acc: (92.00%) (4869/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.3929) |  Loss2: (0.1791) | Acc: (92.00%) (6038/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.3902) |  Loss2: (0.1791) | Acc: (92.00%) (7232/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.3942) |  Loss2: (0.1791) | Acc: (92.00%) (8407/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.3962) |  Loss2: (0.1791) | Acc: (92.00%) (9580/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.3949) |  Loss2: (0.1791) | Acc: (92.00%) (10768/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.3950) |  Loss2: (0.1790) | Acc: (92.00%) (11957/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.3948) |  Loss2: (0.1790) | Acc: (92.00%) (13145/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.3963) |  Loss2: (0.1790) | Acc: (92.00%) (14324/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.3958) |  Loss2: (0.1790) | Acc: (92.00%) (15510/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.3966) |  Loss2: (0.1790) | Acc: (92.00%) (16681/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.3951) |  Loss2: (0.1790) | Acc: (92.00%) (17877/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.3953) |  Loss2: (0.1790) | Acc: (92.00%) (19048/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.3955) |  Loss2: (0.1790) | Acc: (92.00%) (20234/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.3959) |  Loss2: (0.1790) | Acc: (92.00%) (21417/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.3975) |  Loss2: (0.1790) | Acc: (92.00%) (22590/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.3966) |  Loss2: (0.1790) | Acc: (92.00%) (23782/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.3973) |  Loss2: (0.1789) | Acc: (92.00%) (24965/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.3968) |  Loss2: (0.1789) | Acc: (92.00%) (26158/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.3970) |  Loss2: (0.1789) | Acc: (92.00%) (27332/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.3973) |  Loss2: (0.1789) | Acc: (92.00%) (28507/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.3981) |  Loss2: (0.1789) | Acc: (92.00%) (29683/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.3988) |  Loss2: (0.1789) | Acc: (92.00%) (30862/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.3985) |  Loss2: (0.1789) | Acc: (92.00%) (32057/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.3985) |  Loss2: (0.1789) | Acc: (92.00%) (33248/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.3993) |  Loss2: (0.1789) | Acc: (92.00%) (34426/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.3995) |  Loss2: (0.1789) | Acc: (92.00%) (35604/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.4009) |  Loss2: (0.1789) | Acc: (92.00%) (36776/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.4012) |  Loss2: (0.1788) | Acc: (92.00%) (37951/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.4007) |  Loss2: (0.1788) | Acc: (92.00%) (39141/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.4017) |  Loss2: (0.1788) | Acc: (92.00%) (40309/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.4014) |  Loss2: (0.1788) | Acc: (92.00%) (41494/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.4015) |  Loss2: (0.1788) | Acc: (92.00%) (42677/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.4017) |  Loss2: (0.1788) | Acc: (92.00%) (43851/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.4022) |  Loss2: (0.1788) | Acc: (92.00%) (45028/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.4032) |  Loss2: (0.1788) | Acc: (92.00%) (46144/50000)
# TEST : Loss: (0.4213) | Acc: (87.00%) (8726/10000)
percent tensor([0.7800], device='cuda:0')
percent tensor([0.7830], device='cuda:0')
percent tensor([0.8151], device='cuda:0')
percent tensor([0.7366], device='cuda:0')
percent tensor([0.7817], device='cuda:0')
percent tensor([0.8126], device='cuda:0')
percent tensor([0.8451], device='cuda:0')
percent tensor([0.1289], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.3720) |  Loss2: (0.1785) | Acc: (92.00%) (119/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.4173) |  Loss2: (0.1785) | Acc: (91.00%) (1292/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.4026) |  Loss2: (0.1785) | Acc: (92.00%) (2476/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.4072) |  Loss2: (0.1785) | Acc: (92.00%) (3653/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.3993) |  Loss2: (0.1785) | Acc: (92.00%) (4847/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.4022) |  Loss2: (0.1785) | Acc: (92.00%) (6023/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.3987) |  Loss2: (0.1785) | Acc: (92.00%) (7211/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.3963) |  Loss2: (0.1785) | Acc: (92.00%) (8410/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.3953) |  Loss2: (0.1784) | Acc: (92.00%) (9610/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.3944) |  Loss2: (0.1784) | Acc: (92.00%) (10802/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.3970) |  Loss2: (0.1784) | Acc: (92.00%) (11970/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.3977) |  Loss2: (0.1784) | Acc: (92.00%) (13153/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.3967) |  Loss2: (0.1784) | Acc: (92.00%) (14344/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.3970) |  Loss2: (0.1784) | Acc: (92.00%) (15525/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.3949) |  Loss2: (0.1784) | Acc: (92.00%) (16715/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.3950) |  Loss2: (0.1784) | Acc: (92.00%) (17902/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.3945) |  Loss2: (0.1784) | Acc: (92.00%) (19082/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.3938) |  Loss2: (0.1784) | Acc: (92.00%) (20274/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.3945) |  Loss2: (0.1784) | Acc: (92.00%) (21451/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.3951) |  Loss2: (0.1783) | Acc: (92.00%) (22643/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.3948) |  Loss2: (0.1783) | Acc: (92.00%) (23831/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.3933) |  Loss2: (0.1783) | Acc: (92.00%) (25032/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.3935) |  Loss2: (0.1783) | Acc: (92.00%) (26203/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.3943) |  Loss2: (0.1783) | Acc: (92.00%) (27374/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.3949) |  Loss2: (0.1783) | Acc: (92.00%) (28541/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.3949) |  Loss2: (0.1783) | Acc: (92.00%) (29725/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.3953) |  Loss2: (0.1783) | Acc: (92.00%) (30900/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.3964) |  Loss2: (0.1783) | Acc: (92.00%) (32084/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.3965) |  Loss2: (0.1783) | Acc: (92.00%) (33262/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.3962) |  Loss2: (0.1782) | Acc: (92.00%) (34450/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.3966) |  Loss2: (0.1782) | Acc: (92.00%) (35615/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.3964) |  Loss2: (0.1782) | Acc: (92.00%) (36801/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.3962) |  Loss2: (0.1782) | Acc: (92.00%) (37980/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.3959) |  Loss2: (0.1782) | Acc: (92.00%) (39175/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.3969) |  Loss2: (0.1782) | Acc: (92.00%) (40344/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.3970) |  Loss2: (0.1782) | Acc: (92.00%) (41524/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.3974) |  Loss2: (0.1782) | Acc: (92.00%) (42703/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.3969) |  Loss2: (0.1782) | Acc: (92.00%) (43898/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.3962) |  Loss2: (0.1782) | Acc: (92.00%) (45102/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.3957) |  Loss2: (0.1782) | Acc: (92.00%) (46254/50000)
# TEST : Loss: (0.4030) | Acc: (87.00%) (8761/10000)
percent tensor([0.7813], device='cuda:0')
percent tensor([0.7850], device='cuda:0')
percent tensor([0.8159], device='cuda:0')
percent tensor([0.7372], device='cuda:0')
percent tensor([0.7836], device='cuda:0')
percent tensor([0.8127], device='cuda:0')
percent tensor([0.8449], device='cuda:0')
percent tensor([0.1276], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.4046) |  Loss2: (0.1780) | Acc: (92.00%) (119/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.3854) |  Loss2: (0.1780) | Acc: (93.00%) (1317/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.3791) |  Loss2: (0.1780) | Acc: (93.00%) (2506/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.3818) |  Loss2: (0.1780) | Acc: (92.00%) (3690/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.3835) |  Loss2: (0.1779) | Acc: (92.00%) (4874/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.3884) |  Loss2: (0.1779) | Acc: (92.00%) (6051/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.3876) |  Loss2: (0.1779) | Acc: (92.00%) (7243/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.3834) |  Loss2: (0.1779) | Acc: (92.00%) (8427/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.3787) |  Loss2: (0.1779) | Acc: (92.00%) (9633/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.3789) |  Loss2: (0.1779) | Acc: (92.00%) (10820/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.3824) |  Loss2: (0.1779) | Acc: (92.00%) (11988/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.3820) |  Loss2: (0.1778) | Acc: (92.00%) (13171/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.3835) |  Loss2: (0.1778) | Acc: (92.00%) (14355/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.3840) |  Loss2: (0.1778) | Acc: (92.00%) (15550/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.3831) |  Loss2: (0.1778) | Acc: (92.00%) (16740/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.3847) |  Loss2: (0.1778) | Acc: (92.00%) (17913/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.3867) |  Loss2: (0.1778) | Acc: (92.00%) (19089/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.3884) |  Loss2: (0.1778) | Acc: (92.00%) (20275/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.3879) |  Loss2: (0.1778) | Acc: (92.00%) (21467/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.3870) |  Loss2: (0.1778) | Acc: (92.00%) (22663/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.3878) |  Loss2: (0.1777) | Acc: (92.00%) (23853/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.3889) |  Loss2: (0.1777) | Acc: (92.00%) (25034/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.3902) |  Loss2: (0.1777) | Acc: (92.00%) (26207/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.3904) |  Loss2: (0.1777) | Acc: (92.00%) (27389/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.3911) |  Loss2: (0.1777) | Acc: (92.00%) (28571/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.3917) |  Loss2: (0.1777) | Acc: (92.00%) (29756/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.3919) |  Loss2: (0.1777) | Acc: (92.00%) (30936/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.3921) |  Loss2: (0.1777) | Acc: (92.00%) (32113/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.3923) |  Loss2: (0.1777) | Acc: (92.00%) (33301/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.3933) |  Loss2: (0.1777) | Acc: (92.00%) (34472/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.3925) |  Loss2: (0.1777) | Acc: (92.00%) (35664/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.3922) |  Loss2: (0.1777) | Acc: (92.00%) (36847/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.3919) |  Loss2: (0.1777) | Acc: (92.00%) (38042/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.3919) |  Loss2: (0.1777) | Acc: (92.00%) (39221/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.3920) |  Loss2: (0.1777) | Acc: (92.00%) (40391/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.3910) |  Loss2: (0.1776) | Acc: (92.00%) (41596/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.3912) |  Loss2: (0.1776) | Acc: (92.00%) (42773/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.3919) |  Loss2: (0.1776) | Acc: (92.00%) (43947/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.3920) |  Loss2: (0.1776) | Acc: (92.00%) (45138/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.3920) |  Loss2: (0.1776) | Acc: (92.00%) (46277/50000)
# TEST : Loss: (0.4388) | Acc: (86.00%) (8639/10000)
percent tensor([0.7846], device='cuda:0')
percent tensor([0.7872], device='cuda:0')
percent tensor([0.8167], device='cuda:0')
percent tensor([0.7394], device='cuda:0')
percent tensor([0.7842], device='cuda:0')
percent tensor([0.8129], device='cuda:0')
percent tensor([0.8450], device='cuda:0')
percent tensor([0.1263], device='cuda:0')
Epoch: 140 | Batch_idx: 0 |  Loss: (0.3582) |  Loss2: (0.1773) | Acc: (93.00%) (120/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.3756) |  Loss2: (0.1773) | Acc: (93.00%) (1316/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.3899) |  Loss2: (0.1773) | Acc: (92.00%) (2499/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.3801) |  Loss2: (0.1773) | Acc: (93.00%) (3698/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.3776) |  Loss2: (0.1773) | Acc: (93.00%) (4889/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.3755) |  Loss2: (0.1773) | Acc: (93.00%) (6080/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.3779) |  Loss2: (0.1773) | Acc: (92.00%) (7254/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.3748) |  Loss2: (0.1773) | Acc: (93.00%) (8452/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.3748) |  Loss2: (0.1773) | Acc: (93.00%) (9654/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.3739) |  Loss2: (0.1773) | Acc: (93.00%) (10849/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.3745) |  Loss2: (0.1773) | Acc: (93.00%) (12046/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.3752) |  Loss2: (0.1772) | Acc: (93.00%) (13228/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.3759) |  Loss2: (0.1772) | Acc: (93.00%) (14420/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.3778) |  Loss2: (0.1772) | Acc: (92.00%) (15594/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.3778) |  Loss2: (0.1772) | Acc: (93.00%) (16786/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.3782) |  Loss2: (0.1772) | Acc: (92.00%) (17970/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.3798) |  Loss2: (0.1772) | Acc: (92.00%) (19156/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.3792) |  Loss2: (0.1772) | Acc: (93.00%) (20359/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.3806) |  Loss2: (0.1772) | Acc: (92.00%) (21542/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.3806) |  Loss2: (0.1772) | Acc: (92.00%) (22734/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.3819) |  Loss2: (0.1772) | Acc: (92.00%) (23913/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.3847) |  Loss2: (0.1772) | Acc: (92.00%) (25076/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.3865) |  Loss2: (0.1771) | Acc: (92.00%) (26242/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.3885) |  Loss2: (0.1771) | Acc: (92.00%) (27413/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.3887) |  Loss2: (0.1771) | Acc: (92.00%) (28598/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.3899) |  Loss2: (0.1771) | Acc: (92.00%) (29772/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.3908) |  Loss2: (0.1771) | Acc: (92.00%) (30945/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.3901) |  Loss2: (0.1771) | Acc: (92.00%) (32144/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.3902) |  Loss2: (0.1771) | Acc: (92.00%) (33333/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.3897) |  Loss2: (0.1771) | Acc: (92.00%) (34523/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.3903) |  Loss2: (0.1771) | Acc: (92.00%) (35694/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.3904) |  Loss2: (0.1771) | Acc: (92.00%) (36871/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.3907) |  Loss2: (0.1770) | Acc: (92.00%) (38051/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.3910) |  Loss2: (0.1770) | Acc: (92.00%) (39228/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.3903) |  Loss2: (0.1770) | Acc: (92.00%) (40426/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.3908) |  Loss2: (0.1770) | Acc: (92.00%) (41595/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.3908) |  Loss2: (0.1770) | Acc: (92.00%) (42780/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.3899) |  Loss2: (0.1770) | Acc: (92.00%) (43976/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.3901) |  Loss2: (0.1770) | Acc: (92.00%) (45153/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.3907) |  Loss2: (0.1770) | Acc: (92.00%) (46293/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_140.pth.tar'
# TEST : Loss: (0.4261) | Acc: (87.00%) (8721/10000)
percent tensor([0.7864], device='cuda:0')
percent tensor([0.7891], device='cuda:0')
percent tensor([0.8182], device='cuda:0')
percent tensor([0.7412], device='cuda:0')
percent tensor([0.7850], device='cuda:0')
percent tensor([0.8130], device='cuda:0')
percent tensor([0.8457], device='cuda:0')
percent tensor([0.1252], device='cuda:0')
Epoch: 141 | Batch_idx: 0 |  Loss: (0.3673) |  Loss2: (0.1766) | Acc: (92.00%) (119/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.3564) |  Loss2: (0.1766) | Acc: (93.00%) (1316/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.3707) |  Loss2: (0.1766) | Acc: (93.00%) (2504/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.3607) |  Loss2: (0.1766) | Acc: (93.00%) (3712/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.3618) |  Loss2: (0.1766) | Acc: (93.00%) (4912/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.3626) |  Loss2: (0.1766) | Acc: (93.00%) (6111/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.3665) |  Loss2: (0.1766) | Acc: (93.00%) (7298/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.3674) |  Loss2: (0.1766) | Acc: (93.00%) (8490/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.3695) |  Loss2: (0.1766) | Acc: (93.00%) (9687/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.3702) |  Loss2: (0.1766) | Acc: (93.00%) (10875/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.3706) |  Loss2: (0.1765) | Acc: (93.00%) (12073/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.3720) |  Loss2: (0.1765) | Acc: (93.00%) (13257/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.3748) |  Loss2: (0.1765) | Acc: (93.00%) (14435/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.3744) |  Loss2: (0.1765) | Acc: (93.00%) (15632/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.3757) |  Loss2: (0.1765) | Acc: (93.00%) (16813/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.3754) |  Loss2: (0.1765) | Acc: (93.00%) (18009/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.3766) |  Loss2: (0.1765) | Acc: (93.00%) (19195/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.3769) |  Loss2: (0.1765) | Acc: (93.00%) (20382/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.3773) |  Loss2: (0.1764) | Acc: (93.00%) (21572/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.3791) |  Loss2: (0.1764) | Acc: (93.00%) (22760/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.3791) |  Loss2: (0.1764) | Acc: (93.00%) (23954/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.3795) |  Loss2: (0.1764) | Acc: (93.00%) (25138/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.3792) |  Loss2: (0.1764) | Acc: (93.00%) (26328/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.3806) |  Loss2: (0.1764) | Acc: (93.00%) (27515/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.3799) |  Loss2: (0.1764) | Acc: (93.00%) (28713/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.3808) |  Loss2: (0.1764) | Acc: (93.00%) (29891/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.3811) |  Loss2: (0.1763) | Acc: (93.00%) (31082/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.3810) |  Loss2: (0.1763) | Acc: (93.00%) (32279/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.3814) |  Loss2: (0.1763) | Acc: (93.00%) (33459/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.3814) |  Loss2: (0.1763) | Acc: (93.00%) (34649/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.3814) |  Loss2: (0.1763) | Acc: (93.00%) (35835/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.3826) |  Loss2: (0.1763) | Acc: (92.00%) (37002/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.3834) |  Loss2: (0.1763) | Acc: (92.00%) (38176/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.3841) |  Loss2: (0.1763) | Acc: (92.00%) (39367/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.3840) |  Loss2: (0.1762) | Acc: (92.00%) (40559/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.3846) |  Loss2: (0.1762) | Acc: (92.00%) (41740/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.3849) |  Loss2: (0.1762) | Acc: (92.00%) (42925/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.3855) |  Loss2: (0.1762) | Acc: (92.00%) (44101/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.3854) |  Loss2: (0.1762) | Acc: (92.00%) (45296/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.3857) |  Loss2: (0.1762) | Acc: (92.00%) (46436/50000)
# TEST : Loss: (0.4417) | Acc: (87.00%) (8707/10000)
percent tensor([0.7883], device='cuda:0')
percent tensor([0.7910], device='cuda:0')
percent tensor([0.8199], device='cuda:0')
percent tensor([0.7433], device='cuda:0')
percent tensor([0.7860], device='cuda:0')
percent tensor([0.8147], device='cuda:0')
percent tensor([0.8466], device='cuda:0')
percent tensor([0.1240], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.3376) |  Loss2: (0.1757) | Acc: (92.00%) (119/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.3703) |  Loss2: (0.1757) | Acc: (92.00%) (1306/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.3726) |  Loss2: (0.1757) | Acc: (93.00%) (2501/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.3735) |  Loss2: (0.1757) | Acc: (93.00%) (3695/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.3746) |  Loss2: (0.1757) | Acc: (93.00%) (4892/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.3823) |  Loss2: (0.1757) | Acc: (92.00%) (6057/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.3798) |  Loss2: (0.1757) | Acc: (92.00%) (7245/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.3802) |  Loss2: (0.1757) | Acc: (92.00%) (8433/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.3798) |  Loss2: (0.1756) | Acc: (92.00%) (9621/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.3792) |  Loss2: (0.1756) | Acc: (92.00%) (10816/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.3776) |  Loss2: (0.1756) | Acc: (92.00%) (12013/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.3780) |  Loss2: (0.1756) | Acc: (92.00%) (13202/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.3771) |  Loss2: (0.1756) | Acc: (92.00%) (14403/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.3748) |  Loss2: (0.1756) | Acc: (93.00%) (15607/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.3750) |  Loss2: (0.1756) | Acc: (93.00%) (16797/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.3764) |  Loss2: (0.1756) | Acc: (92.00%) (17974/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.3778) |  Loss2: (0.1756) | Acc: (92.00%) (19148/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.3809) |  Loss2: (0.1756) | Acc: (92.00%) (20311/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.3829) |  Loss2: (0.1755) | Acc: (92.00%) (21477/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.3828) |  Loss2: (0.1755) | Acc: (92.00%) (22665/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.3840) |  Loss2: (0.1755) | Acc: (92.00%) (23843/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.3840) |  Loss2: (0.1755) | Acc: (92.00%) (25040/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.3826) |  Loss2: (0.1755) | Acc: (92.00%) (26239/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.3825) |  Loss2: (0.1755) | Acc: (92.00%) (27431/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.3814) |  Loss2: (0.1755) | Acc: (92.00%) (28630/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.3804) |  Loss2: (0.1755) | Acc: (92.00%) (29827/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.3806) |  Loss2: (0.1755) | Acc: (92.00%) (31009/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.3800) |  Loss2: (0.1755) | Acc: (92.00%) (32210/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.3819) |  Loss2: (0.1755) | Acc: (92.00%) (33372/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.3819) |  Loss2: (0.1755) | Acc: (92.00%) (34556/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.3820) |  Loss2: (0.1755) | Acc: (92.00%) (35737/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.3824) |  Loss2: (0.1755) | Acc: (92.00%) (36925/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.3830) |  Loss2: (0.1754) | Acc: (92.00%) (38108/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.3826) |  Loss2: (0.1754) | Acc: (92.00%) (39300/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.3824) |  Loss2: (0.1754) | Acc: (92.00%) (40484/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.3821) |  Loss2: (0.1754) | Acc: (92.00%) (41676/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.3829) |  Loss2: (0.1754) | Acc: (92.00%) (42845/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.3823) |  Loss2: (0.1754) | Acc: (92.00%) (44043/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.3824) |  Loss2: (0.1754) | Acc: (92.00%) (45238/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.3842) |  Loss2: (0.1754) | Acc: (92.00%) (46355/50000)
# TEST : Loss: (0.4661) | Acc: (85.00%) (8569/10000)
percent tensor([0.7905], device='cuda:0')
percent tensor([0.7940], device='cuda:0')
percent tensor([0.8205], device='cuda:0')
percent tensor([0.7447], device='cuda:0')
percent tensor([0.7863], device='cuda:0')
percent tensor([0.8152], device='cuda:0')
percent tensor([0.8475], device='cuda:0')
percent tensor([0.1229], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.3337) |  Loss2: (0.1750) | Acc: (93.00%) (120/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.3582) |  Loss2: (0.1750) | Acc: (93.00%) (1319/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.3640) |  Loss2: (0.1750) | Acc: (93.00%) (2508/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.3610) |  Loss2: (0.1750) | Acc: (93.00%) (3708/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.3602) |  Loss2: (0.1750) | Acc: (93.00%) (4911/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.3586) |  Loss2: (0.1750) | Acc: (93.00%) (6114/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.3619) |  Loss2: (0.1750) | Acc: (93.00%) (7302/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.3643) |  Loss2: (0.1750) | Acc: (93.00%) (8496/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.3654) |  Loss2: (0.1750) | Acc: (93.00%) (9685/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.3649) |  Loss2: (0.1750) | Acc: (93.00%) (10876/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.3647) |  Loss2: (0.1750) | Acc: (93.00%) (12069/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.3674) |  Loss2: (0.1750) | Acc: (93.00%) (13259/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.3696) |  Loss2: (0.1750) | Acc: (93.00%) (14449/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.3701) |  Loss2: (0.1750) | Acc: (93.00%) (15637/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.3703) |  Loss2: (0.1750) | Acc: (93.00%) (16829/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.3701) |  Loss2: (0.1750) | Acc: (93.00%) (18014/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.3689) |  Loss2: (0.1750) | Acc: (93.00%) (19210/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.3697) |  Loss2: (0.1750) | Acc: (93.00%) (20394/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.3694) |  Loss2: (0.1749) | Acc: (93.00%) (21590/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.3688) |  Loss2: (0.1749) | Acc: (93.00%) (22788/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.3690) |  Loss2: (0.1749) | Acc: (93.00%) (23990/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.3710) |  Loss2: (0.1749) | Acc: (93.00%) (25162/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.3711) |  Loss2: (0.1749) | Acc: (93.00%) (26353/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.3716) |  Loss2: (0.1749) | Acc: (93.00%) (27547/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.3724) |  Loss2: (0.1749) | Acc: (93.00%) (28739/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.3724) |  Loss2: (0.1749) | Acc: (93.00%) (29928/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.3734) |  Loss2: (0.1749) | Acc: (93.00%) (31114/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.3737) |  Loss2: (0.1749) | Acc: (93.00%) (32303/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.3730) |  Loss2: (0.1748) | Acc: (93.00%) (33509/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.3725) |  Loss2: (0.1748) | Acc: (93.00%) (34710/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.3720) |  Loss2: (0.1748) | Acc: (93.00%) (35910/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.3719) |  Loss2: (0.1748) | Acc: (93.00%) (37104/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.3716) |  Loss2: (0.1748) | Acc: (93.00%) (38306/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.3721) |  Loss2: (0.1748) | Acc: (93.00%) (39491/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.3718) |  Loss2: (0.1748) | Acc: (93.00%) (40677/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.3728) |  Loss2: (0.1748) | Acc: (93.00%) (41864/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.3732) |  Loss2: (0.1748) | Acc: (93.00%) (43046/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.3736) |  Loss2: (0.1748) | Acc: (93.00%) (44233/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.3741) |  Loss2: (0.1748) | Acc: (93.00%) (45413/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.3740) |  Loss2: (0.1747) | Acc: (93.00%) (46557/50000)
# TEST : Loss: (0.5418) | Acc: (84.00%) (8413/10000)
percent tensor([0.7925], device='cuda:0')
percent tensor([0.7957], device='cuda:0')
percent tensor([0.8222], device='cuda:0')
percent tensor([0.7456], device='cuda:0')
percent tensor([0.7873], device='cuda:0')
percent tensor([0.8154], device='cuda:0')
percent tensor([0.8478], device='cuda:0')
percent tensor([0.1217], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.4062) |  Loss2: (0.1744) | Acc: (92.00%) (118/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.3519) |  Loss2: (0.1744) | Acc: (94.00%) (1324/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.3564) |  Loss2: (0.1744) | Acc: (93.00%) (2526/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.3604) |  Loss2: (0.1744) | Acc: (93.00%) (3713/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.3628) |  Loss2: (0.1744) | Acc: (93.00%) (4906/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.3648) |  Loss2: (0.1744) | Acc: (93.00%) (6082/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.3667) |  Loss2: (0.1744) | Acc: (93.00%) (7274/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.3686) |  Loss2: (0.1744) | Acc: (93.00%) (8456/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.3681) |  Loss2: (0.1744) | Acc: (93.00%) (9660/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.3677) |  Loss2: (0.1744) | Acc: (93.00%) (10854/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.3702) |  Loss2: (0.1744) | Acc: (93.00%) (12037/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.3691) |  Loss2: (0.1744) | Acc: (93.00%) (13238/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.3693) |  Loss2: (0.1744) | Acc: (93.00%) (14428/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.3695) |  Loss2: (0.1744) | Acc: (93.00%) (15626/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.3693) |  Loss2: (0.1744) | Acc: (93.00%) (16824/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.3690) |  Loss2: (0.1744) | Acc: (93.00%) (18013/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.3683) |  Loss2: (0.1743) | Acc: (93.00%) (19214/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.3691) |  Loss2: (0.1743) | Acc: (93.00%) (20406/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.3696) |  Loss2: (0.1743) | Acc: (93.00%) (21595/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.3692) |  Loss2: (0.1743) | Acc: (93.00%) (22793/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.3714) |  Loss2: (0.1743) | Acc: (93.00%) (23969/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.3719) |  Loss2: (0.1743) | Acc: (93.00%) (25161/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.3724) |  Loss2: (0.1743) | Acc: (93.00%) (26347/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.3709) |  Loss2: (0.1743) | Acc: (93.00%) (27554/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.3698) |  Loss2: (0.1742) | Acc: (93.00%) (28756/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.3692) |  Loss2: (0.1742) | Acc: (93.00%) (29958/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.3705) |  Loss2: (0.1742) | Acc: (93.00%) (31127/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.3703) |  Loss2: (0.1742) | Acc: (93.00%) (32319/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.3704) |  Loss2: (0.1742) | Acc: (93.00%) (33512/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.3710) |  Loss2: (0.1742) | Acc: (93.00%) (34697/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.3720) |  Loss2: (0.1742) | Acc: (93.00%) (35881/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.3720) |  Loss2: (0.1742) | Acc: (93.00%) (37071/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.3735) |  Loss2: (0.1742) | Acc: (93.00%) (38241/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.3735) |  Loss2: (0.1741) | Acc: (93.00%) (39425/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.3739) |  Loss2: (0.1741) | Acc: (93.00%) (40611/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.3742) |  Loss2: (0.1741) | Acc: (93.00%) (41807/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.3745) |  Loss2: (0.1741) | Acc: (93.00%) (42994/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.3739) |  Loss2: (0.1741) | Acc: (93.00%) (44199/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.3739) |  Loss2: (0.1741) | Acc: (93.00%) (45397/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.3736) |  Loss2: (0.1741) | Acc: (93.00%) (46544/50000)
# TEST : Loss: (0.4364) | Acc: (87.00%) (8707/10000)
percent tensor([0.7942], device='cuda:0')
percent tensor([0.7976], device='cuda:0')
percent tensor([0.8234], device='cuda:0')
percent tensor([0.7477], device='cuda:0')
percent tensor([0.7880], device='cuda:0')
percent tensor([0.8167], device='cuda:0')
percent tensor([0.8484], device='cuda:0')
percent tensor([0.1206], device='cuda:0')
Epoch: 145 | Batch_idx: 0 |  Loss: (0.2920) |  Loss2: (0.1737) | Acc: (96.00%) (123/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.3520) |  Loss2: (0.1737) | Acc: (94.00%) (1328/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.3529) |  Loss2: (0.1737) | Acc: (94.00%) (2530/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.3526) |  Loss2: (0.1736) | Acc: (94.00%) (3730/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.3621) |  Loss2: (0.1736) | Acc: (93.00%) (4916/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.3607) |  Loss2: (0.1736) | Acc: (93.00%) (6122/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.3589) |  Loss2: (0.1736) | Acc: (93.00%) (7318/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.3594) |  Loss2: (0.1736) | Acc: (93.00%) (8516/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.3570) |  Loss2: (0.1736) | Acc: (93.00%) (9716/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.3573) |  Loss2: (0.1736) | Acc: (93.00%) (10910/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.3572) |  Loss2: (0.1736) | Acc: (93.00%) (12111/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.3596) |  Loss2: (0.1736) | Acc: (93.00%) (13300/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.3609) |  Loss2: (0.1735) | Acc: (93.00%) (14485/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.3602) |  Loss2: (0.1735) | Acc: (93.00%) (15684/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.3621) |  Loss2: (0.1735) | Acc: (93.00%) (16866/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.3613) |  Loss2: (0.1735) | Acc: (93.00%) (18073/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.3605) |  Loss2: (0.1735) | Acc: (93.00%) (19271/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.3605) |  Loss2: (0.1735) | Acc: (93.00%) (20475/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.3613) |  Loss2: (0.1735) | Acc: (93.00%) (21667/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.3619) |  Loss2: (0.1735) | Acc: (93.00%) (22860/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.3625) |  Loss2: (0.1735) | Acc: (93.00%) (24046/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.3616) |  Loss2: (0.1735) | Acc: (93.00%) (25255/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.3614) |  Loss2: (0.1735) | Acc: (93.00%) (26449/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.3617) |  Loss2: (0.1735) | Acc: (93.00%) (27638/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.3619) |  Loss2: (0.1735) | Acc: (93.00%) (28823/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.3630) |  Loss2: (0.1734) | Acc: (93.00%) (30014/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.3636) |  Loss2: (0.1734) | Acc: (93.00%) (31203/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.3637) |  Loss2: (0.1734) | Acc: (93.00%) (32396/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.3639) |  Loss2: (0.1734) | Acc: (93.00%) (33594/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.3637) |  Loss2: (0.1734) | Acc: (93.00%) (34793/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.3632) |  Loss2: (0.1734) | Acc: (93.00%) (35999/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.3647) |  Loss2: (0.1734) | Acc: (93.00%) (37171/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.3660) |  Loss2: (0.1734) | Acc: (93.00%) (38353/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.3663) |  Loss2: (0.1734) | Acc: (93.00%) (39550/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.3656) |  Loss2: (0.1734) | Acc: (93.00%) (40748/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.3657) |  Loss2: (0.1733) | Acc: (93.00%) (41941/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.3662) |  Loss2: (0.1733) | Acc: (93.00%) (43129/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.3665) |  Loss2: (0.1733) | Acc: (93.00%) (44327/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.3668) |  Loss2: (0.1733) | Acc: (93.00%) (45518/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.3678) |  Loss2: (0.1733) | Acc: (93.00%) (46657/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_145.pth.tar'
# TEST : Loss: (0.4370) | Acc: (86.00%) (8696/10000)
percent tensor([0.7974], device='cuda:0')
percent tensor([0.7994], device='cuda:0')
percent tensor([0.8247], device='cuda:0')
percent tensor([0.7481], device='cuda:0')
percent tensor([0.7891], device='cuda:0')
percent tensor([0.8173], device='cuda:0')
percent tensor([0.8492], device='cuda:0')
percent tensor([0.1194], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.3372) |  Loss2: (0.1730) | Acc: (94.00%) (121/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.3785) |  Loss2: (0.1729) | Acc: (92.00%) (1303/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.3738) |  Loss2: (0.1729) | Acc: (92.00%) (2491/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.3763) |  Loss2: (0.1729) | Acc: (92.00%) (3678/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.3737) |  Loss2: (0.1729) | Acc: (92.00%) (4872/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.3690) |  Loss2: (0.1729) | Acc: (93.00%) (6078/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.3672) |  Loss2: (0.1729) | Acc: (93.00%) (7277/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.3628) |  Loss2: (0.1728) | Acc: (93.00%) (8484/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.3618) |  Loss2: (0.1728) | Acc: (93.00%) (9685/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.3605) |  Loss2: (0.1728) | Acc: (93.00%) (10888/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.3613) |  Loss2: (0.1728) | Acc: (93.00%) (12081/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.3615) |  Loss2: (0.1728) | Acc: (93.00%) (13276/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.3619) |  Loss2: (0.1728) | Acc: (93.00%) (14478/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.3647) |  Loss2: (0.1728) | Acc: (93.00%) (15665/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.3646) |  Loss2: (0.1728) | Acc: (93.00%) (16861/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.3651) |  Loss2: (0.1728) | Acc: (93.00%) (18055/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.3656) |  Loss2: (0.1728) | Acc: (93.00%) (19244/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.3661) |  Loss2: (0.1728) | Acc: (93.00%) (20435/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.3653) |  Loss2: (0.1728) | Acc: (93.00%) (21636/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.3645) |  Loss2: (0.1728) | Acc: (93.00%) (22843/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.3636) |  Loss2: (0.1727) | Acc: (93.00%) (24057/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.3645) |  Loss2: (0.1727) | Acc: (93.00%) (25246/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.3649) |  Loss2: (0.1727) | Acc: (93.00%) (26437/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.3655) |  Loss2: (0.1727) | Acc: (93.00%) (27627/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.3659) |  Loss2: (0.1727) | Acc: (93.00%) (28814/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.3668) |  Loss2: (0.1727) | Acc: (93.00%) (29993/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.3673) |  Loss2: (0.1727) | Acc: (93.00%) (31192/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.3679) |  Loss2: (0.1727) | Acc: (93.00%) (32368/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.3676) |  Loss2: (0.1727) | Acc: (93.00%) (33564/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.3676) |  Loss2: (0.1727) | Acc: (93.00%) (34757/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.3684) |  Loss2: (0.1727) | Acc: (93.00%) (35951/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.3684) |  Loss2: (0.1727) | Acc: (93.00%) (37147/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.3676) |  Loss2: (0.1727) | Acc: (93.00%) (38350/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.3675) |  Loss2: (0.1726) | Acc: (93.00%) (39541/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.3672) |  Loss2: (0.1726) | Acc: (93.00%) (40742/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.3669) |  Loss2: (0.1726) | Acc: (93.00%) (41940/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.3676) |  Loss2: (0.1726) | Acc: (93.00%) (43125/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.3674) |  Loss2: (0.1726) | Acc: (93.00%) (44319/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.3684) |  Loss2: (0.1726) | Acc: (93.00%) (45500/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.3683) |  Loss2: (0.1726) | Acc: (93.00%) (46658/50000)
# TEST : Loss: (0.4767) | Acc: (86.00%) (8614/10000)
percent tensor([0.8001], device='cuda:0')
percent tensor([0.8016], device='cuda:0')
percent tensor([0.8259], device='cuda:0')
percent tensor([0.7492], device='cuda:0')
percent tensor([0.7905], device='cuda:0')
percent tensor([0.8164], device='cuda:0')
percent tensor([0.8502], device='cuda:0')
percent tensor([0.1184], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.2946) |  Loss2: (0.1723) | Acc: (94.00%) (121/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.3561) |  Loss2: (0.1723) | Acc: (93.00%) (1319/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.3528) |  Loss2: (0.1723) | Acc: (93.00%) (2521/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.3494) |  Loss2: (0.1723) | Acc: (93.00%) (3727/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.3517) |  Loss2: (0.1723) | Acc: (93.00%) (4921/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.3516) |  Loss2: (0.1722) | Acc: (93.00%) (6125/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.3513) |  Loss2: (0.1722) | Acc: (93.00%) (7331/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.3526) |  Loss2: (0.1722) | Acc: (93.00%) (8524/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.3573) |  Loss2: (0.1722) | Acc: (93.00%) (9709/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.3607) |  Loss2: (0.1722) | Acc: (93.00%) (10891/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.3572) |  Loss2: (0.1722) | Acc: (93.00%) (12107/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.3580) |  Loss2: (0.1722) | Acc: (93.00%) (13297/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.3581) |  Loss2: (0.1722) | Acc: (93.00%) (14487/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.3598) |  Loss2: (0.1721) | Acc: (93.00%) (15679/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.3587) |  Loss2: (0.1721) | Acc: (93.00%) (16873/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.3602) |  Loss2: (0.1721) | Acc: (93.00%) (18050/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.3606) |  Loss2: (0.1721) | Acc: (93.00%) (19249/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.3620) |  Loss2: (0.1721) | Acc: (93.00%) (20446/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.3634) |  Loss2: (0.1721) | Acc: (93.00%) (21628/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.3622) |  Loss2: (0.1721) | Acc: (93.00%) (22844/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.3620) |  Loss2: (0.1721) | Acc: (93.00%) (24039/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.3622) |  Loss2: (0.1721) | Acc: (93.00%) (25235/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.3621) |  Loss2: (0.1720) | Acc: (93.00%) (26437/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.3618) |  Loss2: (0.1720) | Acc: (93.00%) (27639/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.3616) |  Loss2: (0.1720) | Acc: (93.00%) (28835/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.3633) |  Loss2: (0.1720) | Acc: (93.00%) (30004/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.3633) |  Loss2: (0.1720) | Acc: (93.00%) (31199/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.3639) |  Loss2: (0.1720) | Acc: (93.00%) (32399/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.3643) |  Loss2: (0.1720) | Acc: (93.00%) (33590/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.3644) |  Loss2: (0.1720) | Acc: (93.00%) (34780/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.3652) |  Loss2: (0.1720) | Acc: (93.00%) (35965/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.3664) |  Loss2: (0.1720) | Acc: (93.00%) (37149/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.3672) |  Loss2: (0.1720) | Acc: (93.00%) (38326/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.3674) |  Loss2: (0.1720) | Acc: (93.00%) (39520/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.3673) |  Loss2: (0.1720) | Acc: (93.00%) (40710/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.3669) |  Loss2: (0.1719) | Acc: (93.00%) (41908/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.3675) |  Loss2: (0.1719) | Acc: (93.00%) (43091/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.3676) |  Loss2: (0.1719) | Acc: (93.00%) (44286/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.3671) |  Loss2: (0.1719) | Acc: (93.00%) (45491/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.3672) |  Loss2: (0.1719) | Acc: (93.00%) (46639/50000)
# TEST : Loss: (0.4228) | Acc: (87.00%) (8719/10000)
percent tensor([0.8030], device='cuda:0')
percent tensor([0.8029], device='cuda:0')
percent tensor([0.8267], device='cuda:0')
percent tensor([0.7510], device='cuda:0')
percent tensor([0.7906], device='cuda:0')
percent tensor([0.8176], device='cuda:0')
percent tensor([0.8503], device='cuda:0')
percent tensor([0.1174], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.3854) |  Loss2: (0.1716) | Acc: (92.00%) (119/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.3790) |  Loss2: (0.1716) | Acc: (92.00%) (1305/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.3792) |  Loss2: (0.1716) | Acc: (92.00%) (2487/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.3671) |  Loss2: (0.1716) | Acc: (92.00%) (3687/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.3700) |  Loss2: (0.1716) | Acc: (92.00%) (4872/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.3662) |  Loss2: (0.1716) | Acc: (93.00%) (6074/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.3664) |  Loss2: (0.1716) | Acc: (93.00%) (7275/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.3633) |  Loss2: (0.1716) | Acc: (93.00%) (8481/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.3604) |  Loss2: (0.1716) | Acc: (93.00%) (9685/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.3609) |  Loss2: (0.1716) | Acc: (93.00%) (10885/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.3592) |  Loss2: (0.1715) | Acc: (93.00%) (12084/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.3621) |  Loss2: (0.1715) | Acc: (93.00%) (13269/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.3626) |  Loss2: (0.1715) | Acc: (93.00%) (14450/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.3614) |  Loss2: (0.1715) | Acc: (93.00%) (15648/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.3604) |  Loss2: (0.1715) | Acc: (93.00%) (16844/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.3593) |  Loss2: (0.1715) | Acc: (93.00%) (18057/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.3599) |  Loss2: (0.1715) | Acc: (93.00%) (19244/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.3592) |  Loss2: (0.1715) | Acc: (93.00%) (20447/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.3585) |  Loss2: (0.1715) | Acc: (93.00%) (21650/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.3589) |  Loss2: (0.1715) | Acc: (93.00%) (22850/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.3587) |  Loss2: (0.1715) | Acc: (93.00%) (24050/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.3590) |  Loss2: (0.1715) | Acc: (93.00%) (25245/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.3594) |  Loss2: (0.1714) | Acc: (93.00%) (26447/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.3600) |  Loss2: (0.1714) | Acc: (93.00%) (27639/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.3590) |  Loss2: (0.1714) | Acc: (93.00%) (28844/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.3593) |  Loss2: (0.1714) | Acc: (93.00%) (30034/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.3595) |  Loss2: (0.1714) | Acc: (93.00%) (31229/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.3595) |  Loss2: (0.1714) | Acc: (93.00%) (32427/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.3596) |  Loss2: (0.1714) | Acc: (93.00%) (33622/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.3598) |  Loss2: (0.1714) | Acc: (93.00%) (34819/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.3602) |  Loss2: (0.1714) | Acc: (93.00%) (36012/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.3602) |  Loss2: (0.1714) | Acc: (93.00%) (37205/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.3605) |  Loss2: (0.1714) | Acc: (93.00%) (38401/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.3600) |  Loss2: (0.1714) | Acc: (93.00%) (39599/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.3605) |  Loss2: (0.1713) | Acc: (93.00%) (40788/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.3606) |  Loss2: (0.1713) | Acc: (93.00%) (41983/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.3606) |  Loss2: (0.1713) | Acc: (93.00%) (43177/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.3609) |  Loss2: (0.1713) | Acc: (93.00%) (44367/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.3614) |  Loss2: (0.1713) | Acc: (93.00%) (45557/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.3618) |  Loss2: (0.1713) | Acc: (93.00%) (46696/50000)
# TEST : Loss: (0.4567) | Acc: (86.00%) (8619/10000)
percent tensor([0.8037], device='cuda:0')
percent tensor([0.8063], device='cuda:0')
percent tensor([0.8271], device='cuda:0')
percent tensor([0.7532], device='cuda:0')
percent tensor([0.7925], device='cuda:0')
percent tensor([0.8182], device='cuda:0')
percent tensor([0.8509], device='cuda:0')
percent tensor([0.1163], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.4089) |  Loss2: (0.1709) | Acc: (89.00%) (115/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.3558) |  Loss2: (0.1709) | Acc: (94.00%) (1326/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.3513) |  Loss2: (0.1709) | Acc: (94.00%) (2530/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.3543) |  Loss2: (0.1709) | Acc: (93.00%) (3729/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.3531) |  Loss2: (0.1709) | Acc: (93.00%) (4931/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.3550) |  Loss2: (0.1709) | Acc: (93.00%) (6131/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.3554) |  Loss2: (0.1709) | Acc: (93.00%) (7336/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.3550) |  Loss2: (0.1709) | Acc: (93.00%) (8532/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.3559) |  Loss2: (0.1709) | Acc: (93.00%) (9723/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.3544) |  Loss2: (0.1709) | Acc: (93.00%) (10932/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.3573) |  Loss2: (0.1709) | Acc: (93.00%) (12117/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.3584) |  Loss2: (0.1709) | Acc: (93.00%) (13304/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.3577) |  Loss2: (0.1708) | Acc: (93.00%) (14500/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.3596) |  Loss2: (0.1708) | Acc: (93.00%) (15687/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.3604) |  Loss2: (0.1708) | Acc: (93.00%) (16886/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.3607) |  Loss2: (0.1708) | Acc: (93.00%) (18077/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.3604) |  Loss2: (0.1708) | Acc: (93.00%) (19285/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.3596) |  Loss2: (0.1708) | Acc: (93.00%) (20486/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.3591) |  Loss2: (0.1708) | Acc: (93.00%) (21692/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.3578) |  Loss2: (0.1708) | Acc: (93.00%) (22897/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.3564) |  Loss2: (0.1708) | Acc: (93.00%) (24110/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.3554) |  Loss2: (0.1708) | Acc: (93.00%) (25318/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.3559) |  Loss2: (0.1708) | Acc: (93.00%) (26516/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.3551) |  Loss2: (0.1707) | Acc: (93.00%) (27724/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.3562) |  Loss2: (0.1707) | Acc: (93.00%) (28916/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.3566) |  Loss2: (0.1707) | Acc: (93.00%) (30107/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.3559) |  Loss2: (0.1707) | Acc: (93.00%) (31310/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.3557) |  Loss2: (0.1707) | Acc: (93.00%) (32508/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.3553) |  Loss2: (0.1707) | Acc: (93.00%) (33721/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.3540) |  Loss2: (0.1707) | Acc: (93.00%) (34940/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.3553) |  Loss2: (0.1707) | Acc: (93.00%) (36114/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.3560) |  Loss2: (0.1706) | Acc: (93.00%) (37302/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.3568) |  Loss2: (0.1706) | Acc: (93.00%) (38487/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.3579) |  Loss2: (0.1706) | Acc: (93.00%) (39669/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.3579) |  Loss2: (0.1706) | Acc: (93.00%) (40863/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.3584) |  Loss2: (0.1706) | Acc: (93.00%) (42052/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.3580) |  Loss2: (0.1706) | Acc: (93.00%) (43253/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.3574) |  Loss2: (0.1706) | Acc: (93.00%) (44465/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.3572) |  Loss2: (0.1706) | Acc: (93.00%) (45665/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.3573) |  Loss2: (0.1706) | Acc: (93.00%) (46813/50000)
# TEST : Loss: (0.4171) | Acc: (87.00%) (8743/10000)
percent tensor([0.8063], device='cuda:0')
percent tensor([0.8063], device='cuda:0')
percent tensor([0.8291], device='cuda:0')
percent tensor([0.7548], device='cuda:0')
percent tensor([0.7934], device='cuda:0')
percent tensor([0.8196], device='cuda:0')
percent tensor([0.8513], device='cuda:0')
percent tensor([0.1153], device='cuda:0')
Epoch: 150 | Batch_idx: 0 |  Loss: (0.3575) |  Loss2: (0.1702) | Acc: (92.00%) (118/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.3387) |  Loss2: (0.1701) | Acc: (94.00%) (1335/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.3499) |  Loss2: (0.1701) | Acc: (94.00%) (2533/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.3455) |  Loss2: (0.1701) | Acc: (94.00%) (3743/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.3489) |  Loss2: (0.1701) | Acc: (94.00%) (4945/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.3482) |  Loss2: (0.1701) | Acc: (94.00%) (6144/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.3499) |  Loss2: (0.1701) | Acc: (94.00%) (7341/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.3470) |  Loss2: (0.1701) | Acc: (94.00%) (8545/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.3444) |  Loss2: (0.1701) | Acc: (94.00%) (9762/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.3436) |  Loss2: (0.1701) | Acc: (94.00%) (10975/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.3436) |  Loss2: (0.1701) | Acc: (94.00%) (12187/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.3441) |  Loss2: (0.1701) | Acc: (94.00%) (13384/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.3448) |  Loss2: (0.1701) | Acc: (94.00%) (14582/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.3451) |  Loss2: (0.1701) | Acc: (94.00%) (15783/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.3434) |  Loss2: (0.1700) | Acc: (94.00%) (17002/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.3434) |  Loss2: (0.1700) | Acc: (94.00%) (18200/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.3458) |  Loss2: (0.1700) | Acc: (94.00%) (19386/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.3467) |  Loss2: (0.1700) | Acc: (94.00%) (20590/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.3488) |  Loss2: (0.1700) | Acc: (93.00%) (21773/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.3485) |  Loss2: (0.1700) | Acc: (93.00%) (22979/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.3488) |  Loss2: (0.1700) | Acc: (93.00%) (24178/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.3488) |  Loss2: (0.1700) | Acc: (93.00%) (25381/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.3495) |  Loss2: (0.1700) | Acc: (93.00%) (26570/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.3500) |  Loss2: (0.1700) | Acc: (93.00%) (27765/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.3493) |  Loss2: (0.1699) | Acc: (93.00%) (28974/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.3505) |  Loss2: (0.1699) | Acc: (93.00%) (30169/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.3503) |  Loss2: (0.1699) | Acc: (93.00%) (31371/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.3509) |  Loss2: (0.1699) | Acc: (93.00%) (32560/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.3512) |  Loss2: (0.1699) | Acc: (93.00%) (33754/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.3517) |  Loss2: (0.1699) | Acc: (93.00%) (34938/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.3524) |  Loss2: (0.1699) | Acc: (93.00%) (36131/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.3531) |  Loss2: (0.1699) | Acc: (93.00%) (37315/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.3541) |  Loss2: (0.1699) | Acc: (93.00%) (38487/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.3540) |  Loss2: (0.1699) | Acc: (93.00%) (39687/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.3537) |  Loss2: (0.1699) | Acc: (93.00%) (40890/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.3535) |  Loss2: (0.1698) | Acc: (93.00%) (42082/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.3531) |  Loss2: (0.1698) | Acc: (93.00%) (43288/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.3534) |  Loss2: (0.1698) | Acc: (93.00%) (44481/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.3534) |  Loss2: (0.1698) | Acc: (93.00%) (45678/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.3532) |  Loss2: (0.1698) | Acc: (93.00%) (46839/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_150.pth.tar'
# TEST : Loss: (0.4499) | Acc: (86.00%) (8648/10000)
percent tensor([0.8081], device='cuda:0')
percent tensor([0.8091], device='cuda:0')
percent tensor([0.8304], device='cuda:0')
percent tensor([0.7555], device='cuda:0')
percent tensor([0.7947], device='cuda:0')
percent tensor([0.8201], device='cuda:0')
percent tensor([0.8523], device='cuda:0')
percent tensor([0.1143], device='cuda:0')
Epoch: 151 | Batch_idx: 0 |  Loss: (0.3657) |  Loss2: (0.1694) | Acc: (92.00%) (119/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.3420) |  Loss2: (0.1694) | Acc: (93.00%) (1323/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.3464) |  Loss2: (0.1694) | Acc: (93.00%) (2522/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.3439) |  Loss2: (0.1694) | Acc: (93.00%) (3725/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.3455) |  Loss2: (0.1693) | Acc: (93.00%) (4926/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.3457) |  Loss2: (0.1693) | Acc: (93.00%) (6127/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.3462) |  Loss2: (0.1693) | Acc: (93.00%) (7325/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.3437) |  Loss2: (0.1693) | Acc: (93.00%) (8535/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.3419) |  Loss2: (0.1693) | Acc: (93.00%) (9742/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.3435) |  Loss2: (0.1693) | Acc: (93.00%) (10945/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.3440) |  Loss2: (0.1693) | Acc: (94.00%) (12156/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.3438) |  Loss2: (0.1693) | Acc: (94.00%) (13359/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.3432) |  Loss2: (0.1693) | Acc: (94.00%) (14565/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.3440) |  Loss2: (0.1693) | Acc: (94.00%) (15768/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.3460) |  Loss2: (0.1693) | Acc: (93.00%) (16957/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.3476) |  Loss2: (0.1693) | Acc: (93.00%) (18145/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.3480) |  Loss2: (0.1693) | Acc: (93.00%) (19343/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.3488) |  Loss2: (0.1693) | Acc: (93.00%) (20537/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.3494) |  Loss2: (0.1693) | Acc: (93.00%) (21731/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.3489) |  Loss2: (0.1693) | Acc: (93.00%) (22929/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.3478) |  Loss2: (0.1693) | Acc: (93.00%) (24139/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.3485) |  Loss2: (0.1693) | Acc: (93.00%) (25330/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.3482) |  Loss2: (0.1692) | Acc: (93.00%) (26533/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.3489) |  Loss2: (0.1692) | Acc: (93.00%) (27721/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.3500) |  Loss2: (0.1692) | Acc: (93.00%) (28907/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.3505) |  Loss2: (0.1692) | Acc: (93.00%) (30102/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.3514) |  Loss2: (0.1692) | Acc: (93.00%) (31280/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.3519) |  Loss2: (0.1692) | Acc: (93.00%) (32469/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.3518) |  Loss2: (0.1692) | Acc: (93.00%) (33671/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.3524) |  Loss2: (0.1692) | Acc: (93.00%) (34861/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.3519) |  Loss2: (0.1692) | Acc: (93.00%) (36070/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.3514) |  Loss2: (0.1692) | Acc: (93.00%) (37276/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.3506) |  Loss2: (0.1692) | Acc: (93.00%) (38487/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.3511) |  Loss2: (0.1692) | Acc: (93.00%) (39669/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.3506) |  Loss2: (0.1692) | Acc: (93.00%) (40872/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.3504) |  Loss2: (0.1692) | Acc: (93.00%) (42076/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.3509) |  Loss2: (0.1692) | Acc: (93.00%) (43275/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.3501) |  Loss2: (0.1692) | Acc: (93.00%) (44491/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.3503) |  Loss2: (0.1692) | Acc: (93.00%) (45684/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.3507) |  Loss2: (0.1692) | Acc: (93.00%) (46829/50000)
# TEST : Loss: (0.4223) | Acc: (87.00%) (8713/10000)
percent tensor([0.8098], device='cuda:0')
percent tensor([0.8109], device='cuda:0')
percent tensor([0.8309], device='cuda:0')
percent tensor([0.7571], device='cuda:0')
percent tensor([0.7947], device='cuda:0')
percent tensor([0.8210], device='cuda:0')
percent tensor([0.8527], device='cuda:0')
percent tensor([0.1133], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.3011) |  Loss2: (0.1689) | Acc: (95.00%) (122/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.3320) |  Loss2: (0.1688) | Acc: (94.00%) (1327/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.3336) |  Loss2: (0.1688) | Acc: (94.00%) (2532/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.3290) |  Loss2: (0.1688) | Acc: (94.00%) (3749/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.3334) |  Loss2: (0.1688) | Acc: (94.00%) (4946/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.3335) |  Loss2: (0.1688) | Acc: (94.00%) (6151/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.3358) |  Loss2: (0.1688) | Acc: (94.00%) (7356/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.3401) |  Loss2: (0.1688) | Acc: (94.00%) (8548/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.3433) |  Loss2: (0.1688) | Acc: (94.00%) (9748/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.3445) |  Loss2: (0.1688) | Acc: (93.00%) (10947/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.3448) |  Loss2: (0.1688) | Acc: (93.00%) (12150/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.3446) |  Loss2: (0.1688) | Acc: (93.00%) (13351/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.3436) |  Loss2: (0.1687) | Acc: (94.00%) (14565/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.3442) |  Loss2: (0.1687) | Acc: (94.00%) (15764/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.3447) |  Loss2: (0.1687) | Acc: (94.00%) (16969/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.3472) |  Loss2: (0.1687) | Acc: (93.00%) (18146/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.3475) |  Loss2: (0.1687) | Acc: (93.00%) (19347/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.3487) |  Loss2: (0.1687) | Acc: (93.00%) (20548/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.3494) |  Loss2: (0.1687) | Acc: (93.00%) (21731/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.3492) |  Loss2: (0.1687) | Acc: (93.00%) (22935/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.3491) |  Loss2: (0.1686) | Acc: (93.00%) (24137/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.3485) |  Loss2: (0.1686) | Acc: (93.00%) (25342/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.3469) |  Loss2: (0.1686) | Acc: (93.00%) (26565/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.3479) |  Loss2: (0.1686) | Acc: (93.00%) (27750/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.3480) |  Loss2: (0.1686) | Acc: (93.00%) (28957/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.3473) |  Loss2: (0.1686) | Acc: (93.00%) (30164/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.3467) |  Loss2: (0.1686) | Acc: (93.00%) (31369/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.3467) |  Loss2: (0.1686) | Acc: (93.00%) (32573/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.3461) |  Loss2: (0.1686) | Acc: (93.00%) (33780/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.3458) |  Loss2: (0.1685) | Acc: (93.00%) (34984/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.3453) |  Loss2: (0.1685) | Acc: (93.00%) (36202/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.3454) |  Loss2: (0.1685) | Acc: (93.00%) (37411/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.3457) |  Loss2: (0.1685) | Acc: (93.00%) (38609/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.3458) |  Loss2: (0.1685) | Acc: (93.00%) (39806/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.3465) |  Loss2: (0.1685) | Acc: (93.00%) (40998/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.3463) |  Loss2: (0.1685) | Acc: (93.00%) (42193/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.3476) |  Loss2: (0.1685) | Acc: (93.00%) (43364/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.3481) |  Loss2: (0.1685) | Acc: (93.00%) (44554/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.3481) |  Loss2: (0.1685) | Acc: (93.00%) (45753/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.3486) |  Loss2: (0.1685) | Acc: (93.00%) (46902/50000)
# TEST : Loss: (0.4523) | Acc: (86.00%) (8676/10000)
percent tensor([0.8109], device='cuda:0')
percent tensor([0.8126], device='cuda:0')
percent tensor([0.8330], device='cuda:0')
percent tensor([0.7593], device='cuda:0')
percent tensor([0.7961], device='cuda:0')
percent tensor([0.8220], device='cuda:0')
percent tensor([0.8535], device='cuda:0')
percent tensor([0.1123], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.3593) |  Loss2: (0.1680) | Acc: (92.00%) (119/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.3311) |  Loss2: (0.1680) | Acc: (93.00%) (1323/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.3352) |  Loss2: (0.1680) | Acc: (94.00%) (2534/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.3382) |  Loss2: (0.1680) | Acc: (94.00%) (3730/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.3336) |  Loss2: (0.1680) | Acc: (94.00%) (4946/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.3407) |  Loss2: (0.1680) | Acc: (93.00%) (6135/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.3407) |  Loss2: (0.1680) | Acc: (93.00%) (7336/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.3398) |  Loss2: (0.1680) | Acc: (93.00%) (8542/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.3426) |  Loss2: (0.1680) | Acc: (93.00%) (9739/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.3456) |  Loss2: (0.1680) | Acc: (93.00%) (10926/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.3434) |  Loss2: (0.1679) | Acc: (93.00%) (12138/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.3447) |  Loss2: (0.1679) | Acc: (93.00%) (13329/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.3441) |  Loss2: (0.1679) | Acc: (93.00%) (14535/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.3443) |  Loss2: (0.1679) | Acc: (93.00%) (15735/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.3434) |  Loss2: (0.1679) | Acc: (93.00%) (16935/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.3434) |  Loss2: (0.1679) | Acc: (93.00%) (18132/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.3431) |  Loss2: (0.1679) | Acc: (93.00%) (19341/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.3445) |  Loss2: (0.1679) | Acc: (93.00%) (20536/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.3449) |  Loss2: (0.1679) | Acc: (93.00%) (21729/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.3451) |  Loss2: (0.1678) | Acc: (93.00%) (22926/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.3443) |  Loss2: (0.1678) | Acc: (93.00%) (24131/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.3443) |  Loss2: (0.1678) | Acc: (93.00%) (25331/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.3444) |  Loss2: (0.1678) | Acc: (93.00%) (26531/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.3454) |  Loss2: (0.1678) | Acc: (93.00%) (27723/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.3460) |  Loss2: (0.1678) | Acc: (93.00%) (28915/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.3461) |  Loss2: (0.1678) | Acc: (93.00%) (30116/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.3465) |  Loss2: (0.1678) | Acc: (93.00%) (31305/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.3463) |  Loss2: (0.1678) | Acc: (93.00%) (32515/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.3462) |  Loss2: (0.1678) | Acc: (93.00%) (33721/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.3461) |  Loss2: (0.1678) | Acc: (93.00%) (34921/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.3449) |  Loss2: (0.1678) | Acc: (93.00%) (36137/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.3441) |  Loss2: (0.1678) | Acc: (93.00%) (37348/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.3445) |  Loss2: (0.1677) | Acc: (93.00%) (38549/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.3448) |  Loss2: (0.1677) | Acc: (93.00%) (39749/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.3457) |  Loss2: (0.1677) | Acc: (93.00%) (40938/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.3461) |  Loss2: (0.1677) | Acc: (93.00%) (42131/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.3467) |  Loss2: (0.1677) | Acc: (93.00%) (43321/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.3470) |  Loss2: (0.1677) | Acc: (93.00%) (44512/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.3469) |  Loss2: (0.1677) | Acc: (93.00%) (45718/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.3471) |  Loss2: (0.1677) | Acc: (93.00%) (46873/50000)
# TEST : Loss: (0.4340) | Acc: (86.00%) (8696/10000)
percent tensor([0.8140], device='cuda:0')
percent tensor([0.8136], device='cuda:0')
percent tensor([0.8333], device='cuda:0')
percent tensor([0.7609], device='cuda:0')
percent tensor([0.7969], device='cuda:0')
percent tensor([0.8230], device='cuda:0')
percent tensor([0.8545], device='cuda:0')
percent tensor([0.1114], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.3704) |  Loss2: (0.1674) | Acc: (92.00%) (118/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.3496) |  Loss2: (0.1673) | Acc: (93.00%) (1314/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.3540) |  Loss2: (0.1673) | Acc: (93.00%) (2511/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.3435) |  Loss2: (0.1673) | Acc: (93.00%) (3719/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.3404) |  Loss2: (0.1674) | Acc: (93.00%) (4932/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.3408) |  Loss2: (0.1673) | Acc: (94.00%) (6145/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.3391) |  Loss2: (0.1673) | Acc: (94.00%) (7352/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.3379) |  Loss2: (0.1673) | Acc: (94.00%) (8556/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.3361) |  Loss2: (0.1673) | Acc: (94.00%) (9775/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.3359) |  Loss2: (0.1673) | Acc: (94.00%) (10985/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.3349) |  Loss2: (0.1673) | Acc: (94.00%) (12195/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.3328) |  Loss2: (0.1673) | Acc: (94.00%) (13413/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.3333) |  Loss2: (0.1673) | Acc: (94.00%) (14616/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.3343) |  Loss2: (0.1673) | Acc: (94.00%) (15804/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.3343) |  Loss2: (0.1673) | Acc: (94.00%) (17014/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.3363) |  Loss2: (0.1673) | Acc: (94.00%) (18205/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.3379) |  Loss2: (0.1672) | Acc: (94.00%) (19398/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.3396) |  Loss2: (0.1672) | Acc: (94.00%) (20580/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.3395) |  Loss2: (0.1672) | Acc: (94.00%) (21782/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.3401) |  Loss2: (0.1672) | Acc: (94.00%) (22982/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.3422) |  Loss2: (0.1672) | Acc: (93.00%) (24176/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.3406) |  Loss2: (0.1672) | Acc: (94.00%) (25399/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.3406) |  Loss2: (0.1672) | Acc: (94.00%) (26602/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.3404) |  Loss2: (0.1672) | Acc: (94.00%) (27805/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.3417) |  Loss2: (0.1672) | Acc: (94.00%) (29004/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.3428) |  Loss2: (0.1672) | Acc: (93.00%) (30191/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.3426) |  Loss2: (0.1671) | Acc: (93.00%) (31398/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.3427) |  Loss2: (0.1671) | Acc: (93.00%) (32598/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.3435) |  Loss2: (0.1671) | Acc: (93.00%) (33789/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.3427) |  Loss2: (0.1671) | Acc: (93.00%) (35004/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.3427) |  Loss2: (0.1671) | Acc: (93.00%) (36200/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.3429) |  Loss2: (0.1671) | Acc: (93.00%) (37399/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.3434) |  Loss2: (0.1671) | Acc: (93.00%) (38590/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.3435) |  Loss2: (0.1671) | Acc: (93.00%) (39794/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.3435) |  Loss2: (0.1671) | Acc: (93.00%) (40999/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.3430) |  Loss2: (0.1671) | Acc: (93.00%) (42218/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.3430) |  Loss2: (0.1670) | Acc: (93.00%) (43428/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.3427) |  Loss2: (0.1670) | Acc: (93.00%) (44626/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.3428) |  Loss2: (0.1670) | Acc: (93.00%) (45827/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.3437) |  Loss2: (0.1670) | Acc: (93.00%) (46972/50000)
# TEST : Loss: (0.4693) | Acc: (86.00%) (8689/10000)
percent tensor([0.8152], device='cuda:0')
percent tensor([0.8166], device='cuda:0')
percent tensor([0.8355], device='cuda:0')
percent tensor([0.7622], device='cuda:0')
percent tensor([0.7980], device='cuda:0')
percent tensor([0.8236], device='cuda:0')
percent tensor([0.8553], device='cuda:0')
percent tensor([0.1104], device='cuda:0')
Epoch: 155 | Batch_idx: 0 |  Loss: (0.4602) |  Loss2: (0.1665) | Acc: (88.00%) (113/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.3252) |  Loss2: (0.1665) | Acc: (94.00%) (1332/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.3284) |  Loss2: (0.1665) | Acc: (94.00%) (2539/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.3310) |  Loss2: (0.1665) | Acc: (94.00%) (3747/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.3297) |  Loss2: (0.1666) | Acc: (94.00%) (4967/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.3292) |  Loss2: (0.1666) | Acc: (94.00%) (6169/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.3267) |  Loss2: (0.1666) | Acc: (94.00%) (7379/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.3302) |  Loss2: (0.1666) | Acc: (94.00%) (8570/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.3294) |  Loss2: (0.1666) | Acc: (94.00%) (9780/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.3317) |  Loss2: (0.1665) | Acc: (94.00%) (10979/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.3346) |  Loss2: (0.1665) | Acc: (94.00%) (12163/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.3343) |  Loss2: (0.1665) | Acc: (94.00%) (13369/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.3360) |  Loss2: (0.1665) | Acc: (94.00%) (14566/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.3340) |  Loss2: (0.1665) | Acc: (94.00%) (15784/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.3337) |  Loss2: (0.1665) | Acc: (94.00%) (16994/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.3341) |  Loss2: (0.1665) | Acc: (94.00%) (18203/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.3339) |  Loss2: (0.1665) | Acc: (94.00%) (19403/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.3343) |  Loss2: (0.1665) | Acc: (94.00%) (20607/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.3344) |  Loss2: (0.1665) | Acc: (94.00%) (21813/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.3348) |  Loss2: (0.1665) | Acc: (94.00%) (23002/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.3345) |  Loss2: (0.1665) | Acc: (94.00%) (24210/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.3343) |  Loss2: (0.1665) | Acc: (94.00%) (25413/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.3346) |  Loss2: (0.1665) | Acc: (94.00%) (26617/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.3351) |  Loss2: (0.1664) | Acc: (94.00%) (27819/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.3354) |  Loss2: (0.1664) | Acc: (94.00%) (29017/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.3356) |  Loss2: (0.1664) | Acc: (94.00%) (30223/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.3359) |  Loss2: (0.1664) | Acc: (94.00%) (31414/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.3361) |  Loss2: (0.1664) | Acc: (94.00%) (32616/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.3364) |  Loss2: (0.1664) | Acc: (94.00%) (33819/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.3365) |  Loss2: (0.1664) | Acc: (94.00%) (35022/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.3374) |  Loss2: (0.1664) | Acc: (94.00%) (36218/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.3381) |  Loss2: (0.1664) | Acc: (93.00%) (37412/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.3377) |  Loss2: (0.1664) | Acc: (94.00%) (38626/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.3370) |  Loss2: (0.1664) | Acc: (94.00%) (39838/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.3376) |  Loss2: (0.1664) | Acc: (94.00%) (41036/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.3374) |  Loss2: (0.1663) | Acc: (94.00%) (42250/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.3371) |  Loss2: (0.1663) | Acc: (94.00%) (43458/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.3372) |  Loss2: (0.1663) | Acc: (94.00%) (44661/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.3375) |  Loss2: (0.1663) | Acc: (94.00%) (45863/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.3387) |  Loss2: (0.1663) | Acc: (94.00%) (47006/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_155.pth.tar'
# TEST : Loss: (0.4500) | Acc: (86.00%) (8698/10000)
percent tensor([0.8173], device='cuda:0')
percent tensor([0.8174], device='cuda:0')
percent tensor([0.8360], device='cuda:0')
percent tensor([0.7646], device='cuda:0')
percent tensor([0.7993], device='cuda:0')
percent tensor([0.8235], device='cuda:0')
percent tensor([0.8558], device='cuda:0')
percent tensor([0.1094], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.3486) |  Loss2: (0.1659) | Acc: (94.00%) (121/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.3322) |  Loss2: (0.1660) | Acc: (94.00%) (1330/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.3346) |  Loss2: (0.1660) | Acc: (94.00%) (2537/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.3384) |  Loss2: (0.1660) | Acc: (94.00%) (3730/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.3355) |  Loss2: (0.1659) | Acc: (94.00%) (4941/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.3360) |  Loss2: (0.1659) | Acc: (94.00%) (6142/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.3345) |  Loss2: (0.1659) | Acc: (94.00%) (7349/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.3319) |  Loss2: (0.1659) | Acc: (94.00%) (8558/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.3302) |  Loss2: (0.1659) | Acc: (94.00%) (9767/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.3300) |  Loss2: (0.1659) | Acc: (94.00%) (10972/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.3290) |  Loss2: (0.1659) | Acc: (94.00%) (12190/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.3287) |  Loss2: (0.1659) | Acc: (94.00%) (13397/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.3286) |  Loss2: (0.1659) | Acc: (94.00%) (14593/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.3300) |  Loss2: (0.1659) | Acc: (94.00%) (15798/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.3315) |  Loss2: (0.1659) | Acc: (94.00%) (16998/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.3322) |  Loss2: (0.1658) | Acc: (94.00%) (18186/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.3328) |  Loss2: (0.1658) | Acc: (94.00%) (19391/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.3323) |  Loss2: (0.1658) | Acc: (94.00%) (20606/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.3317) |  Loss2: (0.1658) | Acc: (94.00%) (21820/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.3327) |  Loss2: (0.1658) | Acc: (94.00%) (23022/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.3331) |  Loss2: (0.1658) | Acc: (94.00%) (24224/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.3336) |  Loss2: (0.1658) | Acc: (94.00%) (25423/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.3349) |  Loss2: (0.1658) | Acc: (94.00%) (26615/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.3354) |  Loss2: (0.1658) | Acc: (94.00%) (27818/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.3349) |  Loss2: (0.1658) | Acc: (94.00%) (29030/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.3355) |  Loss2: (0.1658) | Acc: (94.00%) (30232/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.3359) |  Loss2: (0.1658) | Acc: (94.00%) (31426/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.3368) |  Loss2: (0.1658) | Acc: (94.00%) (32619/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.3365) |  Loss2: (0.1658) | Acc: (94.00%) (33825/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.3361) |  Loss2: (0.1657) | Acc: (94.00%) (35032/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.3360) |  Loss2: (0.1657) | Acc: (94.00%) (36230/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.3361) |  Loss2: (0.1657) | Acc: (94.00%) (37436/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.3356) |  Loss2: (0.1657) | Acc: (94.00%) (38651/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.3352) |  Loss2: (0.1657) | Acc: (94.00%) (39869/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.3350) |  Loss2: (0.1657) | Acc: (94.00%) (41077/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.3350) |  Loss2: (0.1657) | Acc: (94.00%) (42288/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.3348) |  Loss2: (0.1657) | Acc: (94.00%) (43488/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.3345) |  Loss2: (0.1657) | Acc: (94.00%) (44697/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.3345) |  Loss2: (0.1657) | Acc: (94.00%) (45900/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.3351) |  Loss2: (0.1657) | Acc: (94.00%) (47059/50000)
# TEST : Loss: (0.4388) | Acc: (87.00%) (8704/10000)
percent tensor([0.8192], device='cuda:0')
percent tensor([0.8187], device='cuda:0')
percent tensor([0.8370], device='cuda:0')
percent tensor([0.7648], device='cuda:0')
percent tensor([0.8005], device='cuda:0')
percent tensor([0.8248], device='cuda:0')
percent tensor([0.8563], device='cuda:0')
percent tensor([0.1086], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.2948) |  Loss2: (0.1654) | Acc: (93.00%) (120/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.3192) |  Loss2: (0.1654) | Acc: (95.00%) (1339/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.3160) |  Loss2: (0.1653) | Acc: (95.00%) (2555/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.3182) |  Loss2: (0.1653) | Acc: (94.00%) (3769/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.3239) |  Loss2: (0.1653) | Acc: (94.00%) (4968/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.3275) |  Loss2: (0.1653) | Acc: (94.00%) (6175/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.3256) |  Loss2: (0.1653) | Acc: (94.00%) (7388/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.3276) |  Loss2: (0.1653) | Acc: (94.00%) (8597/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.3282) |  Loss2: (0.1653) | Acc: (94.00%) (9803/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.3275) |  Loss2: (0.1653) | Acc: (94.00%) (11015/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.3264) |  Loss2: (0.1653) | Acc: (94.00%) (12236/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.3259) |  Loss2: (0.1652) | Acc: (94.00%) (13451/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.3279) |  Loss2: (0.1652) | Acc: (94.00%) (14647/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.3292) |  Loss2: (0.1652) | Acc: (94.00%) (15849/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.3311) |  Loss2: (0.1652) | Acc: (94.00%) (17047/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.3321) |  Loss2: (0.1652) | Acc: (94.00%) (18233/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.3313) |  Loss2: (0.1652) | Acc: (94.00%) (19446/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.3312) |  Loss2: (0.1652) | Acc: (94.00%) (20653/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.3292) |  Loss2: (0.1652) | Acc: (94.00%) (21877/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.3302) |  Loss2: (0.1652) | Acc: (94.00%) (23073/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.3298) |  Loss2: (0.1652) | Acc: (94.00%) (24288/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.3300) |  Loss2: (0.1652) | Acc: (94.00%) (25492/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.3309) |  Loss2: (0.1652) | Acc: (94.00%) (26682/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.3313) |  Loss2: (0.1652) | Acc: (94.00%) (27886/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.3316) |  Loss2: (0.1652) | Acc: (94.00%) (29091/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.3323) |  Loss2: (0.1651) | Acc: (94.00%) (30278/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.3321) |  Loss2: (0.1651) | Acc: (94.00%) (31478/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.3330) |  Loss2: (0.1651) | Acc: (94.00%) (32671/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.3347) |  Loss2: (0.1651) | Acc: (94.00%) (33857/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.3340) |  Loss2: (0.1651) | Acc: (94.00%) (35070/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.3350) |  Loss2: (0.1651) | Acc: (94.00%) (36263/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.3346) |  Loss2: (0.1651) | Acc: (94.00%) (37474/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.3352) |  Loss2: (0.1651) | Acc: (94.00%) (38670/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.3343) |  Loss2: (0.1651) | Acc: (94.00%) (39886/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.3347) |  Loss2: (0.1651) | Acc: (94.00%) (41081/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.3344) |  Loss2: (0.1651) | Acc: (94.00%) (42284/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.3347) |  Loss2: (0.1651) | Acc: (94.00%) (43484/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.3352) |  Loss2: (0.1650) | Acc: (94.00%) (44678/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.3352) |  Loss2: (0.1650) | Acc: (94.00%) (45884/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.3350) |  Loss2: (0.1650) | Acc: (94.00%) (47037/50000)
# TEST : Loss: (0.4490) | Acc: (86.00%) (8672/10000)
percent tensor([0.8214], device='cuda:0')
percent tensor([0.8210], device='cuda:0')
percent tensor([0.8376], device='cuda:0')
percent tensor([0.7666], device='cuda:0')
percent tensor([0.8012], device='cuda:0')
percent tensor([0.8250], device='cuda:0')
percent tensor([0.8568], device='cuda:0')
percent tensor([0.1077], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.2989) |  Loss2: (0.1647) | Acc: (94.00%) (121/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.3364) |  Loss2: (0.1647) | Acc: (93.00%) (1320/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.3298) |  Loss2: (0.1647) | Acc: (93.00%) (2526/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.3335) |  Loss2: (0.1647) | Acc: (93.00%) (3729/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.3287) |  Loss2: (0.1647) | Acc: (94.00%) (4943/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.3286) |  Loss2: (0.1647) | Acc: (94.00%) (6147/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.3285) |  Loss2: (0.1647) | Acc: (94.00%) (7354/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.3313) |  Loss2: (0.1647) | Acc: (94.00%) (8555/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.3344) |  Loss2: (0.1647) | Acc: (94.00%) (9753/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.3336) |  Loss2: (0.1646) | Acc: (94.00%) (10952/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.3359) |  Loss2: (0.1646) | Acc: (93.00%) (12142/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.3332) |  Loss2: (0.1646) | Acc: (93.00%) (13350/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.3346) |  Loss2: (0.1646) | Acc: (93.00%) (14553/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.3334) |  Loss2: (0.1646) | Acc: (94.00%) (15775/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.3326) |  Loss2: (0.1646) | Acc: (94.00%) (16981/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.3306) |  Loss2: (0.1646) | Acc: (94.00%) (18212/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.3323) |  Loss2: (0.1646) | Acc: (94.00%) (19404/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.3321) |  Loss2: (0.1646) | Acc: (94.00%) (20607/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.3322) |  Loss2: (0.1646) | Acc: (94.00%) (21806/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.3319) |  Loss2: (0.1646) | Acc: (94.00%) (23013/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.3323) |  Loss2: (0.1646) | Acc: (94.00%) (24216/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.3324) |  Loss2: (0.1645) | Acc: (94.00%) (25423/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.3329) |  Loss2: (0.1645) | Acc: (94.00%) (26630/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.3324) |  Loss2: (0.1645) | Acc: (94.00%) (27841/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.3320) |  Loss2: (0.1645) | Acc: (94.00%) (29050/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.3317) |  Loss2: (0.1645) | Acc: (94.00%) (30252/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.3323) |  Loss2: (0.1645) | Acc: (94.00%) (31456/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.3317) |  Loss2: (0.1645) | Acc: (94.00%) (32679/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.3310) |  Loss2: (0.1645) | Acc: (94.00%) (33900/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.3309) |  Loss2: (0.1645) | Acc: (94.00%) (35116/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.3313) |  Loss2: (0.1645) | Acc: (94.00%) (36304/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.3313) |  Loss2: (0.1644) | Acc: (94.00%) (37508/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.3308) |  Loss2: (0.1644) | Acc: (94.00%) (38712/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.3310) |  Loss2: (0.1644) | Acc: (94.00%) (39908/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.3315) |  Loss2: (0.1644) | Acc: (94.00%) (41110/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.3318) |  Loss2: (0.1644) | Acc: (94.00%) (42313/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.3321) |  Loss2: (0.1644) | Acc: (94.00%) (43512/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.3325) |  Loss2: (0.1644) | Acc: (94.00%) (44718/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.3327) |  Loss2: (0.1644) | Acc: (94.00%) (45918/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.3327) |  Loss2: (0.1644) | Acc: (94.00%) (47081/50000)
# TEST : Loss: (0.4997) | Acc: (85.00%) (8583/10000)
percent tensor([0.8233], device='cuda:0')
percent tensor([0.8211], device='cuda:0')
percent tensor([0.8389], device='cuda:0')
percent tensor([0.7665], device='cuda:0')
percent tensor([0.8031], device='cuda:0')
percent tensor([0.8261], device='cuda:0')
percent tensor([0.8577], device='cuda:0')
percent tensor([0.1069], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.2884) |  Loss2: (0.1641) | Acc: (98.00%) (126/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.3240) |  Loss2: (0.1641) | Acc: (94.00%) (1324/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.3267) |  Loss2: (0.1641) | Acc: (94.00%) (2529/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.3258) |  Loss2: (0.1641) | Acc: (94.00%) (3737/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.3238) |  Loss2: (0.1641) | Acc: (94.00%) (4949/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.3274) |  Loss2: (0.1640) | Acc: (94.00%) (6154/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.3231) |  Loss2: (0.1640) | Acc: (94.00%) (7375/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.3267) |  Loss2: (0.1640) | Acc: (94.00%) (8574/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.3239) |  Loss2: (0.1640) | Acc: (94.00%) (9799/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.3228) |  Loss2: (0.1640) | Acc: (94.00%) (11019/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.3211) |  Loss2: (0.1640) | Acc: (94.00%) (12230/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.3207) |  Loss2: (0.1640) | Acc: (94.00%) (13437/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.3216) |  Loss2: (0.1640) | Acc: (94.00%) (14637/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.3223) |  Loss2: (0.1640) | Acc: (94.00%) (15839/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.3233) |  Loss2: (0.1640) | Acc: (94.00%) (17040/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.3230) |  Loss2: (0.1640) | Acc: (94.00%) (18245/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.3236) |  Loss2: (0.1640) | Acc: (94.00%) (19449/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.3242) |  Loss2: (0.1639) | Acc: (94.00%) (20655/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.3257) |  Loss2: (0.1639) | Acc: (94.00%) (21847/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.3251) |  Loss2: (0.1639) | Acc: (94.00%) (23057/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.3265) |  Loss2: (0.1639) | Acc: (94.00%) (24246/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.3254) |  Loss2: (0.1639) | Acc: (94.00%) (25467/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.3267) |  Loss2: (0.1639) | Acc: (94.00%) (26656/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.3262) |  Loss2: (0.1639) | Acc: (94.00%) (27874/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.3263) |  Loss2: (0.1639) | Acc: (94.00%) (29076/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.3261) |  Loss2: (0.1639) | Acc: (94.00%) (30286/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.3258) |  Loss2: (0.1639) | Acc: (94.00%) (31496/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.3263) |  Loss2: (0.1638) | Acc: (94.00%) (32701/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.3261) |  Loss2: (0.1638) | Acc: (94.00%) (33916/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.3265) |  Loss2: (0.1638) | Acc: (94.00%) (35118/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.3262) |  Loss2: (0.1638) | Acc: (94.00%) (36327/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.3263) |  Loss2: (0.1638) | Acc: (94.00%) (37525/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.3265) |  Loss2: (0.1638) | Acc: (94.00%) (38724/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.3266) |  Loss2: (0.1638) | Acc: (94.00%) (39934/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.3266) |  Loss2: (0.1638) | Acc: (94.00%) (41136/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.3262) |  Loss2: (0.1637) | Acc: (94.00%) (42355/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.3263) |  Loss2: (0.1637) | Acc: (94.00%) (43543/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.3265) |  Loss2: (0.1637) | Acc: (94.00%) (44755/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.3265) |  Loss2: (0.1637) | Acc: (94.00%) (45956/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.3268) |  Loss2: (0.1637) | Acc: (94.00%) (47120/50000)
# TEST : Loss: (0.4470) | Acc: (87.00%) (8734/10000)
percent tensor([0.8247], device='cuda:0')
percent tensor([0.8241], device='cuda:0')
percent tensor([0.8394], device='cuda:0')
percent tensor([0.7695], device='cuda:0')
percent tensor([0.8032], device='cuda:0')
percent tensor([0.8272], device='cuda:0')
percent tensor([0.8584], device='cuda:0')
percent tensor([0.1060], device='cuda:0')
Epoch: 160 | Batch_idx: 0 |  Loss: (0.2794) |  Loss2: (0.1633) | Acc: (96.00%) (124/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.3028) |  Loss2: (0.1633) | Acc: (95.00%) (1338/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.3017) |  Loss2: (0.1633) | Acc: (95.00%) (2566/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.3013) |  Loss2: (0.1633) | Acc: (95.00%) (3784/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.3032) |  Loss2: (0.1633) | Acc: (95.00%) (5003/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.3088) |  Loss2: (0.1632) | Acc: (95.00%) (6216/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.3120) |  Loss2: (0.1632) | Acc: (95.00%) (7426/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.3103) |  Loss2: (0.1632) | Acc: (95.00%) (8645/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.3128) |  Loss2: (0.1632) | Acc: (94.00%) (9848/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.3125) |  Loss2: (0.1632) | Acc: (94.00%) (11065/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.3115) |  Loss2: (0.1631) | Acc: (95.00%) (12283/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.3105) |  Loss2: (0.1631) | Acc: (95.00%) (13504/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.3095) |  Loss2: (0.1631) | Acc: (95.00%) (14723/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.3082) |  Loss2: (0.1631) | Acc: (95.00%) (15941/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.3083) |  Loss2: (0.1631) | Acc: (95.00%) (17157/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.3100) |  Loss2: (0.1631) | Acc: (94.00%) (18355/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.3103) |  Loss2: (0.1631) | Acc: (94.00%) (19566/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.3114) |  Loss2: (0.1630) | Acc: (94.00%) (20769/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.3119) |  Loss2: (0.1630) | Acc: (94.00%) (21978/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.3136) |  Loss2: (0.1630) | Acc: (94.00%) (23171/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.3143) |  Loss2: (0.1630) | Acc: (94.00%) (24377/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.3144) |  Loss2: (0.1630) | Acc: (94.00%) (25586/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.3146) |  Loss2: (0.1630) | Acc: (94.00%) (26799/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.3151) |  Loss2: (0.1630) | Acc: (94.00%) (27998/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.3159) |  Loss2: (0.1630) | Acc: (94.00%) (29196/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.3166) |  Loss2: (0.1630) | Acc: (94.00%) (30401/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.3171) |  Loss2: (0.1630) | Acc: (94.00%) (31606/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.3175) |  Loss2: (0.1630) | Acc: (94.00%) (32816/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.3176) |  Loss2: (0.1629) | Acc: (94.00%) (34020/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.3168) |  Loss2: (0.1629) | Acc: (94.00%) (35238/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.3172) |  Loss2: (0.1629) | Acc: (94.00%) (36436/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.3178) |  Loss2: (0.1629) | Acc: (94.00%) (37637/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.3181) |  Loss2: (0.1629) | Acc: (94.00%) (38846/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.3183) |  Loss2: (0.1629) | Acc: (94.00%) (40057/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.3191) |  Loss2: (0.1629) | Acc: (94.00%) (41253/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.3194) |  Loss2: (0.1629) | Acc: (94.00%) (42456/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.3191) |  Loss2: (0.1629) | Acc: (94.00%) (43676/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.3198) |  Loss2: (0.1629) | Acc: (94.00%) (44878/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.3205) |  Loss2: (0.1629) | Acc: (94.00%) (46076/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.3204) |  Loss2: (0.1629) | Acc: (94.00%) (47238/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_160.pth.tar'
# TEST : Loss: (0.4286) | Acc: (87.00%) (8777/10000)
percent tensor([0.8279], device='cuda:0')
percent tensor([0.8256], device='cuda:0')
percent tensor([0.8410], device='cuda:0')
percent tensor([0.7704], device='cuda:0')
percent tensor([0.8028], device='cuda:0')
percent tensor([0.8279], device='cuda:0')
percent tensor([0.8594], device='cuda:0')
percent tensor([0.1051], device='cuda:0')
Epoch: 161 | Batch_idx: 0 |  Loss: (0.2541) |  Loss2: (0.1626) | Acc: (96.00%) (123/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.3256) |  Loss2: (0.1626) | Acc: (93.00%) (1323/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.3347) |  Loss2: (0.1626) | Acc: (93.00%) (2521/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.3345) |  Loss2: (0.1626) | Acc: (93.00%) (3724/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.3327) |  Loss2: (0.1626) | Acc: (93.00%) (4933/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.3293) |  Loss2: (0.1626) | Acc: (94.00%) (6140/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.3251) |  Loss2: (0.1626) | Acc: (94.00%) (7359/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.3258) |  Loss2: (0.1626) | Acc: (94.00%) (8561/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.3245) |  Loss2: (0.1626) | Acc: (94.00%) (9773/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.3235) |  Loss2: (0.1626) | Acc: (94.00%) (10983/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.3230) |  Loss2: (0.1625) | Acc: (94.00%) (12200/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.3265) |  Loss2: (0.1625) | Acc: (94.00%) (13396/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.3249) |  Loss2: (0.1625) | Acc: (94.00%) (14619/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.3240) |  Loss2: (0.1625) | Acc: (94.00%) (15833/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.3227) |  Loss2: (0.1625) | Acc: (94.00%) (17051/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.3214) |  Loss2: (0.1625) | Acc: (94.00%) (18265/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.3208) |  Loss2: (0.1625) | Acc: (94.00%) (19482/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.3200) |  Loss2: (0.1625) | Acc: (94.00%) (20693/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.3205) |  Loss2: (0.1625) | Acc: (94.00%) (21905/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.3197) |  Loss2: (0.1625) | Acc: (94.00%) (23131/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.3192) |  Loss2: (0.1624) | Acc: (94.00%) (24346/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.3193) |  Loss2: (0.1624) | Acc: (94.00%) (25551/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.3189) |  Loss2: (0.1624) | Acc: (94.00%) (26768/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.3193) |  Loss2: (0.1624) | Acc: (94.00%) (27971/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.3201) |  Loss2: (0.1624) | Acc: (94.00%) (29167/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.3210) |  Loss2: (0.1624) | Acc: (94.00%) (30368/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.3203) |  Loss2: (0.1624) | Acc: (94.00%) (31585/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.3213) |  Loss2: (0.1624) | Acc: (94.00%) (32776/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.3215) |  Loss2: (0.1624) | Acc: (94.00%) (33983/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.3217) |  Loss2: (0.1624) | Acc: (94.00%) (35191/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.3222) |  Loss2: (0.1624) | Acc: (94.00%) (36395/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.3217) |  Loss2: (0.1623) | Acc: (94.00%) (37608/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.3216) |  Loss2: (0.1623) | Acc: (94.00%) (38820/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.3216) |  Loss2: (0.1623) | Acc: (94.00%) (40027/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.3209) |  Loss2: (0.1623) | Acc: (94.00%) (41243/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.3213) |  Loss2: (0.1623) | Acc: (94.00%) (42441/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.3223) |  Loss2: (0.1623) | Acc: (94.00%) (43638/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.3221) |  Loss2: (0.1623) | Acc: (94.00%) (44853/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.3216) |  Loss2: (0.1623) | Acc: (94.00%) (46072/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.3209) |  Loss2: (0.1623) | Acc: (94.00%) (47252/50000)
# TEST : Loss: (0.4361) | Acc: (87.00%) (8754/10000)
percent tensor([0.8296], device='cuda:0')
percent tensor([0.8272], device='cuda:0')
percent tensor([0.8421], device='cuda:0')
percent tensor([0.7722], device='cuda:0')
percent tensor([0.8046], device='cuda:0')
percent tensor([0.8279], device='cuda:0')
percent tensor([0.8602], device='cuda:0')
percent tensor([0.1042], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.2899) |  Loss2: (0.1619) | Acc: (94.00%) (121/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.3093) |  Loss2: (0.1619) | Acc: (94.00%) (1335/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.3208) |  Loss2: (0.1619) | Acc: (94.00%) (2534/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.3181) |  Loss2: (0.1619) | Acc: (94.00%) (3741/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.3183) |  Loss2: (0.1619) | Acc: (94.00%) (4943/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.3188) |  Loss2: (0.1619) | Acc: (94.00%) (6155/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.3188) |  Loss2: (0.1619) | Acc: (94.00%) (7367/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.3174) |  Loss2: (0.1619) | Acc: (94.00%) (8582/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.3165) |  Loss2: (0.1619) | Acc: (94.00%) (9798/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.3164) |  Loss2: (0.1619) | Acc: (94.00%) (11016/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.3147) |  Loss2: (0.1619) | Acc: (94.00%) (12238/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.3168) |  Loss2: (0.1619) | Acc: (94.00%) (13441/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.3156) |  Loss2: (0.1619) | Acc: (94.00%) (14665/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.3160) |  Loss2: (0.1619) | Acc: (94.00%) (15876/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.3165) |  Loss2: (0.1618) | Acc: (94.00%) (17081/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.3158) |  Loss2: (0.1618) | Acc: (94.00%) (18302/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.3157) |  Loss2: (0.1618) | Acc: (94.00%) (19524/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.3159) |  Loss2: (0.1618) | Acc: (94.00%) (20736/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.3162) |  Loss2: (0.1618) | Acc: (94.00%) (21952/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.3161) |  Loss2: (0.1618) | Acc: (94.00%) (23169/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.3161) |  Loss2: (0.1618) | Acc: (94.00%) (24385/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.3154) |  Loss2: (0.1618) | Acc: (94.00%) (25604/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.3151) |  Loss2: (0.1618) | Acc: (94.00%) (26818/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.3156) |  Loss2: (0.1618) | Acc: (94.00%) (28022/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.3154) |  Loss2: (0.1618) | Acc: (94.00%) (29239/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.3161) |  Loss2: (0.1618) | Acc: (94.00%) (30446/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.3163) |  Loss2: (0.1618) | Acc: (94.00%) (31656/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.3158) |  Loss2: (0.1617) | Acc: (94.00%) (32870/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.3149) |  Loss2: (0.1617) | Acc: (94.00%) (34092/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.3145) |  Loss2: (0.1617) | Acc: (94.00%) (35300/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.3142) |  Loss2: (0.1617) | Acc: (94.00%) (36515/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.3144) |  Loss2: (0.1617) | Acc: (94.00%) (37727/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.3143) |  Loss2: (0.1617) | Acc: (94.00%) (38937/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.3142) |  Loss2: (0.1617) | Acc: (94.00%) (40152/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.3138) |  Loss2: (0.1617) | Acc: (94.00%) (41368/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.3137) |  Loss2: (0.1617) | Acc: (94.00%) (42586/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.3140) |  Loss2: (0.1617) | Acc: (94.00%) (43785/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.3142) |  Loss2: (0.1617) | Acc: (94.00%) (44988/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.3147) |  Loss2: (0.1617) | Acc: (94.00%) (46179/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.3146) |  Loss2: (0.1616) | Acc: (94.00%) (47336/50000)
# TEST : Loss: (0.4694) | Acc: (86.00%) (8696/10000)
percent tensor([0.8313], device='cuda:0')
percent tensor([0.8284], device='cuda:0')
percent tensor([0.8431], device='cuda:0')
percent tensor([0.7741], device='cuda:0')
percent tensor([0.8056], device='cuda:0')
percent tensor([0.8283], device='cuda:0')
percent tensor([0.8612], device='cuda:0')
percent tensor([0.1034], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.3061) |  Loss2: (0.1613) | Acc: (93.00%) (120/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.2977) |  Loss2: (0.1612) | Acc: (94.00%) (1334/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.2922) |  Loss2: (0.1612) | Acc: (95.00%) (2561/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.3048) |  Loss2: (0.1612) | Acc: (94.00%) (3758/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.3004) |  Loss2: (0.1612) | Acc: (95.00%) (4988/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.3059) |  Loss2: (0.1612) | Acc: (94.00%) (6198/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.3104) |  Loss2: (0.1612) | Acc: (94.00%) (7401/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.3083) |  Loss2: (0.1612) | Acc: (94.00%) (8626/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.3118) |  Loss2: (0.1612) | Acc: (94.00%) (9823/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.3124) |  Loss2: (0.1612) | Acc: (94.00%) (11027/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.3125) |  Loss2: (0.1612) | Acc: (94.00%) (12231/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.3139) |  Loss2: (0.1612) | Acc: (94.00%) (13441/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.3153) |  Loss2: (0.1612) | Acc: (94.00%) (14644/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.3144) |  Loss2: (0.1612) | Acc: (94.00%) (15851/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.3160) |  Loss2: (0.1612) | Acc: (94.00%) (17048/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.3153) |  Loss2: (0.1612) | Acc: (94.00%) (18263/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.3154) |  Loss2: (0.1612) | Acc: (94.00%) (19484/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.3156) |  Loss2: (0.1612) | Acc: (94.00%) (20695/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.3167) |  Loss2: (0.1611) | Acc: (94.00%) (21895/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.3171) |  Loss2: (0.1611) | Acc: (94.00%) (23105/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.3165) |  Loss2: (0.1611) | Acc: (94.00%) (24321/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.3158) |  Loss2: (0.1611) | Acc: (94.00%) (25531/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.3146) |  Loss2: (0.1611) | Acc: (94.00%) (26757/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.3137) |  Loss2: (0.1611) | Acc: (94.00%) (27977/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.3128) |  Loss2: (0.1611) | Acc: (94.00%) (29199/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.3125) |  Loss2: (0.1611) | Acc: (94.00%) (30421/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.3130) |  Loss2: (0.1611) | Acc: (94.00%) (31632/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.3131) |  Loss2: (0.1611) | Acc: (94.00%) (32847/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.3126) |  Loss2: (0.1611) | Acc: (94.00%) (34065/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.3129) |  Loss2: (0.1610) | Acc: (94.00%) (35273/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.3127) |  Loss2: (0.1610) | Acc: (94.00%) (36494/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.3120) |  Loss2: (0.1610) | Acc: (94.00%) (37715/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.3113) |  Loss2: (0.1610) | Acc: (94.00%) (38936/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.3119) |  Loss2: (0.1610) | Acc: (94.00%) (40139/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.3123) |  Loss2: (0.1610) | Acc: (94.00%) (41342/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.3121) |  Loss2: (0.1610) | Acc: (94.00%) (42555/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.3114) |  Loss2: (0.1610) | Acc: (94.00%) (43775/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.3119) |  Loss2: (0.1609) | Acc: (94.00%) (44978/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.3127) |  Loss2: (0.1609) | Acc: (94.00%) (46179/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.3132) |  Loss2: (0.1609) | Acc: (94.00%) (47334/50000)
# TEST : Loss: (0.4744) | Acc: (86.00%) (8661/10000)
percent tensor([0.8330], device='cuda:0')
percent tensor([0.8308], device='cuda:0')
percent tensor([0.8446], device='cuda:0')
percent tensor([0.7762], device='cuda:0')
percent tensor([0.8078], device='cuda:0')
percent tensor([0.8297], device='cuda:0')
percent tensor([0.8611], device='cuda:0')
percent tensor([0.1025], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.2678) |  Loss2: (0.1604) | Acc: (96.00%) (124/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.3286) |  Loss2: (0.1604) | Acc: (93.00%) (1322/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.3176) |  Loss2: (0.1604) | Acc: (94.00%) (2544/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.3134) |  Loss2: (0.1604) | Acc: (94.00%) (3754/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.3153) |  Loss2: (0.1604) | Acc: (94.00%) (4963/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.3175) |  Loss2: (0.1604) | Acc: (94.00%) (6175/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.3195) |  Loss2: (0.1604) | Acc: (94.00%) (7376/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.3176) |  Loss2: (0.1604) | Acc: (94.00%) (8587/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.3171) |  Loss2: (0.1604) | Acc: (94.00%) (9794/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.3154) |  Loss2: (0.1603) | Acc: (94.00%) (11017/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.3152) |  Loss2: (0.1603) | Acc: (94.00%) (12235/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.3146) |  Loss2: (0.1603) | Acc: (94.00%) (13445/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.3153) |  Loss2: (0.1603) | Acc: (94.00%) (14649/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.3156) |  Loss2: (0.1603) | Acc: (94.00%) (15855/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.3143) |  Loss2: (0.1603) | Acc: (94.00%) (17073/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.3150) |  Loss2: (0.1603) | Acc: (94.00%) (18271/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.3158) |  Loss2: (0.1603) | Acc: (94.00%) (19478/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.3157) |  Loss2: (0.1602) | Acc: (94.00%) (20687/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.3152) |  Loss2: (0.1602) | Acc: (94.00%) (21905/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.3169) |  Loss2: (0.1602) | Acc: (94.00%) (23107/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.3168) |  Loss2: (0.1602) | Acc: (94.00%) (24317/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.3168) |  Loss2: (0.1602) | Acc: (94.00%) (25532/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.3163) |  Loss2: (0.1602) | Acc: (94.00%) (26748/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.3164) |  Loss2: (0.1602) | Acc: (94.00%) (27955/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.3160) |  Loss2: (0.1602) | Acc: (94.00%) (29177/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.3162) |  Loss2: (0.1602) | Acc: (94.00%) (30385/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.3158) |  Loss2: (0.1602) | Acc: (94.00%) (31604/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.3169) |  Loss2: (0.1602) | Acc: (94.00%) (32795/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.3168) |  Loss2: (0.1602) | Acc: (94.00%) (34008/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.3171) |  Loss2: (0.1601) | Acc: (94.00%) (35214/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.3171) |  Loss2: (0.1601) | Acc: (94.00%) (36429/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.3176) |  Loss2: (0.1601) | Acc: (94.00%) (37635/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.3178) |  Loss2: (0.1601) | Acc: (94.00%) (38847/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.3171) |  Loss2: (0.1601) | Acc: (94.00%) (40070/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.3170) |  Loss2: (0.1601) | Acc: (94.00%) (41271/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.3170) |  Loss2: (0.1601) | Acc: (94.00%) (42486/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.3169) |  Loss2: (0.1601) | Acc: (94.00%) (43702/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.3169) |  Loss2: (0.1601) | Acc: (94.00%) (44914/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.3167) |  Loss2: (0.1601) | Acc: (94.00%) (46125/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.3168) |  Loss2: (0.1601) | Acc: (94.00%) (47283/50000)
# TEST : Loss: (0.4777) | Acc: (86.00%) (8677/10000)
percent tensor([0.8358], device='cuda:0')
percent tensor([0.8327], device='cuda:0')
percent tensor([0.8453], device='cuda:0')
percent tensor([0.7763], device='cuda:0')
percent tensor([0.8077], device='cuda:0')
percent tensor([0.8303], device='cuda:0')
percent tensor([0.8622], device='cuda:0')
percent tensor([0.1018], device='cuda:0')
0 hours 59 mins 15 secs for training