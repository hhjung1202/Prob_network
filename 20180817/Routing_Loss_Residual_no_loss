Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3151) |  Loss2: (0.0000) | Acc: (9.00%) (12/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.3080) |  Loss2: (0.0000) | Acc: (9.00%) (135/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2981) |  Loss2: (0.0000) | Acc: (11.00%) (302/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2914) |  Loss2: (0.0000) | Acc: (12.00%) (488/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2860) |  Loss2: (0.0000) | Acc: (13.00%) (688/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2778) |  Loss2: (0.0000) | Acc: (14.00%) (931/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2704) |  Loss2: (0.0000) | Acc: (15.00%) (1196/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2608) |  Loss2: (0.0000) | Acc: (16.00%) (1479/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2525) |  Loss2: (0.0000) | Acc: (16.00%) (1729/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2435) |  Loss2: (0.0000) | Acc: (17.00%) (1996/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2340) |  Loss2: (0.0000) | Acc: (17.00%) (2282/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2240) |  Loss2: (0.0000) | Acc: (18.00%) (2582/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2147) |  Loss2: (0.0000) | Acc: (18.00%) (2919/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.2052) |  Loss2: (0.0000) | Acc: (19.00%) (3268/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1970) |  Loss2: (0.0000) | Acc: (19.00%) (3594/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1882) |  Loss2: (0.0000) | Acc: (20.00%) (3919/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1803) |  Loss2: (0.0000) | Acc: (20.00%) (4234/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1715) |  Loss2: (0.0000) | Acc: (20.00%) (4574/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1638) |  Loss2: (0.0000) | Acc: (21.00%) (4930/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1566) |  Loss2: (0.0000) | Acc: (21.00%) (5284/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1480) |  Loss2: (0.0000) | Acc: (22.00%) (5674/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1391) |  Loss2: (0.0000) | Acc: (22.00%) (6049/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1314) |  Loss2: (0.0000) | Acc: (22.00%) (6409/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1237) |  Loss2: (0.0000) | Acc: (22.00%) (6767/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1160) |  Loss2: (0.0000) | Acc: (23.00%) (7159/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1086) |  Loss2: (0.0000) | Acc: (23.00%) (7549/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.1022) |  Loss2: (0.0000) | Acc: (23.00%) (7933/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0956) |  Loss2: (0.0000) | Acc: (23.00%) (8317/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0879) |  Loss2: (0.0000) | Acc: (24.00%) (8737/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0812) |  Loss2: (0.0000) | Acc: (24.00%) (9116/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0747) |  Loss2: (0.0000) | Acc: (24.00%) (9520/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0670) |  Loss2: (0.0000) | Acc: (24.00%) (9947/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0607) |  Loss2: (0.0000) | Acc: (25.00%) (10372/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0541) |  Loss2: (0.0000) | Acc: (25.00%) (10806/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0475) |  Loss2: (0.0000) | Acc: (25.00%) (11238/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0413) |  Loss2: (0.0000) | Acc: (25.00%) (11651/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0347) |  Loss2: (0.0000) | Acc: (26.00%) (12134/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0287) |  Loss2: (0.0000) | Acc: (26.00%) (12569/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0232) |  Loss2: (0.0000) | Acc: (26.00%) (13011/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0171) |  Loss2: (0.0000) | Acc: (26.00%) (13442/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_000.pth.tar'
# TEST : Loss: (1.7571) | Acc: (35.00%) (3537/10000)
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8461) |  Loss2: (0.0000) | Acc: (33.00%) (43/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8042) |  Loss2: (0.0000) | Acc: (33.00%) (475/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.7968) |  Loss2: (0.0000) | Acc: (35.00%) (957/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.7800) |  Loss2: (0.0000) | Acc: (36.00%) (1433/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.7722) |  Loss2: (0.0000) | Acc: (36.00%) (1901/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.7691) |  Loss2: (0.0000) | Acc: (36.00%) (2363/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.7653) |  Loss2: (0.0000) | Acc: (36.00%) (2831/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.7627) |  Loss2: (0.0000) | Acc: (36.00%) (3282/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.7574) |  Loss2: (0.0000) | Acc: (36.00%) (3740/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.7564) |  Loss2: (0.0000) | Acc: (36.00%) (4203/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7541) |  Loss2: (0.0000) | Acc: (36.00%) (4680/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7518) |  Loss2: (0.0000) | Acc: (36.00%) (5172/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7501) |  Loss2: (0.0000) | Acc: (36.00%) (5642/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7425) |  Loss2: (0.0000) | Acc: (36.00%) (6161/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7392) |  Loss2: (0.0000) | Acc: (36.00%) (6635/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7363) |  Loss2: (0.0000) | Acc: (36.00%) (7127/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7338) |  Loss2: (0.0000) | Acc: (36.00%) (7612/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7301) |  Loss2: (0.0000) | Acc: (36.00%) (8094/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7284) |  Loss2: (0.0000) | Acc: (36.00%) (8564/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7253) |  Loss2: (0.0000) | Acc: (36.00%) (9031/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7234) |  Loss2: (0.0000) | Acc: (36.00%) (9496/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7224) |  Loss2: (0.0000) | Acc: (36.00%) (9940/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7201) |  Loss2: (0.0000) | Acc: (36.00%) (10413/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7160) |  Loss2: (0.0000) | Acc: (36.00%) (10925/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7126) |  Loss2: (0.0000) | Acc: (37.00%) (11444/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7098) |  Loss2: (0.0000) | Acc: (37.00%) (11936/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7067) |  Loss2: (0.0000) | Acc: (37.00%) (12469/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7042) |  Loss2: (0.0000) | Acc: (37.00%) (12987/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7009) |  Loss2: (0.0000) | Acc: (37.00%) (13524/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.6980) |  Loss2: (0.0000) | Acc: (37.00%) (14049/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.6965) |  Loss2: (0.0000) | Acc: (37.00%) (14547/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.6946) |  Loss2: (0.0000) | Acc: (37.00%) (15061/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.6916) |  Loss2: (0.0000) | Acc: (38.00%) (15618/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.6894) |  Loss2: (0.0000) | Acc: (38.00%) (16120/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.6870) |  Loss2: (0.0000) | Acc: (38.00%) (16620/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.6839) |  Loss2: (0.0000) | Acc: (38.00%) (17159/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.6812) |  Loss2: (0.0000) | Acc: (38.00%) (17674/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.6795) |  Loss2: (0.0000) | Acc: (38.00%) (18194/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.6768) |  Loss2: (0.0000) | Acc: (38.00%) (18725/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.6744) |  Loss2: (0.0000) | Acc: (38.00%) (19224/50000)
# TEST : Loss: (1.5714) | Acc: (40.00%) (4038/10000)
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.4848) |  Loss2: (0.0000) | Acc: (42.00%) (55/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5554) |  Loss2: (0.0000) | Acc: (42.00%) (604/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5605) |  Loss2: (0.0000) | Acc: (41.00%) (1116/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5685) |  Loss2: (0.0000) | Acc: (41.00%) (1629/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5614) |  Loss2: (0.0000) | Acc: (41.00%) (2165/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5685) |  Loss2: (0.0000) | Acc: (41.00%) (2678/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5669) |  Loss2: (0.0000) | Acc: (41.00%) (3217/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5651) |  Loss2: (0.0000) | Acc: (41.00%) (3744/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5645) |  Loss2: (0.0000) | Acc: (41.00%) (4281/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5624) |  Loss2: (0.0000) | Acc: (41.00%) (4840/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5617) |  Loss2: (0.0000) | Acc: (41.00%) (5387/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5609) |  Loss2: (0.0000) | Acc: (41.00%) (5915/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5579) |  Loss2: (0.0000) | Acc: (41.00%) (6466/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5527) |  Loss2: (0.0000) | Acc: (42.00%) (7047/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5502) |  Loss2: (0.0000) | Acc: (42.00%) (7623/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5496) |  Loss2: (0.0000) | Acc: (42.00%) (8204/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5462) |  Loss2: (0.0000) | Acc: (42.00%) (8775/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5444) |  Loss2: (0.0000) | Acc: (42.00%) (9336/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5420) |  Loss2: (0.0000) | Acc: (42.00%) (9886/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5407) |  Loss2: (0.0000) | Acc: (42.00%) (10447/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5379) |  Loss2: (0.0000) | Acc: (42.00%) (11037/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5343) |  Loss2: (0.0000) | Acc: (43.00%) (11637/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5327) |  Loss2: (0.0000) | Acc: (43.00%) (12208/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5300) |  Loss2: (0.0000) | Acc: (43.00%) (12819/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5281) |  Loss2: (0.0000) | Acc: (43.00%) (13397/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5244) |  Loss2: (0.0000) | Acc: (43.00%) (14009/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5224) |  Loss2: (0.0000) | Acc: (43.00%) (14578/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5208) |  Loss2: (0.0000) | Acc: (43.00%) (15179/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5194) |  Loss2: (0.0000) | Acc: (43.00%) (15757/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5171) |  Loss2: (0.0000) | Acc: (43.00%) (16362/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5155) |  Loss2: (0.0000) | Acc: (43.00%) (16952/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5135) |  Loss2: (0.0000) | Acc: (44.00%) (17545/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5108) |  Loss2: (0.0000) | Acc: (44.00%) (18165/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5084) |  Loss2: (0.0000) | Acc: (44.00%) (18756/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5052) |  Loss2: (0.0000) | Acc: (44.00%) (19388/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5038) |  Loss2: (0.0000) | Acc: (44.00%) (19993/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5020) |  Loss2: (0.0000) | Acc: (44.00%) (20621/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.4996) |  Loss2: (0.0000) | Acc: (44.00%) (21255/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.4979) |  Loss2: (0.0000) | Acc: (44.00%) (21871/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.4956) |  Loss2: (0.0000) | Acc: (44.00%) (22494/50000)
# TEST : Loss: (1.3726) | Acc: (49.00%) (4928/10000)
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.4162) |  Loss2: (0.0000) | Acc: (48.00%) (62/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.3967) |  Loss2: (0.0000) | Acc: (49.00%) (701/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.3900) |  Loss2: (0.0000) | Acc: (49.00%) (1342/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.3977) |  Loss2: (0.0000) | Acc: (49.00%) (1960/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.3957) |  Loss2: (0.0000) | Acc: (49.00%) (2600/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.3945) |  Loss2: (0.0000) | Acc: (49.00%) (3228/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.3935) |  Loss2: (0.0000) | Acc: (49.00%) (3855/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.3896) |  Loss2: (0.0000) | Acc: (49.00%) (4491/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.3897) |  Loss2: (0.0000) | Acc: (49.00%) (5139/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.3848) |  Loss2: (0.0000) | Acc: (49.00%) (5810/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.3792) |  Loss2: (0.0000) | Acc: (50.00%) (6491/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.3797) |  Loss2: (0.0000) | Acc: (50.00%) (7113/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.3767) |  Loss2: (0.0000) | Acc: (50.00%) (7777/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.3744) |  Loss2: (0.0000) | Acc: (50.00%) (8428/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.3734) |  Loss2: (0.0000) | Acc: (50.00%) (9085/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.3729) |  Loss2: (0.0000) | Acc: (50.00%) (9740/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.3715) |  Loss2: (0.0000) | Acc: (50.00%) (10378/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.3711) |  Loss2: (0.0000) | Acc: (50.00%) (11046/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.3695) |  Loss2: (0.0000) | Acc: (50.00%) (11693/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.3686) |  Loss2: (0.0000) | Acc: (50.00%) (12353/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.3681) |  Loss2: (0.0000) | Acc: (50.00%) (13028/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.3668) |  Loss2: (0.0000) | Acc: (50.00%) (13712/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.3640) |  Loss2: (0.0000) | Acc: (50.00%) (14377/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.3609) |  Loss2: (0.0000) | Acc: (50.00%) (15072/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.3587) |  Loss2: (0.0000) | Acc: (51.00%) (15761/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.3559) |  Loss2: (0.0000) | Acc: (51.00%) (16449/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.3550) |  Loss2: (0.0000) | Acc: (51.00%) (17115/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.3541) |  Loss2: (0.0000) | Acc: (51.00%) (17783/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.3528) |  Loss2: (0.0000) | Acc: (51.00%) (18468/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.3527) |  Loss2: (0.0000) | Acc: (51.00%) (19102/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.3528) |  Loss2: (0.0000) | Acc: (51.00%) (19760/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.3514) |  Loss2: (0.0000) | Acc: (51.00%) (20430/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.3492) |  Loss2: (0.0000) | Acc: (51.00%) (21123/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.3471) |  Loss2: (0.0000) | Acc: (51.00%) (21795/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.3450) |  Loss2: (0.0000) | Acc: (51.00%) (22478/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3430) |  Loss2: (0.0000) | Acc: (51.00%) (23201/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3416) |  Loss2: (0.0000) | Acc: (51.00%) (23873/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3398) |  Loss2: (0.0000) | Acc: (51.00%) (24572/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3369) |  Loss2: (0.0000) | Acc: (51.00%) (25312/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3351) |  Loss2: (0.0000) | Acc: (51.00%) (25990/50000)
# TEST : Loss: (1.2565) | Acc: (53.00%) (5314/10000)
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.2988) |  Loss2: (0.0000) | Acc: (58.00%) (75/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.2363) |  Loss2: (0.0000) | Acc: (55.00%) (780/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.2451) |  Loss2: (0.0000) | Acc: (55.00%) (1481/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.2469) |  Loss2: (0.0000) | Acc: (55.00%) (2192/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.2522) |  Loss2: (0.0000) | Acc: (55.00%) (2903/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.2535) |  Loss2: (0.0000) | Acc: (55.00%) (3614/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.2533) |  Loss2: (0.0000) | Acc: (55.00%) (4323/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.2555) |  Loss2: (0.0000) | Acc: (55.00%) (5033/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.2541) |  Loss2: (0.0000) | Acc: (55.00%) (5707/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.2531) |  Loss2: (0.0000) | Acc: (55.00%) (6410/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.2526) |  Loss2: (0.0000) | Acc: (55.00%) (7115/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.2493) |  Loss2: (0.0000) | Acc: (55.00%) (7848/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.2474) |  Loss2: (0.0000) | Acc: (55.00%) (8566/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.2454) |  Loss2: (0.0000) | Acc: (55.00%) (9283/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.2385) |  Loss2: (0.0000) | Acc: (55.00%) (10033/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2390) |  Loss2: (0.0000) | Acc: (55.00%) (10757/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2390) |  Loss2: (0.0000) | Acc: (55.00%) (11485/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (55.00%) (12194/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2377) |  Loss2: (0.0000) | Acc: (55.00%) (12921/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2366) |  Loss2: (0.0000) | Acc: (55.00%) (13659/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2343) |  Loss2: (0.0000) | Acc: (56.00%) (14411/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2337) |  Loss2: (0.0000) | Acc: (55.00%) (15123/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2325) |  Loss2: (0.0000) | Acc: (56.00%) (15875/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2301) |  Loss2: (0.0000) | Acc: (56.00%) (16638/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2303) |  Loss2: (0.0000) | Acc: (56.00%) (17352/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2294) |  Loss2: (0.0000) | Acc: (56.00%) (18077/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (56.00%) (18794/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2280) |  Loss2: (0.0000) | Acc: (56.00%) (19511/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2258) |  Loss2: (0.0000) | Acc: (56.00%) (20258/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2224) |  Loss2: (0.0000) | Acc: (56.00%) (21035/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2203) |  Loss2: (0.0000) | Acc: (56.00%) (21782/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2186) |  Loss2: (0.0000) | Acc: (56.00%) (22540/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2183) |  Loss2: (0.0000) | Acc: (56.00%) (23270/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2178) |  Loss2: (0.0000) | Acc: (56.00%) (24000/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2176) |  Loss2: (0.0000) | Acc: (56.00%) (24740/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2162) |  Loss2: (0.0000) | Acc: (56.00%) (25476/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2157) |  Loss2: (0.0000) | Acc: (56.00%) (26217/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2149) |  Loss2: (0.0000) | Acc: (56.00%) (26953/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2135) |  Loss2: (0.0000) | Acc: (56.00%) (27684/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2131) |  Loss2: (0.0000) | Acc: (56.00%) (28373/50000)
# TEST : Loss: (1.1826) | Acc: (57.00%) (5745/10000)
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
percent tensor([0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.1716) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2142) |  Loss2: (0.0000) | Acc: (55.00%) (777/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.2455) |  Loss2: (0.0000) | Acc: (53.00%) (1440/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.2515) |  Loss2: (0.0000) | Acc: (54.00%) (2151/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.2730) |  Loss2: (0.0000) | Acc: (53.00%) (2813/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.2826) |  Loss2: (0.0000) | Acc: (53.00%) (3473/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.2880) |  Loss2: (0.0000) | Acc: (53.00%) (4168/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.2876) |  Loss2: (0.0000) | Acc: (53.00%) (4858/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.2877) |  Loss2: (0.0000) | Acc: (53.00%) (5529/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.2852) |  Loss2: (0.0000) | Acc: (53.00%) (6233/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.2849) |  Loss2: (0.0000) | Acc: (53.00%) (6920/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.2834) |  Loss2: (0.0000) | Acc: (53.00%) (7626/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.2844) |  Loss2: (0.0000) | Acc: (53.00%) (8315/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.2820) |  Loss2: (0.0000) | Acc: (53.00%) (9022/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.2837) |  Loss2: (0.0000) | Acc: (53.00%) (9708/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.2797) |  Loss2: (0.0000) | Acc: (53.00%) (10433/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.2757) |  Loss2: (0.0000) | Acc: (54.00%) (11143/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.2769) |  Loss2: (0.0000) | Acc: (54.00%) (11835/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.2775) |  Loss2: (0.0000) | Acc: (54.00%) (12546/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.2761) |  Loss2: (0.0000) | Acc: (54.00%) (13262/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.2754) |  Loss2: (0.0000) | Acc: (54.00%) (13965/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.2754) |  Loss2: (0.0000) | Acc: (54.00%) (14669/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.2714) |  Loss2: (0.0000) | Acc: (54.00%) (15428/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.2693) |  Loss2: (0.0000) | Acc: (54.00%) (16150/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.2676) |  Loss2: (0.0000) | Acc: (54.00%) (16886/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.2643) |  Loss2: (0.0000) | Acc: (54.00%) (17624/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.2623) |  Loss2: (0.0000) | Acc: (54.00%) (18336/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.2622) |  Loss2: (0.0000) | Acc: (54.00%) (19029/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.2610) |  Loss2: (0.0000) | Acc: (54.00%) (19738/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.2597) |  Loss2: (0.0000) | Acc: (54.00%) (20451/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.2578) |  Loss2: (0.0000) | Acc: (54.00%) (21172/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.2572) |  Loss2: (0.0000) | Acc: (54.00%) (21863/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.2558) |  Loss2: (0.0000) | Acc: (54.00%) (22584/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.2528) |  Loss2: (0.0000) | Acc: (55.00%) (23357/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.2521) |  Loss2: (0.0000) | Acc: (55.00%) (24085/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.2509) |  Loss2: (0.0000) | Acc: (55.00%) (24781/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.2494) |  Loss2: (0.0000) | Acc: (55.00%) (25521/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.2467) |  Loss2: (0.0000) | Acc: (55.00%) (26271/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.2447) |  Loss2: (0.0000) | Acc: (55.00%) (27005/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.2437) |  Loss2: (0.0000) | Acc: (55.00%) (27706/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_005.pth.tar'
# TEST : Loss: (1.1853) | Acc: (56.00%) (5676/10000)
percent tensor([0.4932], device='cuda:0')
percent tensor([0.4966], device='cuda:0')
percent tensor([0.5182], device='cuda:0')
percent tensor([0.5051], device='cuda:0')
percent tensor([0.5097], device='cuda:0')
percent tensor([0.4929], device='cuda:0')
percent tensor([0.4926], device='cuda:0')
percent tensor([0.4737], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2020) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.1897) |  Loss2: (0.0000) | Acc: (56.00%) (790/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (1510/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.1953) |  Loss2: (0.0000) | Acc: (56.00%) (2239/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.1997) |  Loss2: (0.0000) | Acc: (56.00%) (2959/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.1973) |  Loss2: (0.0000) | Acc: (56.00%) (3684/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.1879) |  Loss2: (0.0000) | Acc: (57.00%) (4451/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.1914) |  Loss2: (0.0000) | Acc: (57.00%) (5183/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.1875) |  Loss2: (0.0000) | Acc: (57.00%) (5947/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.1908) |  Loss2: (0.0000) | Acc: (57.00%) (6664/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.1907) |  Loss2: (0.0000) | Acc: (57.00%) (7388/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.1886) |  Loss2: (0.0000) | Acc: (57.00%) (8135/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.1896) |  Loss2: (0.0000) | Acc: (57.00%) (8855/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.1902) |  Loss2: (0.0000) | Acc: (57.00%) (9584/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.1926) |  Loss2: (0.0000) | Acc: (57.00%) (10295/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.1895) |  Loss2: (0.0000) | Acc: (57.00%) (11032/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.1899) |  Loss2: (0.0000) | Acc: (57.00%) (11794/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.1900) |  Loss2: (0.0000) | Acc: (57.00%) (12533/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.1874) |  Loss2: (0.0000) | Acc: (57.00%) (13260/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.1885) |  Loss2: (0.0000) | Acc: (57.00%) (14002/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.1879) |  Loss2: (0.0000) | Acc: (57.00%) (14756/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.1898) |  Loss2: (0.0000) | Acc: (57.00%) (15442/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.1898) |  Loss2: (0.0000) | Acc: (57.00%) (16170/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.1892) |  Loss2: (0.0000) | Acc: (57.00%) (16916/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (57.00%) (17655/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.1880) |  Loss2: (0.0000) | Acc: (57.00%) (18396/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.1856) |  Loss2: (0.0000) | Acc: (57.00%) (19163/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.1832) |  Loss2: (0.0000) | Acc: (57.00%) (19909/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.1811) |  Loss2: (0.0000) | Acc: (57.00%) (20676/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.1802) |  Loss2: (0.0000) | Acc: (57.00%) (21419/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.1803) |  Loss2: (0.0000) | Acc: (57.00%) (22163/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.1787) |  Loss2: (0.0000) | Acc: (57.00%) (22936/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.1781) |  Loss2: (0.0000) | Acc: (57.00%) (23686/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.1778) |  Loss2: (0.0000) | Acc: (57.00%) (24430/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.1765) |  Loss2: (0.0000) | Acc: (57.00%) (25196/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.1765) |  Loss2: (0.0000) | Acc: (57.00%) (25939/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.1761) |  Loss2: (0.0000) | Acc: (57.00%) (26670/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.1754) |  Loss2: (0.0000) | Acc: (57.00%) (27432/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.1752) |  Loss2: (0.0000) | Acc: (57.00%) (28173/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.1753) |  Loss2: (0.0000) | Acc: (57.00%) (28875/50000)
# TEST : Loss: (1.1513) | Acc: (57.00%) (5790/10000)
percent tensor([0.4862], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.5151], device='cuda:0')
percent tensor([0.5036], device='cuda:0')
percent tensor([0.5124], device='cuda:0')
percent tensor([0.4854], device='cuda:0')
percent tensor([0.4844], device='cuda:0')
percent tensor([0.4413], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.1213) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.1643) |  Loss2: (0.0000) | Acc: (57.00%) (804/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.1740) |  Loss2: (0.0000) | Acc: (57.00%) (1537/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.1672) |  Loss2: (0.0000) | Acc: (57.00%) (2279/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.1644) |  Loss2: (0.0000) | Acc: (57.00%) (3034/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.1633) |  Loss2: (0.0000) | Acc: (58.00%) (3794/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.1614) |  Loss2: (0.0000) | Acc: (58.00%) (4536/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.1595) |  Loss2: (0.0000) | Acc: (58.00%) (5274/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.1572) |  Loss2: (0.0000) | Acc: (58.00%) (6034/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.1588) |  Loss2: (0.0000) | Acc: (58.00%) (6777/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.1568) |  Loss2: (0.0000) | Acc: (58.00%) (7547/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.1527) |  Loss2: (0.0000) | Acc: (58.00%) (8318/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.1499) |  Loss2: (0.0000) | Acc: (58.00%) (9074/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.1498) |  Loss2: (0.0000) | Acc: (58.00%) (9827/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.1524) |  Loss2: (0.0000) | Acc: (58.00%) (10553/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.1514) |  Loss2: (0.0000) | Acc: (58.00%) (11315/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.1526) |  Loss2: (0.0000) | Acc: (58.00%) (12062/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.1528) |  Loss2: (0.0000) | Acc: (58.00%) (12816/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.1537) |  Loss2: (0.0000) | Acc: (58.00%) (13559/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.1525) |  Loss2: (0.0000) | Acc: (58.00%) (14306/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.1523) |  Loss2: (0.0000) | Acc: (58.00%) (15063/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.1533) |  Loss2: (0.0000) | Acc: (58.00%) (15796/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.1533) |  Loss2: (0.0000) | Acc: (58.00%) (16534/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.1544) |  Loss2: (0.0000) | Acc: (58.00%) (17274/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.1547) |  Loss2: (0.0000) | Acc: (58.00%) (18015/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.1573) |  Loss2: (0.0000) | Acc: (58.00%) (18740/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.1579) |  Loss2: (0.0000) | Acc: (58.00%) (19471/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.1569) |  Loss2: (0.0000) | Acc: (58.00%) (20212/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.1572) |  Loss2: (0.0000) | Acc: (58.00%) (20964/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.1572) |  Loss2: (0.0000) | Acc: (58.00%) (21690/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.1581) |  Loss2: (0.0000) | Acc: (58.00%) (22444/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.1599) |  Loss2: (0.0000) | Acc: (58.00%) (23164/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.1588) |  Loss2: (0.0000) | Acc: (58.00%) (23917/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.1585) |  Loss2: (0.0000) | Acc: (58.00%) (24675/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.1580) |  Loss2: (0.0000) | Acc: (58.00%) (25441/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.1585) |  Loss2: (0.0000) | Acc: (58.00%) (26190/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.1574) |  Loss2: (0.0000) | Acc: (58.00%) (26943/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.1574) |  Loss2: (0.0000) | Acc: (58.00%) (27671/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.1568) |  Loss2: (0.0000) | Acc: (58.00%) (28443/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.1558) |  Loss2: (0.0000) | Acc: (58.00%) (29177/50000)
# TEST : Loss: (1.1393) | Acc: (58.00%) (5853/10000)
percent tensor([0.4811], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.5108], device='cuda:0')
percent tensor([0.4990], device='cuda:0')
percent tensor([0.5120], device='cuda:0')
percent tensor([0.4795], device='cuda:0')
percent tensor([0.4779], device='cuda:0')
percent tensor([0.4137], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (0.9908) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.1476) |  Loss2: (0.0000) | Acc: (58.00%) (821/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.1565) |  Loss2: (0.0000) | Acc: (57.00%) (1558/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.1532) |  Loss2: (0.0000) | Acc: (58.00%) (2303/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.1461) |  Loss2: (0.0000) | Acc: (58.00%) (3083/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.1421) |  Loss2: (0.0000) | Acc: (59.00%) (3867/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.1420) |  Loss2: (0.0000) | Acc: (59.00%) (4624/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (59.00%) (5364/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.1366) |  Loss2: (0.0000) | Acc: (59.00%) (6124/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.1362) |  Loss2: (0.0000) | Acc: (59.00%) (6884/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.1373) |  Loss2: (0.0000) | Acc: (58.00%) (7624/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.1399) |  Loss2: (0.0000) | Acc: (58.00%) (8348/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.1406) |  Loss2: (0.0000) | Acc: (58.00%) (9092/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.1392) |  Loss2: (0.0000) | Acc: (58.00%) (9856/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (58.00%) (10591/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.1397) |  Loss2: (0.0000) | Acc: (58.00%) (11337/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.1417) |  Loss2: (0.0000) | Acc: (58.00%) (12101/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.1415) |  Loss2: (0.0000) | Acc: (58.00%) (12869/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.1408) |  Loss2: (0.0000) | Acc: (58.00%) (13638/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.1450) |  Loss2: (0.0000) | Acc: (58.00%) (14360/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.1467) |  Loss2: (0.0000) | Acc: (58.00%) (15107/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.1457) |  Loss2: (0.0000) | Acc: (58.00%) (15887/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.1461) |  Loss2: (0.0000) | Acc: (58.00%) (16622/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.1449) |  Loss2: (0.0000) | Acc: (58.00%) (17375/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.1452) |  Loss2: (0.0000) | Acc: (58.00%) (18131/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.1457) |  Loss2: (0.0000) | Acc: (58.00%) (18875/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.1436) |  Loss2: (0.0000) | Acc: (58.00%) (19635/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.1440) |  Loss2: (0.0000) | Acc: (58.00%) (20370/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.1438) |  Loss2: (0.0000) | Acc: (58.00%) (21120/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.1429) |  Loss2: (0.0000) | Acc: (58.00%) (21877/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.1443) |  Loss2: (0.0000) | Acc: (58.00%) (22627/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.1446) |  Loss2: (0.0000) | Acc: (58.00%) (23388/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.1458) |  Loss2: (0.0000) | Acc: (58.00%) (24125/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.1462) |  Loss2: (0.0000) | Acc: (58.00%) (24874/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.1468) |  Loss2: (0.0000) | Acc: (58.00%) (25618/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.1470) |  Loss2: (0.0000) | Acc: (58.00%) (26378/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.1465) |  Loss2: (0.0000) | Acc: (58.00%) (27145/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.1466) |  Loss2: (0.0000) | Acc: (58.00%) (27872/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.1457) |  Loss2: (0.0000) | Acc: (58.00%) (28636/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.1451) |  Loss2: (0.0000) | Acc: (58.00%) (29371/50000)
# TEST : Loss: (1.1318) | Acc: (58.00%) (5877/10000)
percent tensor([0.4786], device='cuda:0')
percent tensor([0.4959], device='cuda:0')
percent tensor([0.5035], device='cuda:0')
percent tensor([0.4951], device='cuda:0')
percent tensor([0.5109], device='cuda:0')
percent tensor([0.4754], device='cuda:0')
percent tensor([0.4722], device='cuda:0')
percent tensor([0.3905], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1807) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.1581) |  Loss2: (0.0000) | Acc: (58.00%) (821/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1365) |  Loss2: (0.0000) | Acc: (58.00%) (1571/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1401) |  Loss2: (0.0000) | Acc: (59.00%) (2342/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (59.00%) (3110/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1364) |  Loss2: (0.0000) | Acc: (59.00%) (3869/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1352) |  Loss2: (0.0000) | Acc: (59.00%) (4624/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1271) |  Loss2: (0.0000) | Acc: (59.00%) (5410/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1346) |  Loss2: (0.0000) | Acc: (59.00%) (6163/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1308) |  Loss2: (0.0000) | Acc: (59.00%) (6915/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1339) |  Loss2: (0.0000) | Acc: (59.00%) (7675/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1369) |  Loss2: (0.0000) | Acc: (59.00%) (8421/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1355) |  Loss2: (0.0000) | Acc: (59.00%) (9175/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1335) |  Loss2: (0.0000) | Acc: (59.00%) (9947/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1327) |  Loss2: (0.0000) | Acc: (59.00%) (10711/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1343) |  Loss2: (0.0000) | Acc: (59.00%) (11466/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1338) |  Loss2: (0.0000) | Acc: (59.00%) (12225/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1358) |  Loss2: (0.0000) | Acc: (59.00%) (12952/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1371) |  Loss2: (0.0000) | Acc: (59.00%) (13710/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1398) |  Loss2: (0.0000) | Acc: (59.00%) (14450/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1411) |  Loss2: (0.0000) | Acc: (59.00%) (15201/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1423) |  Loss2: (0.0000) | Acc: (59.00%) (15935/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1392) |  Loss2: (0.0000) | Acc: (59.00%) (16720/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1383) |  Loss2: (0.0000) | Acc: (59.00%) (17494/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1374) |  Loss2: (0.0000) | Acc: (59.00%) (18265/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1373) |  Loss2: (0.0000) | Acc: (59.00%) (19028/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (59.00%) (19791/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (59.00%) (20549/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1371) |  Loss2: (0.0000) | Acc: (59.00%) (21309/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1364) |  Loss2: (0.0000) | Acc: (59.00%) (22059/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1371) |  Loss2: (0.0000) | Acc: (59.00%) (22784/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (59.00%) (23527/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1382) |  Loss2: (0.0000) | Acc: (59.00%) (24270/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1373) |  Loss2: (0.0000) | Acc: (59.00%) (25040/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.1367) |  Loss2: (0.0000) | Acc: (59.00%) (25803/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.1364) |  Loss2: (0.0000) | Acc: (59.00%) (26575/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.1365) |  Loss2: (0.0000) | Acc: (59.00%) (27318/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.1355) |  Loss2: (0.0000) | Acc: (59.00%) (28090/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.1356) |  Loss2: (0.0000) | Acc: (59.00%) (28847/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.1352) |  Loss2: (0.0000) | Acc: (59.00%) (29570/50000)
# TEST : Loss: (1.1278) | Acc: (58.00%) (5886/10000)
percent tensor([0.4751], device='cuda:0')
percent tensor([0.4939], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5088], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4674], device='cuda:0')
percent tensor([0.3701], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (0.9972) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1073) |  Loss2: (0.0000) | Acc: (59.00%) (834/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.1393) |  Loss2: (0.0000) | Acc: (58.00%) (1575/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1360) |  Loss2: (0.0000) | Acc: (58.00%) (2332/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1355) |  Loss2: (0.0000) | Acc: (59.00%) (3105/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1434) |  Loss2: (0.0000) | Acc: (58.00%) (3846/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1397) |  Loss2: (0.0000) | Acc: (59.00%) (4626/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1343) |  Loss2: (0.0000) | Acc: (59.00%) (5402/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1319) |  Loss2: (0.0000) | Acc: (59.00%) (6170/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1311) |  Loss2: (0.0000) | Acc: (59.00%) (6935/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1324) |  Loss2: (0.0000) | Acc: (59.00%) (7678/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1303) |  Loss2: (0.0000) | Acc: (59.00%) (8435/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1284) |  Loss2: (0.0000) | Acc: (59.00%) (9222/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1285) |  Loss2: (0.0000) | Acc: (59.00%) (9992/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1237) |  Loss2: (0.0000) | Acc: (59.00%) (10778/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1219) |  Loss2: (0.0000) | Acc: (59.00%) (11569/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1186) |  Loss2: (0.0000) | Acc: (59.00%) (12354/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1177) |  Loss2: (0.0000) | Acc: (59.00%) (13130/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1184) |  Loss2: (0.0000) | Acc: (59.00%) (13883/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1169) |  Loss2: (0.0000) | Acc: (59.00%) (14666/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1153) |  Loss2: (0.0000) | Acc: (60.00%) (15452/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1146) |  Loss2: (0.0000) | Acc: (60.00%) (16211/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1142) |  Loss2: (0.0000) | Acc: (60.00%) (16980/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1136) |  Loss2: (0.0000) | Acc: (60.00%) (17742/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1132) |  Loss2: (0.0000) | Acc: (60.00%) (18520/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1130) |  Loss2: (0.0000) | Acc: (60.00%) (19302/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1105) |  Loss2: (0.0000) | Acc: (60.00%) (20103/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1085) |  Loss2: (0.0000) | Acc: (60.00%) (20905/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1096) |  Loss2: (0.0000) | Acc: (60.00%) (21646/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1098) |  Loss2: (0.0000) | Acc: (60.00%) (22414/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1085) |  Loss2: (0.0000) | Acc: (60.00%) (23203/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1063) |  Loss2: (0.0000) | Acc: (60.00%) (24006/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1060) |  Loss2: (0.0000) | Acc: (60.00%) (24790/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1060) |  Loss2: (0.0000) | Acc: (60.00%) (25568/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1055) |  Loss2: (0.0000) | Acc: (60.00%) (26360/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1041) |  Loss2: (0.0000) | Acc: (60.00%) (27158/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1032) |  Loss2: (0.0000) | Acc: (60.00%) (27947/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1019) |  Loss2: (0.0000) | Acc: (60.00%) (28729/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.0993) |  Loss2: (0.0000) | Acc: (60.00%) (29541/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.0981) |  Loss2: (0.0000) | Acc: (60.00%) (30320/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_010.pth.tar'
# TEST : Loss: (1.0744) | Acc: (60.00%) (6042/10000)
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4719], device='cuda:0')
percent tensor([0.4673], device='cuda:0')
percent tensor([0.3697], device='cuda:0')
Epoch: 11 | Batch_idx: 0 |  Loss: (1.0801) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0785) |  Loss2: (0.0000) | Acc: (62.00%) (874/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0751) |  Loss2: (0.0000) | Acc: (61.00%) (1658/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (62.00%) (2480/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0666) |  Loss2: (0.0000) | Acc: (61.00%) (3252/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0590) |  Loss2: (0.0000) | Acc: (62.00%) (4069/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0471) |  Loss2: (0.0000) | Acc: (62.00%) (4911/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0445) |  Loss2: (0.0000) | Acc: (63.00%) (5727/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0429) |  Loss2: (0.0000) | Acc: (62.00%) (6524/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0393) |  Loss2: (0.0000) | Acc: (62.00%) (7310/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0364) |  Loss2: (0.0000) | Acc: (62.00%) (8133/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0385) |  Loss2: (0.0000) | Acc: (62.00%) (8920/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0353) |  Loss2: (0.0000) | Acc: (63.00%) (9758/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0320) |  Loss2: (0.0000) | Acc: (63.00%) (10588/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0313) |  Loss2: (0.0000) | Acc: (63.00%) (11416/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0300) |  Loss2: (0.0000) | Acc: (63.00%) (12223/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0313) |  Loss2: (0.0000) | Acc: (63.00%) (12998/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0310) |  Loss2: (0.0000) | Acc: (63.00%) (13819/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0281) |  Loss2: (0.0000) | Acc: (63.00%) (14667/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0261) |  Loss2: (0.0000) | Acc: (63.00%) (15500/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0255) |  Loss2: (0.0000) | Acc: (63.00%) (16301/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0237) |  Loss2: (0.0000) | Acc: (63.00%) (17143/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0218) |  Loss2: (0.0000) | Acc: (63.00%) (17948/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0232) |  Loss2: (0.0000) | Acc: (63.00%) (18726/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0227) |  Loss2: (0.0000) | Acc: (63.00%) (19534/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0219) |  Loss2: (0.0000) | Acc: (63.00%) (20360/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0208) |  Loss2: (0.0000) | Acc: (63.00%) (21171/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0195) |  Loss2: (0.0000) | Acc: (63.00%) (22002/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0190) |  Loss2: (0.0000) | Acc: (63.00%) (22839/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0183) |  Loss2: (0.0000) | Acc: (63.00%) (23671/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0179) |  Loss2: (0.0000) | Acc: (63.00%) (24495/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0168) |  Loss2: (0.0000) | Acc: (63.00%) (25314/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0167) |  Loss2: (0.0000) | Acc: (63.00%) (26120/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0165) |  Loss2: (0.0000) | Acc: (63.00%) (26946/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0149) |  Loss2: (0.0000) | Acc: (63.00%) (27793/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0151) |  Loss2: (0.0000) | Acc: (63.00%) (28590/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0136) |  Loss2: (0.0000) | Acc: (63.00%) (29417/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0126) |  Loss2: (0.0000) | Acc: (63.00%) (30244/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0112) |  Loss2: (0.0000) | Acc: (63.00%) (31091/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0101) |  Loss2: (0.0000) | Acc: (63.00%) (31895/50000)
# TEST : Loss: (0.9712) | Acc: (65.00%) (6518/10000)
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4673], device='cuda:0')
percent tensor([0.3698], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (1.2937) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (1.0008) |  Loss2: (0.0000) | Acc: (65.00%) (917/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9906) |  Loss2: (0.0000) | Acc: (65.00%) (1762/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9880) |  Loss2: (0.0000) | Acc: (65.00%) (2589/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9751) |  Loss2: (0.0000) | Acc: (65.00%) (3442/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9717) |  Loss2: (0.0000) | Acc: (65.00%) (4284/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9608) |  Loss2: (0.0000) | Acc: (65.00%) (5153/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9578) |  Loss2: (0.0000) | Acc: (66.00%) (6019/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9597) |  Loss2: (0.0000) | Acc: (66.00%) (6843/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (7682/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9621) |  Loss2: (0.0000) | Acc: (65.00%) (8506/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9620) |  Loss2: (0.0000) | Acc: (65.00%) (9345/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9606) |  Loss2: (0.0000) | Acc: (65.00%) (10199/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9621) |  Loss2: (0.0000) | Acc: (65.00%) (11031/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9587) |  Loss2: (0.0000) | Acc: (65.00%) (11911/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (12745/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9627) |  Loss2: (0.0000) | Acc: (65.00%) (13598/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9643) |  Loss2: (0.0000) | Acc: (65.00%) (14439/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9608) |  Loss2: (0.0000) | Acc: (66.00%) (15306/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9609) |  Loss2: (0.0000) | Acc: (66.00%) (16157/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9600) |  Loss2: (0.0000) | Acc: (66.00%) (17022/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9585) |  Loss2: (0.0000) | Acc: (66.00%) (17871/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9547) |  Loss2: (0.0000) | Acc: (66.00%) (18758/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9516) |  Loss2: (0.0000) | Acc: (66.00%) (19624/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9521) |  Loss2: (0.0000) | Acc: (66.00%) (20457/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9507) |  Loss2: (0.0000) | Acc: (66.00%) (21322/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9502) |  Loss2: (0.0000) | Acc: (66.00%) (22179/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9508) |  Loss2: (0.0000) | Acc: (66.00%) (23010/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9521) |  Loss2: (0.0000) | Acc: (66.00%) (23816/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9516) |  Loss2: (0.0000) | Acc: (66.00%) (24667/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9518) |  Loss2: (0.0000) | Acc: (66.00%) (25510/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9517) |  Loss2: (0.0000) | Acc: (66.00%) (26358/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9519) |  Loss2: (0.0000) | Acc: (66.00%) (27213/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9503) |  Loss2: (0.0000) | Acc: (66.00%) (28088/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9504) |  Loss2: (0.0000) | Acc: (66.00%) (28925/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9491) |  Loss2: (0.0000) | Acc: (66.00%) (29794/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9487) |  Loss2: (0.0000) | Acc: (66.00%) (30649/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9481) |  Loss2: (0.0000) | Acc: (66.00%) (31513/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9476) |  Loss2: (0.0000) | Acc: (66.00%) (32383/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9463) |  Loss2: (0.0000) | Acc: (66.00%) (33227/50000)
# TEST : Loss: (1.1242) | Acc: (60.00%) (6034/10000)
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4673], device='cuda:0')
percent tensor([0.3698], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (1.0208) |  Loss2: (0.0000) | Acc: (66.00%) (85/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9366) |  Loss2: (0.0000) | Acc: (66.00%) (935/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9249) |  Loss2: (0.0000) | Acc: (67.00%) (1810/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.9244) |  Loss2: (0.0000) | Acc: (66.00%) (2655/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (3525/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.9195) |  Loss2: (0.0000) | Acc: (67.00%) (4393/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.9177) |  Loss2: (0.0000) | Acc: (67.00%) (5262/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.9214) |  Loss2: (0.0000) | Acc: (67.00%) (6108/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.9149) |  Loss2: (0.0000) | Acc: (67.00%) (6989/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.9123) |  Loss2: (0.0000) | Acc: (67.00%) (7878/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.9157) |  Loss2: (0.0000) | Acc: (67.00%) (8724/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.9120) |  Loss2: (0.0000) | Acc: (67.00%) (9608/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.9092) |  Loss2: (0.0000) | Acc: (67.00%) (10490/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.9042) |  Loss2: (0.0000) | Acc: (67.00%) (11390/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.9009) |  Loss2: (0.0000) | Acc: (67.00%) (12272/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.9026) |  Loss2: (0.0000) | Acc: (67.00%) (13135/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.9030) |  Loss2: (0.0000) | Acc: (68.00%) (14014/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.9021) |  Loss2: (0.0000) | Acc: (67.00%) (14876/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.9019) |  Loss2: (0.0000) | Acc: (67.00%) (15753/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.9013) |  Loss2: (0.0000) | Acc: (67.00%) (16622/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.9014) |  Loss2: (0.0000) | Acc: (67.00%) (17485/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.9012) |  Loss2: (0.0000) | Acc: (67.00%) (18362/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8983) |  Loss2: (0.0000) | Acc: (68.00%) (19268/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8964) |  Loss2: (0.0000) | Acc: (68.00%) (20150/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8959) |  Loss2: (0.0000) | Acc: (68.00%) (21033/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8936) |  Loss2: (0.0000) | Acc: (68.00%) (21943/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (22796/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8942) |  Loss2: (0.0000) | Acc: (68.00%) (23662/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8936) |  Loss2: (0.0000) | Acc: (68.00%) (24560/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8928) |  Loss2: (0.0000) | Acc: (68.00%) (25424/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8923) |  Loss2: (0.0000) | Acc: (68.00%) (26308/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (27189/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8930) |  Loss2: (0.0000) | Acc: (68.00%) (28044/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (28921/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8913) |  Loss2: (0.0000) | Acc: (68.00%) (29799/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8901) |  Loss2: (0.0000) | Acc: (68.00%) (30700/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8899) |  Loss2: (0.0000) | Acc: (68.00%) (31579/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (32458/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8901) |  Loss2: (0.0000) | Acc: (68.00%) (33337/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (34160/50000)
# TEST : Loss: (0.9698) | Acc: (64.00%) (6497/10000)
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4673], device='cuda:0')
percent tensor([0.3699], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.9304) |  Loss2: (0.0000) | Acc: (64.00%) (82/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8466) |  Loss2: (0.0000) | Acc: (70.00%) (989/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8442) |  Loss2: (0.0000) | Acc: (70.00%) (1894/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8327) |  Loss2: (0.0000) | Acc: (71.00%) (2821/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8385) |  Loss2: (0.0000) | Acc: (70.00%) (3710/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8458) |  Loss2: (0.0000) | Acc: (70.00%) (4615/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8474) |  Loss2: (0.0000) | Acc: (70.00%) (5502/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8476) |  Loss2: (0.0000) | Acc: (70.00%) (6393/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8413) |  Loss2: (0.0000) | Acc: (70.00%) (7303/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8427) |  Loss2: (0.0000) | Acc: (70.00%) (8192/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8469) |  Loss2: (0.0000) | Acc: (70.00%) (9079/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8490) |  Loss2: (0.0000) | Acc: (70.00%) (9965/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8533) |  Loss2: (0.0000) | Acc: (69.00%) (10834/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8528) |  Loss2: (0.0000) | Acc: (69.00%) (11729/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8541) |  Loss2: (0.0000) | Acc: (69.00%) (12622/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8530) |  Loss2: (0.0000) | Acc: (69.00%) (13518/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8527) |  Loss2: (0.0000) | Acc: (69.00%) (14414/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (70.00%) (15327/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8509) |  Loss2: (0.0000) | Acc: (70.00%) (16240/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (70.00%) (17148/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8501) |  Loss2: (0.0000) | Acc: (70.00%) (18030/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8493) |  Loss2: (0.0000) | Acc: (70.00%) (18940/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8495) |  Loss2: (0.0000) | Acc: (70.00%) (19837/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8498) |  Loss2: (0.0000) | Acc: (70.00%) (20718/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8493) |  Loss2: (0.0000) | Acc: (70.00%) (21614/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8482) |  Loss2: (0.0000) | Acc: (70.00%) (22524/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8477) |  Loss2: (0.0000) | Acc: (70.00%) (23436/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8481) |  Loss2: (0.0000) | Acc: (70.00%) (24307/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8480) |  Loss2: (0.0000) | Acc: (70.00%) (25210/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8469) |  Loss2: (0.0000) | Acc: (70.00%) (26126/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8452) |  Loss2: (0.0000) | Acc: (70.00%) (27039/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8452) |  Loss2: (0.0000) | Acc: (70.00%) (27944/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8449) |  Loss2: (0.0000) | Acc: (70.00%) (28842/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8452) |  Loss2: (0.0000) | Acc: (70.00%) (29724/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8450) |  Loss2: (0.0000) | Acc: (70.00%) (30629/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8439) |  Loss2: (0.0000) | Acc: (70.00%) (31542/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8426) |  Loss2: (0.0000) | Acc: (70.00%) (32480/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8428) |  Loss2: (0.0000) | Acc: (70.00%) (33380/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8422) |  Loss2: (0.0000) | Acc: (70.00%) (34292/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8412) |  Loss2: (0.0000) | Acc: (70.00%) (35191/50000)
# TEST : Loss: (1.1507) | Acc: (60.00%) (6049/10000)
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4910], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4673], device='cuda:0')
percent tensor([0.3699], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.9785) |  Loss2: (0.0000) | Acc: (64.00%) (83/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.9725) |  Loss2: (0.0000) | Acc: (66.00%) (932/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (1.0907) |  Loss2: (0.0000) | Acc: (62.00%) (1675/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (1.1643) |  Loss2: (0.0000) | Acc: (59.00%) (2378/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (1.1990) |  Loss2: (0.0000) | Acc: (58.00%) (3089/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (1.1963) |  Loss2: (0.0000) | Acc: (58.00%) (3835/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (1.1912) |  Loss2: (0.0000) | Acc: (58.00%) (4593/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (1.1758) |  Loss2: (0.0000) | Acc: (59.00%) (5370/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (1.1753) |  Loss2: (0.0000) | Acc: (59.00%) (6122/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (1.1673) |  Loss2: (0.0000) | Acc: (59.00%) (6904/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (1.1605) |  Loss2: (0.0000) | Acc: (59.00%) (7690/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (1.1535) |  Loss2: (0.0000) | Acc: (59.00%) (8482/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (1.1486) |  Loss2: (0.0000) | Acc: (59.00%) (9274/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (1.1448) |  Loss2: (0.0000) | Acc: (60.00%) (10061/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (1.1400) |  Loss2: (0.0000) | Acc: (60.00%) (10854/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (1.1330) |  Loss2: (0.0000) | Acc: (60.00%) (11676/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (1.1267) |  Loss2: (0.0000) | Acc: (60.00%) (12494/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (1.1225) |  Loss2: (0.0000) | Acc: (60.00%) (13283/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (1.1164) |  Loss2: (0.0000) | Acc: (60.00%) (14093/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (1.1124) |  Loss2: (0.0000) | Acc: (60.00%) (14903/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (1.1067) |  Loss2: (0.0000) | Acc: (61.00%) (15722/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (1.0967) |  Loss2: (0.0000) | Acc: (61.00%) (16596/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (1.0919) |  Loss2: (0.0000) | Acc: (61.00%) (17426/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (1.0873) |  Loss2: (0.0000) | Acc: (61.00%) (18257/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (1.0834) |  Loss2: (0.0000) | Acc: (61.00%) (19099/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (1.0798) |  Loss2: (0.0000) | Acc: (62.00%) (19932/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (1.0754) |  Loss2: (0.0000) | Acc: (62.00%) (20769/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (1.0712) |  Loss2: (0.0000) | Acc: (62.00%) (21608/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (1.0685) |  Loss2: (0.0000) | Acc: (62.00%) (22432/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (1.0642) |  Loss2: (0.0000) | Acc: (62.00%) (23266/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (1.0611) |  Loss2: (0.0000) | Acc: (62.00%) (24099/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (1.0577) |  Loss2: (0.0000) | Acc: (62.00%) (24949/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (1.0546) |  Loss2: (0.0000) | Acc: (62.00%) (25790/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (1.0508) |  Loss2: (0.0000) | Acc: (62.00%) (26639/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (1.0480) |  Loss2: (0.0000) | Acc: (62.00%) (27478/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (1.0456) |  Loss2: (0.0000) | Acc: (63.00%) (28332/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (1.0415) |  Loss2: (0.0000) | Acc: (63.00%) (29216/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (1.0400) |  Loss2: (0.0000) | Acc: (63.00%) (30061/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (1.0384) |  Loss2: (0.0000) | Acc: (63.00%) (30893/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (1.0360) |  Loss2: (0.0000) | Acc: (63.00%) (31710/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_015.pth.tar'
# TEST : Loss: (0.9699) | Acc: (65.00%) (6577/10000)
percent tensor([0.4588], device='cuda:0')
percent tensor([0.5168], device='cuda:0')
percent tensor([0.4950], device='cuda:0')
percent tensor([0.5154], device='cuda:0')
percent tensor([0.5073], device='cuda:0')
percent tensor([0.4827], device='cuda:0')
percent tensor([0.4761], device='cuda:0')
percent tensor([0.3756], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (1.0845) |  Loss2: (0.0000) | Acc: (59.00%) (76/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.9456) |  Loss2: (0.0000) | Acc: (66.00%) (943/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.9404) |  Loss2: (0.0000) | Acc: (66.00%) (1796/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.9405) |  Loss2: (0.0000) | Acc: (67.00%) (2666/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.9236) |  Loss2: (0.0000) | Acc: (67.00%) (3547/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.9267) |  Loss2: (0.0000) | Acc: (67.00%) (4403/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.9338) |  Loss2: (0.0000) | Acc: (67.00%) (5261/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.9344) |  Loss2: (0.0000) | Acc: (67.00%) (6124/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.9336) |  Loss2: (0.0000) | Acc: (67.00%) (6973/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.9326) |  Loss2: (0.0000) | Acc: (67.00%) (7835/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.9297) |  Loss2: (0.0000) | Acc: (67.00%) (8711/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.9305) |  Loss2: (0.0000) | Acc: (67.00%) (9557/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.9319) |  Loss2: (0.0000) | Acc: (67.00%) (10413/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.9351) |  Loss2: (0.0000) | Acc: (66.00%) (11225/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.9312) |  Loss2: (0.0000) | Acc: (67.00%) (12111/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.9270) |  Loss2: (0.0000) | Acc: (67.00%) (13001/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.9254) |  Loss2: (0.0000) | Acc: (67.00%) (13857/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.9249) |  Loss2: (0.0000) | Acc: (67.00%) (14713/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.9245) |  Loss2: (0.0000) | Acc: (67.00%) (15554/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.9233) |  Loss2: (0.0000) | Acc: (67.00%) (16416/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.9238) |  Loss2: (0.0000) | Acc: (67.00%) (17264/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.9219) |  Loss2: (0.0000) | Acc: (67.00%) (18127/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (18995/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.9232) |  Loss2: (0.0000) | Acc: (67.00%) (19833/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.9233) |  Loss2: (0.0000) | Acc: (67.00%) (20694/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.9226) |  Loss2: (0.0000) | Acc: (67.00%) (21546/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.9218) |  Loss2: (0.0000) | Acc: (67.00%) (22420/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.9231) |  Loss2: (0.0000) | Acc: (67.00%) (23263/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.9228) |  Loss2: (0.0000) | Acc: (67.00%) (24133/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.9222) |  Loss2: (0.0000) | Acc: (67.00%) (24981/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.9221) |  Loss2: (0.0000) | Acc: (67.00%) (25849/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.9218) |  Loss2: (0.0000) | Acc: (67.00%) (26721/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.9235) |  Loss2: (0.0000) | Acc: (67.00%) (27555/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.9232) |  Loss2: (0.0000) | Acc: (67.00%) (28432/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.9213) |  Loss2: (0.0000) | Acc: (67.00%) (29318/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.9212) |  Loss2: (0.0000) | Acc: (67.00%) (30175/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.9199) |  Loss2: (0.0000) | Acc: (67.00%) (31039/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.9199) |  Loss2: (0.0000) | Acc: (67.00%) (31925/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.9186) |  Loss2: (0.0000) | Acc: (67.00%) (32821/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.9171) |  Loss2: (0.0000) | Acc: (67.00%) (33663/50000)
# TEST : Loss: (0.9099) | Acc: (67.00%) (6742/10000)
percent tensor([0.4584], device='cuda:0')
percent tensor([0.5136], device='cuda:0')
percent tensor([0.5057], device='cuda:0')
percent tensor([0.5182], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.4835], device='cuda:0')
percent tensor([0.4726], device='cuda:0')
percent tensor([0.3622], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.8594) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.9199) |  Loss2: (0.0000) | Acc: (68.00%) (959/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.9086) |  Loss2: (0.0000) | Acc: (68.00%) (1842/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8811) |  Loss2: (0.0000) | Acc: (69.00%) (2752/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8797) |  Loss2: (0.0000) | Acc: (69.00%) (3640/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8719) |  Loss2: (0.0000) | Acc: (69.00%) (4521/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8769) |  Loss2: (0.0000) | Acc: (69.00%) (5390/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8814) |  Loss2: (0.0000) | Acc: (68.00%) (6249/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8821) |  Loss2: (0.0000) | Acc: (68.00%) (7136/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8852) |  Loss2: (0.0000) | Acc: (68.00%) (8012/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8897) |  Loss2: (0.0000) | Acc: (68.00%) (8861/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8914) |  Loss2: (0.0000) | Acc: (68.00%) (9724/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8948) |  Loss2: (0.0000) | Acc: (68.00%) (10564/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8909) |  Loss2: (0.0000) | Acc: (68.00%) (11465/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8953) |  Loss2: (0.0000) | Acc: (68.00%) (12310/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8935) |  Loss2: (0.0000) | Acc: (68.00%) (13218/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8913) |  Loss2: (0.0000) | Acc: (68.00%) (14101/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8887) |  Loss2: (0.0000) | Acc: (68.00%) (14996/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8882) |  Loss2: (0.0000) | Acc: (68.00%) (15888/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8893) |  Loss2: (0.0000) | Acc: (68.00%) (16748/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8901) |  Loss2: (0.0000) | Acc: (68.00%) (17609/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8918) |  Loss2: (0.0000) | Acc: (68.00%) (18466/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (19326/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8938) |  Loss2: (0.0000) | Acc: (68.00%) (20160/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (21069/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8907) |  Loss2: (0.0000) | Acc: (68.00%) (21945/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (22808/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8896) |  Loss2: (0.0000) | Acc: (68.00%) (23694/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8884) |  Loss2: (0.0000) | Acc: (68.00%) (24593/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8873) |  Loss2: (0.0000) | Acc: (68.00%) (25484/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8854) |  Loss2: (0.0000) | Acc: (68.00%) (26391/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8863) |  Loss2: (0.0000) | Acc: (68.00%) (27272/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8863) |  Loss2: (0.0000) | Acc: (68.00%) (28138/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8871) |  Loss2: (0.0000) | Acc: (68.00%) (29006/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8860) |  Loss2: (0.0000) | Acc: (68.00%) (29900/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8867) |  Loss2: (0.0000) | Acc: (68.00%) (30751/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8869) |  Loss2: (0.0000) | Acc: (68.00%) (31638/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8860) |  Loss2: (0.0000) | Acc: (68.00%) (32529/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8864) |  Loss2: (0.0000) | Acc: (68.00%) (33391/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8861) |  Loss2: (0.0000) | Acc: (68.00%) (34232/50000)
# TEST : Loss: (0.8828) | Acc: (68.00%) (6866/10000)
percent tensor([0.4610], device='cuda:0')
percent tensor([0.5102], device='cuda:0')
percent tensor([0.5124], device='cuda:0')
percent tensor([0.5170], device='cuda:0')
percent tensor([0.5017], device='cuda:0')
percent tensor([0.4817], device='cuda:0')
percent tensor([0.4671], device='cuda:0')
percent tensor([0.3464], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.9203) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.8581) |  Loss2: (0.0000) | Acc: (68.00%) (971/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.8567) |  Loss2: (0.0000) | Acc: (69.00%) (1865/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.8602) |  Loss2: (0.0000) | Acc: (69.00%) (2768/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.8640) |  Loss2: (0.0000) | Acc: (69.00%) (3651/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.8552) |  Loss2: (0.0000) | Acc: (69.00%) (4551/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8630) |  Loss2: (0.0000) | Acc: (69.00%) (5425/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8634) |  Loss2: (0.0000) | Acc: (69.00%) (6321/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8653) |  Loss2: (0.0000) | Acc: (69.00%) (7193/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8659) |  Loss2: (0.0000) | Acc: (69.00%) (8084/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8658) |  Loss2: (0.0000) | Acc: (69.00%) (8954/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8684) |  Loss2: (0.0000) | Acc: (69.00%) (9813/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8697) |  Loss2: (0.0000) | Acc: (68.00%) (10675/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8728) |  Loss2: (0.0000) | Acc: (68.00%) (11546/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8739) |  Loss2: (0.0000) | Acc: (68.00%) (12412/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8718) |  Loss2: (0.0000) | Acc: (68.00%) (13309/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8727) |  Loss2: (0.0000) | Acc: (68.00%) (14167/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8734) |  Loss2: (0.0000) | Acc: (68.00%) (15053/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8743) |  Loss2: (0.0000) | Acc: (68.00%) (15916/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8740) |  Loss2: (0.0000) | Acc: (68.00%) (16797/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8705) |  Loss2: (0.0000) | Acc: (68.00%) (17729/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8707) |  Loss2: (0.0000) | Acc: (68.00%) (18615/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8722) |  Loss2: (0.0000) | Acc: (68.00%) (19485/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8703) |  Loss2: (0.0000) | Acc: (68.00%) (20373/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8709) |  Loss2: (0.0000) | Acc: (68.00%) (21247/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8695) |  Loss2: (0.0000) | Acc: (68.00%) (22157/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8701) |  Loss2: (0.0000) | Acc: (68.00%) (23043/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.8699) |  Loss2: (0.0000) | Acc: (68.00%) (23925/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8691) |  Loss2: (0.0000) | Acc: (69.00%) (24822/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.8672) |  Loss2: (0.0000) | Acc: (69.00%) (25719/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.8669) |  Loss2: (0.0000) | Acc: (69.00%) (26591/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.8664) |  Loss2: (0.0000) | Acc: (69.00%) (27495/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.8665) |  Loss2: (0.0000) | Acc: (69.00%) (28376/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.8672) |  Loss2: (0.0000) | Acc: (69.00%) (29235/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.8656) |  Loss2: (0.0000) | Acc: (69.00%) (30159/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.8654) |  Loss2: (0.0000) | Acc: (69.00%) (31056/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.8656) |  Loss2: (0.0000) | Acc: (69.00%) (31952/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.8677) |  Loss2: (0.0000) | Acc: (69.00%) (32806/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.8682) |  Loss2: (0.0000) | Acc: (69.00%) (33683/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.8689) |  Loss2: (0.0000) | Acc: (69.00%) (34528/50000)
# TEST : Loss: (0.8718) | Acc: (69.00%) (6906/10000)
percent tensor([0.4604], device='cuda:0')
percent tensor([0.5071], device='cuda:0')
percent tensor([0.5147], device='cuda:0')
percent tensor([0.5140], device='cuda:0')
percent tensor([0.4995], device='cuda:0')
percent tensor([0.4792], device='cuda:0')
percent tensor([0.4609], device='cuda:0')
percent tensor([0.3310], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.7645) |  Loss2: (0.0000) | Acc: (71.00%) (92/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.8609) |  Loss2: (0.0000) | Acc: (70.00%) (997/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.8540) |  Loss2: (0.0000) | Acc: (70.00%) (1889/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.8434) |  Loss2: (0.0000) | Acc: (70.00%) (2786/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.8483) |  Loss2: (0.0000) | Acc: (69.00%) (3662/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (69.00%) (4548/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (5430/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.8632) |  Loss2: (0.0000) | Acc: (69.00%) (6304/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.8542) |  Loss2: (0.0000) | Acc: (69.00%) (7219/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.8579) |  Loss2: (0.0000) | Acc: (69.00%) (8074/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.8564) |  Loss2: (0.0000) | Acc: (69.00%) (8978/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.8515) |  Loss2: (0.0000) | Acc: (69.00%) (9901/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.8538) |  Loss2: (0.0000) | Acc: (69.00%) (10775/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.8547) |  Loss2: (0.0000) | Acc: (69.00%) (11652/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.8564) |  Loss2: (0.0000) | Acc: (69.00%) (12539/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.8548) |  Loss2: (0.0000) | Acc: (69.00%) (13438/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.8546) |  Loss2: (0.0000) | Acc: (69.00%) (14330/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (15211/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.8566) |  Loss2: (0.0000) | Acc: (69.00%) (16073/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.8562) |  Loss2: (0.0000) | Acc: (69.00%) (16981/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.8580) |  Loss2: (0.0000) | Acc: (69.00%) (17847/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.8595) |  Loss2: (0.0000) | Acc: (69.00%) (18708/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.8613) |  Loss2: (0.0000) | Acc: (69.00%) (19583/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.8619) |  Loss2: (0.0000) | Acc: (69.00%) (20457/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.8617) |  Loss2: (0.0000) | Acc: (69.00%) (21342/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.8599) |  Loss2: (0.0000) | Acc: (69.00%) (22252/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.8580) |  Loss2: (0.0000) | Acc: (69.00%) (23164/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.8586) |  Loss2: (0.0000) | Acc: (69.00%) (24048/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.8568) |  Loss2: (0.0000) | Acc: (69.00%) (24955/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.8564) |  Loss2: (0.0000) | Acc: (69.00%) (25857/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.8561) |  Loss2: (0.0000) | Acc: (69.00%) (26744/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.8553) |  Loss2: (0.0000) | Acc: (69.00%) (27649/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8549) |  Loss2: (0.0000) | Acc: (69.00%) (28560/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.8547) |  Loss2: (0.0000) | Acc: (69.00%) (29452/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.8550) |  Loss2: (0.0000) | Acc: (69.00%) (30342/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.8548) |  Loss2: (0.0000) | Acc: (69.00%) (31231/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8557) |  Loss2: (0.0000) | Acc: (69.00%) (32098/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.8564) |  Loss2: (0.0000) | Acc: (69.00%) (32981/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.8579) |  Loss2: (0.0000) | Acc: (69.00%) (33837/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.8585) |  Loss2: (0.0000) | Acc: (69.00%) (34680/50000)
# TEST : Loss: (0.8646) | Acc: (69.00%) (6926/10000)
percent tensor([0.4585], device='cuda:0')
percent tensor([0.5049], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5115], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4772], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3172], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.7994) |  Loss2: (0.0000) | Acc: (68.00%) (88/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8155) |  Loss2: (0.0000) | Acc: (72.00%) (1021/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8386) |  Loss2: (0.0000) | Acc: (71.00%) (1912/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8472) |  Loss2: (0.0000) | Acc: (70.00%) (2801/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8523) |  Loss2: (0.0000) | Acc: (70.00%) (3693/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.8467) |  Loss2: (0.0000) | Acc: (70.00%) (4607/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.8430) |  Loss2: (0.0000) | Acc: (70.00%) (5523/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.8360) |  Loss2: (0.0000) | Acc: (70.00%) (6427/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.8438) |  Loss2: (0.0000) | Acc: (70.00%) (7291/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.8447) |  Loss2: (0.0000) | Acc: (70.00%) (8186/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.8393) |  Loss2: (0.0000) | Acc: (70.00%) (9099/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.8427) |  Loss2: (0.0000) | Acc: (70.00%) (9974/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.8389) |  Loss2: (0.0000) | Acc: (70.00%) (10903/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.8399) |  Loss2: (0.0000) | Acc: (70.00%) (11793/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.8400) |  Loss2: (0.0000) | Acc: (70.00%) (12676/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.8416) |  Loss2: (0.0000) | Acc: (70.00%) (13572/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.8403) |  Loss2: (0.0000) | Acc: (70.00%) (14478/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.8400) |  Loss2: (0.0000) | Acc: (70.00%) (15361/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.8361) |  Loss2: (0.0000) | Acc: (70.00%) (16302/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.8365) |  Loss2: (0.0000) | Acc: (70.00%) (17191/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.8363) |  Loss2: (0.0000) | Acc: (70.00%) (18076/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.8352) |  Loss2: (0.0000) | Acc: (70.00%) (18992/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.8337) |  Loss2: (0.0000) | Acc: (70.00%) (19902/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.8359) |  Loss2: (0.0000) | Acc: (70.00%) (20780/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.8346) |  Loss2: (0.0000) | Acc: (70.00%) (21690/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.8357) |  Loss2: (0.0000) | Acc: (70.00%) (22572/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.8339) |  Loss2: (0.0000) | Acc: (70.00%) (23509/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.8319) |  Loss2: (0.0000) | Acc: (70.00%) (24423/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.8307) |  Loss2: (0.0000) | Acc: (70.00%) (25337/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.8316) |  Loss2: (0.0000) | Acc: (70.00%) (26229/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.8299) |  Loss2: (0.0000) | Acc: (70.00%) (27154/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.8282) |  Loss2: (0.0000) | Acc: (70.00%) (28088/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.8278) |  Loss2: (0.0000) | Acc: (70.00%) (29003/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.8266) |  Loss2: (0.0000) | Acc: (70.00%) (29918/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.8247) |  Loss2: (0.0000) | Acc: (70.00%) (30848/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.8247) |  Loss2: (0.0000) | Acc: (70.00%) (31739/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.8234) |  Loss2: (0.0000) | Acc: (70.00%) (32660/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.8230) |  Loss2: (0.0000) | Acc: (70.00%) (33582/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.8222) |  Loss2: (0.0000) | Acc: (70.00%) (34495/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.8230) |  Loss2: (0.0000) | Acc: (70.00%) (35373/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_020.pth.tar'
# TEST : Loss: (0.9187) | Acc: (67.00%) (6796/10000)
percent tensor([0.4581], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5113], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4773], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3172], device='cuda:0')
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.8180) |  Loss2: (0.0000) | Acc: (70.00%) (996/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.8030) |  Loss2: (0.0000) | Acc: (71.00%) (1915/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7937) |  Loss2: (0.0000) | Acc: (71.00%) (2848/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.8070) |  Loss2: (0.0000) | Acc: (71.00%) (3743/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (4665/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7938) |  Loss2: (0.0000) | Acc: (71.00%) (5595/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7881) |  Loss2: (0.0000) | Acc: (72.00%) (6547/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7899) |  Loss2: (0.0000) | Acc: (71.00%) (7450/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7864) |  Loss2: (0.0000) | Acc: (71.00%) (8383/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7851) |  Loss2: (0.0000) | Acc: (72.00%) (9334/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7793) |  Loss2: (0.0000) | Acc: (72.00%) (10289/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7748) |  Loss2: (0.0000) | Acc: (72.00%) (11243/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (12183/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7711) |  Loss2: (0.0000) | Acc: (72.00%) (13158/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7708) |  Loss2: (0.0000) | Acc: (72.00%) (14079/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7728) |  Loss2: (0.0000) | Acc: (72.00%) (14997/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7741) |  Loss2: (0.0000) | Acc: (72.00%) (15924/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7739) |  Loss2: (0.0000) | Acc: (72.00%) (16847/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7730) |  Loss2: (0.0000) | Acc: (72.00%) (17792/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7746) |  Loss2: (0.0000) | Acc: (72.00%) (18689/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7744) |  Loss2: (0.0000) | Acc: (72.00%) (19624/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7749) |  Loss2: (0.0000) | Acc: (72.00%) (20543/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7760) |  Loss2: (0.0000) | Acc: (72.00%) (21477/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7774) |  Loss2: (0.0000) | Acc: (72.00%) (22393/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7782) |  Loss2: (0.0000) | Acc: (72.00%) (23299/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7779) |  Loss2: (0.0000) | Acc: (72.00%) (24229/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7785) |  Loss2: (0.0000) | Acc: (72.00%) (25136/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7773) |  Loss2: (0.0000) | Acc: (72.00%) (26082/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7779) |  Loss2: (0.0000) | Acc: (72.00%) (27001/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7791) |  Loss2: (0.0000) | Acc: (72.00%) (27909/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7802) |  Loss2: (0.0000) | Acc: (72.00%) (28826/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7796) |  Loss2: (0.0000) | Acc: (72.00%) (29762/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7782) |  Loss2: (0.0000) | Acc: (72.00%) (30717/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7762) |  Loss2: (0.0000) | Acc: (72.00%) (31683/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7757) |  Loss2: (0.0000) | Acc: (72.00%) (32612/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7755) |  Loss2: (0.0000) | Acc: (72.00%) (33525/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7751) |  Loss2: (0.0000) | Acc: (72.00%) (34452/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7747) |  Loss2: (0.0000) | Acc: (72.00%) (35406/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7741) |  Loss2: (0.0000) | Acc: (72.00%) (36311/50000)
# TEST : Loss: (0.9053) | Acc: (69.00%) (6926/10000)
percent tensor([0.4581], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5113], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4773], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3172], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.7469) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.7644) |  Loss2: (0.0000) | Acc: (72.00%) (1017/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7566) |  Loss2: (0.0000) | Acc: (72.00%) (1960/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.7536) |  Loss2: (0.0000) | Acc: (73.00%) (2905/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.7448) |  Loss2: (0.0000) | Acc: (73.00%) (3870/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.7479) |  Loss2: (0.0000) | Acc: (73.00%) (4806/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.7502) |  Loss2: (0.0000) | Acc: (73.00%) (5720/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.7486) |  Loss2: (0.0000) | Acc: (73.00%) (6678/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (7626/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.7441) |  Loss2: (0.0000) | Acc: (73.00%) (8582/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (9508/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.7443) |  Loss2: (0.0000) | Acc: (73.00%) (10440/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.7442) |  Loss2: (0.0000) | Acc: (73.00%) (11377/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.7479) |  Loss2: (0.0000) | Acc: (73.00%) (12305/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7450) |  Loss2: (0.0000) | Acc: (73.00%) (13246/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7444) |  Loss2: (0.0000) | Acc: (73.00%) (14174/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7442) |  Loss2: (0.0000) | Acc: (73.00%) (15129/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7453) |  Loss2: (0.0000) | Acc: (73.00%) (16068/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7470) |  Loss2: (0.0000) | Acc: (73.00%) (17004/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (17955/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7466) |  Loss2: (0.0000) | Acc: (73.00%) (18892/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7487) |  Loss2: (0.0000) | Acc: (73.00%) (19823/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7466) |  Loss2: (0.0000) | Acc: (73.00%) (20777/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7445) |  Loss2: (0.0000) | Acc: (73.00%) (21740/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7426) |  Loss2: (0.0000) | Acc: (73.00%) (22721/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.7429) |  Loss2: (0.0000) | Acc: (73.00%) (23662/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (24616/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.7411) |  Loss2: (0.0000) | Acc: (73.00%) (25571/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.7404) |  Loss2: (0.0000) | Acc: (73.00%) (26514/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7410) |  Loss2: (0.0000) | Acc: (73.00%) (27447/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.7384) |  Loss2: (0.0000) | Acc: (73.00%) (28430/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.7375) |  Loss2: (0.0000) | Acc: (73.00%) (29400/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.7368) |  Loss2: (0.0000) | Acc: (73.00%) (30370/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.7365) |  Loss2: (0.0000) | Acc: (73.00%) (31321/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.7371) |  Loss2: (0.0000) | Acc: (73.00%) (32271/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.7379) |  Loss2: (0.0000) | Acc: (73.00%) (33207/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.7370) |  Loss2: (0.0000) | Acc: (73.00%) (34176/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.7376) |  Loss2: (0.0000) | Acc: (73.00%) (35122/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.7377) |  Loss2: (0.0000) | Acc: (73.00%) (36070/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.7365) |  Loss2: (0.0000) | Acc: (74.00%) (37006/50000)
# TEST : Loss: (0.8035) | Acc: (72.00%) (7203/10000)
percent tensor([0.4582], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5112], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4774], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3173], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.7738) |  Loss2: (0.0000) | Acc: (71.00%) (92/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6906) |  Loss2: (0.0000) | Acc: (76.00%) (1079/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.7033) |  Loss2: (0.0000) | Acc: (75.00%) (2029/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.7110) |  Loss2: (0.0000) | Acc: (75.00%) (2985/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.7102) |  Loss2: (0.0000) | Acc: (74.00%) (3933/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.7040) |  Loss2: (0.0000) | Acc: (75.00%) (4922/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.7021) |  Loss2: (0.0000) | Acc: (75.00%) (5897/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.7043) |  Loss2: (0.0000) | Acc: (75.00%) (6855/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.7025) |  Loss2: (0.0000) | Acc: (75.00%) (7836/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.7013) |  Loss2: (0.0000) | Acc: (75.00%) (8797/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6960) |  Loss2: (0.0000) | Acc: (75.00%) (9799/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6963) |  Loss2: (0.0000) | Acc: (75.00%) (10774/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (11752/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6948) |  Loss2: (0.0000) | Acc: (75.00%) (12723/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6947) |  Loss2: (0.0000) | Acc: (75.00%) (13687/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6982) |  Loss2: (0.0000) | Acc: (75.00%) (14636/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6982) |  Loss2: (0.0000) | Acc: (75.00%) (15591/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6997) |  Loss2: (0.0000) | Acc: (75.00%) (16547/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.7004) |  Loss2: (0.0000) | Acc: (75.00%) (17489/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6980) |  Loss2: (0.0000) | Acc: (75.00%) (18494/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6977) |  Loss2: (0.0000) | Acc: (75.00%) (19470/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6990) |  Loss2: (0.0000) | Acc: (75.00%) (20424/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6980) |  Loss2: (0.0000) | Acc: (75.00%) (21406/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6979) |  Loss2: (0.0000) | Acc: (75.00%) (22379/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6990) |  Loss2: (0.0000) | Acc: (75.00%) (23333/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6989) |  Loss2: (0.0000) | Acc: (75.00%) (24306/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (25259/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (75.00%) (26232/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6979) |  Loss2: (0.0000) | Acc: (75.00%) (27200/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6978) |  Loss2: (0.0000) | Acc: (75.00%) (28168/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (29144/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6958) |  Loss2: (0.0000) | Acc: (75.00%) (30124/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6957) |  Loss2: (0.0000) | Acc: (75.00%) (31094/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6952) |  Loss2: (0.0000) | Acc: (75.00%) (32071/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6950) |  Loss2: (0.0000) | Acc: (75.00%) (33029/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6954) |  Loss2: (0.0000) | Acc: (75.00%) (33998/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6937) |  Loss2: (0.0000) | Acc: (75.00%) (34993/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6943) |  Loss2: (0.0000) | Acc: (75.00%) (35958/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6953) |  Loss2: (0.0000) | Acc: (75.00%) (36927/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (37859/50000)
# TEST : Loss: (0.7202) | Acc: (74.00%) (7439/10000)
percent tensor([0.4582], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5112], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4774], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3174], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6861) |  Loss2: (0.0000) | Acc: (76.00%) (1078/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6674) |  Loss2: (0.0000) | Acc: (76.00%) (2050/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6740) |  Loss2: (0.0000) | Acc: (75.00%) (3012/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6697) |  Loss2: (0.0000) | Acc: (76.00%) (3997/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6698) |  Loss2: (0.0000) | Acc: (76.00%) (4969/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6719) |  Loss2: (0.0000) | Acc: (76.00%) (5943/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6716) |  Loss2: (0.0000) | Acc: (76.00%) (6925/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6768) |  Loss2: (0.0000) | Acc: (76.00%) (7890/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6796) |  Loss2: (0.0000) | Acc: (75.00%) (8852/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6761) |  Loss2: (0.0000) | Acc: (76.00%) (9856/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6755) |  Loss2: (0.0000) | Acc: (76.00%) (10849/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6733) |  Loss2: (0.0000) | Acc: (76.00%) (11829/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6730) |  Loss2: (0.0000) | Acc: (76.00%) (12803/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6708) |  Loss2: (0.0000) | Acc: (76.00%) (13783/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6735) |  Loss2: (0.0000) | Acc: (76.00%) (14754/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6744) |  Loss2: (0.0000) | Acc: (76.00%) (15720/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6743) |  Loss2: (0.0000) | Acc: (76.00%) (16709/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6754) |  Loss2: (0.0000) | Acc: (76.00%) (17687/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6756) |  Loss2: (0.0000) | Acc: (76.00%) (18659/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6745) |  Loss2: (0.0000) | Acc: (76.00%) (19652/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6781) |  Loss2: (0.0000) | Acc: (76.00%) (20586/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6774) |  Loss2: (0.0000) | Acc: (76.00%) (21543/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6769) |  Loss2: (0.0000) | Acc: (76.00%) (22512/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6774) |  Loss2: (0.0000) | Acc: (76.00%) (23487/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6768) |  Loss2: (0.0000) | Acc: (76.00%) (24463/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (25438/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6778) |  Loss2: (0.0000) | Acc: (76.00%) (26420/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6778) |  Loss2: (0.0000) | Acc: (76.00%) (27390/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6771) |  Loss2: (0.0000) | Acc: (76.00%) (28362/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6758) |  Loss2: (0.0000) | Acc: (76.00%) (29350/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6738) |  Loss2: (0.0000) | Acc: (76.00%) (30355/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6712) |  Loss2: (0.0000) | Acc: (76.00%) (31357/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6715) |  Loss2: (0.0000) | Acc: (76.00%) (32327/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6703) |  Loss2: (0.0000) | Acc: (76.00%) (33339/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6697) |  Loss2: (0.0000) | Acc: (76.00%) (34335/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6695) |  Loss2: (0.0000) | Acc: (76.00%) (35324/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6682) |  Loss2: (0.0000) | Acc: (76.00%) (36327/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6679) |  Loss2: (0.0000) | Acc: (76.00%) (37319/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6667) |  Loss2: (0.0000) | Acc: (76.00%) (38285/50000)
# TEST : Loss: (0.7120) | Acc: (74.00%) (7465/10000)
percent tensor([0.4582], device='cuda:0')
percent tensor([0.5048], device='cuda:0')
percent tensor([0.5167], device='cuda:0')
percent tensor([0.5112], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4774], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.3174], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.6588) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6607) |  Loss2: (0.0000) | Acc: (78.00%) (1099/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6759) |  Loss2: (0.0000) | Acc: (76.00%) (2067/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6831) |  Loss2: (0.0000) | Acc: (76.00%) (3049/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.6907) |  Loss2: (0.0000) | Acc: (76.00%) (4005/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.6949) |  Loss2: (0.0000) | Acc: (76.00%) (4972/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.6974) |  Loss2: (0.0000) | Acc: (76.00%) (5950/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.6954) |  Loss2: (0.0000) | Acc: (76.00%) (6919/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.6981) |  Loss2: (0.0000) | Acc: (75.00%) (7874/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7068) |  Loss2: (0.0000) | Acc: (75.00%) (8816/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7084) |  Loss2: (0.0000) | Acc: (75.00%) (9772/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7078) |  Loss2: (0.0000) | Acc: (75.00%) (10739/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (75.00%) (11730/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7046) |  Loss2: (0.0000) | Acc: (75.00%) (12690/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7059) |  Loss2: (0.0000) | Acc: (75.00%) (13640/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7042) |  Loss2: (0.0000) | Acc: (75.00%) (14619/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7038) |  Loss2: (0.0000) | Acc: (75.00%) (15594/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7052) |  Loss2: (0.0000) | Acc: (75.00%) (16544/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7034) |  Loss2: (0.0000) | Acc: (75.00%) (17521/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7032) |  Loss2: (0.0000) | Acc: (75.00%) (18472/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (19456/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7009) |  Loss2: (0.0000) | Acc: (75.00%) (20468/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.6991) |  Loss2: (0.0000) | Acc: (75.00%) (21448/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6991) |  Loss2: (0.0000) | Acc: (75.00%) (22415/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6981) |  Loss2: (0.0000) | Acc: (75.00%) (23382/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6966) |  Loss2: (0.0000) | Acc: (75.00%) (24373/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6959) |  Loss2: (0.0000) | Acc: (75.00%) (25338/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6948) |  Loss2: (0.0000) | Acc: (75.00%) (26309/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6951) |  Loss2: (0.0000) | Acc: (75.00%) (27282/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6949) |  Loss2: (0.0000) | Acc: (75.00%) (28252/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6939) |  Loss2: (0.0000) | Acc: (75.00%) (29234/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6924) |  Loss2: (0.0000) | Acc: (75.00%) (30229/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6929) |  Loss2: (0.0000) | Acc: (75.00%) (31194/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6922) |  Loss2: (0.0000) | Acc: (75.00%) (32160/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6917) |  Loss2: (0.0000) | Acc: (75.00%) (33130/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6905) |  Loss2: (0.0000) | Acc: (75.00%) (34123/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6917) |  Loss2: (0.0000) | Acc: (75.00%) (35069/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6904) |  Loss2: (0.0000) | Acc: (75.00%) (36054/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6902) |  Loss2: (0.0000) | Acc: (75.00%) (37035/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6882) |  Loss2: (0.0000) | Acc: (76.00%) (38012/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_025.pth.tar'
# TEST : Loss: (0.7054) | Acc: (75.00%) (7561/10000)
percent tensor([0.4526], device='cuda:0')
percent tensor([0.5129], device='cuda:0')
percent tensor([0.5123], device='cuda:0')
percent tensor([0.5087], device='cuda:0')
percent tensor([0.5031], device='cuda:0')
percent tensor([0.4728], device='cuda:0')
percent tensor([0.4533], device='cuda:0')
percent tensor([0.3045], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.5803) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.6930) |  Loss2: (0.0000) | Acc: (76.00%) (1083/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.6762) |  Loss2: (0.0000) | Acc: (77.00%) (2089/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.6799) |  Loss2: (0.0000) | Acc: (77.00%) (3063/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6765) |  Loss2: (0.0000) | Acc: (77.00%) (4041/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6691) |  Loss2: (0.0000) | Acc: (77.00%) (5046/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6727) |  Loss2: (0.0000) | Acc: (77.00%) (6014/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6776) |  Loss2: (0.0000) | Acc: (76.00%) (6976/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6760) |  Loss2: (0.0000) | Acc: (76.00%) (7944/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6715) |  Loss2: (0.0000) | Acc: (76.00%) (8949/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6706) |  Loss2: (0.0000) | Acc: (76.00%) (9922/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6666) |  Loss2: (0.0000) | Acc: (76.00%) (10917/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6702) |  Loss2: (0.0000) | Acc: (76.00%) (11855/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6639) |  Loss2: (0.0000) | Acc: (76.00%) (12866/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6632) |  Loss2: (0.0000) | Acc: (76.00%) (13850/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6613) |  Loss2: (0.0000) | Acc: (76.00%) (14863/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (76.00%) (15842/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6633) |  Loss2: (0.0000) | Acc: (76.00%) (16808/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6634) |  Loss2: (0.0000) | Acc: (76.00%) (17792/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6623) |  Loss2: (0.0000) | Acc: (76.00%) (18780/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6616) |  Loss2: (0.0000) | Acc: (76.00%) (19760/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6612) |  Loss2: (0.0000) | Acc: (76.00%) (20752/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6599) |  Loss2: (0.0000) | Acc: (76.00%) (21743/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6604) |  Loss2: (0.0000) | Acc: (76.00%) (22720/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6602) |  Loss2: (0.0000) | Acc: (76.00%) (23716/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6589) |  Loss2: (0.0000) | Acc: (76.00%) (24707/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6573) |  Loss2: (0.0000) | Acc: (76.00%) (25721/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6569) |  Loss2: (0.0000) | Acc: (76.00%) (26706/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6573) |  Loss2: (0.0000) | Acc: (76.00%) (27693/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6575) |  Loss2: (0.0000) | Acc: (76.00%) (28676/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6570) |  Loss2: (0.0000) | Acc: (76.00%) (29659/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6566) |  Loss2: (0.0000) | Acc: (77.00%) (30671/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (31675/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (32689/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (33673/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6531) |  Loss2: (0.0000) | Acc: (77.00%) (34658/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6526) |  Loss2: (0.0000) | Acc: (77.00%) (35668/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6547) |  Loss2: (0.0000) | Acc: (77.00%) (36624/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (77.00%) (37628/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6538) |  Loss2: (0.0000) | Acc: (77.00%) (38604/50000)
# TEST : Loss: (0.6824) | Acc: (76.00%) (7622/10000)
percent tensor([0.4531], device='cuda:0')
percent tensor([0.5160], device='cuda:0')
percent tensor([0.5072], device='cuda:0')
percent tensor([0.5102], device='cuda:0')
percent tensor([0.5034], device='cuda:0')
percent tensor([0.4670], device='cuda:0')
percent tensor([0.4480], device='cuda:0')
percent tensor([0.2900], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.7954) |  Loss2: (0.0000) | Acc: (68.00%) (88/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6751) |  Loss2: (0.0000) | Acc: (76.00%) (1078/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6450) |  Loss2: (0.0000) | Acc: (77.00%) (2094/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6501) |  Loss2: (0.0000) | Acc: (77.00%) (3081/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6609) |  Loss2: (0.0000) | Acc: (77.00%) (4048/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6598) |  Loss2: (0.0000) | Acc: (77.00%) (5035/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6537) |  Loss2: (0.0000) | Acc: (77.00%) (6032/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6474) |  Loss2: (0.0000) | Acc: (77.00%) (7044/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6469) |  Loss2: (0.0000) | Acc: (77.00%) (8043/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6480) |  Loss2: (0.0000) | Acc: (77.00%) (9025/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6477) |  Loss2: (0.0000) | Acc: (77.00%) (10012/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (11006/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6413) |  Loss2: (0.0000) | Acc: (77.00%) (12035/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6391) |  Loss2: (0.0000) | Acc: (77.00%) (13044/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (14007/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6406) |  Loss2: (0.0000) | Acc: (77.00%) (15021/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6426) |  Loss2: (0.0000) | Acc: (77.00%) (15989/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6403) |  Loss2: (0.0000) | Acc: (77.00%) (17000/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6411) |  Loss2: (0.0000) | Acc: (77.00%) (17976/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6414) |  Loss2: (0.0000) | Acc: (77.00%) (18954/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6414) |  Loss2: (0.0000) | Acc: (77.00%) (19939/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6426) |  Loss2: (0.0000) | Acc: (77.00%) (20919/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (21903/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6415) |  Loss2: (0.0000) | Acc: (77.00%) (22897/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6424) |  Loss2: (0.0000) | Acc: (77.00%) (23892/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6429) |  Loss2: (0.0000) | Acc: (77.00%) (24885/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (25886/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (26856/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6449) |  Loss2: (0.0000) | Acc: (77.00%) (27848/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6441) |  Loss2: (0.0000) | Acc: (77.00%) (28849/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6435) |  Loss2: (0.0000) | Acc: (77.00%) (29857/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6435) |  Loss2: (0.0000) | Acc: (77.00%) (30862/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6433) |  Loss2: (0.0000) | Acc: (77.00%) (31855/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6415) |  Loss2: (0.0000) | Acc: (77.00%) (32877/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6403) |  Loss2: (0.0000) | Acc: (77.00%) (33887/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (34907/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6395) |  Loss2: (0.0000) | Acc: (77.00%) (35889/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6396) |  Loss2: (0.0000) | Acc: (77.00%) (36881/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6401) |  Loss2: (0.0000) | Acc: (77.00%) (37874/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6403) |  Loss2: (0.0000) | Acc: (77.00%) (38844/50000)
# TEST : Loss: (0.6741) | Acc: (76.00%) (7637/10000)
percent tensor([0.4541], device='cuda:0')
percent tensor([0.5133], device='cuda:0')
percent tensor([0.5040], device='cuda:0')
percent tensor([0.5099], device='cuda:0')
percent tensor([0.5033], device='cuda:0')
percent tensor([0.4620], device='cuda:0')
percent tensor([0.4433], device='cuda:0')
percent tensor([0.2775], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.7255) |  Loss2: (0.0000) | Acc: (75.00%) (97/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6800) |  Loss2: (0.0000) | Acc: (75.00%) (1065/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6543) |  Loss2: (0.0000) | Acc: (76.00%) (2065/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (77.00%) (3071/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6373) |  Loss2: (0.0000) | Acc: (77.00%) (4055/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (77.00%) (5054/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6369) |  Loss2: (0.0000) | Acc: (77.00%) (6055/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6390) |  Loss2: (0.0000) | Acc: (77.00%) (7033/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6447) |  Loss2: (0.0000) | Acc: (77.00%) (7987/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (77.00%) (9004/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6431) |  Loss2: (0.0000) | Acc: (77.00%) (9980/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6473) |  Loss2: (0.0000) | Acc: (76.00%) (10931/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6478) |  Loss2: (0.0000) | Acc: (76.00%) (11924/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6469) |  Loss2: (0.0000) | Acc: (77.00%) (12930/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6465) |  Loss2: (0.0000) | Acc: (77.00%) (13937/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6454) |  Loss2: (0.0000) | Acc: (77.00%) (14931/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (15955/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (77.00%) (16982/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6367) |  Loss2: (0.0000) | Acc: (77.00%) (17976/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6376) |  Loss2: (0.0000) | Acc: (77.00%) (18977/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6366) |  Loss2: (0.0000) | Acc: (77.00%) (19983/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6368) |  Loss2: (0.0000) | Acc: (77.00%) (20994/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (21998/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (23000/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6363) |  Loss2: (0.0000) | Acc: (77.00%) (24008/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6349) |  Loss2: (0.0000) | Acc: (77.00%) (25017/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (77.00%) (26000/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (77.00%) (26980/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6352) |  Loss2: (0.0000) | Acc: (77.00%) (27989/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (77.00%) (29014/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (77.00%) (29991/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (30996/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6332) |  Loss2: (0.0000) | Acc: (77.00%) (32006/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (33006/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6332) |  Loss2: (0.0000) | Acc: (77.00%) (34016/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6343) |  Loss2: (0.0000) | Acc: (77.00%) (35004/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6347) |  Loss2: (0.0000) | Acc: (77.00%) (35978/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (36992/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6348) |  Loss2: (0.0000) | Acc: (77.00%) (37986/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6356) |  Loss2: (0.0000) | Acc: (77.00%) (38929/50000)
# TEST : Loss: (0.6672) | Acc: (76.00%) (7673/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5129], device='cuda:0')
percent tensor([0.5017], device='cuda:0')
percent tensor([0.5102], device='cuda:0')
percent tensor([0.5010], device='cuda:0')
percent tensor([0.4568], device='cuda:0')
percent tensor([0.4388], device='cuda:0')
percent tensor([0.2665], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6355) |  Loss2: (0.0000) | Acc: (78.00%) (1103/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (78.00%) (2120/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.6335) |  Loss2: (0.0000) | Acc: (78.00%) (3103/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.6409) |  Loss2: (0.0000) | Acc: (77.00%) (4078/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.6373) |  Loss2: (0.0000) | Acc: (77.00%) (5090/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.6383) |  Loss2: (0.0000) | Acc: (77.00%) (6084/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (77.00%) (7079/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (8064/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (9069/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.6329) |  Loss2: (0.0000) | Acc: (77.00%) (10060/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6341) |  Loss2: (0.0000) | Acc: (77.00%) (11037/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6362) |  Loss2: (0.0000) | Acc: (77.00%) (12013/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6385) |  Loss2: (0.0000) | Acc: (77.00%) (12995/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6344) |  Loss2: (0.0000) | Acc: (77.00%) (14013/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6375) |  Loss2: (0.0000) | Acc: (77.00%) (14986/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6353) |  Loss2: (0.0000) | Acc: (77.00%) (15991/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6354) |  Loss2: (0.0000) | Acc: (77.00%) (16993/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6353) |  Loss2: (0.0000) | Acc: (77.00%) (17998/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6357) |  Loss2: (0.0000) | Acc: (77.00%) (18979/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6341) |  Loss2: (0.0000) | Acc: (77.00%) (19996/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6342) |  Loss2: (0.0000) | Acc: (77.00%) (20999/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6335) |  Loss2: (0.0000) | Acc: (77.00%) (22004/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6336) |  Loss2: (0.0000) | Acc: (77.00%) (22992/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6331) |  Loss2: (0.0000) | Acc: (77.00%) (23998/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6350) |  Loss2: (0.0000) | Acc: (77.00%) (24971/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6325) |  Loss2: (0.0000) | Acc: (77.00%) (25997/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (77.00%) (26995/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.6318) |  Loss2: (0.0000) | Acc: (77.00%) (28015/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (29022/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (30017/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.6328) |  Loss2: (0.0000) | Acc: (77.00%) (30994/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.6324) |  Loss2: (0.0000) | Acc: (77.00%) (31984/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.6319) |  Loss2: (0.0000) | Acc: (77.00%) (32989/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.6322) |  Loss2: (0.0000) | Acc: (77.00%) (33975/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (77.00%) (34976/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.6313) |  Loss2: (0.0000) | Acc: (77.00%) (35977/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.6305) |  Loss2: (0.0000) | Acc: (77.00%) (36993/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.6303) |  Loss2: (0.0000) | Acc: (77.00%) (38002/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.6314) |  Loss2: (0.0000) | Acc: (77.00%) (38947/50000)
# TEST : Loss: (0.6629) | Acc: (76.00%) (7689/10000)
percent tensor([0.4548], device='cuda:0')
percent tensor([0.5124], device='cuda:0')
percent tensor([0.5015], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4992], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4358], device='cuda:0')
percent tensor([0.2573], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.7164) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.6749) |  Loss2: (0.0000) | Acc: (78.00%) (1101/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.6617) |  Loss2: (0.0000) | Acc: (77.00%) (2074/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (3078/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.6566) |  Loss2: (0.0000) | Acc: (77.00%) (4048/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.6660) |  Loss2: (0.0000) | Acc: (76.00%) (5022/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.6685) |  Loss2: (0.0000) | Acc: (76.00%) (6008/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6676) |  Loss2: (0.0000) | Acc: (76.00%) (6983/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.6633) |  Loss2: (0.0000) | Acc: (76.00%) (7971/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6628) |  Loss2: (0.0000) | Acc: (77.00%) (8972/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6575) |  Loss2: (0.0000) | Acc: (77.00%) (9974/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6535) |  Loss2: (0.0000) | Acc: (77.00%) (10973/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6522) |  Loss2: (0.0000) | Acc: (77.00%) (11959/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (12932/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6498) |  Loss2: (0.0000) | Acc: (77.00%) (13962/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6483) |  Loss2: (0.0000) | Acc: (77.00%) (14969/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6461) |  Loss2: (0.0000) | Acc: (77.00%) (15980/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (16961/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6472) |  Loss2: (0.0000) | Acc: (77.00%) (17937/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6442) |  Loss2: (0.0000) | Acc: (77.00%) (18946/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (19931/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6445) |  Loss2: (0.0000) | Acc: (77.00%) (20922/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6444) |  Loss2: (0.0000) | Acc: (77.00%) (21917/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.6447) |  Loss2: (0.0000) | Acc: (77.00%) (22910/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.6417) |  Loss2: (0.0000) | Acc: (77.00%) (23939/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.6437) |  Loss2: (0.0000) | Acc: (77.00%) (24901/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.6422) |  Loss2: (0.0000) | Acc: (77.00%) (25917/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.6410) |  Loss2: (0.0000) | Acc: (77.00%) (26918/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.6414) |  Loss2: (0.0000) | Acc: (77.00%) (27900/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.6419) |  Loss2: (0.0000) | Acc: (77.00%) (28880/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.6412) |  Loss2: (0.0000) | Acc: (77.00%) (29884/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.6400) |  Loss2: (0.0000) | Acc: (77.00%) (30895/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.6400) |  Loss2: (0.0000) | Acc: (77.00%) (31893/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.6406) |  Loss2: (0.0000) | Acc: (77.00%) (32859/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.6408) |  Loss2: (0.0000) | Acc: (77.00%) (33848/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.6410) |  Loss2: (0.0000) | Acc: (77.00%) (34819/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.6401) |  Loss2: (0.0000) | Acc: (77.00%) (35837/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (36806/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.6407) |  Loss2: (0.0000) | Acc: (77.00%) (37798/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.6402) |  Loss2: (0.0000) | Acc: (77.00%) (38757/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_030.pth.tar'
# TEST : Loss: (0.7787) | Acc: (73.00%) (7330/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5125], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4359], device='cuda:0')
percent tensor([0.2573], device='cuda:0')
Epoch: 31 | Batch_idx: 0 |  Loss: (0.5480) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.6051) |  Loss2: (0.0000) | Acc: (79.00%) (1113/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.6037) |  Loss2: (0.0000) | Acc: (78.00%) (2113/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.6013) |  Loss2: (0.0000) | Acc: (78.00%) (3125/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (4113/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.6182) |  Loss2: (0.0000) | Acc: (78.00%) (5130/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (78.00%) (6135/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (7136/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.6196) |  Loss2: (0.0000) | Acc: (78.00%) (8125/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.6244) |  Loss2: (0.0000) | Acc: (78.00%) (9095/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.6236) |  Loss2: (0.0000) | Acc: (78.00%) (10113/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (78.00%) (11108/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.6218) |  Loss2: (0.0000) | Acc: (78.00%) (12111/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (13114/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.6240) |  Loss2: (0.0000) | Acc: (78.00%) (14109/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.6246) |  Loss2: (0.0000) | Acc: (78.00%) (15105/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (16125/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (17139/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.6195) |  Loss2: (0.0000) | Acc: (78.00%) (18146/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (19137/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.6184) |  Loss2: (0.0000) | Acc: (78.00%) (20171/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (21175/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.6183) |  Loss2: (0.0000) | Acc: (78.00%) (22200/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.6172) |  Loss2: (0.0000) | Acc: (78.00%) (23214/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.6169) |  Loss2: (0.0000) | Acc: (78.00%) (24214/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.6148) |  Loss2: (0.0000) | Acc: (78.00%) (25235/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.6144) |  Loss2: (0.0000) | Acc: (78.00%) (26250/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.6141) |  Loss2: (0.0000) | Acc: (78.00%) (27253/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (28260/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.6126) |  Loss2: (0.0000) | Acc: (78.00%) (29284/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.6128) |  Loss2: (0.0000) | Acc: (78.00%) (30286/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.6131) |  Loss2: (0.0000) | Acc: (78.00%) (31301/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.6122) |  Loss2: (0.0000) | Acc: (78.00%) (32331/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.6129) |  Loss2: (0.0000) | Acc: (78.00%) (33328/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (34311/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.6121) |  Loss2: (0.0000) | Acc: (78.00%) (35339/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (36347/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.6116) |  Loss2: (0.0000) | Acc: (78.00%) (37354/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.6110) |  Loss2: (0.0000) | Acc: (78.00%) (38366/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.6105) |  Loss2: (0.0000) | Acc: (78.00%) (39333/50000)
# TEST : Loss: (0.6302) | Acc: (77.00%) (7792/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5125], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4359], device='cuda:0')
percent tensor([0.2574], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.6524) |  Loss2: (0.0000) | Acc: (77.00%) (1085/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.6140) |  Loss2: (0.0000) | Acc: (78.00%) (2106/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.6029) |  Loss2: (0.0000) | Acc: (78.00%) (3124/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (78.00%) (4125/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (78.00%) (5132/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (78.00%) (6147/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5942) |  Loss2: (0.0000) | Acc: (79.00%) (7200/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5903) |  Loss2: (0.0000) | Acc: (79.00%) (8232/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5873) |  Loss2: (0.0000) | Acc: (79.00%) (9263/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5881) |  Loss2: (0.0000) | Acc: (79.00%) (10272/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5896) |  Loss2: (0.0000) | Acc: (79.00%) (11292/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (12302/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5906) |  Loss2: (0.0000) | Acc: (79.00%) (13327/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (79.00%) (14362/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5843) |  Loss2: (0.0000) | Acc: (79.00%) (15401/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5852) |  Loss2: (0.0000) | Acc: (79.00%) (16416/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5856) |  Loss2: (0.0000) | Acc: (79.00%) (17430/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5866) |  Loss2: (0.0000) | Acc: (79.00%) (18436/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5847) |  Loss2: (0.0000) | Acc: (79.00%) (19474/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5854) |  Loss2: (0.0000) | Acc: (79.00%) (20497/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5849) |  Loss2: (0.0000) | Acc: (79.00%) (21520/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5854) |  Loss2: (0.0000) | Acc: (79.00%) (22532/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5857) |  Loss2: (0.0000) | Acc: (79.00%) (23532/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5838) |  Loss2: (0.0000) | Acc: (79.00%) (24578/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5847) |  Loss2: (0.0000) | Acc: (79.00%) (25593/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5844) |  Loss2: (0.0000) | Acc: (79.00%) (26622/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5837) |  Loss2: (0.0000) | Acc: (79.00%) (27666/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5836) |  Loss2: (0.0000) | Acc: (79.00%) (28687/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5836) |  Loss2: (0.0000) | Acc: (79.00%) (29712/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5830) |  Loss2: (0.0000) | Acc: (79.00%) (30751/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5824) |  Loss2: (0.0000) | Acc: (79.00%) (31781/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5815) |  Loss2: (0.0000) | Acc: (79.00%) (32818/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5826) |  Loss2: (0.0000) | Acc: (79.00%) (33833/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5833) |  Loss2: (0.0000) | Acc: (79.00%) (34847/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5842) |  Loss2: (0.0000) | Acc: (79.00%) (35850/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5852) |  Loss2: (0.0000) | Acc: (79.00%) (36844/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5843) |  Loss2: (0.0000) | Acc: (79.00%) (37888/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5833) |  Loss2: (0.0000) | Acc: (79.00%) (38922/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5831) |  Loss2: (0.0000) | Acc: (79.00%) (39919/50000)
# TEST : Loss: (0.6696) | Acc: (76.00%) (7634/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5125], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4359], device='cuda:0')
percent tensor([0.2574], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.5425) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (80.00%) (1137/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5735) |  Loss2: (0.0000) | Acc: (80.00%) (2168/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (3208/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5580) |  Loss2: (0.0000) | Acc: (80.00%) (4231/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5546) |  Loss2: (0.0000) | Acc: (80.00%) (5270/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5497) |  Loss2: (0.0000) | Acc: (80.00%) (6320/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5513) |  Loss2: (0.0000) | Acc: (80.00%) (7347/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5539) |  Loss2: (0.0000) | Acc: (80.00%) (8360/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5551) |  Loss2: (0.0000) | Acc: (80.00%) (9382/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5578) |  Loss2: (0.0000) | Acc: (80.00%) (10390/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5585) |  Loss2: (0.0000) | Acc: (80.00%) (11410/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (12426/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (13481/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5615) |  Loss2: (0.0000) | Acc: (80.00%) (14505/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (15543/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (16564/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5614) |  Loss2: (0.0000) | Acc: (80.00%) (17612/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5612) |  Loss2: (0.0000) | Acc: (80.00%) (18634/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5616) |  Loss2: (0.0000) | Acc: (80.00%) (19652/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5638) |  Loss2: (0.0000) | Acc: (80.00%) (20661/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5637) |  Loss2: (0.0000) | Acc: (80.00%) (21705/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (80.00%) (22730/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (23749/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (24782/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (25838/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (26873/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (27902/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (28927/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5613) |  Loss2: (0.0000) | Acc: (80.00%) (29982/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (30981/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5633) |  Loss2: (0.0000) | Acc: (80.00%) (32005/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (33040/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5617) |  Loss2: (0.0000) | Acc: (80.00%) (34083/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5623) |  Loss2: (0.0000) | Acc: (80.00%) (35095/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (36115/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (37135/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5618) |  Loss2: (0.0000) | Acc: (80.00%) (38160/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (39218/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5609) |  Loss2: (0.0000) | Acc: (80.00%) (40215/50000)
# TEST : Loss: (0.6092) | Acc: (79.00%) (7920/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5125], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4360], device='cuda:0')
percent tensor([0.2575], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5172) |  Loss2: (0.0000) | Acc: (82.00%) (1160/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5251) |  Loss2: (0.0000) | Acc: (81.00%) (2200/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.5481) |  Loss2: (0.0000) | Acc: (81.00%) (3216/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (4263/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (5326/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5325) |  Loss2: (0.0000) | Acc: (81.00%) (6366/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (7408/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.5320) |  Loss2: (0.0000) | Acc: (81.00%) (8467/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5384) |  Loss2: (0.0000) | Acc: (81.00%) (9483/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5385) |  Loss2: (0.0000) | Acc: (81.00%) (10526/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (11562/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5368) |  Loss2: (0.0000) | Acc: (81.00%) (12614/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (13672/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5347) |  Loss2: (0.0000) | Acc: (81.00%) (14713/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5358) |  Loss2: (0.0000) | Acc: (81.00%) (15758/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (81.00%) (16793/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5364) |  Loss2: (0.0000) | Acc: (81.00%) (17852/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5357) |  Loss2: (0.0000) | Acc: (81.00%) (18904/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5375) |  Loss2: (0.0000) | Acc: (81.00%) (19931/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (20962/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5381) |  Loss2: (0.0000) | Acc: (81.00%) (22009/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5374) |  Loss2: (0.0000) | Acc: (81.00%) (23061/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5370) |  Loss2: (0.0000) | Acc: (81.00%) (24099/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5373) |  Loss2: (0.0000) | Acc: (81.00%) (25130/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5393) |  Loss2: (0.0000) | Acc: (81.00%) (26149/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5383) |  Loss2: (0.0000) | Acc: (81.00%) (27210/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (28238/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (29268/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (30307/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (31370/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5379) |  Loss2: (0.0000) | Acc: (81.00%) (32427/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5379) |  Loss2: (0.0000) | Acc: (81.00%) (33464/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (34511/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5376) |  Loss2: (0.0000) | Acc: (81.00%) (35544/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (36580/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5372) |  Loss2: (0.0000) | Acc: (81.00%) (37654/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5363) |  Loss2: (0.0000) | Acc: (81.00%) (38705/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5360) |  Loss2: (0.0000) | Acc: (81.00%) (39756/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (40746/50000)
# TEST : Loss: (0.6281) | Acc: (78.00%) (7839/10000)
percent tensor([0.4550], device='cuda:0')
percent tensor([0.5125], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.5111], device='cuda:0')
percent tensor([0.4993], device='cuda:0')
percent tensor([0.4535], device='cuda:0')
percent tensor([0.4360], device='cuda:0')
percent tensor([0.2576], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.4889) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.5137) |  Loss2: (0.0000) | Acc: (81.00%) (1154/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (2178/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5402) |  Loss2: (0.0000) | Acc: (81.00%) (3219/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (80.00%) (4244/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (5265/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.5559) |  Loss2: (0.0000) | Acc: (80.00%) (6291/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (7303/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.5597) |  Loss2: (0.0000) | Acc: (80.00%) (8344/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.5559) |  Loss2: (0.0000) | Acc: (80.00%) (9395/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (10406/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.5612) |  Loss2: (0.0000) | Acc: (80.00%) (11425/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.5626) |  Loss2: (0.0000) | Acc: (80.00%) (12461/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.5639) |  Loss2: (0.0000) | Acc: (80.00%) (13479/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (14533/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.5644) |  Loss2: (0.0000) | Acc: (80.00%) (15552/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.5670) |  Loss2: (0.0000) | Acc: (80.00%) (16559/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.5651) |  Loss2: (0.0000) | Acc: (80.00%) (17594/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.5645) |  Loss2: (0.0000) | Acc: (80.00%) (18631/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.5634) |  Loss2: (0.0000) | Acc: (80.00%) (19674/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.5621) |  Loss2: (0.0000) | Acc: (80.00%) (20707/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.5611) |  Loss2: (0.0000) | Acc: (80.00%) (21752/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.5613) |  Loss2: (0.0000) | Acc: (80.00%) (22781/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (23821/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.5607) |  Loss2: (0.0000) | Acc: (80.00%) (24835/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.5600) |  Loss2: (0.0000) | Acc: (80.00%) (25871/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.5612) |  Loss2: (0.0000) | Acc: (80.00%) (26878/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.5605) |  Loss2: (0.0000) | Acc: (80.00%) (27912/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.5595) |  Loss2: (0.0000) | Acc: (80.00%) (28962/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (30002/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (31041/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.5573) |  Loss2: (0.0000) | Acc: (80.00%) (32089/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (80.00%) (33111/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.5577) |  Loss2: (0.0000) | Acc: (80.00%) (34139/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (35170/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (36205/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.5575) |  Loss2: (0.0000) | Acc: (80.00%) (37244/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.5572) |  Loss2: (0.0000) | Acc: (80.00%) (38273/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.5564) |  Loss2: (0.0000) | Acc: (80.00%) (39303/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (40289/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_035.pth.tar'
# TEST : Loss: (0.6005) | Acc: (79.00%) (7936/10000)
percent tensor([0.4455], device='cuda:0')
percent tensor([0.5043], device='cuda:0')
percent tensor([0.4969], device='cuda:0')
percent tensor([0.4999], device='cuda:0')
percent tensor([0.4901], device='cuda:0')
percent tensor([0.4461], device='cuda:0')
percent tensor([0.4309], device='cuda:0')
percent tensor([0.2483], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.5115) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5723) |  Loss2: (0.0000) | Acc: (80.00%) (1134/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5540) |  Loss2: (0.0000) | Acc: (80.00%) (2172/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5658) |  Loss2: (0.0000) | Acc: (80.00%) (3188/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5574) |  Loss2: (0.0000) | Acc: (80.00%) (4238/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5543) |  Loss2: (0.0000) | Acc: (80.00%) (5280/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5534) |  Loss2: (0.0000) | Acc: (80.00%) (6321/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (7396/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5489) |  Loss2: (0.0000) | Acc: (81.00%) (8426/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5495) |  Loss2: (0.0000) | Acc: (81.00%) (9461/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5458) |  Loss2: (0.0000) | Acc: (81.00%) (10511/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (81.00%) (11553/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (12580/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (81.00%) (13594/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (14632/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (15666/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (16739/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (17796/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5427) |  Loss2: (0.0000) | Acc: (81.00%) (18848/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (81.00%) (19891/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5420) |  Loss2: (0.0000) | Acc: (81.00%) (20946/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (22011/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5395) |  Loss2: (0.0000) | Acc: (81.00%) (23058/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5388) |  Loss2: (0.0000) | Acc: (81.00%) (24104/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5382) |  Loss2: (0.0000) | Acc: (81.00%) (25155/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5394) |  Loss2: (0.0000) | Acc: (81.00%) (26198/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5387) |  Loss2: (0.0000) | Acc: (81.00%) (27238/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5377) |  Loss2: (0.0000) | Acc: (81.00%) (28291/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5365) |  Loss2: (0.0000) | Acc: (81.00%) (29345/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5360) |  Loss2: (0.0000) | Acc: (81.00%) (30398/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (31464/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (81.00%) (32536/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5325) |  Loss2: (0.0000) | Acc: (81.00%) (33567/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5326) |  Loss2: (0.0000) | Acc: (81.00%) (34600/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5324) |  Loss2: (0.0000) | Acc: (81.00%) (35644/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5316) |  Loss2: (0.0000) | Acc: (81.00%) (36693/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5332) |  Loss2: (0.0000) | Acc: (81.00%) (37713/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5338) |  Loss2: (0.0000) | Acc: (81.00%) (38742/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5341) |  Loss2: (0.0000) | Acc: (81.00%) (39779/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (40798/50000)
# TEST : Loss: (0.5915) | Acc: (79.00%) (7984/10000)
percent tensor([0.4475], device='cuda:0')
percent tensor([0.5023], device='cuda:0')
percent tensor([0.4955], device='cuda:0')
percent tensor([0.4947], device='cuda:0')
percent tensor([0.4848], device='cuda:0')
percent tensor([0.4404], device='cuda:0')
percent tensor([0.4267], device='cuda:0')
percent tensor([0.2389], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.6754) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5430) |  Loss2: (0.0000) | Acc: (80.00%) (1136/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (2204/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (82.00%) (3262/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5150) |  Loss2: (0.0000) | Acc: (82.00%) (4318/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (82.00%) (5367/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5212) |  Loss2: (0.0000) | Acc: (82.00%) (6426/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (7484/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (8540/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5225) |  Loss2: (0.0000) | Acc: (82.00%) (9595/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (82.00%) (10654/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (11715/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (12775/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5167) |  Loss2: (0.0000) | Acc: (82.00%) (13850/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5127) |  Loss2: (0.0000) | Acc: (82.00%) (14926/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5124) |  Loss2: (0.0000) | Acc: (82.00%) (15985/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5140) |  Loss2: (0.0000) | Acc: (82.00%) (17034/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5139) |  Loss2: (0.0000) | Acc: (82.00%) (18103/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5130) |  Loss2: (0.0000) | Acc: (82.00%) (19152/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5123) |  Loss2: (0.0000) | Acc: (82.00%) (20211/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5138) |  Loss2: (0.0000) | Acc: (82.00%) (21259/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (82.00%) (22280/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (23333/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (24376/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (82.00%) (25412/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (82.00%) (26465/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (82.00%) (27525/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (28592/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (29634/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (82.00%) (30700/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (31729/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (32773/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5222) |  Loss2: (0.0000) | Acc: (82.00%) (33792/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5221) |  Loss2: (0.0000) | Acc: (82.00%) (34836/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (82.00%) (35888/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (82.00%) (36937/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (37997/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (82.00%) (39035/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (40086/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (41087/50000)
# TEST : Loss: (0.5788) | Acc: (80.00%) (8017/10000)
percent tensor([0.4500], device='cuda:0')
percent tensor([0.5009], device='cuda:0')
percent tensor([0.4976], device='cuda:0')
percent tensor([0.4922], device='cuda:0')
percent tensor([0.4811], device='cuda:0')
percent tensor([0.4360], device='cuda:0')
percent tensor([0.4229], device='cuda:0')
percent tensor([0.2302], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.5980) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.5348) |  Loss2: (0.0000) | Acc: (81.00%) (1151/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (2220/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (82.00%) (3267/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (4327/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (5363/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (6416/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (7482/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (8527/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5134) |  Loss2: (0.0000) | Acc: (82.00%) (9593/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5149) |  Loss2: (0.0000) | Acc: (82.00%) (10646/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (11694/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (12737/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5144) |  Loss2: (0.0000) | Acc: (82.00%) (13799/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5166) |  Loss2: (0.0000) | Acc: (82.00%) (14840/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5161) |  Loss2: (0.0000) | Acc: (82.00%) (15909/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5164) |  Loss2: (0.0000) | Acc: (82.00%) (16951/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (17998/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (82.00%) (19034/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5206) |  Loss2: (0.0000) | Acc: (82.00%) (20090/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (21145/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5192) |  Loss2: (0.0000) | Acc: (82.00%) (22209/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (82.00%) (23247/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (24291/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (82.00%) (25354/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (82.00%) (26397/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (27460/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5195) |  Loss2: (0.0000) | Acc: (82.00%) (28511/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (29572/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (30624/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (31662/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (32722/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (33776/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5169) |  Loss2: (0.0000) | Acc: (82.00%) (34846/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (35906/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5170) |  Loss2: (0.0000) | Acc: (82.00%) (36953/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (37984/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5167) |  Loss2: (0.0000) | Acc: (82.00%) (39043/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (40082/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (41105/50000)
# TEST : Loss: (0.5742) | Acc: (80.00%) (8041/10000)
percent tensor([0.4503], device='cuda:0')
percent tensor([0.4988], device='cuda:0')
percent tensor([0.4968], device='cuda:0')
percent tensor([0.4914], device='cuda:0')
percent tensor([0.4775], device='cuda:0')
percent tensor([0.4327], device='cuda:0')
percent tensor([0.4193], device='cuda:0')
percent tensor([0.2224], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5325) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (2219/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (3274/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (82.00%) (4329/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (5403/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (6461/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.5044) |  Loss2: (0.0000) | Acc: (82.00%) (7515/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (8579/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (82.00%) (9642/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (10683/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (11743/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.5052) |  Loss2: (0.0000) | Acc: (82.00%) (12787/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.5084) |  Loss2: (0.0000) | Acc: (82.00%) (13835/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.5074) |  Loss2: (0.0000) | Acc: (82.00%) (14893/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (15970/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (17020/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (18048/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (19091/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.5092) |  Loss2: (0.0000) | Acc: (82.00%) (20158/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.5090) |  Loss2: (0.0000) | Acc: (82.00%) (21213/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (22242/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.5087) |  Loss2: (0.0000) | Acc: (82.00%) (23311/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (24378/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.5088) |  Loss2: (0.0000) | Acc: (82.00%) (25436/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.5089) |  Loss2: (0.0000) | Acc: (82.00%) (26486/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.5096) |  Loss2: (0.0000) | Acc: (82.00%) (27538/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (28606/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (29668/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.5075) |  Loss2: (0.0000) | Acc: (82.00%) (30710/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.5064) |  Loss2: (0.0000) | Acc: (82.00%) (31781/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (32837/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.5069) |  Loss2: (0.0000) | Acc: (82.00%) (33895/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.5082) |  Loss2: (0.0000) | Acc: (82.00%) (34930/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.5086) |  Loss2: (0.0000) | Acc: (82.00%) (35972/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.5079) |  Loss2: (0.0000) | Acc: (82.00%) (37039/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.5080) |  Loss2: (0.0000) | Acc: (82.00%) (38092/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.5076) |  Loss2: (0.0000) | Acc: (82.00%) (39164/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.5078) |  Loss2: (0.0000) | Acc: (82.00%) (40207/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.5077) |  Loss2: (0.0000) | Acc: (82.00%) (41216/50000)
# TEST : Loss: (0.5706) | Acc: (80.00%) (8044/10000)
percent tensor([0.4479], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4298], device='cuda:0')
percent tensor([0.4157], device='cuda:0')
percent tensor([0.2152], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.5276) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.4982) |  Loss2: (0.0000) | Acc: (82.00%) (1163/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5147) |  Loss2: (0.0000) | Acc: (82.00%) (2217/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (82.00%) (3271/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (82.00%) (4323/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5256) |  Loss2: (0.0000) | Acc: (82.00%) (5353/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (82.00%) (6405/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (7450/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.5246) |  Loss2: (0.0000) | Acc: (81.00%) (8492/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.5239) |  Loss2: (0.0000) | Acc: (82.00%) (9553/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (10588/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (11636/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (82.00%) (12701/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.5228) |  Loss2: (0.0000) | Acc: (81.00%) (13735/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.5241) |  Loss2: (0.0000) | Acc: (81.00%) (14773/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.5235) |  Loss2: (0.0000) | Acc: (81.00%) (15822/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.5253) |  Loss2: (0.0000) | Acc: (81.00%) (16859/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.5272) |  Loss2: (0.0000) | Acc: (81.00%) (17878/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.5252) |  Loss2: (0.0000) | Acc: (81.00%) (18947/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (20012/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (81.00%) (21041/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (22115/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (81.00%) (23176/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (24239/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.5202) |  Loss2: (0.0000) | Acc: (81.00%) (25285/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.5206) |  Loss2: (0.0000) | Acc: (81.00%) (26327/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.5217) |  Loss2: (0.0000) | Acc: (81.00%) (27360/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.5205) |  Loss2: (0.0000) | Acc: (81.00%) (28410/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (29494/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.5182) |  Loss2: (0.0000) | Acc: (81.00%) (30536/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (31599/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (32659/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (81.00%) (33692/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.5194) |  Loss2: (0.0000) | Acc: (81.00%) (34726/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.5188) |  Loss2: (0.0000) | Acc: (81.00%) (35788/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.5179) |  Loss2: (0.0000) | Acc: (82.00%) (36842/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (37891/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (81.00%) (38935/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.5180) |  Loss2: (0.0000) | Acc: (82.00%) (39990/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (41011/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_040.pth.tar'
# TEST : Loss: (0.6356) | Acc: (78.00%) (7858/10000)
percent tensor([0.4481], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4747], device='cuda:0')
percent tensor([0.4297], device='cuda:0')
percent tensor([0.4157], device='cuda:0')
percent tensor([0.2151], device='cuda:0')
Epoch: 41 | Batch_idx: 0 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4860) |  Loss2: (0.0000) | Acc: (83.00%) (1182/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4764) |  Loss2: (0.0000) | Acc: (84.00%) (2264/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4778) |  Loss2: (0.0000) | Acc: (83.00%) (3330/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4968) |  Loss2: (0.0000) | Acc: (83.00%) (4363/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (5439/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (83.00%) (6499/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (7562/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4957) |  Loss2: (0.0000) | Acc: (83.00%) (8631/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4988) |  Loss2: (0.0000) | Acc: (83.00%) (9683/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (83.00%) (10742/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (11775/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (12837/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (13893/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (14938/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (15987/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (17042/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (18116/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (19204/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (20245/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (21292/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (22343/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (23377/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (24436/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.5056) |  Loss2: (0.0000) | Acc: (82.00%) (25480/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (26560/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (27621/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (28677/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (29736/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (30792/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (31849/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.5026) |  Loss2: (0.0000) | Acc: (82.00%) (32915/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (33958/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (34997/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (36031/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (37087/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (38144/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.5070) |  Loss2: (0.0000) | Acc: (82.00%) (39176/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (40264/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (41309/50000)
# TEST : Loss: (0.5869) | Acc: (80.00%) (8032/10000)
percent tensor([0.4481], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4747], device='cuda:0')
percent tensor([0.4297], device='cuda:0')
percent tensor([0.4157], device='cuda:0')
percent tensor([0.2152], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (1156/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.5009) |  Loss2: (0.0000) | Acc: (82.00%) (2223/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (83.00%) (3307/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (83.00%) (4371/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4921) |  Loss2: (0.0000) | Acc: (83.00%) (5428/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (82.00%) (6478/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4906) |  Loss2: (0.0000) | Acc: (83.00%) (7567/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4870) |  Loss2: (0.0000) | Acc: (83.00%) (8643/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4837) |  Loss2: (0.0000) | Acc: (83.00%) (9738/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4875) |  Loss2: (0.0000) | Acc: (83.00%) (10793/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4861) |  Loss2: (0.0000) | Acc: (83.00%) (11856/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (13950/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4866) |  Loss2: (0.0000) | Acc: (83.00%) (15021/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (16094/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (83.00%) (17169/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (83.00%) (18239/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4878) |  Loss2: (0.0000) | Acc: (83.00%) (19290/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4873) |  Loss2: (0.0000) | Acc: (83.00%) (20365/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (21440/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4847) |  Loss2: (0.0000) | Acc: (83.00%) (22508/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (23570/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (83.00%) (24660/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4828) |  Loss2: (0.0000) | Acc: (83.00%) (25737/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4833) |  Loss2: (0.0000) | Acc: (83.00%) (26788/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4840) |  Loss2: (0.0000) | Acc: (83.00%) (27834/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (28907/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (83.00%) (29970/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4839) |  Loss2: (0.0000) | Acc: (83.00%) (31024/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4847) |  Loss2: (0.0000) | Acc: (83.00%) (32079/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4855) |  Loss2: (0.0000) | Acc: (83.00%) (33127/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4864) |  Loss2: (0.0000) | Acc: (83.00%) (34186/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4857) |  Loss2: (0.0000) | Acc: (83.00%) (35264/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4851) |  Loss2: (0.0000) | Acc: (83.00%) (36348/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4841) |  Loss2: (0.0000) | Acc: (83.00%) (37431/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4842) |  Loss2: (0.0000) | Acc: (83.00%) (38499/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4856) |  Loss2: (0.0000) | Acc: (83.00%) (39551/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4865) |  Loss2: (0.0000) | Acc: (83.00%) (40611/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4858) |  Loss2: (0.0000) | Acc: (83.00%) (41655/50000)
# TEST : Loss: (0.5781) | Acc: (80.00%) (8045/10000)
percent tensor([0.4482], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4747], device='cuda:0')
percent tensor([0.4297], device='cuda:0')
percent tensor([0.4157], device='cuda:0')
percent tensor([0.2153], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.5208) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4459) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4642) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4681) |  Loss2: (0.0000) | Acc: (84.00%) (3339/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4600) |  Loss2: (0.0000) | Acc: (84.00%) (4422/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (5500/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (84.00%) (6565/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (7637/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (8717/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4622) |  Loss2: (0.0000) | Acc: (84.00%) (9802/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4621) |  Loss2: (0.0000) | Acc: (84.00%) (10874/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4670) |  Loss2: (0.0000) | Acc: (83.00%) (11926/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (13005/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (14081/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4705) |  Loss2: (0.0000) | Acc: (83.00%) (15155/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (84.00%) (16246/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (84.00%) (17317/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4674) |  Loss2: (0.0000) | Acc: (83.00%) (18374/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (19439/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (20516/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4690) |  Loss2: (0.0000) | Acc: (83.00%) (21600/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4687) |  Loss2: (0.0000) | Acc: (83.00%) (22676/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (23737/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4690) |  Loss2: (0.0000) | Acc: (83.00%) (24807/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4684) |  Loss2: (0.0000) | Acc: (83.00%) (25894/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4675) |  Loss2: (0.0000) | Acc: (83.00%) (26971/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4663) |  Loss2: (0.0000) | Acc: (83.00%) (28046/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (29113/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (30192/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4670) |  Loss2: (0.0000) | Acc: (83.00%) (31257/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4678) |  Loss2: (0.0000) | Acc: (83.00%) (32316/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4676) |  Loss2: (0.0000) | Acc: (83.00%) (33398/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4682) |  Loss2: (0.0000) | Acc: (83.00%) (34463/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4686) |  Loss2: (0.0000) | Acc: (83.00%) (35528/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4683) |  Loss2: (0.0000) | Acc: (83.00%) (36611/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4681) |  Loss2: (0.0000) | Acc: (83.00%) (37685/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4684) |  Loss2: (0.0000) | Acc: (83.00%) (38763/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4677) |  Loss2: (0.0000) | Acc: (83.00%) (39838/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4675) |  Loss2: (0.0000) | Acc: (83.00%) (40916/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (83.00%) (41961/50000)
# TEST : Loss: (0.5566) | Acc: (80.00%) (8092/10000)
percent tensor([0.4482], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4298], device='cuda:0')
percent tensor([0.4158], device='cuda:0')
percent tensor([0.2154], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4528) |  Loss2: (0.0000) | Acc: (83.00%) (1176/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4701) |  Loss2: (0.0000) | Acc: (83.00%) (2238/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4754) |  Loss2: (0.0000) | Acc: (83.00%) (3298/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4739) |  Loss2: (0.0000) | Acc: (83.00%) (4382/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (5459/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4612) |  Loss2: (0.0000) | Acc: (83.00%) (6550/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (83.00%) (7623/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4623) |  Loss2: (0.0000) | Acc: (84.00%) (8711/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (84.00%) (9794/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (10881/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4591) |  Loss2: (0.0000) | Acc: (84.00%) (11948/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (13043/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (14142/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4578) |  Loss2: (0.0000) | Acc: (84.00%) (15197/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (16266/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (17350/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (18423/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4567) |  Loss2: (0.0000) | Acc: (84.00%) (19503/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (20598/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4572) |  Loss2: (0.0000) | Acc: (84.00%) (21665/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (22722/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4580) |  Loss2: (0.0000) | Acc: (84.00%) (23803/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4575) |  Loss2: (0.0000) | Acc: (84.00%) (24891/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (25968/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (27040/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (28124/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4560) |  Loss2: (0.0000) | Acc: (84.00%) (29196/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4564) |  Loss2: (0.0000) | Acc: (84.00%) (30270/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4557) |  Loss2: (0.0000) | Acc: (84.00%) (31366/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (32448/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (33527/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4536) |  Loss2: (0.0000) | Acc: (84.00%) (34631/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (35716/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4544) |  Loss2: (0.0000) | Acc: (84.00%) (36781/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (37859/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4545) |  Loss2: (0.0000) | Acc: (84.00%) (38936/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (39993/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4551) |  Loss2: (0.0000) | Acc: (84.00%) (41075/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4546) |  Loss2: (0.0000) | Acc: (84.00%) (42114/50000)
# TEST : Loss: (0.6098) | Acc: (79.00%) (7981/10000)
percent tensor([0.4482], device='cuda:0')
percent tensor([0.4981], device='cuda:0')
percent tensor([0.4957], device='cuda:0')
percent tensor([0.4925], device='cuda:0')
percent tensor([0.4748], device='cuda:0')
percent tensor([0.4298], device='cuda:0')
percent tensor([0.4158], device='cuda:0')
percent tensor([0.2154], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (85.00%) (1199/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4793) |  Loss2: (0.0000) | Acc: (83.00%) (2251/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (83.00%) (3299/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (4327/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.5201) |  Loss2: (0.0000) | Acc: (82.00%) (5375/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (82.00%) (6413/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (82.00%) (7463/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (8519/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.5203) |  Loss2: (0.0000) | Acc: (82.00%) (9566/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (82.00%) (10602/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.5210) |  Loss2: (0.0000) | Acc: (81.00%) (11632/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.5213) |  Loss2: (0.0000) | Acc: (81.00%) (12676/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (13729/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (14770/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (81.00%) (15826/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.5148) |  Loss2: (0.0000) | Acc: (82.00%) (16907/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.5153) |  Loss2: (0.0000) | Acc: (81.00%) (17941/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.5152) |  Loss2: (0.0000) | Acc: (82.00%) (19009/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.5147) |  Loss2: (0.0000) | Acc: (82.00%) (20057/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.5122) |  Loss2: (0.0000) | Acc: (82.00%) (21136/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.5095) |  Loss2: (0.0000) | Acc: (82.00%) (22217/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.5093) |  Loss2: (0.0000) | Acc: (82.00%) (23269/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (24332/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (25367/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.5114) |  Loss2: (0.0000) | Acc: (82.00%) (26414/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.5112) |  Loss2: (0.0000) | Acc: (82.00%) (27452/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.5112) |  Loss2: (0.0000) | Acc: (82.00%) (28507/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.5111) |  Loss2: (0.0000) | Acc: (82.00%) (29564/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (30632/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.5103) |  Loss2: (0.0000) | Acc: (82.00%) (31680/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (32726/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.5091) |  Loss2: (0.0000) | Acc: (82.00%) (33809/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.5081) |  Loss2: (0.0000) | Acc: (82.00%) (34885/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.5065) |  Loss2: (0.0000) | Acc: (82.00%) (35974/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.5059) |  Loss2: (0.0000) | Acc: (82.00%) (37039/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (38110/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (39150/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.5052) |  Loss2: (0.0000) | Acc: (82.00%) (40207/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (41239/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_045.pth.tar'
# TEST : Loss: (0.5504) | Acc: (81.00%) (8123/10000)
percent tensor([0.4486], device='cuda:0')
percent tensor([0.5024], device='cuda:0')
percent tensor([0.5011], device='cuda:0')
percent tensor([0.4675], device='cuda:0')
percent tensor([0.4722], device='cuda:0')
percent tensor([0.4450], device='cuda:0')
percent tensor([0.4227], device='cuda:0')
percent tensor([0.2119], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (1183/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4771) |  Loss2: (0.0000) | Acc: (83.00%) (2239/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4845) |  Loss2: (0.0000) | Acc: (83.00%) (3295/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4822) |  Loss2: (0.0000) | Acc: (83.00%) (4360/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4835) |  Loss2: (0.0000) | Acc: (83.00%) (5427/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4805) |  Loss2: (0.0000) | Acc: (83.00%) (6501/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4732) |  Loss2: (0.0000) | Acc: (83.00%) (7583/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4733) |  Loss2: (0.0000) | Acc: (83.00%) (8663/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4729) |  Loss2: (0.0000) | Acc: (83.00%) (9738/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4688) |  Loss2: (0.0000) | Acc: (83.00%) (10820/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (11881/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4692) |  Loss2: (0.0000) | Acc: (83.00%) (12943/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4677) |  Loss2: (0.0000) | Acc: (83.00%) (14020/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4668) |  Loss2: (0.0000) | Acc: (83.00%) (15095/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4680) |  Loss2: (0.0000) | Acc: (83.00%) (16166/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4669) |  Loss2: (0.0000) | Acc: (83.00%) (17253/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (83.00%) (18338/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4666) |  Loss2: (0.0000) | Acc: (83.00%) (19410/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4647) |  Loss2: (0.0000) | Acc: (83.00%) (20499/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4646) |  Loss2: (0.0000) | Acc: (83.00%) (21585/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4667) |  Loss2: (0.0000) | Acc: (83.00%) (22635/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4653) |  Loss2: (0.0000) | Acc: (83.00%) (23731/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4643) |  Loss2: (0.0000) | Acc: (83.00%) (24833/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4643) |  Loss2: (0.0000) | Acc: (84.00%) (25913/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4644) |  Loss2: (0.0000) | Acc: (83.00%) (26982/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (84.00%) (28070/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4640) |  Loss2: (0.0000) | Acc: (84.00%) (29138/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4633) |  Loss2: (0.0000) | Acc: (84.00%) (30225/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4631) |  Loss2: (0.0000) | Acc: (84.00%) (31297/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4624) |  Loss2: (0.0000) | Acc: (84.00%) (32386/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4628) |  Loss2: (0.0000) | Acc: (84.00%) (33461/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4629) |  Loss2: (0.0000) | Acc: (84.00%) (34533/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4625) |  Loss2: (0.0000) | Acc: (84.00%) (35610/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4614) |  Loss2: (0.0000) | Acc: (84.00%) (36697/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4618) |  Loss2: (0.0000) | Acc: (84.00%) (37780/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4611) |  Loss2: (0.0000) | Acc: (84.00%) (38865/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4616) |  Loss2: (0.0000) | Acc: (84.00%) (39924/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4615) |  Loss2: (0.0000) | Acc: (84.00%) (41006/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4617) |  Loss2: (0.0000) | Acc: (84.00%) (42042/50000)
# TEST : Loss: (0.5259) | Acc: (81.00%) (8187/10000)
percent tensor([0.4472], device='cuda:0')
percent tensor([0.5064], device='cuda:0')
percent tensor([0.5018], device='cuda:0')
percent tensor([0.4572], device='cuda:0')
percent tensor([0.4693], device='cuda:0')
percent tensor([0.4503], device='cuda:0')
percent tensor([0.4235], device='cuda:0')
percent tensor([0.2057], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (1203/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4400) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4464) |  Loss2: (0.0000) | Acc: (84.00%) (3350/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4513) |  Loss2: (0.0000) | Acc: (84.00%) (4418/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4583) |  Loss2: (0.0000) | Acc: (84.00%) (5493/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (6563/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (7638/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4569) |  Loss2: (0.0000) | Acc: (84.00%) (8720/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (9806/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4549) |  Loss2: (0.0000) | Acc: (84.00%) (10890/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (11961/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4587) |  Loss2: (0.0000) | Acc: (84.00%) (13036/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4586) |  Loss2: (0.0000) | Acc: (84.00%) (14105/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (15190/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (16263/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (17346/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4568) |  Loss2: (0.0000) | Acc: (84.00%) (18429/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4554) |  Loss2: (0.0000) | Acc: (84.00%) (19519/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4544) |  Loss2: (0.0000) | Acc: (84.00%) (20600/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4539) |  Loss2: (0.0000) | Acc: (84.00%) (21689/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4550) |  Loss2: (0.0000) | Acc: (84.00%) (22768/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4543) |  Loss2: (0.0000) | Acc: (84.00%) (23863/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4536) |  Loss2: (0.0000) | Acc: (84.00%) (24950/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4534) |  Loss2: (0.0000) | Acc: (84.00%) (26033/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4530) |  Loss2: (0.0000) | Acc: (84.00%) (27129/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (28231/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (29327/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4524) |  Loss2: (0.0000) | Acc: (84.00%) (30391/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4508) |  Loss2: (0.0000) | Acc: (84.00%) (31483/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (32578/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4497) |  Loss2: (0.0000) | Acc: (84.00%) (33655/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4487) |  Loss2: (0.0000) | Acc: (84.00%) (34746/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4480) |  Loss2: (0.0000) | Acc: (84.00%) (35831/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4481) |  Loss2: (0.0000) | Acc: (84.00%) (36913/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4482) |  Loss2: (0.0000) | Acc: (84.00%) (38002/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (39100/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4475) |  Loss2: (0.0000) | Acc: (84.00%) (40176/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (41260/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (42292/50000)
# TEST : Loss: (0.5144) | Acc: (82.00%) (8231/10000)
percent tensor([0.4483], device='cuda:0')
percent tensor([0.5075], device='cuda:0')
percent tensor([0.4983], device='cuda:0')
percent tensor([0.4548], device='cuda:0')
percent tensor([0.4669], device='cuda:0')
percent tensor([0.4524], device='cuda:0')
percent tensor([0.4221], device='cuda:0')
percent tensor([0.1989], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4075) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (85.00%) (1199/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4249) |  Loss2: (0.0000) | Acc: (85.00%) (2291/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (3391/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (4506/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4223) |  Loss2: (0.0000) | Acc: (85.00%) (5589/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4263) |  Loss2: (0.0000) | Acc: (85.00%) (6667/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (7751/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4326) |  Loss2: (0.0000) | Acc: (85.00%) (8838/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4299) |  Loss2: (0.0000) | Acc: (85.00%) (9932/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (11019/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (12091/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (13161/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (14230/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (15288/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (16369/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4417) |  Loss2: (0.0000) | Acc: (84.00%) (17462/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (18521/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (19611/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (20700/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (21786/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (22896/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (23948/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (25044/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (26114/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (27169/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (28241/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (29329/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4430) |  Loss2: (0.0000) | Acc: (84.00%) (30432/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4435) |  Loss2: (0.0000) | Acc: (84.00%) (31526/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (32600/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (33682/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4461) |  Loss2: (0.0000) | Acc: (84.00%) (34747/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4460) |  Loss2: (0.0000) | Acc: (84.00%) (35832/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4453) |  Loss2: (0.0000) | Acc: (84.00%) (36931/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (38018/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (39105/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (40194/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4439) |  Loss2: (0.0000) | Acc: (84.00%) (41268/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (42320/50000)
# TEST : Loss: (0.5062) | Acc: (82.00%) (8275/10000)
percent tensor([0.4459], device='cuda:0')
percent tensor([0.5058], device='cuda:0')
percent tensor([0.4985], device='cuda:0')
percent tensor([0.4534], device='cuda:0')
percent tensor([0.4656], device='cuda:0')
percent tensor([0.4519], device='cuda:0')
percent tensor([0.4197], device='cuda:0')
percent tensor([0.1930], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.4330) |  Loss2: (0.0000) | Acc: (84.00%) (2284/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.4284) |  Loss2: (0.0000) | Acc: (84.00%) (3367/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (4437/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (84.00%) (5524/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.4457) |  Loss2: (0.0000) | Acc: (84.00%) (6599/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (7679/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (8763/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (9844/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.4452) |  Loss2: (0.0000) | Acc: (84.00%) (10936/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.4425) |  Loss2: (0.0000) | Acc: (84.00%) (12036/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (13130/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (14241/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (15331/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (16403/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.4399) |  Loss2: (0.0000) | Acc: (84.00%) (17496/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (18595/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (19685/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (20805/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (85.00%) (21890/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (85.00%) (22972/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (85.00%) (24047/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (85.00%) (25159/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (85.00%) (26242/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.4364) |  Loss2: (0.0000) | Acc: (85.00%) (27337/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (85.00%) (28417/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (85.00%) (29492/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (85.00%) (30578/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.4376) |  Loss2: (0.0000) | Acc: (84.00%) (31641/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (84.00%) (32728/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (85.00%) (33841/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (85.00%) (34938/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.4357) |  Loss2: (0.0000) | Acc: (85.00%) (36038/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (85.00%) (37132/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (85.00%) (38216/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (39324/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (40418/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (85.00%) (41500/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (85.00%) (42575/50000)
# TEST : Loss: (0.5020) | Acc: (82.00%) (8286/10000)
percent tensor([0.4454], device='cuda:0')
percent tensor([0.5065], device='cuda:0')
percent tensor([0.4962], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4636], device='cuda:0')
percent tensor([0.4504], device='cuda:0')
percent tensor([0.4169], device='cuda:0')
percent tensor([0.1870], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (85.00%) (1203/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.4265) |  Loss2: (0.0000) | Acc: (85.00%) (2288/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (3396/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4251) |  Loss2: (0.0000) | Acc: (85.00%) (4479/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (5566/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4298) |  Loss2: (0.0000) | Acc: (85.00%) (6649/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (7726/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4384) |  Loss2: (0.0000) | Acc: (84.00%) (8806/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (9887/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4390) |  Loss2: (0.0000) | Acc: (84.00%) (10967/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (12035/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (13117/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (14218/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4418) |  Loss2: (0.0000) | Acc: (84.00%) (15275/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (16336/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (17433/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4424) |  Loss2: (0.0000) | Acc: (84.00%) (18509/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (19596/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4464) |  Loss2: (0.0000) | Acc: (84.00%) (20629/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4469) |  Loss2: (0.0000) | Acc: (84.00%) (21706/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4469) |  Loss2: (0.0000) | Acc: (84.00%) (22788/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4477) |  Loss2: (0.0000) | Acc: (84.00%) (23850/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (24951/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4468) |  Loss2: (0.0000) | Acc: (84.00%) (26031/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4469) |  Loss2: (0.0000) | Acc: (84.00%) (27121/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4480) |  Loss2: (0.0000) | Acc: (84.00%) (28187/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4481) |  Loss2: (0.0000) | Acc: (84.00%) (29273/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4475) |  Loss2: (0.0000) | Acc: (84.00%) (30359/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4477) |  Loss2: (0.0000) | Acc: (84.00%) (31455/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (32531/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (33601/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (34689/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (35770/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (36847/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4471) |  Loss2: (0.0000) | Acc: (84.00%) (37939/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (39017/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4473) |  Loss2: (0.0000) | Acc: (84.00%) (40096/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (84.00%) (41197/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4456) |  Loss2: (0.0000) | Acc: (84.00%) (42227/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_050.pth.tar'
# TEST : Loss: (0.5488) | Acc: (81.00%) (8141/10000)
percent tensor([0.4453], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4960], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4636], device='cuda:0')
percent tensor([0.4504], device='cuda:0')
percent tensor([0.4169], device='cuda:0')
percent tensor([0.1869], device='cuda:0')
Epoch: 51 | Batch_idx: 0 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4369) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (2275/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (3366/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (4463/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (5540/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (6625/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4405) |  Loss2: (0.0000) | Acc: (84.00%) (7714/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (85.00%) (8823/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (85.00%) (9925/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (11021/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (12097/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4309) |  Loss2: (0.0000) | Acc: (85.00%) (13187/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4374) |  Loss2: (0.0000) | Acc: (84.00%) (14247/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4355) |  Loss2: (0.0000) | Acc: (85.00%) (15343/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4355) |  Loss2: (0.0000) | Acc: (85.00%) (16439/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (85.00%) (17523/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (85.00%) (18633/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (19718/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4338) |  Loss2: (0.0000) | Acc: (85.00%) (20799/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (85.00%) (21885/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4346) |  Loss2: (0.0000) | Acc: (85.00%) (22978/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (85.00%) (24060/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (85.00%) (25156/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4357) |  Loss2: (0.0000) | Acc: (85.00%) (26242/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (85.00%) (27320/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (85.00%) (28418/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (85.00%) (29489/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4360) |  Loss2: (0.0000) | Acc: (85.00%) (30592/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4358) |  Loss2: (0.0000) | Acc: (85.00%) (31666/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4352) |  Loss2: (0.0000) | Acc: (85.00%) (32759/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (85.00%) (33851/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (85.00%) (34952/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (36048/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4320) |  Loss2: (0.0000) | Acc: (85.00%) (37163/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4332) |  Loss2: (0.0000) | Acc: (85.00%) (38233/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (85.00%) (39323/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4324) |  Loss2: (0.0000) | Acc: (85.00%) (40429/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4323) |  Loss2: (0.0000) | Acc: (85.00%) (41510/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4319) |  Loss2: (0.0000) | Acc: (85.00%) (42576/50000)
# TEST : Loss: (0.5770) | Acc: (80.00%) (8085/10000)
percent tensor([0.4453], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4960], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4636], device='cuda:0')
percent tensor([0.4504], device='cuda:0')
percent tensor([0.4169], device='cuda:0')
percent tensor([0.1870], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.2898) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (1207/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (2306/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (85.00%) (3411/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (4485/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (5577/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (6686/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (7785/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.4139) |  Loss2: (0.0000) | Acc: (85.00%) (8894/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (9999/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (11100/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (12198/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (13284/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (14393/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (85.00%) (15500/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (16579/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.4161) |  Loss2: (0.0000) | Acc: (85.00%) (17651/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (18763/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (19849/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (20936/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (22040/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (23126/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (24215/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (25319/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.4143) |  Loss2: (0.0000) | Acc: (85.00%) (26416/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.4144) |  Loss2: (0.0000) | Acc: (85.00%) (27510/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (28594/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (29699/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (30792/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (31872/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (32966/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (34057/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.4173) |  Loss2: (0.0000) | Acc: (85.00%) (35154/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.4165) |  Loss2: (0.0000) | Acc: (85.00%) (36256/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (37364/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.4157) |  Loss2: (0.0000) | Acc: (85.00%) (38483/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (39591/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (40693/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.4156) |  Loss2: (0.0000) | Acc: (85.00%) (41794/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.4151) |  Loss2: (0.0000) | Acc: (85.00%) (42850/50000)
# TEST : Loss: (0.5134) | Acc: (82.00%) (8278/10000)
percent tensor([0.4454], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4960], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4636], device='cuda:0')
percent tensor([0.4505], device='cuda:0')
percent tensor([0.4169], device='cuda:0')
percent tensor([0.1871], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.3291) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (2326/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (87.00%) (3456/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (4563/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (87.00%) (5680/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (6767/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (7870/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (8999/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (86.00%) (10091/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (86.00%) (11205/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (12296/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3941) |  Loss2: (0.0000) | Acc: (86.00%) (13398/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (14483/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3921) |  Loss2: (0.0000) | Acc: (86.00%) (15599/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (16703/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (17823/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3935) |  Loss2: (0.0000) | Acc: (86.00%) (18921/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (20001/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (21087/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3977) |  Loss2: (0.0000) | Acc: (86.00%) (22179/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (23274/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (24371/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (25486/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (26584/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.4007) |  Loss2: (0.0000) | Acc: (86.00%) (27673/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (28740/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (29842/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (30942/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.4019) |  Loss2: (0.0000) | Acc: (86.00%) (32057/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (33170/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (34282/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.4030) |  Loss2: (0.0000) | Acc: (86.00%) (35367/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (36454/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (37565/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (86.00%) (38670/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (39792/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.4018) |  Loss2: (0.0000) | Acc: (86.00%) (40899/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (86.00%) (42007/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (86.00%) (43070/50000)
# TEST : Loss: (0.5579) | Acc: (81.00%) (8156/10000)
percent tensor([0.4454], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4960], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4636], device='cuda:0')
percent tensor([0.4505], device='cuda:0')
percent tensor([0.4170], device='cuda:0')
percent tensor([0.1872], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3593) |  Loss2: (0.0000) | Acc: (87.00%) (2363/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (3480/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (4582/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3681) |  Loss2: (0.0000) | Acc: (87.00%) (5704/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (87.00%) (6800/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (86.00%) (7895/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3792) |  Loss2: (0.0000) | Acc: (86.00%) (8999/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3769) |  Loss2: (0.0000) | Acc: (86.00%) (10124/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3777) |  Loss2: (0.0000) | Acc: (86.00%) (11233/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (86.00%) (12356/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (86.00%) (13467/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (14556/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (15649/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3800) |  Loss2: (0.0000) | Acc: (86.00%) (16769/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (17872/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (18988/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (20112/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (21225/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (22338/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (23442/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (24550/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (25647/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (26754/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (27858/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3876) |  Loss2: (0.0000) | Acc: (86.00%) (28959/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (30057/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (31140/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (32211/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (33307/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (34405/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (35530/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3934) |  Loss2: (0.0000) | Acc: (86.00%) (36627/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (37713/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (38808/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3943) |  Loss2: (0.0000) | Acc: (86.00%) (39917/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (41035/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3932) |  Loss2: (0.0000) | Acc: (86.00%) (42141/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3939) |  Loss2: (0.0000) | Acc: (86.00%) (43190/50000)
# TEST : Loss: (0.6047) | Acc: (80.00%) (8034/10000)
percent tensor([0.4454], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4960], device='cuda:0')
percent tensor([0.4543], device='cuda:0')
percent tensor([0.4637], device='cuda:0')
percent tensor([0.4505], device='cuda:0')
percent tensor([0.4170], device='cuda:0')
percent tensor([0.1873], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.4347) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (1198/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.4694) |  Loss2: (0.0000) | Acc: (83.00%) (2255/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4867) |  Loss2: (0.0000) | Acc: (83.00%) (3299/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (82.00%) (4327/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (5357/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.5261) |  Loss2: (0.0000) | Acc: (81.00%) (6385/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (81.00%) (7444/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.5133) |  Loss2: (0.0000) | Acc: (82.00%) (8504/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (81.00%) (9530/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.5157) |  Loss2: (0.0000) | Acc: (81.00%) (10575/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.5157) |  Loss2: (0.0000) | Acc: (81.00%) (11629/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.5165) |  Loss2: (0.0000) | Acc: (81.00%) (12666/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (81.00%) (13696/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (81.00%) (14758/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (15804/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.5193) |  Loss2: (0.0000) | Acc: (81.00%) (16862/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (81.00%) (17926/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.5160) |  Loss2: (0.0000) | Acc: (81.00%) (18982/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.5142) |  Loss2: (0.0000) | Acc: (82.00%) (20051/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.5117) |  Loss2: (0.0000) | Acc: (82.00%) (21114/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.5104) |  Loss2: (0.0000) | Acc: (82.00%) (22175/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (23253/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.5061) |  Loss2: (0.0000) | Acc: (82.00%) (24308/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.5039) |  Loss2: (0.0000) | Acc: (82.00%) (25394/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (26473/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (27518/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.5019) |  Loss2: (0.0000) | Acc: (82.00%) (28580/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (29632/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4987) |  Loss2: (0.0000) | Acc: (82.00%) (30706/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4969) |  Loss2: (0.0000) | Acc: (82.00%) (31795/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4967) |  Loss2: (0.0000) | Acc: (82.00%) (32850/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4954) |  Loss2: (0.0000) | Acc: (82.00%) (33938/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (82.00%) (35013/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4946) |  Loss2: (0.0000) | Acc: (82.00%) (36079/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (82.00%) (37169/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4918) |  Loss2: (0.0000) | Acc: (82.00%) (38246/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (82.00%) (39344/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4895) |  Loss2: (0.0000) | Acc: (82.00%) (40404/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4881) |  Loss2: (0.0000) | Acc: (82.00%) (41462/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_055.pth.tar'
# TEST : Loss: (0.5410) | Acc: (81.00%) (8187/10000)
percent tensor([0.4250], device='cuda:0')
percent tensor([0.5052], device='cuda:0')
percent tensor([0.4869], device='cuda:0')
percent tensor([0.4451], device='cuda:0')
percent tensor([0.4694], device='cuda:0')
percent tensor([0.4339], device='cuda:0')
percent tensor([0.4269], device='cuda:0')
percent tensor([0.1860], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.4599) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.4645) |  Loss2: (0.0000) | Acc: (84.00%) (2266/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.4638) |  Loss2: (0.0000) | Acc: (83.00%) (3327/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (83.00%) (4405/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (5498/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (6583/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.4497) |  Loss2: (0.0000) | Acc: (84.00%) (7653/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.4476) |  Loss2: (0.0000) | Acc: (84.00%) (8745/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (9850/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (10935/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (12006/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (13111/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.4412) |  Loss2: (0.0000) | Acc: (84.00%) (14177/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (15261/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (16361/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (17445/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (18558/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (19640/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.4382) |  Loss2: (0.0000) | Acc: (84.00%) (20727/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.4392) |  Loss2: (0.0000) | Acc: (84.00%) (21802/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.4399) |  Loss2: (0.0000) | Acc: (84.00%) (22888/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.4393) |  Loss2: (0.0000) | Acc: (84.00%) (23978/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (84.00%) (25090/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (26174/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.4368) |  Loss2: (0.0000) | Acc: (84.00%) (27259/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (28344/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.4380) |  Loss2: (0.0000) | Acc: (84.00%) (29415/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.4372) |  Loss2: (0.0000) | Acc: (84.00%) (30498/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (31602/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.4365) |  Loss2: (0.0000) | Acc: (84.00%) (32696/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (84.00%) (33771/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.4357) |  Loss2: (0.0000) | Acc: (84.00%) (34858/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (84.00%) (35947/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (84.00%) (37041/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.4335) |  Loss2: (0.0000) | Acc: (84.00%) (38157/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.4325) |  Loss2: (0.0000) | Acc: (84.00%) (39256/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (84.00%) (40348/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (84.00%) (41442/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.4312) |  Loss2: (0.0000) | Acc: (84.00%) (42496/50000)
# TEST : Loss: (0.5048) | Acc: (83.00%) (8317/10000)
percent tensor([0.4224], device='cuda:0')
percent tensor([0.5082], device='cuda:0')
percent tensor([0.4849], device='cuda:0')
percent tensor([0.4458], device='cuda:0')
percent tensor([0.4676], device='cuda:0')
percent tensor([0.4261], device='cuda:0')
percent tensor([0.4272], device='cuda:0')
percent tensor([0.1820], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.4187) |  Loss2: (0.0000) | Acc: (85.00%) (1198/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.4120) |  Loss2: (0.0000) | Acc: (85.00%) (2300/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (3382/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.4186) |  Loss2: (0.0000) | Acc: (85.00%) (4490/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.4179) |  Loss2: (0.0000) | Acc: (85.00%) (5578/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.4202) |  Loss2: (0.0000) | Acc: (85.00%) (6674/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.4265) |  Loss2: (0.0000) | Acc: (85.00%) (7738/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.4263) |  Loss2: (0.0000) | Acc: (85.00%) (8826/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (9908/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.4229) |  Loss2: (0.0000) | Acc: (85.00%) (11012/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (12116/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.4216) |  Loss2: (0.0000) | Acc: (85.00%) (13202/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.4209) |  Loss2: (0.0000) | Acc: (85.00%) (14306/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.4193) |  Loss2: (0.0000) | Acc: (85.00%) (15421/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (16524/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (17615/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.4185) |  Loss2: (0.0000) | Acc: (85.00%) (18719/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.4184) |  Loss2: (0.0000) | Acc: (85.00%) (19807/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.4196) |  Loss2: (0.0000) | Acc: (85.00%) (20878/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.4177) |  Loss2: (0.0000) | Acc: (85.00%) (21996/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (23105/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.4152) |  Loss2: (0.0000) | Acc: (85.00%) (24216/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.4141) |  Loss2: (0.0000) | Acc: (85.00%) (25318/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.4154) |  Loss2: (0.0000) | Acc: (85.00%) (26390/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (27515/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (28623/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (29725/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (30824/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.4121) |  Loss2: (0.0000) | Acc: (85.00%) (31917/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (33028/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (34140/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (85.00%) (35214/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.4123) |  Loss2: (0.0000) | Acc: (85.00%) (36310/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.4122) |  Loss2: (0.0000) | Acc: (85.00%) (37399/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (85.00%) (38513/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.4107) |  Loss2: (0.0000) | Acc: (85.00%) (39606/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.4098) |  Loss2: (0.0000) | Acc: (85.00%) (40712/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (41827/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.4089) |  Loss2: (0.0000) | Acc: (85.00%) (42892/50000)
# TEST : Loss: (0.4906) | Acc: (83.00%) (8340/10000)
percent tensor([0.4214], device='cuda:0')
percent tensor([0.5099], device='cuda:0')
percent tensor([0.4840], device='cuda:0')
percent tensor([0.4448], device='cuda:0')
percent tensor([0.4622], device='cuda:0')
percent tensor([0.4213], device='cuda:0')
percent tensor([0.4259], device='cuda:0')
percent tensor([0.1775], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (85.00%) (1206/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (2296/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.4038) |  Loss2: (0.0000) | Acc: (85.00%) (3402/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.4019) |  Loss2: (0.0000) | Acc: (85.00%) (4505/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (5615/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (6703/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (85.00%) (7805/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (85.00%) (8914/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (10025/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (85.00%) (11104/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (85.00%) (12210/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (85.00%) (13311/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.4003) |  Loss2: (0.0000) | Acc: (86.00%) (14431/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (85.00%) (15517/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (16629/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.4005) |  Loss2: (0.0000) | Acc: (86.00%) (17740/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3990) |  Loss2: (0.0000) | Acc: (86.00%) (18848/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (19949/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3980) |  Loss2: (0.0000) | Acc: (86.00%) (21079/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3975) |  Loss2: (0.0000) | Acc: (86.00%) (22180/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3976) |  Loss2: (0.0000) | Acc: (86.00%) (23285/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (24378/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (25492/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (26574/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (86.00%) (27682/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.4003) |  Loss2: (0.0000) | Acc: (86.00%) (28808/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (29911/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (31009/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (32122/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.4006) |  Loss2: (0.0000) | Acc: (86.00%) (33202/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (34310/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (35440/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3984) |  Loss2: (0.0000) | Acc: (86.00%) (36547/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (37649/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (38745/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3989) |  Loss2: (0.0000) | Acc: (86.00%) (39849/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (40975/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (86.00%) (42079/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3985) |  Loss2: (0.0000) | Acc: (86.00%) (43151/50000)
# TEST : Loss: (0.4844) | Acc: (83.00%) (8380/10000)
percent tensor([0.4227], device='cuda:0')
percent tensor([0.5108], device='cuda:0')
percent tensor([0.4828], device='cuda:0')
percent tensor([0.4454], device='cuda:0')
percent tensor([0.4584], device='cuda:0')
percent tensor([0.4194], device='cuda:0')
percent tensor([0.4231], device='cuda:0')
percent tensor([0.1730], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.4684) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (1201/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.4019) |  Loss2: (0.0000) | Acc: (86.00%) (2315/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (3445/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (87.00%) (4569/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (86.00%) (5672/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (6781/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (7885/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (8989/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3832) |  Loss2: (0.0000) | Acc: (86.00%) (10100/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3868) |  Loss2: (0.0000) | Acc: (86.00%) (11185/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (12301/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (13401/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (14521/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (15626/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (16730/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (17833/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3887) |  Loss2: (0.0000) | Acc: (86.00%) (18946/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (20047/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3891) |  Loss2: (0.0000) | Acc: (86.00%) (21157/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (22261/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (23355/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (24477/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (25588/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (26707/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3874) |  Loss2: (0.0000) | Acc: (86.00%) (27832/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (28928/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3890) |  Loss2: (0.0000) | Acc: (86.00%) (30014/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3891) |  Loss2: (0.0000) | Acc: (86.00%) (31130/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (32205/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (33319/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (34412/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3923) |  Loss2: (0.0000) | Acc: (86.00%) (35508/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (36629/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (37730/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3931) |  Loss2: (0.0000) | Acc: (86.00%) (38825/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (39948/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (41036/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (42151/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (43208/50000)
# TEST : Loss: (0.4793) | Acc: (84.00%) (8401/10000)
percent tensor([0.4217], device='cuda:0')
percent tensor([0.5109], device='cuda:0')
percent tensor([0.4817], device='cuda:0')
percent tensor([0.4439], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.4187], device='cuda:0')
percent tensor([0.4197], device='cuda:0')
percent tensor([0.1689], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (2338/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (87.00%) (3459/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3942) |  Loss2: (0.0000) | Acc: (86.00%) (4525/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3916) |  Loss2: (0.0000) | Acc: (86.00%) (5639/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (6748/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3927) |  Loss2: (0.0000) | Acc: (86.00%) (7843/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (8945/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (10058/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (11190/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (12296/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (13388/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (86.00%) (14493/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (15620/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (16733/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3903) |  Loss2: (0.0000) | Acc: (86.00%) (17815/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3910) |  Loss2: (0.0000) | Acc: (86.00%) (18920/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3905) |  Loss2: (0.0000) | Acc: (86.00%) (20029/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (21146/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3900) |  Loss2: (0.0000) | Acc: (86.00%) (22256/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (23371/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (24470/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (25580/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3902) |  Loss2: (0.0000) | Acc: (86.00%) (26684/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (27790/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (28893/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (29979/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3906) |  Loss2: (0.0000) | Acc: (86.00%) (31088/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (32211/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3892) |  Loss2: (0.0000) | Acc: (86.00%) (33327/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (34443/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3881) |  Loss2: (0.0000) | Acc: (86.00%) (35557/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (36668/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3875) |  Loss2: (0.0000) | Acc: (86.00%) (37789/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3884) |  Loss2: (0.0000) | Acc: (86.00%) (38884/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (39973/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (41094/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (42199/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3901) |  Loss2: (0.0000) | Acc: (86.00%) (43270/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_060.pth.tar'
# TEST : Loss: (0.5263) | Acc: (82.00%) (8241/10000)
percent tensor([0.4218], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4820], device='cuda:0')
percent tensor([0.4438], device='cuda:0')
percent tensor([0.4560], device='cuda:0')
percent tensor([0.4188], device='cuda:0')
percent tensor([0.4197], device='cuda:0')
percent tensor([0.1689], device='cuda:0')
Epoch: 61 | Batch_idx: 0 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (1230/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (2343/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3568) |  Loss2: (0.0000) | Acc: (87.00%) (3473/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (5708/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (6816/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (7941/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (9065/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (10160/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (11279/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3695) |  Loss2: (0.0000) | Acc: (87.00%) (12397/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3670) |  Loss2: (0.0000) | Acc: (87.00%) (13522/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (14664/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3668) |  Loss2: (0.0000) | Acc: (87.00%) (15765/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3691) |  Loss2: (0.0000) | Acc: (87.00%) (16873/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (17987/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3676) |  Loss2: (0.0000) | Acc: (87.00%) (19124/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (20226/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (21350/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (22476/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3692) |  Loss2: (0.0000) | Acc: (87.00%) (23585/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3700) |  Loss2: (0.0000) | Acc: (87.00%) (24685/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (25788/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (26898/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3711) |  Loss2: (0.0000) | Acc: (87.00%) (28026/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3731) |  Loss2: (0.0000) | Acc: (87.00%) (29117/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3738) |  Loss2: (0.0000) | Acc: (87.00%) (30203/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3741) |  Loss2: (0.0000) | Acc: (87.00%) (31318/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (32416/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (33528/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3729) |  Loss2: (0.0000) | Acc: (87.00%) (34659/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (35793/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (36888/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3732) |  Loss2: (0.0000) | Acc: (87.00%) (37998/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3729) |  Loss2: (0.0000) | Acc: (87.00%) (39106/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (40221/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (41337/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (42440/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3718) |  Loss2: (0.0000) | Acc: (87.00%) (43519/50000)
# TEST : Loss: (0.5859) | Acc: (80.00%) (8078/10000)
percent tensor([0.4218], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4820], device='cuda:0')
percent tensor([0.4438], device='cuda:0')
percent tensor([0.4560], device='cuda:0')
percent tensor([0.4188], device='cuda:0')
percent tensor([0.4197], device='cuda:0')
percent tensor([0.1690], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (1257/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (2387/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (3499/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (88.00%) (4622/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (87.00%) (5742/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (6864/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (7986/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (9103/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (10244/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3536) |  Loss2: (0.0000) | Acc: (87.00%) (11347/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (12459/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (13565/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (14679/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (15794/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (16911/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (18012/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (19133/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (20235/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (21358/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (22475/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (23597/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (24728/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (25860/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (26973/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3656) |  Loss2: (0.0000) | Acc: (87.00%) (28076/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (29207/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (30347/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (31461/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (32586/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (33697/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (34841/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (35969/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (37085/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (38191/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (39330/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (40442/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (41543/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (42648/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (43719/50000)
# TEST : Loss: (0.4789) | Acc: (83.00%) (8386/10000)
percent tensor([0.4219], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4820], device='cuda:0')
percent tensor([0.4438], device='cuda:0')
percent tensor([0.4560], device='cuda:0')
percent tensor([0.4188], device='cuda:0')
percent tensor([0.4198], device='cuda:0')
percent tensor([0.1690], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3395) |  Loss2: (0.0000) | Acc: (88.00%) (3522/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (4637/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (88.00%) (5752/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (6876/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (8015/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3553) |  Loss2: (0.0000) | Acc: (87.00%) (9120/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3525) |  Loss2: (0.0000) | Acc: (88.00%) (10263/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (88.00%) (11387/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (12493/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (13615/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (14733/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (15852/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3539) |  Loss2: (0.0000) | Acc: (87.00%) (16967/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3516) |  Loss2: (0.0000) | Acc: (87.00%) (18107/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3540) |  Loss2: (0.0000) | Acc: (87.00%) (19210/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (20330/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (21453/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (22586/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (23740/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (24848/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (25963/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3528) |  Loss2: (0.0000) | Acc: (87.00%) (27083/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (28192/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29317/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (30444/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (31582/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3518) |  Loss2: (0.0000) | Acc: (87.00%) (32713/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (33860/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (34964/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (36095/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (37217/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (38348/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (39478/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3493) |  Loss2: (0.0000) | Acc: (87.00%) (40600/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (41728/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (42847/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3496) |  Loss2: (0.0000) | Acc: (87.00%) (43941/50000)
# TEST : Loss: (0.5376) | Acc: (82.00%) (8217/10000)
percent tensor([0.4219], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4820], device='cuda:0')
percent tensor([0.4438], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.4189], device='cuda:0')
percent tensor([0.4198], device='cuda:0')
percent tensor([0.1691], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (2389/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (89.00%) (3542/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (89.00%) (4671/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (5795/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (6930/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (8071/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (9188/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (10330/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (11454/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3401) |  Loss2: (0.0000) | Acc: (88.00%) (12564/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (13676/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (14794/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (15913/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (17033/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (88.00%) (18170/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (19302/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (20426/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (21551/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (88.00%) (22674/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (23798/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (24905/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (26040/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (27170/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (28283/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (87.00%) (29391/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (87.00%) (30504/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (31628/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (32771/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (33920/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (35033/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (36166/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (37301/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (38407/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3489) |  Loss2: (0.0000) | Acc: (87.00%) (39519/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3488) |  Loss2: (0.0000) | Acc: (87.00%) (40629/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3494) |  Loss2: (0.0000) | Acc: (87.00%) (41760/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (42887/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (43981/50000)
# TEST : Loss: (0.4675) | Acc: (84.00%) (8427/10000)
percent tensor([0.4219], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4820], device='cuda:0')
percent tensor([0.4439], device='cuda:0')
percent tensor([0.4561], device='cuda:0')
percent tensor([0.4189], device='cuda:0')
percent tensor([0.4198], device='cuda:0')
percent tensor([0.1692], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (88.00%) (1243/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3709) |  Loss2: (0.0000) | Acc: (87.00%) (2358/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.3882) |  Loss2: (0.0000) | Acc: (86.00%) (3437/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (4540/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (5632/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4003) |  Loss2: (0.0000) | Acc: (86.00%) (6718/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.3969) |  Loss2: (0.0000) | Acc: (86.00%) (7832/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (8931/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (86.00%) (10023/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4004) |  Loss2: (0.0000) | Acc: (85.00%) (11117/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (85.00%) (12206/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4014) |  Loss2: (0.0000) | Acc: (85.00%) (13309/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (14412/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4000) |  Loss2: (0.0000) | Acc: (86.00%) (15523/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.3983) |  Loss2: (0.0000) | Acc: (86.00%) (16643/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (17770/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (18875/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.3928) |  Loss2: (0.0000) | Acc: (86.00%) (20000/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.3912) |  Loss2: (0.0000) | Acc: (86.00%) (21129/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (22224/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (23338/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (24451/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.3895) |  Loss2: (0.0000) | Acc: (86.00%) (25564/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.3896) |  Loss2: (0.0000) | Acc: (86.00%) (26675/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (27801/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (86.00%) (28925/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (30036/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (31160/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (32267/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (33374/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.3836) |  Loss2: (0.0000) | Acc: (86.00%) (34489/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (35606/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.3817) |  Loss2: (0.0000) | Acc: (86.00%) (36723/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (37839/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.3826) |  Loss2: (0.0000) | Acc: (86.00%) (38940/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (40059/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (41172/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.3806) |  Loss2: (0.0000) | Acc: (86.00%) (42313/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (43381/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_065.pth.tar'
# TEST : Loss: (0.4724) | Acc: (84.00%) (8415/10000)
percent tensor([0.4218], device='cuda:0')
percent tensor([0.5291], device='cuda:0')
percent tensor([0.4794], device='cuda:0')
percent tensor([0.4455], device='cuda:0')
percent tensor([0.4484], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.4171], device='cuda:0')
percent tensor([0.1666], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.5526) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (1214/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3843) |  Loss2: (0.0000) | Acc: (86.00%) (2322/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (3447/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (4575/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3720) |  Loss2: (0.0000) | Acc: (87.00%) (5681/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3673) |  Loss2: (0.0000) | Acc: (87.00%) (6803/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (7928/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (9064/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (10188/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3605) |  Loss2: (0.0000) | Acc: (87.00%) (11292/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3596) |  Loss2: (0.0000) | Acc: (87.00%) (12421/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (13533/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (14678/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (15809/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3553) |  Loss2: (0.0000) | Acc: (87.00%) (16937/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3571) |  Loss2: (0.0000) | Acc: (87.00%) (18054/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (19182/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (20289/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (21394/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (22513/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (23631/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (24756/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (25878/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (27007/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (28130/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (87.00%) (29251/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3555) |  Loss2: (0.0000) | Acc: (87.00%) (30378/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3543) |  Loss2: (0.0000) | Acc: (87.00%) (31524/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3553) |  Loss2: (0.0000) | Acc: (87.00%) (32625/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (33740/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3563) |  Loss2: (0.0000) | Acc: (87.00%) (34853/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3571) |  Loss2: (0.0000) | Acc: (87.00%) (35964/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (37113/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3557) |  Loss2: (0.0000) | Acc: (87.00%) (38228/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (39360/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (40487/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (41612/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (42758/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3549) |  Loss2: (0.0000) | Acc: (87.00%) (43846/50000)
# TEST : Loss: (0.4550) | Acc: (84.00%) (8450/10000)
percent tensor([0.4209], device='cuda:0')
percent tensor([0.5330], device='cuda:0')
percent tensor([0.4805], device='cuda:0')
percent tensor([0.4447], device='cuda:0')
percent tensor([0.4435], device='cuda:0')
percent tensor([0.3989], device='cuda:0')
percent tensor([0.4128], device='cuda:0')
percent tensor([0.1630], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (2377/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3396) |  Loss2: (0.0000) | Acc: (87.00%) (3488/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (4626/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (5766/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3319) |  Loss2: (0.0000) | Acc: (88.00%) (6898/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3372) |  Loss2: (0.0000) | Acc: (88.00%) (8021/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (9155/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (10282/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (11413/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (88.00%) (12515/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (13649/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (14780/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (15905/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3420) |  Loss2: (0.0000) | Acc: (88.00%) (17043/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (88.00%) (18171/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (19310/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (88.00%) (20439/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (21576/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (22719/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (88.00%) (23832/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (24962/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (26090/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (88.00%) (27234/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (28368/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (29498/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (30615/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (31722/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (32861/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (33996/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (35123/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (36256/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3398) |  Loss2: (0.0000) | Acc: (88.00%) (37397/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (38543/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (39650/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (40768/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (41897/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3412) |  Loss2: (0.0000) | Acc: (88.00%) (43029/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (44116/50000)
# TEST : Loss: (0.4437) | Acc: (84.00%) (8495/10000)
percent tensor([0.4210], device='cuda:0')
percent tensor([0.5320], device='cuda:0')
percent tensor([0.4800], device='cuda:0')
percent tensor([0.4450], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.3933], device='cuda:0')
percent tensor([0.4082], device='cuda:0')
percent tensor([0.1590], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (87.00%) (1229/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (87.00%) (2354/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3304) |  Loss2: (0.0000) | Acc: (88.00%) (3515/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (4659/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (5788/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (6920/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (8043/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (9161/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (88.00%) (10300/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (11450/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (12597/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (13739/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (14862/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (15995/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (17123/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (18254/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (19365/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (20513/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (21645/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (22762/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (23909/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (25048/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (26177/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (27318/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (28469/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3347) |  Loss2: (0.0000) | Acc: (88.00%) (29610/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (30757/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (31889/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (33016/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (34154/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35292/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (36428/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (37575/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (38700/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (39835/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3342) |  Loss2: (0.0000) | Acc: (88.00%) (40961/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (42105/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (43218/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (44316/50000)
# TEST : Loss: (0.4434) | Acc: (84.00%) (8480/10000)
percent tensor([0.4199], device='cuda:0')
percent tensor([0.5315], device='cuda:0')
percent tensor([0.4804], device='cuda:0')
percent tensor([0.4441], device='cuda:0')
percent tensor([0.4419], device='cuda:0')
percent tensor([0.3903], device='cuda:0')
percent tensor([0.4040], device='cuda:0')
percent tensor([0.1553], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (2390/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (3517/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (4651/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (5792/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3390) |  Loss2: (0.0000) | Acc: (88.00%) (6926/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (8038/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (9174/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (10314/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (11458/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (12596/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (13716/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (14842/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (15993/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (17115/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (18231/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3371) |  Loss2: (0.0000) | Acc: (88.00%) (19364/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (20508/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3369) |  Loss2: (0.0000) | Acc: (88.00%) (21632/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (22781/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (23915/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (25049/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (26183/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (27318/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (28448/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (29603/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (30742/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (31873/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3316) |  Loss2: (0.0000) | Acc: (88.00%) (33023/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (34161/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (35280/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (36432/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (37568/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (38711/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (39846/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (40984/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (42100/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (43252/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (44342/50000)
# TEST : Loss: (0.4392) | Acc: (85.00%) (8520/10000)
percent tensor([0.4205], device='cuda:0')
percent tensor([0.5305], device='cuda:0')
percent tensor([0.4798], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.4407], device='cuda:0')
percent tensor([0.3882], device='cuda:0')
percent tensor([0.4018], device='cuda:0')
percent tensor([0.1518], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (2387/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (89.00%) (3540/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (4697/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (89.00%) (5817/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (6947/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3255) |  Loss2: (0.0000) | Acc: (89.00%) (8090/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (9223/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (10340/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (11463/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (12573/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (88.00%) (13682/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3383) |  Loss2: (0.0000) | Acc: (88.00%) (14816/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (15928/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (17055/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (18185/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (19318/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3424) |  Loss2: (0.0000) | Acc: (88.00%) (20445/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (21580/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (22710/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (23842/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (88.00%) (24986/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (26108/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (27234/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (28358/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (29491/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (30604/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (31712/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (32861/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3417) |  Loss2: (0.0000) | Acc: (88.00%) (33985/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (35111/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (36251/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (37383/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (38503/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (39640/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3421) |  Loss2: (0.0000) | Acc: (88.00%) (40753/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (41876/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (43004/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (44090/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_070.pth.tar'
# TEST : Loss: (0.5306) | Acc: (82.00%) (8278/10000)
percent tensor([0.4203], device='cuda:0')
percent tensor([0.5302], device='cuda:0')
percent tensor([0.4797], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.4408], device='cuda:0')
percent tensor([0.3884], device='cuda:0')
percent tensor([0.4018], device='cuda:0')
percent tensor([0.1518], device='cuda:0')
Epoch: 71 | Batch_idx: 0 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (89.00%) (1262/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (88.00%) (2392/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (88.00%) (3520/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (4646/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (5793/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (88.00%) (6941/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (8077/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3160) |  Loss2: (0.0000) | Acc: (88.00%) (9220/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (88.00%) (10347/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (11507/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (12641/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (13764/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (14894/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (16011/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (17134/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3274) |  Loss2: (0.0000) | Acc: (88.00%) (18266/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (19390/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (20516/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (21636/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3325) |  Loss2: (0.0000) | Acc: (88.00%) (22779/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (23887/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3338) |  Loss2: (0.0000) | Acc: (88.00%) (25034/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (26195/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3312) |  Loss2: (0.0000) | Acc: (88.00%) (27330/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (28463/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3313) |  Loss2: (0.0000) | Acc: (88.00%) (29600/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (30736/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (31874/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3320) |  Loss2: (0.0000) | Acc: (88.00%) (33006/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (34112/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (35250/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (36382/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (37517/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3306) |  Loss2: (0.0000) | Acc: (88.00%) (38678/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (39817/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (40952/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (42086/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (43229/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (44319/50000)
# TEST : Loss: (0.4601) | Acc: (84.00%) (8427/10000)
percent tensor([0.4204], device='cuda:0')
percent tensor([0.5302], device='cuda:0')
percent tensor([0.4797], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.4409], device='cuda:0')
percent tensor([0.3885], device='cuda:0')
percent tensor([0.4019], device='cuda:0')
percent tensor([0.1519], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (89.00%) (1257/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (3538/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (4679/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (88.00%) (5805/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.3074) |  Loss2: (0.0000) | Acc: (89.00%) (6958/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (88.00%) (8083/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (9229/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (10371/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (88.00%) (11500/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (88.00%) (12623/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (13758/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (14912/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (88.00%) (16046/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (17180/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (18310/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (19451/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (20594/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (21733/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (88.00%) (22873/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (23995/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3228) |  Loss2: (0.0000) | Acc: (88.00%) (25126/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (26277/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (27428/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3211) |  Loss2: (0.0000) | Acc: (88.00%) (28578/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (29719/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (88.00%) (30851/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (31983/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (88.00%) (33119/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (34250/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (88.00%) (35398/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (88.00%) (36530/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (88.00%) (37670/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (88.00%) (38816/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (39941/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (88.00%) (41075/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.3197) |  Loss2: (0.0000) | Acc: (88.00%) (42221/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (43372/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (44452/50000)
# TEST : Loss: (0.4872) | Acc: (83.00%) (8368/10000)
percent tensor([0.4204], device='cuda:0')
percent tensor([0.5302], device='cuda:0')
percent tensor([0.4797], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.4409], device='cuda:0')
percent tensor([0.3885], device='cuda:0')
percent tensor([0.4019], device='cuda:0')
percent tensor([0.1520], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3645) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (2403/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (3557/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (4717/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (7011/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (8169/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (9304/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (10435/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (11579/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (12713/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (13849/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (14982/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (16119/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (17266/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (18412/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (19539/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (20683/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (89.00%) (21820/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (22955/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (24085/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (25222/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (26368/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (27501/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.3138) |  Loss2: (0.0000) | Acc: (89.00%) (28637/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (29783/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (30918/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (32056/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.3154) |  Loss2: (0.0000) | Acc: (89.00%) (33174/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (34297/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (35444/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (36600/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (37740/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (38880/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (40018/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (41157/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (42298/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (43447/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (44521/50000)
# TEST : Loss: (0.5026) | Acc: (83.00%) (8386/10000)
percent tensor([0.4204], device='cuda:0')
percent tensor([0.5302], device='cuda:0')
percent tensor([0.4797], device='cuda:0')
percent tensor([0.4426], device='cuda:0')
percent tensor([0.4409], device='cuda:0')
percent tensor([0.3885], device='cuda:0')
percent tensor([0.4019], device='cuda:0')
percent tensor([0.1521], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (2425/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (3557/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (4716/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (5856/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (7018/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (8179/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (9327/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (10475/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (11616/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2978) |  Loss2: (0.0000) | Acc: (89.00%) (12747/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (13883/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (15000/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (16145/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (17287/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (18432/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (19588/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (20742/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (21888/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (23031/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (24177/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (25310/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (26468/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (27620/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (28772/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (29910/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (31071/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (32216/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (33359/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.3043) |  Loss2: (0.0000) | Acc: (89.00%) (34492/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (35628/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (36770/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (37909/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (39061/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (40220/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (41361/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (42499/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.3070) |  Loss2: (0.0000) | Acc: (89.00%) (43629/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (44745/50000)
# TEST : Loss: (0.5022) | Acc: (83.00%) (8364/10000)
percent tensor([0.4205], device='cuda:0')
percent tensor([0.5302], device='cuda:0')
percent tensor([0.4797], device='cuda:0')
percent tensor([0.4427], device='cuda:0')
percent tensor([0.4409], device='cuda:0')
percent tensor([0.3886], device='cuda:0')
percent tensor([0.4020], device='cuda:0')
percent tensor([0.1521], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (87.00%) (1235/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (86.00%) (3452/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (86.00%) (4543/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (86.00%) (5651/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (86.00%) (6773/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (86.00%) (7891/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (9029/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (10158/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (11266/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (12388/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (13510/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (14628/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (15757/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (16887/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (18000/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3598) |  Loss2: (0.0000) | Acc: (87.00%) (19108/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (20245/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (21355/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (22479/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (87.00%) (23612/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (24748/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (25884/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (27019/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (28156/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (29288/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (30413/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3525) |  Loss2: (0.0000) | Acc: (87.00%) (31530/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (32644/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (33779/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (34916/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (36041/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (37154/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (87.00%) (38297/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (39416/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (87.00%) (40545/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (87.00%) (41699/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3482) |  Loss2: (0.0000) | Acc: (87.00%) (42835/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (43951/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_075.pth.tar'
# TEST : Loss: (0.4803) | Acc: (83.00%) (8383/10000)
percent tensor([0.4102], device='cuda:0')
percent tensor([0.5296], device='cuda:0')
percent tensor([0.4738], device='cuda:0')
percent tensor([0.4361], device='cuda:0')
percent tensor([0.4475], device='cuda:0')
percent tensor([0.3866], device='cuda:0')
percent tensor([0.4115], device='cuda:0')
percent tensor([0.1497], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (88.00%) (1253/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (88.00%) (2392/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (3538/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (5787/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3257) |  Loss2: (0.0000) | Acc: (88.00%) (6910/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (8028/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3266) |  Loss2: (0.0000) | Acc: (88.00%) (9174/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (10305/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (11443/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (12587/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (13718/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (14861/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (15999/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (17135/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3277) |  Loss2: (0.0000) | Acc: (88.00%) (18270/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (19439/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (88.00%) (20576/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (21719/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3241) |  Loss2: (0.0000) | Acc: (88.00%) (22831/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3230) |  Loss2: (0.0000) | Acc: (88.00%) (23979/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (25134/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (88.00%) (26270/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (88.00%) (27411/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (28546/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (29701/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3209) |  Loss2: (0.0000) | Acc: (88.00%) (30823/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3214) |  Loss2: (0.0000) | Acc: (88.00%) (31970/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3200) |  Loss2: (0.0000) | Acc: (88.00%) (33133/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (34286/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (89.00%) (35434/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (88.00%) (36566/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (88.00%) (37680/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (38853/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (88.00%) (39980/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (41147/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (88.00%) (42263/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (43419/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (44517/50000)
# TEST : Loss: (0.4524) | Acc: (85.00%) (8510/10000)
percent tensor([0.4136], device='cuda:0')
percent tensor([0.5305], device='cuda:0')
percent tensor([0.4721], device='cuda:0')
percent tensor([0.4330], device='cuda:0')
percent tensor([0.4496], device='cuda:0')
percent tensor([0.3829], device='cuda:0')
percent tensor([0.4149], device='cuda:0')
percent tensor([0.1466], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3195) |  Loss2: (0.0000) | Acc: (89.00%) (1254/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3081) |  Loss2: (0.0000) | Acc: (89.00%) (3542/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (4676/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (5834/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (6983/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (8134/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (9291/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (10434/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (11600/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (12733/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (13857/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (15010/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (16165/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (17315/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (18476/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (19605/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (20761/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (21907/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (23041/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (24182/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (25337/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (26473/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (27618/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (28762/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (29914/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (31063/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (32211/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (89.00%) (33356/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.3031) |  Loss2: (0.0000) | Acc: (89.00%) (34504/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (35640/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (36779/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (37940/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.3039) |  Loss2: (0.0000) | Acc: (89.00%) (39092/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (40242/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (41401/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (42542/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (43690/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (44781/50000)
# TEST : Loss: (0.4409) | Acc: (85.00%) (8518/10000)
percent tensor([0.4145], device='cuda:0')
percent tensor([0.5290], device='cuda:0')
percent tensor([0.4715], device='cuda:0')
percent tensor([0.4328], device='cuda:0')
percent tensor([0.4500], device='cuda:0')
percent tensor([0.3800], device='cuda:0')
percent tensor([0.4162], device='cuda:0')
percent tensor([0.1432], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (2384/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (88.00%) (3525/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (4680/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (5839/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (6999/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (8153/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (9294/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (10426/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (11579/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (12714/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (13860/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (15017/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (16167/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (17317/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (18470/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (19625/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (20764/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (21919/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (23049/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (24204/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (25353/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2970) |  Loss2: (0.0000) | Acc: (89.00%) (26517/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (27668/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (28821/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (29960/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (31135/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (32299/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2947) |  Loss2: (0.0000) | Acc: (89.00%) (33462/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (34628/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (35767/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2941) |  Loss2: (0.0000) | Acc: (89.00%) (36928/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2943) |  Loss2: (0.0000) | Acc: (89.00%) (38070/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (39241/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2931) |  Loss2: (0.0000) | Acc: (89.00%) (40403/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (41535/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (42683/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (43823/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (44919/50000)
# TEST : Loss: (0.4414) | Acc: (85.00%) (8537/10000)
percent tensor([0.4172], device='cuda:0')
percent tensor([0.5262], device='cuda:0')
percent tensor([0.4716], device='cuda:0')
percent tensor([0.4313], device='cuda:0')
percent tensor([0.4483], device='cuda:0')
percent tensor([0.3771], device='cuda:0')
percent tensor([0.4146], device='cuda:0')
percent tensor([0.1397], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.3237) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.3075) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (3556/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (4694/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (5840/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (6993/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (8165/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (9298/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (10455/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (11601/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (12755/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (13902/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (15045/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (16166/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (17309/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (18486/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (19633/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (20794/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (21954/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (23097/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (89.00%) (24245/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (25396/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (26567/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (27703/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (28868/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (30030/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (31193/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (32347/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (33495/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (34626/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (35790/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (36952/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (38110/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (90.00%) (39286/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (90.00%) (40438/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (90.00%) (41599/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (90.00%) (42762/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (90.00%) (43910/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (90.00%) (45024/50000)
# TEST : Loss: (0.4348) | Acc: (85.00%) (8554/10000)
percent tensor([0.4175], device='cuda:0')
percent tensor([0.5253], device='cuda:0')
percent tensor([0.4721], device='cuda:0')
percent tensor([0.4301], device='cuda:0')
percent tensor([0.4467], device='cuda:0')
percent tensor([0.3749], device='cuda:0')
percent tensor([0.4133], device='cuda:0')
percent tensor([0.1364], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (2422/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (90.00%) (3573/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (90.00%) (4724/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (5881/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (7003/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (8146/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (9303/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (10455/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (11600/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (12761/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (13911/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2948) |  Loss2: (0.0000) | Acc: (89.00%) (15065/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (16204/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (17353/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (18505/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (19658/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (20794/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (21944/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2946) |  Loss2: (0.0000) | Acc: (89.00%) (23092/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2937) |  Loss2: (0.0000) | Acc: (89.00%) (24252/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (25415/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (89.00%) (26551/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (89.00%) (27703/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2954) |  Loss2: (0.0000) | Acc: (89.00%) (28847/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (29979/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (31122/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (32269/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2969) |  Loss2: (0.0000) | Acc: (89.00%) (33416/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (34545/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (35690/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (36832/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (37979/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (39125/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (40283/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (41434/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (42577/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (43729/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (89.00%) (44833/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_080.pth.tar'
# TEST : Loss: (0.4247) | Acc: (85.00%) (8599/10000)
percent tensor([0.4176], device='cuda:0')
percent tensor([0.5247], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4302], device='cuda:0')
percent tensor([0.4465], device='cuda:0')
percent tensor([0.3749], device='cuda:0')
percent tensor([0.4132], device='cuda:0')
percent tensor([0.1364], device='cuda:0')
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (2437/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (91.00%) (4783/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (5934/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (7089/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (8231/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (9378/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (10534/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (11689/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (12838/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2811) |  Loss2: (0.0000) | Acc: (90.00%) (13997/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2813) |  Loss2: (0.0000) | Acc: (90.00%) (15151/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (16287/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (17444/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (18594/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (19728/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (90.00%) (20886/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (22025/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (90.00%) (23180/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (24329/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (25472/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (90.00%) (26617/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (27762/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (28912/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (90.00%) (30079/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (90.00%) (31220/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (89.00%) (32368/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (33519/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (34662/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (35817/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2907) |  Loss2: (0.0000) | Acc: (90.00%) (36986/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (89.00%) (38127/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (39284/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (90.00%) (40436/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (41584/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (42736/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (43887/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (44984/50000)
# TEST : Loss: (0.4456) | Acc: (85.00%) (8539/10000)
percent tensor([0.4176], device='cuda:0')
percent tensor([0.5247], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4302], device='cuda:0')
percent tensor([0.4465], device='cuda:0')
percent tensor([0.3749], device='cuda:0')
percent tensor([0.4132], device='cuda:0')
percent tensor([0.1365], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (89.00%) (1264/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (2426/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (3585/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (4737/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (5901/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (7072/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (8241/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (9421/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (10595/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (11747/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (12897/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (14050/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (15199/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (16349/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (17502/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (18653/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (19814/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (20992/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (22151/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (23298/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (24461/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (25618/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (26758/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (27905/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (29068/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (30222/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (31372/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (32534/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (33690/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (34839/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (35993/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (37128/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (90.00%) (38262/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (39421/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (40568/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (41726/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (42887/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (44038/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (45146/50000)
# TEST : Loss: (0.4965) | Acc: (84.00%) (8432/10000)
percent tensor([0.4176], device='cuda:0')
percent tensor([0.5246], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4303], device='cuda:0')
percent tensor([0.4465], device='cuda:0')
percent tensor([0.3750], device='cuda:0')
percent tensor([0.4133], device='cuda:0')
percent tensor([0.1366], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (2416/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (3585/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2823) |  Loss2: (0.0000) | Acc: (90.00%) (4736/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (5899/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (7058/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (8221/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (9374/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (10540/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (11688/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (12839/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (14013/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (15157/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (16329/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2766) |  Loss2: (0.0000) | Acc: (90.00%) (17499/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (18662/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (19809/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (20963/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (22102/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (23267/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (24426/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (25585/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2797) |  Loss2: (0.0000) | Acc: (90.00%) (26720/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (27881/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (29037/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (30186/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (31347/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (32514/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (33689/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (34830/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (35984/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (37134/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (38278/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (39429/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (40588/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (41749/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (42903/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (44049/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (45174/50000)
# TEST : Loss: (0.4915) | Acc: (84.00%) (8440/10000)
percent tensor([0.4177], device='cuda:0')
percent tensor([0.5246], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4303], device='cuda:0')
percent tensor([0.4465], device='cuda:0')
percent tensor([0.3750], device='cuda:0')
percent tensor([0.4133], device='cuda:0')
percent tensor([0.1367], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (3622/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (4804/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (5965/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (91.00%) (7119/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (8289/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (9438/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (10596/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (11747/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (12901/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (14049/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (15206/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (16353/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (17512/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (18656/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (19806/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (20953/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (22093/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (23237/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (24409/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (25567/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (26736/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (27902/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (29066/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (30228/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (31376/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (32524/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (33681/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (34829/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2757) |  Loss2: (0.0000) | Acc: (90.00%) (35992/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (37140/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (38296/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (39459/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (40634/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (41789/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (42961/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (44134/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (45252/50000)
# TEST : Loss: (0.4994) | Acc: (84.00%) (8428/10000)
percent tensor([0.4177], device='cuda:0')
percent tensor([0.5246], device='cuda:0')
percent tensor([0.4720], device='cuda:0')
percent tensor([0.4303], device='cuda:0')
percent tensor([0.4466], device='cuda:0')
percent tensor([0.3751], device='cuda:0')
percent tensor([0.4133], device='cuda:0')
percent tensor([0.1367], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (1262/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.3110) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (3499/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (87.00%) (4615/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.3441) |  Loss2: (0.0000) | Acc: (87.00%) (5738/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (87.00%) (6863/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (7977/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (9070/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (10193/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (11297/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (12396/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (13513/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (14645/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (15755/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (16850/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (17979/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (19107/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (20233/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (21368/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (22488/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (23615/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.3554) |  Loss2: (0.0000) | Acc: (87.00%) (24762/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (25891/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (27009/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (28133/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.3515) |  Loss2: (0.0000) | Acc: (87.00%) (29265/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (30390/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (87.00%) (31529/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (87.00%) (32652/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (33790/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (87.00%) (34928/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (87.00%) (36067/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.3438) |  Loss2: (0.0000) | Acc: (87.00%) (37211/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (87.00%) (38348/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (87.00%) (39494/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (87.00%) (40623/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (87.00%) (41751/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (87.00%) (42894/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (87.00%) (43969/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_085.pth.tar'
# TEST : Loss: (0.4871) | Acc: (84.00%) (8443/10000)
percent tensor([0.4190], device='cuda:0')
percent tensor([0.5255], device='cuda:0')
percent tensor([0.4602], device='cuda:0')
percent tensor([0.4303], device='cuda:0')
percent tensor([0.4266], device='cuda:0')
percent tensor([0.3582], device='cuda:0')
percent tensor([0.4196], device='cuda:0')
percent tensor([0.1366], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (89.00%) (2393/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (3526/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (4680/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.3166) |  Loss2: (0.0000) | Acc: (89.00%) (5830/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (88.00%) (6948/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (88.00%) (8087/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (9239/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (10388/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (11532/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (12666/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (13805/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (14951/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (16095/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (17231/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (18382/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (19511/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (20678/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (21832/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.3060) |  Loss2: (0.0000) | Acc: (89.00%) (22978/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.3058) |  Loss2: (0.0000) | Acc: (89.00%) (24128/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (25265/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (26420/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (27558/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (28699/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (29847/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.3061) |  Loss2: (0.0000) | Acc: (89.00%) (30984/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (32128/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (33287/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (34434/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (35591/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (36740/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (37901/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (39047/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (40204/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (41351/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (42484/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (43649/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (44769/50000)
# TEST : Loss: (0.4559) | Acc: (85.00%) (8536/10000)
percent tensor([0.4156], device='cuda:0')
percent tensor([0.5238], device='cuda:0')
percent tensor([0.4566], device='cuda:0')
percent tensor([0.4305], device='cuda:0')
percent tensor([0.4191], device='cuda:0')
percent tensor([0.3486], device='cuda:0')
percent tensor([0.4219], device='cuda:0')
percent tensor([0.1350], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (2403/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2955) |  Loss2: (0.0000) | Acc: (88.00%) (3527/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2953) |  Loss2: (0.0000) | Acc: (88.00%) (4666/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (89.00%) (5831/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (89.00%) (6973/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (8115/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (9264/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (10398/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (11538/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (12694/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2906) |  Loss2: (0.0000) | Acc: (89.00%) (13858/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (15025/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (16190/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (89.00%) (17344/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (89.00%) (18503/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (89.00%) (19664/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (89.00%) (20815/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (21969/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2849) |  Loss2: (0.0000) | Acc: (89.00%) (23122/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (89.00%) (24262/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2864) |  Loss2: (0.0000) | Acc: (89.00%) (25426/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (89.00%) (26580/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2882) |  Loss2: (0.0000) | Acc: (89.00%) (27726/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (89.00%) (28871/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (89.00%) (30031/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (31185/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (32335/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2875) |  Loss2: (0.0000) | Acc: (89.00%) (33496/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (34653/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (35801/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2877) |  Loss2: (0.0000) | Acc: (89.00%) (36952/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (38112/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (89.00%) (39273/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2869) |  Loss2: (0.0000) | Acc: (89.00%) (40423/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (41595/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (90.00%) (42756/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (43922/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (45043/50000)
# TEST : Loss: (0.4370) | Acc: (85.00%) (8582/10000)
percent tensor([0.4172], device='cuda:0')
percent tensor([0.5246], device='cuda:0')
percent tensor([0.4582], device='cuda:0')
percent tensor([0.4288], device='cuda:0')
percent tensor([0.4156], device='cuda:0')
percent tensor([0.3433], device='cuda:0')
percent tensor([0.4235], device='cuda:0')
percent tensor([0.1329], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.3418) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (91.00%) (2451/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (91.00%) (3612/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (4771/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (5925/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (7094/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (8240/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (9395/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (10555/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (11714/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (12867/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (14036/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (15171/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (16347/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (17492/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (18639/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2751) |  Loss2: (0.0000) | Acc: (90.00%) (19804/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (20939/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (22114/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (23254/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (24408/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (25576/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (26750/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (27910/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (29079/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (30241/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (31415/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (32574/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (33719/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2708) |  Loss2: (0.0000) | Acc: (90.00%) (34885/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2704) |  Loss2: (0.0000) | Acc: (90.00%) (36050/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (37212/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2698) |  Loss2: (0.0000) | Acc: (90.00%) (38380/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (39557/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (40714/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (41856/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (43015/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (44173/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (45270/50000)
# TEST : Loss: (0.4302) | Acc: (86.00%) (8600/10000)
percent tensor([0.4184], device='cuda:0')
percent tensor([0.5224], device='cuda:0')
percent tensor([0.4597], device='cuda:0')
percent tensor([0.4273], device='cuda:0')
percent tensor([0.4146], device='cuda:0')
percent tensor([0.3397], device='cuda:0')
percent tensor([0.4235], device='cuda:0')
percent tensor([0.1304], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (1261/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (2426/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2712) |  Loss2: (0.0000) | Acc: (90.00%) (3594/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (4762/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (5937/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (7096/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (8265/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (91.00%) (9436/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2641) |  Loss2: (0.0000) | Acc: (91.00%) (10601/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (11762/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (12919/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (14076/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (15241/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (16383/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (17558/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (18710/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (19883/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (21030/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2683) |  Loss2: (0.0000) | Acc: (90.00%) (22197/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (23364/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (24498/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (25652/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (26814/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (27985/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (29137/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (30274/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (31430/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (32591/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (33751/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (34923/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (36079/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (37243/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (90.00%) (38411/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (39563/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (40741/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (41910/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (43077/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (44235/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (45360/50000)
# TEST : Loss: (0.4255) | Acc: (86.00%) (8612/10000)
percent tensor([0.4161], device='cuda:0')
percent tensor([0.5231], device='cuda:0')
percent tensor([0.4613], device='cuda:0')
percent tensor([0.4263], device='cuda:0')
percent tensor([0.4146], device='cuda:0')
percent tensor([0.3376], device='cuda:0')
percent tensor([0.4215], device='cuda:0')
percent tensor([0.1280], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (2457/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (3634/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (4786/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (90.00%) (5937/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (7108/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (90.00%) (8263/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (9425/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (10588/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2675) |  Loss2: (0.0000) | Acc: (90.00%) (11741/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2707) |  Loss2: (0.0000) | Acc: (90.00%) (12884/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (14038/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (15193/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (16366/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (17523/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (18663/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (19818/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (20958/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (22132/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (23289/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (24441/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (25594/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (26749/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (27902/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (29041/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (30209/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (31362/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (32526/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (33686/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (34836/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (35995/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (37156/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (38292/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (39455/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (40601/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (41777/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (42949/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (44114/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (45218/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_090.pth.tar'
# TEST : Loss: (0.4566) | Acc: (85.00%) (8543/10000)
percent tensor([0.4164], device='cuda:0')
percent tensor([0.5230], device='cuda:0')
percent tensor([0.4617], device='cuda:0')
percent tensor([0.4265], device='cuda:0')
percent tensor([0.4146], device='cuda:0')
percent tensor([0.3378], device='cuda:0')
percent tensor([0.4216], device='cuda:0')
percent tensor([0.1281], device='cuda:0')
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (2470/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (4814/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (5987/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (7151/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (9488/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (10643/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (11811/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (12989/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (14158/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (15310/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (16465/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (17635/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (18801/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (19969/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (21118/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (22289/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (23458/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (91.00%) (24612/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (25779/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (26949/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (28125/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (29276/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (30437/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (31618/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (32780/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (33949/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (35140/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (36290/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (37438/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (38604/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (39757/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (40936/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (42108/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (43261/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (44408/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (45532/50000)
# TEST : Loss: (0.4520) | Acc: (85.00%) (8559/10000)
percent tensor([0.4164], device='cuda:0')
percent tensor([0.5230], device='cuda:0')
percent tensor([0.4617], device='cuda:0')
percent tensor([0.4266], device='cuda:0')
percent tensor([0.4147], device='cuda:0')
percent tensor([0.3379], device='cuda:0')
percent tensor([0.4216], device='cuda:0')
percent tensor([0.1282], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (90.00%) (3601/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (4745/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (5918/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (7079/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (8238/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (90.00%) (9401/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (10557/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (11734/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (90.00%) (12906/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (90.00%) (14069/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (90.00%) (15250/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (16424/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (17598/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (90.00%) (18752/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (90.00%) (19912/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (90.00%) (21063/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (22239/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (90.00%) (23410/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (90.00%) (24573/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (90.00%) (25736/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (26912/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (28090/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (29250/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (30425/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (31596/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (32747/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (33925/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (35083/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (36244/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (37398/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (38569/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (39725/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (40889/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (42056/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (43231/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (44403/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (45522/50000)
# TEST : Loss: (0.5130) | Acc: (84.00%) (8407/10000)
percent tensor([0.4165], device='cuda:0')
percent tensor([0.5230], device='cuda:0')
percent tensor([0.4617], device='cuda:0')
percent tensor([0.4266], device='cuda:0')
percent tensor([0.4147], device='cuda:0')
percent tensor([0.3379], device='cuda:0')
percent tensor([0.4217], device='cuda:0')
percent tensor([0.1283], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (92.00%) (2486/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (3639/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (4826/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (92.00%) (6010/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (92.00%) (7197/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (8366/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (9523/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (10688/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (11851/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (13026/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (14180/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (15370/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (16531/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (17702/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (18874/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (20054/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (21227/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (22404/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (23580/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (24753/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (25934/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (27103/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (28267/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (29427/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (30584/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (31738/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (32898/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (34069/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (35229/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (36407/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (37576/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (38718/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (39882/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (41046/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (42236/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (43397/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (44583/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (45698/50000)
# TEST : Loss: (0.4426) | Acc: (85.00%) (8590/10000)
percent tensor([0.4165], device='cuda:0')
percent tensor([0.5230], device='cuda:0')
percent tensor([0.4617], device='cuda:0')
percent tensor([0.4266], device='cuda:0')
percent tensor([0.4147], device='cuda:0')
percent tensor([0.3380], device='cuda:0')
percent tensor([0.4217], device='cuda:0')
percent tensor([0.1283], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (2460/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (3631/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (4812/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (5993/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (7168/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (8348/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (9520/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (10682/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (13056/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (14246/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (15423/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (16591/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (17767/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (18935/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (20111/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (21294/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (22467/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2394) |  Loss2: (0.0000) | Acc: (91.00%) (23641/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (24831/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (25995/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (27176/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (28327/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (29500/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (30664/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (31824/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (33002/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (34181/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (35360/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (36527/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (37692/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (38862/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (40032/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (41201/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (42392/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (43568/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (44723/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (45841/50000)
# TEST : Loss: (0.4870) | Acc: (84.00%) (8490/10000)
percent tensor([0.4165], device='cuda:0')
percent tensor([0.5230], device='cuda:0')
percent tensor([0.4617], device='cuda:0')
percent tensor([0.4267], device='cuda:0')
percent tensor([0.4148], device='cuda:0')
percent tensor([0.3380], device='cuda:0')
percent tensor([0.4217], device='cuda:0')
percent tensor([0.1284], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (90.00%) (4742/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (90.00%) (5885/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (7003/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (8133/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.3076) |  Loss2: (0.0000) | Acc: (89.00%) (9275/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (10437/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (11576/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (12719/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (13869/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (15021/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.3055) |  Loss2: (0.0000) | Acc: (89.00%) (16161/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.3035) |  Loss2: (0.0000) | Acc: (89.00%) (17309/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (18467/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (19627/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (20769/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.3000) |  Loss2: (0.0000) | Acc: (89.00%) (21898/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (23028/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (24168/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (25320/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (26471/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (27616/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (28774/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (29928/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (31075/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (32222/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (33380/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (34538/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (35689/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (36849/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (38012/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2957) |  Loss2: (0.0000) | Acc: (89.00%) (39168/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (40337/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (41496/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (42645/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (43825/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (44932/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_095.pth.tar'
# TEST : Loss: (0.4460) | Acc: (85.00%) (8518/10000)
percent tensor([0.3923], device='cuda:0')
percent tensor([0.5279], device='cuda:0')
percent tensor([0.4563], device='cuda:0')
percent tensor([0.4112], device='cuda:0')
percent tensor([0.4129], device='cuda:0')
percent tensor([0.3528], device='cuda:0')
percent tensor([0.4368], device='cuda:0')
percent tensor([0.1269], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (1286/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (90.00%) (2434/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (3596/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (5927/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (7079/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (8249/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (9427/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (10604/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (11759/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (12956/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (14107/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (15260/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (16452/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (91.00%) (17610/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (18785/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (19962/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (21126/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (22281/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (23434/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (24621/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2590) |  Loss2: (0.0000) | Acc: (91.00%) (25776/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (26940/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (28119/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (29281/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (30442/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (91.00%) (31615/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (32791/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (91.00%) (33954/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (35146/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (36326/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (37502/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (38679/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (39861/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (41029/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (91.00%) (42201/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (43371/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (44548/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (45682/50000)
# TEST : Loss: (0.4269) | Acc: (85.00%) (8580/10000)
percent tensor([0.3907], device='cuda:0')
percent tensor([0.5274], device='cuda:0')
percent tensor([0.4552], device='cuda:0')
percent tensor([0.4068], device='cuda:0')
percent tensor([0.4120], device='cuda:0')
percent tensor([0.3596], device='cuda:0')
percent tensor([0.4398], device='cuda:0')
percent tensor([0.1245], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (1285/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2469) |  Loss2: (0.0000) | Acc: (91.00%) (3618/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (4802/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (5975/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (7139/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (8306/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (9474/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (10639/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (11825/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (12989/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (14156/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (15337/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (16505/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (17688/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2403) |  Loss2: (0.0000) | Acc: (91.00%) (18879/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (20047/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2398) |  Loss2: (0.0000) | Acc: (91.00%) (21226/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2395) |  Loss2: (0.0000) | Acc: (91.00%) (22404/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (23577/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (24757/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (25933/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (27109/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (28291/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (29482/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (30650/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (31827/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (33011/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (34197/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (35373/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (36573/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (37744/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (38922/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (40087/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (41259/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (42423/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (43583/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (44766/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (45886/50000)
# TEST : Loss: (0.4109) | Acc: (86.00%) (8633/10000)
percent tensor([0.3901], device='cuda:0')
percent tensor([0.5265], device='cuda:0')
percent tensor([0.4546], device='cuda:0')
percent tensor([0.4043], device='cuda:0')
percent tensor([0.4144], device='cuda:0')
percent tensor([0.3622], device='cuda:0')
percent tensor([0.4384], device='cuda:0')
percent tensor([0.1218], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (2462/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (3643/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (4821/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (6000/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (7176/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (8344/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (9524/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (10708/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (11877/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (13068/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (14249/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (15426/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (16614/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (17790/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (18973/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (92.00%) (20147/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (92.00%) (21325/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (92.00%) (22493/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (92.00%) (23676/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (92.00%) (24852/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (26037/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (92.00%) (27207/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (28403/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (92.00%) (29571/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (30724/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (31890/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (33084/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (34264/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (35432/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (36605/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (37787/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (38976/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (92.00%) (40165/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (92.00%) (41360/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (42557/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (43751/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (92.00%) (44905/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (46060/50000)
# TEST : Loss: (0.4044) | Acc: (86.00%) (8665/10000)
percent tensor([0.3899], device='cuda:0')
percent tensor([0.5273], device='cuda:0')
percent tensor([0.4542], device='cuda:0')
percent tensor([0.4042], device='cuda:0')
percent tensor([0.4150], device='cuda:0')
percent tensor([0.3622], device='cuda:0')
percent tensor([0.4358], device='cuda:0')
percent tensor([0.1192], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2053) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (4836/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (6011/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (7185/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2347) |  Loss2: (0.0000) | Acc: (92.00%) (8362/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (92.00%) (9542/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (10709/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (11882/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (13070/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (91.00%) (14247/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (92.00%) (15429/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (16606/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (92.00%) (18967/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (20146/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (21330/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (22496/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (23679/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (24863/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (26031/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (27219/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (28404/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (92.00%) (29599/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (30783/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (31956/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (33137/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (34315/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (35468/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (36661/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (37824/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (38999/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (40174/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (41353/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (42524/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (43725/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (44912/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (46054/50000)
# TEST : Loss: (0.3980) | Acc: (86.00%) (8687/10000)
percent tensor([0.3897], device='cuda:0')
percent tensor([0.5281], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4041], device='cuda:0')
percent tensor([0.4142], device='cuda:0')
percent tensor([0.3635], device='cuda:0')
percent tensor([0.4333], device='cuda:0')
percent tensor([0.1167], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (3635/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (4798/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (5982/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (7151/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (8324/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (9475/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (10633/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (11800/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (12979/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (14152/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (15328/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (16508/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2399) |  Loss2: (0.0000) | Acc: (91.00%) (17692/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (18826/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (19992/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (21171/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (22329/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (23484/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (24630/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (25808/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (26973/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (28159/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (29334/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (30504/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (31671/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (32851/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (34030/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (35214/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (36391/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (37571/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (38753/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (39918/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (41090/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (42268/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (43422/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (44583/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (45706/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_100.pth.tar'
# TEST : Loss: (0.4249) | Acc: (86.00%) (8636/10000)
percent tensor([0.3897], device='cuda:0')
percent tensor([0.5280], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4041], device='cuda:0')
percent tensor([0.4142], device='cuda:0')
percent tensor([0.3635], device='cuda:0')
percent tensor([0.4332], device='cuda:0')
percent tensor([0.1167], device='cuda:0')
Epoch: 101 | Batch_idx: 0 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (2490/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (3669/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (4860/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (6035/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (7229/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (8410/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (9590/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (10756/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (11936/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (13103/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (92.00%) (14263/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (15426/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (92.00%) (16624/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (17816/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (18997/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (20188/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (21357/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (22519/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (23686/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (24858/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (26031/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (27211/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (28377/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (29546/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (30721/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (31918/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (33096/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (34277/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (92.00%) (35448/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (36635/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (92.00%) (37804/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (38956/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (40129/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (41301/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (42480/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (43674/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (44851/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (45978/50000)
# TEST : Loss: (0.4957) | Acc: (84.00%) (8480/10000)
percent tensor([0.3897], device='cuda:0')
percent tensor([0.5280], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4041], device='cuda:0')
percent tensor([0.4142], device='cuda:0')
percent tensor([0.3635], device='cuda:0')
percent tensor([0.4333], device='cuda:0')
percent tensor([0.1168], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (2453/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (3633/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (4808/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2332) |  Loss2: (0.0000) | Acc: (91.00%) (5982/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (7163/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (8346/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (9522/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (10707/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (11902/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (13093/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (14280/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (15455/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (16638/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (17804/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (18969/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (20154/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (21330/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (22507/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (23706/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (24872/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (26051/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (27227/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (28409/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (29584/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (30761/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (31930/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (33114/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (34293/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (35465/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (36632/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (37818/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (39013/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (40178/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (41355/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (42529/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (43718/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (44905/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (46038/50000)
# TEST : Loss: (0.4477) | Acc: (85.00%) (8582/10000)
percent tensor([0.3897], device='cuda:0')
percent tensor([0.5280], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4041], device='cuda:0')
percent tensor([0.4142], device='cuda:0')
percent tensor([0.3636], device='cuda:0')
percent tensor([0.4333], device='cuda:0')
percent tensor([0.1169], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (3673/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (4854/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (6049/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (7235/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (8422/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (9603/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (10802/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (11986/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (13165/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (14356/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (15539/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (16717/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (17896/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.2138) |  Loss2: (0.0000) | Acc: (92.00%) (19089/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (20283/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (21463/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (22637/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (23809/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (24976/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (26156/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (27318/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (28492/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (29666/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (30846/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (32006/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (33179/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (34362/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (35541/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (36708/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (37864/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (39052/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (40224/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (41407/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (42580/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (43776/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (44952/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (46082/50000)
# TEST : Loss: (0.4801) | Acc: (85.00%) (8535/10000)
percent tensor([0.3898], device='cuda:0')
percent tensor([0.5280], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4042], device='cuda:0')
percent tensor([0.4143], device='cuda:0')
percent tensor([0.3636], device='cuda:0')
percent tensor([0.4333], device='cuda:0')
percent tensor([0.1170], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (4876/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2066) |  Loss2: (0.0000) | Acc: (92.00%) (6066/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (7247/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (92.00%) (8444/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (9631/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (10816/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (12004/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (13188/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (14381/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (15562/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (16740/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (17935/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (19111/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (20294/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (21481/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (22666/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (23859/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (25045/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (26229/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (27414/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (28581/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (29777/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (30943/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (32145/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2163) |  Loss2: (0.0000) | Acc: (92.00%) (33317/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (34494/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (35679/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (36867/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (38056/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (39234/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (40416/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (41599/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (42774/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (43965/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (45152/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (46265/50000)
# TEST : Loss: (0.4397) | Acc: (86.00%) (8638/10000)
percent tensor([0.3898], device='cuda:0')
percent tensor([0.5280], device='cuda:0')
percent tensor([0.4559], device='cuda:0')
percent tensor([0.4042], device='cuda:0')
percent tensor([0.4143], device='cuda:0')
percent tensor([0.3637], device='cuda:0')
percent tensor([0.4333], device='cuda:0')
percent tensor([0.1171], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (2463/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (90.00%) (3605/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (4749/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (5896/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2682) |  Loss2: (0.0000) | Acc: (90.00%) (7035/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (8181/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2693) |  Loss2: (0.0000) | Acc: (90.00%) (9342/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2699) |  Loss2: (0.0000) | Acc: (90.00%) (10495/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (90.00%) (11648/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (12798/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (13946/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (89.00%) (15080/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (89.00%) (16227/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (89.00%) (17395/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (18560/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (19720/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (20868/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (22019/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2754) |  Loss2: (0.0000) | Acc: (90.00%) (23177/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (24341/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (25502/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2750) |  Loss2: (0.0000) | Acc: (90.00%) (26648/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (27811/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (28977/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (30164/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (31330/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2701) |  Loss2: (0.0000) | Acc: (90.00%) (32492/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (33658/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (34839/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (36006/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (37169/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2677) |  Loss2: (0.0000) | Acc: (90.00%) (38334/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (39503/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (40668/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (41823/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (42989/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (44148/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (90.00%) (45270/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_105.pth.tar'
# TEST : Loss: (0.4364) | Acc: (86.00%) (8628/10000)
percent tensor([0.3902], device='cuda:0')
percent tensor([0.5090], device='cuda:0')
percent tensor([0.4450], device='cuda:0')
percent tensor([0.4010], device='cuda:0')
percent tensor([0.4114], device='cuda:0')
percent tensor([0.3675], device='cuda:0')
percent tensor([0.4381], device='cuda:0')
percent tensor([0.1168], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (90.00%) (2440/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (3615/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (4780/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (90.00%) (5936/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (7106/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (8276/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (90.00%) (9430/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (10610/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (11769/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (12946/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (14120/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (15307/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (16496/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (17685/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (18869/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (20047/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (21226/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (22397/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (23572/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (24749/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (25928/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (27115/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (28298/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (29474/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (30650/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (31829/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (32991/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (34163/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (35342/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (36543/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (37707/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (38882/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (40051/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2347) |  Loss2: (0.0000) | Acc: (91.00%) (41241/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (42426/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (43606/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (44767/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (45912/50000)
# TEST : Loss: (0.4150) | Acc: (86.00%) (8685/10000)
percent tensor([0.3881], device='cuda:0')
percent tensor([0.5047], device='cuda:0')
percent tensor([0.4417], device='cuda:0')
percent tensor([0.3981], device='cuda:0')
percent tensor([0.4100], device='cuda:0')
percent tensor([0.3682], device='cuda:0')
percent tensor([0.4382], device='cuda:0')
percent tensor([0.1156], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (3662/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (4834/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (5996/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (7163/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (8343/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (9518/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (10686/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (11874/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (13050/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.2327) |  Loss2: (0.0000) | Acc: (91.00%) (14232/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (15417/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (91.00%) (16593/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (17767/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (18952/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (20134/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (21323/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (22505/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (23685/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (24868/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (26042/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (27215/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.2273) |  Loss2: (0.0000) | Acc: (92.00%) (28397/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (29571/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (92.00%) (30741/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (31930/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (33119/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (34293/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (35486/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (36671/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (37875/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (39056/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (40247/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (41429/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (42610/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (43807/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.2230) |  Loss2: (0.0000) | Acc: (92.00%) (44992/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (46139/50000)
# TEST : Loss: (0.4059) | Acc: (87.00%) (8702/10000)
percent tensor([0.3881], device='cuda:0')
percent tensor([0.5040], device='cuda:0')
percent tensor([0.4417], device='cuda:0')
percent tensor([0.3953], device='cuda:0')
percent tensor([0.4086], device='cuda:0')
percent tensor([0.3674], device='cuda:0')
percent tensor([0.4362], device='cuda:0')
percent tensor([0.1140], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (92.00%) (2475/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (4855/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (6043/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (7233/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (92.00%) (8395/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (9590/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (10778/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (11965/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (13154/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (14339/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (15531/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (16722/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (17906/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (19093/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (20262/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (21442/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (22605/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (23779/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.2225) |  Loss2: (0.0000) | Acc: (92.00%) (24958/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.2217) |  Loss2: (0.0000) | Acc: (92.00%) (26148/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (27331/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (28508/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (29706/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (30909/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (32094/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (33272/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (34460/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (35640/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (36835/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (37998/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (39169/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (40348/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (41539/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (42704/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (43898/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (45106/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (46261/50000)
# TEST : Loss: (0.3974) | Acc: (87.00%) (8727/10000)
percent tensor([0.3869], device='cuda:0')
percent tensor([0.5067], device='cuda:0')
percent tensor([0.4425], device='cuda:0')
percent tensor([0.3923], device='cuda:0')
percent tensor([0.4074], device='cuda:0')
percent tensor([0.3667], device='cuda:0')
percent tensor([0.4342], device='cuda:0')
percent tensor([0.1124], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (3682/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (4872/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (6053/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (7240/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (8437/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (9616/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (10820/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (12002/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.2130) |  Loss2: (0.0000) | Acc: (92.00%) (13177/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (14382/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (15566/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (16753/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (17925/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (19108/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (20301/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (21483/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (22670/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (23866/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (25076/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (26249/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (27428/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (28617/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (29809/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (31012/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (32194/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (33369/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (34557/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (35755/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (36944/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (38127/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (39322/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (40515/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (41703/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (42882/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (44077/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (45273/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (46409/50000)
# TEST : Loss: (0.3923) | Acc: (87.00%) (8749/10000)
percent tensor([0.3864], device='cuda:0')
percent tensor([0.5087], device='cuda:0')
percent tensor([0.4417], device='cuda:0')
percent tensor([0.3901], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4307], device='cuda:0')
percent tensor([0.1106], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (1305/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (2493/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (4861/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.2044) |  Loss2: (0.0000) | Acc: (92.00%) (6047/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (7230/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (8412/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (9586/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (10767/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (11948/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (13131/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (14296/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (15476/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (16678/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (17852/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (19022/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (20202/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (21391/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (22576/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (23748/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (24933/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (26114/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (27309/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (28483/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (29658/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (30840/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (32018/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (33184/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (34365/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (35562/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (36756/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (37949/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2199) |  Loss2: (0.0000) | Acc: (92.00%) (39120/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (40307/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (41495/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (42671/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (43868/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (45044/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (46153/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
# TEST : Loss: (0.5163) | Acc: (84.00%) (8460/10000)
percent tensor([0.3863], device='cuda:0')
percent tensor([0.5084], device='cuda:0')
percent tensor([0.4415], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4306], device='cuda:0')
percent tensor([0.1106], device='cuda:0')

Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (4863/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (6064/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1959) |  Loss2: (0.0000) | Acc: (93.00%) (8475/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (9650/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (10844/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (12020/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (13210/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (14388/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (15569/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (16749/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (17943/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (92.00%) (19138/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (20334/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (21511/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.2052) |  Loss2: (0.0000) | Acc: (92.00%) (22688/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (23860/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2074) |  Loss2: (0.0000) | Acc: (92.00%) (25039/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (26215/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (27408/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (28589/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2090) |  Loss2: (0.0000) | Acc: (92.00%) (29778/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (30970/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32152/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (33341/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (34526/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (35727/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (36907/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (38083/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (39268/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (40459/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (41640/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (42822/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (44016/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (45192/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (46325/50000)
# TEST : Loss: (0.4367) | Acc: (86.00%) (8645/10000)
percent tensor([0.3863], device='cuda:0')
percent tensor([0.5084], device='cuda:0')
percent tensor([0.4415], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4306], device='cuda:0')
percent tensor([0.1106], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (3692/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (4880/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.2037) |  Loss2: (0.0000) | Acc: (92.00%) (6054/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (7255/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (92.00%) (8447/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (92.00%) (9634/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (10822/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (12002/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (13184/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (14388/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.2012) |  Loss2: (0.0000) | Acc: (92.00%) (15571/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (16769/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (92.00%) (17975/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (19168/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (20367/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (21565/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (22765/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23951/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (25149/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (26340/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1997) |  Loss2: (0.0000) | Acc: (93.00%) (27518/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (28692/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (29898/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (31089/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (32277/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (33474/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (34652/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (35837/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (37021/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (38194/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (39396/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (40581/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (41764/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (42961/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (44142/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (45332/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (92.00%) (46476/50000)
# TEST : Loss: (0.4188) | Acc: (86.00%) (8696/10000)
percent tensor([0.3863], device='cuda:0')
percent tensor([0.5084], device='cuda:0')
percent tensor([0.4415], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4306], device='cuda:0')
percent tensor([0.1106], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (3678/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (4878/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (6070/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (7273/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (8466/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (9653/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (10847/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (12036/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (13229/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (14439/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (15633/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (16837/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (18030/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (19230/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (20422/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (21625/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (22828/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (24026/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (25203/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (26393/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (27594/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (28776/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (29960/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (31152/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (32346/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (33528/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (34718/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (35898/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (37094/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (38288/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (39464/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (40648/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (41834/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (43014/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (93.00%) (44197/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (45386/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (46545/50000)
# TEST : Loss: (0.4103) | Acc: (87.00%) (8717/10000)
percent tensor([0.3863], device='cuda:0')
percent tensor([0.5084], device='cuda:0')
percent tensor([0.4415], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4306], device='cuda:0')
percent tensor([0.1106], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (94.00%) (1324/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (2521/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (3723/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (4910/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (6092/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1872) |  Loss2: (0.0000) | Acc: (93.00%) (7299/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (8490/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (9695/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (10888/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (12076/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (13269/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (14463/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (15662/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (16841/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (18044/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (19242/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (20435/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (21628/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (22827/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (24002/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (25201/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (26397/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (27590/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (28789/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (29973/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (31167/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (32364/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (33554/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (34743/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1943) |  Loss2: (0.0000) | Acc: (93.00%) (35930/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (37135/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (38316/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (39516/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (40714/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (41901/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (43084/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (44266/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (45468/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (46613/50000)
# TEST : Loss: (0.4700) | Acc: (85.00%) (8578/10000)
percent tensor([0.3863], device='cuda:0')
percent tensor([0.5084], device='cuda:0')
percent tensor([0.4415], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.4066], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.4306], device='cuda:0')
percent tensor([0.1106], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (2458/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (4781/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2544) |  Loss2: (0.0000) | Acc: (90.00%) (5928/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (90.00%) (7082/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (8217/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2637) |  Loss2: (0.0000) | Acc: (90.00%) (9385/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (90.00%) (10570/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (11722/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (12908/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (14066/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (15229/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (16389/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2630) |  Loss2: (0.0000) | Acc: (90.00%) (17557/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (18716/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (90.00%) (19875/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (90.00%) (21033/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (22212/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (90.00%) (23384/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (24564/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (25745/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (26922/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (28098/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (91.00%) (29274/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (30457/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2515) |  Loss2: (0.0000) | Acc: (91.00%) (31633/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (32814/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (33987/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (35143/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (36314/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (37495/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (38655/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (39829/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (41019/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2475) |  Loss2: (0.0000) | Acc: (91.00%) (42200/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (43380/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (44569/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (45713/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_115.pth.tar'
# TEST : Loss: (0.4542) | Acc: (86.00%) (8633/10000)
percent tensor([0.3992], device='cuda:0')
percent tensor([0.5245], device='cuda:0')
percent tensor([0.4443], device='cuda:0')
percent tensor([0.3865], device='cuda:0')
percent tensor([0.4007], device='cuda:0')
percent tensor([0.3565], device='cuda:0')
percent tensor([0.4322], device='cuda:0')
percent tensor([0.1101], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2348) |  Loss2: (0.0000) | Acc: (92.00%) (1303/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (3647/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (4833/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (6002/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (91.00%) (8349/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (91.00%) (9538/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (10717/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (11900/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (13081/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (14251/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (15439/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (16634/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (17800/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (19002/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (20183/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (21377/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (22569/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (23762/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (24952/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (26124/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.2184) |  Loss2: (0.0000) | Acc: (92.00%) (27296/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (28474/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (29669/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (30856/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (32035/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (33211/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (34391/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (35577/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (36753/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (37947/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (39144/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (40329/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (41516/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (42707/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (43890/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (45060/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (46207/50000)
# TEST : Loss: (0.4236) | Acc: (87.00%) (8707/10000)
percent tensor([0.4037], device='cuda:0')
percent tensor([0.5272], device='cuda:0')
percent tensor([0.4418], device='cuda:0')
percent tensor([0.3836], device='cuda:0')
percent tensor([0.3976], device='cuda:0')
percent tensor([0.3502], device='cuda:0')
percent tensor([0.4314], device='cuda:0')
percent tensor([0.1090], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (2504/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (3702/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (4893/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (6072/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (92.00%) (7259/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (8435/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.2049) |  Loss2: (0.0000) | Acc: (92.00%) (9620/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (10830/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (12018/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (13209/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (14415/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.2020) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.2036) |  Loss2: (0.0000) | Acc: (92.00%) (16780/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (17981/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (19168/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (20383/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (21555/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (93.00%) (22744/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (23937/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (25139/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (26325/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (93.00%) (27521/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (28700/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (29888/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (93.00%) (31087/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (93.00%) (32269/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (93.00%) (33455/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (93.00%) (34650/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (35852/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (93.00%) (37051/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (93.00%) (38254/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (39445/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (40637/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (41823/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (43018/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (44230/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (45409/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (46556/50000)
# TEST : Loss: (0.4146) | Acc: (87.00%) (8734/10000)
percent tensor([0.4063], device='cuda:0')
percent tensor([0.5245], device='cuda:0')
percent tensor([0.4438], device='cuda:0')
percent tensor([0.3822], device='cuda:0')
percent tensor([0.3961], device='cuda:0')
percent tensor([0.3454], device='cuda:0')
percent tensor([0.4296], device='cuda:0')
percent tensor([0.1075], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (2512/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (3702/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1957) |  Loss2: (0.0000) | Acc: (93.00%) (4903/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (6096/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1995) |  Loss2: (0.0000) | Acc: (93.00%) (7287/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (8492/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (9696/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1951) |  Loss2: (0.0000) | Acc: (93.00%) (10894/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (12086/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1953) |  Loss2: (0.0000) | Acc: (93.00%) (13267/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (14486/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1929) |  Loss2: (0.0000) | Acc: (93.00%) (15680/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1935) |  Loss2: (0.0000) | Acc: (93.00%) (16869/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (18052/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1947) |  Loss2: (0.0000) | Acc: (93.00%) (19254/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (20447/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (21644/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (22832/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (24008/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (25205/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1956) |  Loss2: (0.0000) | Acc: (93.00%) (26405/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (27594/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (28782/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (29974/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (31176/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (32373/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (33566/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (34750/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (35945/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (37129/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (38318/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (39523/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (40725/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (41937/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1946) |  Loss2: (0.0000) | Acc: (93.00%) (43135/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (44339/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (45537/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (46692/50000)
# TEST : Loss: (0.4040) | Acc: (87.00%) (8749/10000)
percent tensor([0.4073], device='cuda:0')
percent tensor([0.5220], device='cuda:0')
percent tensor([0.4435], device='cuda:0')
percent tensor([0.3801], device='cuda:0')
percent tensor([0.3956], device='cuda:0')
percent tensor([0.3434], device='cuda:0')
percent tensor([0.4268], device='cuda:0')
percent tensor([0.1059], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (2478/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (3667/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.2031) |  Loss2: (0.0000) | Acc: (92.00%) (4862/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (92.00%) (6069/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (7266/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (8455/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (9649/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (10856/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (12064/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (13255/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (14453/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (15645/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (16843/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (18048/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (19257/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (20452/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (21656/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (22849/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (24038/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (25239/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (26416/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (27623/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (28829/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (30028/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (31218/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (32413/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (33607/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (34802/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (36010/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (37201/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (38391/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (39575/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (40751/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (41953/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (43155/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (44370/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (45564/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (46726/50000)
# TEST : Loss: (0.4060) | Acc: (87.00%) (8766/10000)
percent tensor([0.4069], device='cuda:0')
percent tensor([0.5209], device='cuda:0')
percent tensor([0.4423], device='cuda:0')
percent tensor([0.3792], device='cuda:0')
percent tensor([0.3951], device='cuda:0')
percent tensor([0.3413], device='cuda:0')
percent tensor([0.4240], device='cuda:0')
percent tensor([0.1043], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (1316/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (2500/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (4898/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (6084/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (7283/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (8481/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (9683/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (10876/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (12079/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (13282/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (14480/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (15678/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (16883/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (18072/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (19276/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (20467/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (21665/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (22845/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (24040/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (25225/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1923) |  Loss2: (0.0000) | Acc: (93.00%) (26412/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (27601/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1927) |  Loss2: (0.0000) | Acc: (93.00%) (28808/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (30001/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (31199/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (32413/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (33592/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (34781/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (35950/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1937) |  Loss2: (0.0000) | Acc: (93.00%) (37141/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (38326/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (39521/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (40708/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1952) |  Loss2: (0.0000) | Acc: (93.00%) (41900/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (43088/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (44298/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1945) |  Loss2: (0.0000) | Acc: (93.00%) (45494/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (46629/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_120.pth.tar'
# TEST : Loss: (0.5081) | Acc: (85.00%) (8523/10000)
percent tensor([0.4006], device='cuda:0')
percent tensor([0.5166], device='cuda:0')
percent tensor([0.4399], device='cuda:0')
percent tensor([0.3805], device='cuda:0')
percent tensor([0.3937], device='cuda:0')
percent tensor([0.3399], device='cuda:0')
percent tensor([0.4214], device='cuda:0')
percent tensor([0.1030], device='cuda:0')
Epoch: 121 | Batch_idx: 0 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (3755/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (94.00%) (4951/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (94.00%) (6144/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (94.00%) (8562/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (94.00%) (9749/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (94.00%) (10957/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (94.00%) (12172/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (94.00%) (13378/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (94.00%) (14569/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (94.00%) (15769/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (16958/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (18145/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (19329/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (20527/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (21723/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (22919/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (24113/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (25320/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1856) |  Loss2: (0.0000) | Acc: (93.00%) (26505/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (27703/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1850) |  Loss2: (0.0000) | Acc: (93.00%) (28899/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (30082/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (31283/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (32474/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (33657/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (34863/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (36063/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (37255/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (38439/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (39641/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (40824/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (42026/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (43224/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (44417/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (45599/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (46752/50000)
# TEST : Loss: (0.4612) | Acc: (86.00%) (8644/10000)
percent tensor([0.3983], device='cuda:0')
percent tensor([0.5144], device='cuda:0')
percent tensor([0.4389], device='cuda:0')
percent tensor([0.3790], device='cuda:0')
percent tensor([0.3928], device='cuda:0')
percent tensor([0.3389], device='cuda:0')
percent tensor([0.4201], device='cuda:0')
percent tensor([0.1017], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (3734/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (94.00%) (4935/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (6140/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (7343/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (94.00%) (8546/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (9761/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (10961/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (12168/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (13368/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (94.00%) (14564/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (15769/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (94.00%) (16971/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (94.00%) (18180/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (94.00%) (19383/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (94.00%) (20590/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (94.00%) (21780/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (22971/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (24175/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (25370/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (26563/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (27764/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (28950/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (30124/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (31328/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (32534/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (93.00%) (33740/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (34943/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (36140/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (37348/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (38530/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (39729/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (40913/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (42090/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (43283/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (44460/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (45668/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (46821/50000)
# TEST : Loss: (0.4163) | Acc: (87.00%) (8742/10000)
percent tensor([0.3971], device='cuda:0')
percent tensor([0.5119], device='cuda:0')
percent tensor([0.4368], device='cuda:0')
percent tensor([0.3776], device='cuda:0')
percent tensor([0.3929], device='cuda:0')
percent tensor([0.3372], device='cuda:0')
percent tensor([0.4175], device='cuda:0')
percent tensor([0.1005], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1876) |  Loss2: (0.0000) | Acc: (93.00%) (1311/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (94.00%) (3730/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (4932/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (6147/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1710) |  Loss2: (0.0000) | Acc: (94.00%) (7357/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (8559/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (9760/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (10968/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (12146/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (13343/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (93.00%) (14539/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (15748/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (16962/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (93.00%) (18152/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (19350/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (20536/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (21746/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (22940/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (24132/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (25326/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (26504/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (27692/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (28896/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (30108/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (31303/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (32507/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (33699/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (93.00%) (34890/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (36093/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (37288/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (38482/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (39678/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (40860/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (42061/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (43263/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1817) |  Loss2: (0.0000) | Acc: (93.00%) (44474/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1811) |  Loss2: (0.0000) | Acc: (93.00%) (45687/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (46836/50000)
# TEST : Loss: (0.4412) | Acc: (86.00%) (8691/10000)
percent tensor([0.3963], device='cuda:0')
percent tensor([0.5118], device='cuda:0')
percent tensor([0.4369], device='cuda:0')
percent tensor([0.3778], device='cuda:0')
percent tensor([0.3927], device='cuda:0')
percent tensor([0.3361], device='cuda:0')
percent tensor([0.4149], device='cuda:0')
percent tensor([0.0993], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (3757/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1512) |  Loss2: (0.0000) | Acc: (94.00%) (4974/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (6182/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (8591/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (9793/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (10973/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (12169/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (13369/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (14581/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (15783/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (16985/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (18190/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (19383/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (94.00%) (20575/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (21769/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (22954/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1763) |  Loss2: (0.0000) | Acc: (93.00%) (24150/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (25346/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (26549/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (27749/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (28948/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (30161/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (31360/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (32571/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (33767/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1766) |  Loss2: (0.0000) | Acc: (93.00%) (34962/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1767) |  Loss2: (0.0000) | Acc: (93.00%) (36162/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (37357/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (38560/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (39762/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (40967/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (42148/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (43355/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (44555/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1778) |  Loss2: (0.0000) | Acc: (93.00%) (45752/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (46901/50000)
# TEST : Loss: (0.4217) | Acc: (87.00%) (8742/10000)
percent tensor([0.3952], device='cuda:0')
percent tensor([0.5104], device='cuda:0')
percent tensor([0.4351], device='cuda:0')
percent tensor([0.3783], device='cuda:0')
percent tensor([0.3902], device='cuda:0')
percent tensor([0.3348], device='cuda:0')
percent tensor([0.4125], device='cuda:0')
percent tensor([0.0981], device='cuda:0')
Epoch: 125 | Batch_idx: 0 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (94.00%) (2542/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (3750/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1808) |  Loss2: (0.0000) | Acc: (94.00%) (6154/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (94.00%) (7346/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (94.00%) (8553/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1769) |  Loss2: (0.0000) | Acc: (94.00%) (9761/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (94.00%) (10967/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1788) |  Loss2: (0.0000) | Acc: (94.00%) (12164/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (13372/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (94.00%) (14572/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (15776/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (94.00%) (16984/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (18180/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (94.00%) (19384/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (20585/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (21795/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (23012/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (24222/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (25430/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (26646/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (27839/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (29052/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (30271/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (31483/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (32680/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (33887/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (35078/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (94.00%) (36268/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (37481/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (38691/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (39907/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (41128/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (42317/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (43517/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (94.00%) (44715/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (45919/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (47065/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_125.pth.tar'
# TEST : Loss: (0.4098) | Acc: (87.00%) (8782/10000)
percent tensor([0.3927], device='cuda:0')
percent tensor([0.5086], device='cuda:0')
percent tensor([0.4331], device='cuda:0')
percent tensor([0.3777], device='cuda:0')
percent tensor([0.3889], device='cuda:0')
percent tensor([0.3333], device='cuda:0')
percent tensor([0.4102], device='cuda:0')
percent tensor([0.0969], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.1820) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (3730/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (4928/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (94.00%) (6142/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (7350/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (8563/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (94.00%) (9769/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (10998/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (12226/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (13415/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (14617/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (15827/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (17034/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (18256/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (19458/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (20666/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23088/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (24301/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (25505/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (26705/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (27903/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (29112/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1649) |  Loss2: (0.0000) | Acc: (94.00%) (30317/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (31532/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (32755/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (33958/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (35166/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (36364/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (37564/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (38776/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (39974/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (41163/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (42373/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (94.00%) (43575/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (44783/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1665) |  Loss2: (0.0000) | Acc: (94.00%) (45986/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (47148/50000)
# TEST : Loss: (0.4350) | Acc: (87.00%) (8726/10000)
percent tensor([0.3919], device='cuda:0')
percent tensor([0.5063], device='cuda:0')
percent tensor([0.4319], device='cuda:0')
percent tensor([0.3773], device='cuda:0')
percent tensor([0.3882], device='cuda:0')
percent tensor([0.3321], device='cuda:0')
percent tensor([0.4084], device='cuda:0')
percent tensor([0.0958], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1476) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (2548/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (3767/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (4982/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (6186/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (7411/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (8618/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (9815/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (11025/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (12241/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (13465/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (14680/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (15890/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (17103/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (18311/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (19529/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (20743/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (21956/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (23157/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (24374/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (25583/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (26785/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (27977/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (29189/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (30375/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (31581/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (32793/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (34009/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (35227/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (36419/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (37620/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (38815/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (40021/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (41232/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (42456/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (43668/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (44854/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (46056/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (47199/50000)
# TEST : Loss: (0.5521) | Acc: (84.00%) (8485/10000)
percent tensor([0.3922], device='cuda:0')
percent tensor([0.5076], device='cuda:0')
percent tensor([0.4323], device='cuda:0')
percent tensor([0.3760], device='cuda:0')
percent tensor([0.3868], device='cuda:0')
percent tensor([0.3306], device='cuda:0')
percent tensor([0.4067], device='cuda:0')
percent tensor([0.0946], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (93.00%) (2522/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (4968/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (6182/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (7384/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (8602/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (9819/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (11020/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (12226/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (13438/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (14637/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (15843/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (17047/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (18246/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (19458/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (20662/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (21863/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1600) |  Loss2: (0.0000) | Acc: (94.00%) (23082/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (24299/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (25494/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1615) |  Loss2: (0.0000) | Acc: (94.00%) (26702/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (27896/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (29091/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (30301/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (31498/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (32716/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (33930/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (35144/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (36344/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (37550/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (38754/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (39968/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (41170/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (42377/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (43581/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (44782/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (45984/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (47137/50000)
# TEST : Loss: (0.4209) | Acc: (87.00%) (8752/10000)
percent tensor([0.3916], device='cuda:0')
percent tensor([0.5078], device='cuda:0')
percent tensor([0.4304], device='cuda:0')
percent tensor([0.3755], device='cuda:0')
percent tensor([0.3868], device='cuda:0')
percent tensor([0.3286], device='cuda:0')
percent tensor([0.4060], device='cuda:0')
percent tensor([0.0936], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (2555/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (3771/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (4983/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1484) |  Loss2: (0.0000) | Acc: (94.00%) (6190/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (7398/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (8619/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (11025/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (12243/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (13457/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (14670/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (15876/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (17086/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (94.00%) (18297/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1538) |  Loss2: (0.0000) | Acc: (94.00%) (19512/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (20723/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (21943/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (23146/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (24356/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (25573/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (26788/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (27995/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (29204/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (30418/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (31621/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (32819/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (34036/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (35249/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (36462/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (37660/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (38862/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (40072/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (41278/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (42494/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (43713/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (44927/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (46129/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (47297/50000)
# TEST : Loss: (0.4413) | Acc: (87.00%) (8722/10000)
percent tensor([0.3908], device='cuda:0')
percent tensor([0.5073], device='cuda:0')
percent tensor([0.4303], device='cuda:0')
percent tensor([0.3732], device='cuda:0')
percent tensor([0.3846], device='cuda:0')
percent tensor([0.3270], device='cuda:0')
percent tensor([0.4040], device='cuda:0')
percent tensor([0.0925], device='cuda:0')
Epoch: 130 | Batch_idx: 0 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (2541/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1554) |  Loss2: (0.0000) | Acc: (94.00%) (3755/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (4981/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (6197/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (7415/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (8634/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (9858/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (11076/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (12300/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (13508/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (14725/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (15946/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (17157/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (18368/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (19577/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (20795/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (22008/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (23224/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (24436/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (25648/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (26855/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (28071/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (29294/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1491) |  Loss2: (0.0000) | Acc: (94.00%) (30497/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (31700/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (32910/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (34123/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (35326/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (36538/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (37747/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (38960/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (40184/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (41379/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (42569/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (43781/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (45004/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (46220/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (47386/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_130.pth.tar'
# TEST : Loss: (0.4565) | Acc: (87.00%) (8722/10000)
percent tensor([0.3891], device='cuda:0')
percent tensor([0.5051], device='cuda:0')
percent tensor([0.4294], device='cuda:0')
percent tensor([0.3726], device='cuda:0')
percent tensor([0.3822], device='cuda:0')
percent tensor([0.3256], device='cuda:0')
percent tensor([0.4021], device='cuda:0')
percent tensor([0.0914], device='cuda:0')
Epoch: 131 | Batch_idx: 0 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (5019/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (7464/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (8693/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (9920/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (12349/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (13566/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1404) |  Loss2: (0.0000) | Acc: (95.00%) (14758/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1402) |  Loss2: (0.0000) | Acc: (95.00%) (15975/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (17179/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (18390/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (95.00%) (19599/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (20814/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (22014/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (23235/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (24443/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1453) |  Loss2: (0.0000) | Acc: (95.00%) (25659/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (26874/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (28082/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (29287/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (30495/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (31703/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (32921/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (34116/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (35317/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (36530/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (37749/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (38974/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (40191/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (41407/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (42616/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1511) |  Loss2: (0.0000) | Acc: (94.00%) (43824/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (45030/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (46253/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (47413/50000)
# TEST : Loss: (0.5038) | Acc: (85.00%) (8571/10000)
percent tensor([0.3882], device='cuda:0')
percent tensor([0.5038], device='cuda:0')
percent tensor([0.4290], device='cuda:0')
percent tensor([0.3719], device='cuda:0')
percent tensor([0.3812], device='cuda:0')
percent tensor([0.3241], device='cuda:0')
percent tensor([0.4006], device='cuda:0')
percent tensor([0.0905], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (3760/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (4984/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (6201/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (8626/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (9832/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (11051/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (12266/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1498) |  Loss2: (0.0000) | Acc: (94.00%) (13487/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (14710/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (15926/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1495) |  Loss2: (0.0000) | Acc: (94.00%) (17133/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (94.00%) (18356/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (95.00%) (19584/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (95.00%) (20807/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (95.00%) (22013/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (23225/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1488) |  Loss2: (0.0000) | Acc: (94.00%) (24437/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1497) |  Loss2: (0.0000) | Acc: (94.00%) (25642/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (26848/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1513) |  Loss2: (0.0000) | Acc: (94.00%) (28050/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (29253/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (30470/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (31679/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (32892/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (34102/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (35323/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (36536/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (37756/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (38968/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (40181/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (41395/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (42614/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (43806/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (45031/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (46249/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1515) |  Loss2: (0.0000) | Acc: (94.00%) (47418/50000)
# TEST : Loss: (0.4511) | Acc: (86.00%) (8694/10000)
percent tensor([0.3868], device='cuda:0')
percent tensor([0.5034], device='cuda:0')
percent tensor([0.4267], device='cuda:0')
percent tensor([0.3713], device='cuda:0')
percent tensor([0.3795], device='cuda:0')
percent tensor([0.3224], device='cuda:0')
percent tensor([0.4000], device='cuda:0')
percent tensor([0.0896], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (1332/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (94.00%) (2550/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (5008/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (6234/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (7459/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (8676/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (9903/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (11110/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (12343/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (13558/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (14782/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (16011/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (17218/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (18440/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (19666/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (20871/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (22085/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (23303/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (24500/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (25711/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (26930/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (28149/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (29375/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (30591/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (31783/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (32988/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (34199/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (35409/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (36638/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (37855/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1431) |  Loss2: (0.0000) | Acc: (95.00%) (39077/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (40294/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (41502/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (42721/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (43927/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (45120/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (46333/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1452) |  Loss2: (0.0000) | Acc: (95.00%) (47512/50000)
# TEST : Loss: (0.4725) | Acc: (86.00%) (8608/10000)
percent tensor([0.3864], device='cuda:0')
percent tensor([0.5032], device='cuda:0')
percent tensor([0.4237], device='cuda:0')
percent tensor([0.3719], device='cuda:0')
percent tensor([0.3794], device='cuda:0')
percent tensor([0.3215], device='cuda:0')
percent tensor([0.3987], device='cuda:0')
percent tensor([0.0887], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1378) |  Loss2: (0.0000) | Acc: (94.00%) (2553/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (3773/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (94.00%) (4980/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (94.00%) (6200/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (7419/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1376) |  Loss2: (0.0000) | Acc: (95.00%) (8642/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (9856/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (11069/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (12290/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (13508/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (14718/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (15936/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (17158/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (18380/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (19596/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1400) |  Loss2: (0.0000) | Acc: (95.00%) (20804/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (22030/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (23261/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (24468/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (25689/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1401) |  Loss2: (0.0000) | Acc: (95.00%) (26910/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (95.00%) (28124/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1418) |  Loss2: (0.0000) | Acc: (95.00%) (29334/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (95.00%) (30540/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (31763/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (32991/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (34200/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (35439/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (36656/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (37871/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (39090/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1413) |  Loss2: (0.0000) | Acc: (95.00%) (40302/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (41516/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (42729/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (43944/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (45156/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (46366/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1421) |  Loss2: (0.0000) | Acc: (95.00%) (47538/50000)
# TEST : Loss: (0.4696) | Acc: (86.00%) (8697/10000)
percent tensor([0.3852], device='cuda:0')
percent tensor([0.5024], device='cuda:0')
percent tensor([0.4230], device='cuda:0')
percent tensor([0.3703], device='cuda:0')
percent tensor([0.3789], device='cuda:0')
percent tensor([0.3199], device='cuda:0')
percent tensor([0.3974], device='cuda:0')
percent tensor([0.0877], device='cuda:0')
Epoch: 135 | Batch_idx: 0 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (2572/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (3792/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (5023/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (6238/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (7456/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (8680/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (9906/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (11130/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (12355/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (13568/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (14783/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (16013/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (17245/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1320) |  Loss2: (0.0000) | Acc: (95.00%) (18470/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (19696/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (20926/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (22149/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (23367/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (24578/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (25787/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (26993/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (28210/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1346) |  Loss2: (0.0000) | Acc: (95.00%) (29427/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (30644/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (31867/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (33076/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (34287/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (35501/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (36719/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (37930/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (39132/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (40343/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1391) |  Loss2: (0.0000) | Acc: (95.00%) (41551/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (42778/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (44007/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (45220/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (46442/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1383) |  Loss2: (0.0000) | Acc: (95.00%) (47618/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_135.pth.tar'
# TEST : Loss: (0.4320) | Acc: (87.00%) (8747/10000)
percent tensor([0.3852], device='cuda:0')
percent tensor([0.5024], device='cuda:0')
percent tensor([0.4227], device='cuda:0')
percent tensor([0.3691], device='cuda:0')
percent tensor([0.3764], device='cuda:0')
percent tensor([0.3185], device='cuda:0')
percent tensor([0.3966], device='cuda:0')
percent tensor([0.0868], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (2567/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (6251/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (7486/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (8701/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (9929/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (11146/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (12371/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (13603/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (14833/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (16065/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (17287/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (18512/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (19738/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (20967/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (22186/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (23411/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (24635/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (25859/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (27074/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (28308/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (29525/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (30737/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (31955/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (33177/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (34408/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (35625/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (36855/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (38082/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (39293/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (40516/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (41733/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (42935/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (44137/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (45351/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (46564/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (47738/50000)
# TEST : Loss: (0.4315) | Acc: (87.00%) (8752/10000)
percent tensor([0.3846], device='cuda:0')
percent tensor([0.5022], device='cuda:0')
percent tensor([0.4230], device='cuda:0')
percent tensor([0.3680], device='cuda:0')
percent tensor([0.3746], device='cuda:0')
percent tensor([0.3169], device='cuda:0')
percent tensor([0.3949], device='cuda:0')
percent tensor([0.0859], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (2567/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (3785/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (5004/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (6216/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (7450/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (8675/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (9890/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (11113/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (12342/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (13561/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (14792/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (16027/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (17253/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (18484/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (19716/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (20932/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (22151/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (23367/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (24583/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (25806/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (27026/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (28251/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (29475/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (30710/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (31936/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (33158/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (34382/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1276) |  Loss2: (0.0000) | Acc: (95.00%) (35590/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1274) |  Loss2: (0.0000) | Acc: (95.00%) (36820/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (38038/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (39252/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (40475/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (41688/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (42895/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (44114/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (45337/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (46565/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (95.00%) (47744/50000)
# TEST : Loss: (0.4761) | Acc: (86.00%) (8652/10000)
percent tensor([0.3853], device='cuda:0')
percent tensor([0.5010], device='cuda:0')
percent tensor([0.4227], device='cuda:0')
percent tensor([0.3678], device='cuda:0')
percent tensor([0.3742], device='cuda:0')
percent tensor([0.3156], device='cuda:0')
percent tensor([0.3938], device='cuda:0')
percent tensor([0.0850], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (3823/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1142) |  Loss2: (0.0000) | Acc: (96.00%) (5059/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (96.00%) (6288/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (96.00%) (7509/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (8742/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (96.00%) (9964/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (96.00%) (11189/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (12407/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (13632/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (14852/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (16079/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (17292/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (18515/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (19738/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (20948/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (22160/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (23376/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (24588/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (25818/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (27041/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1303) |  Loss2: (0.0000) | Acc: (95.00%) (28263/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (29485/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (30694/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (31924/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (33140/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (34365/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (35568/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (36782/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1329) |  Loss2: (0.0000) | Acc: (95.00%) (38000/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (39226/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (40434/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (41663/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (42879/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (44089/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (45298/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (46521/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1340) |  Loss2: (0.0000) | Acc: (95.00%) (47691/50000)
# TEST : Loss: (0.4315) | Acc: (87.00%) (8723/10000)
percent tensor([0.3842], device='cuda:0')
percent tensor([0.5001], device='cuda:0')
percent tensor([0.4222], device='cuda:0')
percent tensor([0.3680], device='cuda:0')
percent tensor([0.3728], device='cuda:0')
percent tensor([0.3147], device='cuda:0')
percent tensor([0.3928], device='cuda:0')
percent tensor([0.0843], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (3821/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (96.00%) (5052/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (96.00%) (6273/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (7482/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (8713/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (9924/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (11144/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (12362/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (13598/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (14826/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (16056/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (17280/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (18508/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (19734/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (20977/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (22203/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (23425/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (24650/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (25856/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (27065/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (28293/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (29506/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (30711/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (31932/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (33158/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (34381/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (35613/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (36834/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (38067/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (39296/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (40525/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (41761/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (42972/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (44195/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (45416/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (46637/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (47816/50000)
# TEST : Loss: (0.4499) | Acc: (87.00%) (8749/10000)
percent tensor([0.3829], device='cuda:0')
percent tensor([0.5004], device='cuda:0')
percent tensor([0.4201], device='cuda:0')
percent tensor([0.3669], device='cuda:0')
percent tensor([0.3722], device='cuda:0')
percent tensor([0.3140], device='cuda:0')
percent tensor([0.3912], device='cuda:0')
percent tensor([0.0834], device='cuda:0')
Epoch: 140 | Batch_idx: 0 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (3807/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (5031/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (6259/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (7476/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (8711/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (9935/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (11157/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (12377/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (13584/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (14812/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (16031/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (17260/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (18493/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (19716/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (20927/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (22163/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (23390/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (24605/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (25834/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (27050/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (28282/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (29494/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (30721/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (31943/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (33183/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (34407/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (35638/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (36877/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (38096/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (39327/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (40552/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (41773/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (42998/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (44223/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (45453/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1235) |  Loss2: (0.0000) | Acc: (95.00%) (46690/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (47877/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_140.pth.tar'
# TEST : Loss: (0.4441) | Acc: (87.00%) (8792/10000)
percent tensor([0.3838], device='cuda:0')
percent tensor([0.4995], device='cuda:0')
percent tensor([0.4207], device='cuda:0')
percent tensor([0.3661], device='cuda:0')
percent tensor([0.3723], device='cuda:0')
percent tensor([0.3120], device='cuda:0')
percent tensor([0.3897], device='cuda:0')
percent tensor([0.0826], device='cuda:0')
Epoch: 141 | Batch_idx: 0 |  Loss: (0.1428) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (2584/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (5051/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (6286/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (7514/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (8746/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (9974/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (11209/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (12444/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (13683/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (14916/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (16147/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (17377/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (18608/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (19837/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (21069/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (22295/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (23519/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (24755/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (25982/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (27200/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (28425/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (29662/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (30880/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (32098/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (33317/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (34535/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (35765/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (36983/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (38211/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (39446/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (95.00%) (40663/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (41892/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1167) |  Loss2: (0.0000) | Acc: (95.00%) (43121/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (95.00%) (44335/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (45542/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (46766/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (47930/50000)
# TEST : Loss: (0.5032) | Acc: (86.00%) (8614/10000)
percent tensor([0.3831], device='cuda:0')
percent tensor([0.4988], device='cuda:0')
percent tensor([0.4185], device='cuda:0')
percent tensor([0.3656], device='cuda:0')
percent tensor([0.3704], device='cuda:0')
percent tensor([0.3113], device='cuda:0')
percent tensor([0.3883], device='cuda:0')
percent tensor([0.0818], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1341) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (3781/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (5016/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (6258/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (9936/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (11164/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (12392/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (13617/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1181) |  Loss2: (0.0000) | Acc: (95.00%) (14853/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (16080/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (17302/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (18532/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (19745/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (20971/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (22201/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (23422/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (24652/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (25882/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (27115/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (28327/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (29548/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1209) |  Loss2: (0.0000) | Acc: (95.00%) (30771/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (31999/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (33229/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (34448/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (35672/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (36898/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (38121/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (39349/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (40585/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (41812/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (43042/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (44273/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (45484/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (46706/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (47888/50000)
# TEST : Loss: (0.4225) | Acc: (87.00%) (8789/10000)
percent tensor([0.3825], device='cuda:0')
percent tensor([0.4988], device='cuda:0')
percent tensor([0.4182], device='cuda:0')
percent tensor([0.3647], device='cuda:0')
percent tensor([0.3685], device='cuda:0')
percent tensor([0.3101], device='cuda:0')
percent tensor([0.3877], device='cuda:0')
percent tensor([0.0810], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (1352/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (95.00%) (3808/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (5020/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (6244/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1139) |  Loss2: (0.0000) | Acc: (95.00%) (7481/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (95.00%) (8712/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1110) |  Loss2: (0.0000) | Acc: (95.00%) (9945/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (11183/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (95.00%) (12405/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (95.00%) (13625/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (95.00%) (14868/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (16099/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (17331/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (18569/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (19807/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (21039/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (22272/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (23500/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (24727/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (25973/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (27200/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (28430/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (29660/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (30886/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (96.00%) (32117/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (33341/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1146) |  Loss2: (0.0000) | Acc: (96.00%) (34557/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (35770/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (36995/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (38221/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (39445/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (40679/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (41904/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (43127/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (44347/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (45582/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (46810/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (47992/50000)
# TEST : Loss: (0.4201) | Acc: (88.00%) (8839/10000)
percent tensor([0.3824], device='cuda:0')
percent tensor([0.4971], device='cuda:0')
percent tensor([0.4177], device='cuda:0')
percent tensor([0.3636], device='cuda:0')
percent tensor([0.3676], device='cuda:0')
percent tensor([0.3086], device='cuda:0')
percent tensor([0.3867], device='cuda:0')
percent tensor([0.0803], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (3830/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (5076/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (6317/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (7551/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (8792/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (10019/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (11246/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (12483/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (13701/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (14935/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (16175/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (17395/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (18643/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (19881/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (21108/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (22341/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (23575/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (24808/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (26036/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (27254/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (28481/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (29716/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (30951/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (32172/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (33397/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (34600/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (35820/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (37049/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (96.00%) (38279/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (39511/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (40728/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (41962/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (43189/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (44423/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (45661/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (46887/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (48073/50000)
# TEST : Loss: (0.4011) | Acc: (88.00%) (8876/10000)
percent tensor([0.3809], device='cuda:0')
percent tensor([0.4963], device='cuda:0')
percent tensor([0.4167], device='cuda:0')
percent tensor([0.3627], device='cuda:0')
percent tensor([0.3672], device='cuda:0')
percent tensor([0.3085], device='cuda:0')
percent tensor([0.3858], device='cuda:0')
percent tensor([0.0796], device='cuda:0')
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (6298/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (7532/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (8765/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (9999/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (11232/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (12466/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (13697/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (14935/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (16171/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (17411/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (18647/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (19872/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (21111/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (22341/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (23565/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (24797/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (26033/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (27266/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (28506/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (29742/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (30957/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (32181/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (33420/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (34650/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (35867/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (37098/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (38328/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1084) |  Loss2: (0.0000) | Acc: (96.00%) (39565/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (40784/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (42016/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (43246/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (44471/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (45715/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (46955/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (48145/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_145.pth.tar'
# TEST : Loss: (0.5077) | Acc: (86.00%) (8665/10000)
percent tensor([0.3799], device='cuda:0')
percent tensor([0.4950], device='cuda:0')
percent tensor([0.4152], device='cuda:0')
percent tensor([0.3626], device='cuda:0')
percent tensor([0.3659], device='cuda:0')
percent tensor([0.3067], device='cuda:0')
percent tensor([0.3849], device='cuda:0')
percent tensor([0.0788], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (3816/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (5056/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (6289/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (8762/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (10002/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (11236/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (12458/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (13696/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (14918/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (16162/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (17389/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (18622/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (19845/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (21081/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (22320/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (23568/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (24803/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (26020/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (27246/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (28482/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (29722/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (30945/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (32175/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (33406/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (34645/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (35868/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (37106/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (38349/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (39573/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (40799/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (42031/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (43255/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (44498/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (45727/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (46960/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (48147/50000)
# TEST : Loss: (0.4225) | Acc: (88.00%) (8829/10000)
percent tensor([0.3806], device='cuda:0')
percent tensor([0.4951], device='cuda:0')
percent tensor([0.4150], device='cuda:0')
percent tensor([0.3615], device='cuda:0')
percent tensor([0.3657], device='cuda:0')
percent tensor([0.3064], device='cuda:0')
percent tensor([0.3840], device='cuda:0')
percent tensor([0.0781], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (2601/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (3837/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (5071/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (6301/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (8772/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (9997/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1040) |  Loss2: (0.0000) | Acc: (96.00%) (11234/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1037) |  Loss2: (0.0000) | Acc: (96.00%) (12470/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (13700/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (14928/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (16163/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (17400/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (18635/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (19872/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (21110/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (22344/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1038) |  Loss2: (0.0000) | Acc: (96.00%) (23570/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (24806/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (26036/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (27271/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (28505/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (29743/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (30981/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (32208/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (33432/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (34661/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (35908/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (37141/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (38373/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (39615/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (40855/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (42079/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (43307/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (44536/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (45774/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (46998/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (48190/50000)
# TEST : Loss: (0.4441) | Acc: (87.00%) (8774/10000)
percent tensor([0.3794], device='cuda:0')
percent tensor([0.4952], device='cuda:0')
percent tensor([0.4149], device='cuda:0')
percent tensor([0.3609], device='cuda:0')
percent tensor([0.3644], device='cuda:0')
percent tensor([0.3052], device='cuda:0')
percent tensor([0.3832], device='cuda:0')
percent tensor([0.0774], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1035) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (2592/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1042) |  Loss2: (0.0000) | Acc: (96.00%) (3812/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (5044/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (6276/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (7499/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (8725/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (9965/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (11193/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (12432/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (13663/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1044) |  Loss2: (0.0000) | Acc: (96.00%) (14902/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1046) |  Loss2: (0.0000) | Acc: (96.00%) (16129/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (17340/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (18572/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (19793/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (21023/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (22257/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (23487/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (24726/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (25952/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (27184/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (28427/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1047) |  Loss2: (0.0000) | Acc: (96.00%) (29668/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (30907/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1049) |  Loss2: (0.0000) | Acc: (96.00%) (32132/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (33363/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (34596/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (35827/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (37060/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (38286/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (39527/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (40751/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (41972/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (43207/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (44438/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (45667/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (46907/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (48096/50000)
# TEST : Loss: (0.4366) | Acc: (88.00%) (8803/10000)
percent tensor([0.3804], device='cuda:0')
percent tensor([0.4948], device='cuda:0')
percent tensor([0.4140], device='cuda:0')
percent tensor([0.3600], device='cuda:0')
percent tensor([0.3630], device='cuda:0')
percent tensor([0.3042], device='cuda:0')
percent tensor([0.3833], device='cuda:0')
percent tensor([0.0768], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (2595/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (3838/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (6323/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (7551/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (8783/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (10014/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (11243/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (12479/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (13722/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (14963/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (16202/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (17439/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (18679/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1004) |  Loss2: (0.0000) | Acc: (96.00%) (19920/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (21156/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (22400/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (23637/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (24880/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (26126/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (27356/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (28582/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (29822/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (31071/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (32302/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (33529/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (34774/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (36005/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (37247/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (38483/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (39728/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (40960/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (42193/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (43429/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (44663/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (45906/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (47140/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (48321/50000)
# TEST : Loss: (0.4261) | Acc: (88.00%) (8868/10000)
percent tensor([0.3794], device='cuda:0')
percent tensor([0.4935], device='cuda:0')
percent tensor([0.4138], device='cuda:0')
percent tensor([0.3585], device='cuda:0')
percent tensor([0.3634], device='cuda:0')
percent tensor([0.3026], device='cuda:0')
percent tensor([0.3814], device='cuda:0')
percent tensor([0.0761], device='cuda:0')
Epoch: 150 | Batch_idx: 0 |  Loss: (0.0427) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (1363/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (2592/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (3833/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (5074/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (6311/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.0969) |  Loss2: (0.0000) | Acc: (96.00%) (7553/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (8807/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (10046/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (11287/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (12531/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (13776/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (15010/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (16240/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (17484/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (18733/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (19980/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (21216/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (22457/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (23689/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (24930/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (26161/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (27403/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (28640/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (29885/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (31130/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (32356/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (33591/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (34828/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (36060/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (37302/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (38536/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (39769/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (40988/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (42228/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (43461/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (44691/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (45927/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (47171/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (48356/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_150.pth.tar'
# TEST : Loss: (0.4434) | Acc: (88.00%) (8829/10000)
percent tensor([0.3784], device='cuda:0')
percent tensor([0.4938], device='cuda:0')
percent tensor([0.4129], device='cuda:0')
percent tensor([0.3579], device='cuda:0')
percent tensor([0.3626], device='cuda:0')
percent tensor([0.3020], device='cuda:0')
percent tensor([0.3799], device='cuda:0')
percent tensor([0.0755], device='cuda:0')
Epoch: 151 | Batch_idx: 0 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (2604/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (3832/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (5062/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (6308/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (7547/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (8786/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (10026/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (11268/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (12507/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (13752/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (14971/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (16218/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (17458/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (18687/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (19919/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (21157/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (22397/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (23630/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (24874/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (26112/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (27341/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (28585/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (29823/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (31063/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (32304/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (33544/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (34786/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (36032/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (37273/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (38497/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (39740/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (40969/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (42201/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (43441/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (44681/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (45922/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (47160/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (48356/50000)
# TEST : Loss: (0.5366) | Acc: (86.00%) (8603/10000)
percent tensor([0.3792], device='cuda:0')
percent tensor([0.4933], device='cuda:0')
percent tensor([0.4122], device='cuda:0')
percent tensor([0.3573], device='cuda:0')
percent tensor([0.3620], device='cuda:0')
percent tensor([0.3006], device='cuda:0')
percent tensor([0.3794], device='cuda:0')
percent tensor([0.0748], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (3840/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (96.00%) (5080/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (7565/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (8803/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.0876) |  Loss2: (0.0000) | Acc: (96.00%) (10046/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (11279/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (12532/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (13763/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (15006/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (16250/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (96.00%) (17498/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (18741/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (19994/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (96.00%) (21226/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (22461/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (23693/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (24930/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (26170/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (27415/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (28657/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (29894/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (31127/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (32356/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (33592/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (34837/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (36077/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (37311/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (38545/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (39782/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (41024/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (42262/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (43513/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (44755/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (45985/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (47224/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (48416/50000)
# TEST : Loss: (0.4720) | Acc: (87.00%) (8774/10000)
percent tensor([0.3783], device='cuda:0')
percent tensor([0.4945], device='cuda:0')
percent tensor([0.4125], device='cuda:0')
percent tensor([0.3575], device='cuda:0')
percent tensor([0.3608], device='cuda:0')
percent tensor([0.3000], device='cuda:0')
percent tensor([0.3793], device='cuda:0')
percent tensor([0.0742], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (2606/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (3850/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (6333/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (7575/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (8818/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (10057/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (11295/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (12533/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (13771/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (15018/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (16264/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (17513/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (18752/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (19982/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (21214/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (22449/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (23688/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (24923/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (26163/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (27404/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (28638/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (29881/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (96.00%) (31116/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (96.00%) (32358/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (33573/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (34803/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (36028/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (37276/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (38519/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (39761/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (40995/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (42242/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (43493/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (44733/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (45972/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (47214/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (48420/50000)
# TEST : Loss: (0.4272) | Acc: (88.00%) (8867/10000)
percent tensor([0.3779], device='cuda:0')
percent tensor([0.4942], device='cuda:0')
percent tensor([0.4113], device='cuda:0')
percent tensor([0.3580], device='cuda:0')
percent tensor([0.3602], device='cuda:0')
percent tensor([0.2993], device='cuda:0')
percent tensor([0.3780], device='cuda:0')
percent tensor([0.0736], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (3858/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (5115/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (6371/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (7617/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (8874/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (10122/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (11365/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (12607/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (13850/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (15093/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (16335/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (17583/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (18831/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (20078/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (21318/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (22571/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (23803/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (25051/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (26296/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (27541/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (28785/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (30029/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (31270/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (32512/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (33763/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (35003/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (36232/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (37477/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (38712/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (39954/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (41195/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (42441/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (43675/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (44912/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (46155/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (47378/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (48566/50000)
# TEST : Loss: (0.4360) | Acc: (88.00%) (8808/10000)
percent tensor([0.3775], device='cuda:0')
percent tensor([0.4931], device='cuda:0')
percent tensor([0.4099], device='cuda:0')
percent tensor([0.3573], device='cuda:0')
percent tensor([0.3597], device='cuda:0')
percent tensor([0.2983], device='cuda:0')
percent tensor([0.3780], device='cuda:0')
percent tensor([0.0730], device='cuda:0')
Epoch: 155 | Batch_idx: 0 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (3872/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (5115/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (7598/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (8838/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (10088/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (11326/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (12568/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (13823/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (15066/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (16323/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (17574/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (18829/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (20082/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (21333/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (22577/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (23812/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (25059/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (26308/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (27557/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (28789/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (30045/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (31284/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (32527/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (33758/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (34997/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (36247/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (37486/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (38722/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (39962/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (41198/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (42428/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (43667/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (44905/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (46154/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (47394/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (97.00%) (48586/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_155.pth.tar'
# TEST : Loss: (0.4869) | Acc: (87.00%) (8717/10000)
percent tensor([0.3757], device='cuda:0')
percent tensor([0.4916], device='cuda:0')
percent tensor([0.4090], device='cuda:0')
percent tensor([0.3563], device='cuda:0')
percent tensor([0.3574], device='cuda:0')
percent tensor([0.2976], device='cuda:0')
percent tensor([0.3778], device='cuda:0')
percent tensor([0.0724], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (3868/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (5113/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (6360/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (7608/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (8848/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (10101/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (11341/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (12576/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (13830/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (15070/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (16309/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (17557/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (18807/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (20055/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (21305/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (22560/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (23809/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (25045/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (26278/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (27517/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (28751/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (29982/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (31224/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (32471/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (33706/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (34949/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (36193/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (37438/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (38664/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (39904/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (41142/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (42387/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.0865) |  Loss2: (0.0000) | Acc: (97.00%) (43631/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.0863) |  Loss2: (0.0000) | Acc: (97.00%) (44883/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (46129/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (47371/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (48566/50000)
# TEST : Loss: (0.4369) | Acc: (88.00%) (8828/10000)
percent tensor([0.3763], device='cuda:0')
percent tensor([0.4913], device='cuda:0')
percent tensor([0.4076], device='cuda:0')
percent tensor([0.3548], device='cuda:0')
percent tensor([0.3568], device='cuda:0')
percent tensor([0.2961], device='cuda:0')
percent tensor([0.3775], device='cuda:0')
percent tensor([0.0718], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.0922) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (3867/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (5111/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (7600/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (8846/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (11335/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (12576/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (13816/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (15060/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (16305/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (17543/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (18791/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (20020/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (21266/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (22498/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (23733/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (24976/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.0856) |  Loss2: (0.0000) | Acc: (97.00%) (26215/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (27464/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (28714/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (29955/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (31195/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (32443/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (33685/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (34923/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (36156/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (37400/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (38656/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (39899/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.0846) |  Loss2: (0.0000) | Acc: (97.00%) (41140/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (42379/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (43614/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (44849/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (46088/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (47336/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (48527/50000)
# TEST : Loss: (0.4474) | Acc: (88.00%) (8823/10000)
percent tensor([0.3749], device='cuda:0')
percent tensor([0.4911], device='cuda:0')
percent tensor([0.4079], device='cuda:0')
percent tensor([0.3545], device='cuda:0')
percent tensor([0.3562], device='cuda:0')
percent tensor([0.2954], device='cuda:0')
percent tensor([0.3763], device='cuda:0')
percent tensor([0.0713], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (2608/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (5103/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (6351/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (7598/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (8849/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (10101/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (11339/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (12593/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (13842/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (15083/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (16336/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (17591/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (18834/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (20079/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (21322/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (22565/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (23809/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (25057/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (26297/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (27530/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (28781/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (30023/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (31262/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (32508/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (33747/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (34987/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (36236/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (37473/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (38712/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (39956/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (41200/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (42447/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (43689/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (44940/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (46174/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (47403/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (48605/50000)
# TEST : Loss: (0.4840) | Acc: (87.00%) (8736/10000)
percent tensor([0.3750], device='cuda:0')
percent tensor([0.4908], device='cuda:0')
percent tensor([0.4077], device='cuda:0')
percent tensor([0.3544], device='cuda:0')
percent tensor([0.3552], device='cuda:0')
percent tensor([0.2944], device='cuda:0')
percent tensor([0.3765], device='cuda:0')
percent tensor([0.0707], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (2600/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (96.00%) (3843/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (6347/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (7591/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (8828/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (10071/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (11325/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (12569/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (13798/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (15044/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (16284/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (17533/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (18766/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (20009/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (21258/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (22501/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (23741/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (24983/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (26223/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (27474/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (28716/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (29973/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (31219/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (32469/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (33716/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (34959/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (36202/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (37424/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (38674/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (39917/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (41160/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (42399/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (43636/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (44871/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (46112/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (47349/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (48538/50000)
# TEST : Loss: (0.4223) | Acc: (88.00%) (8864/10000)
percent tensor([0.3748], device='cuda:0')
percent tensor([0.4905], device='cuda:0')
percent tensor([0.4070], device='cuda:0')
percent tensor([0.3534], device='cuda:0')
percent tensor([0.3543], device='cuda:0')
percent tensor([0.2935], device='cuda:0')
percent tensor([0.3761], device='cuda:0')
percent tensor([0.0702], device='cuda:0')
Epoch: 160 | Batch_idx: 0 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (1370/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (3860/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (5109/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (6359/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (7593/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (8846/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (10100/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (11356/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (12594/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (13838/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (15090/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (16329/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (17575/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (18817/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (20072/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (21318/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (22555/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (23800/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (25046/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (26294/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (27543/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (28785/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (30041/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (31291/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (32527/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (33781/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (35023/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (36272/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (37510/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (38749/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (39994/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (41237/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (42485/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (43719/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (44973/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (46219/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (47464/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (48665/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_160.pth.tar'
# TEST : Loss: (0.4456) | Acc: (88.00%) (8816/10000)
percent tensor([0.3754], device='cuda:0')
percent tensor([0.4904], device='cuda:0')
percent tensor([0.4072], device='cuda:0')
percent tensor([0.3535], device='cuda:0')
percent tensor([0.3549], device='cuda:0')
percent tensor([0.2927], device='cuda:0')
percent tensor([0.3749], device='cuda:0')
percent tensor([0.0697], device='cuda:0')
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (3865/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (5114/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (6372/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (7623/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (8870/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (10122/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (11369/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (12611/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (13858/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (15098/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (16341/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (17592/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (18850/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (20095/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (21338/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (22578/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (23823/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (25072/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (26317/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (27561/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (28803/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (30050/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (31296/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (32543/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (33793/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (35041/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (36285/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (37523/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (38760/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (39993/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (41245/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (42491/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (43739/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (44984/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (46234/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (47480/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (48682/50000)
# TEST : Loss: (0.4993) | Acc: (87.00%) (8763/10000)
percent tensor([0.3740], device='cuda:0')
percent tensor([0.4894], device='cuda:0')
percent tensor([0.4064], device='cuda:0')
percent tensor([0.3523], device='cuda:0')
percent tensor([0.3551], device='cuda:0')
percent tensor([0.2924], device='cuda:0')
percent tensor([0.3745], device='cuda:0')
percent tensor([0.0692], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (2614/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (6365/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (7622/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (8873/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (10125/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (11380/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (12629/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (13868/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (15118/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (16369/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (17613/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (18866/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (20111/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (21358/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (22608/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (23861/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (25097/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (26350/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (27589/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (28834/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (30084/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (31325/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (32563/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (33806/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (35053/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (36291/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (37529/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (38776/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (40019/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (41250/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (42482/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (43724/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (44968/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (46215/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (47459/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (48657/50000)
# TEST : Loss: (0.4692) | Acc: (88.00%) (8813/10000)
percent tensor([0.3729], device='cuda:0')
percent tensor([0.4879], device='cuda:0')
percent tensor([0.4060], device='cuda:0')
percent tensor([0.3516], device='cuda:0')
percent tensor([0.3541], device='cuda:0')
percent tensor([0.2912], device='cuda:0')
percent tensor([0.3742], device='cuda:0')
percent tensor([0.0688], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.0154) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (2630/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (3882/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (5139/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (6391/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.0649) |  Loss2: (0.0000) | Acc: (97.00%) (7648/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.0648) |  Loss2: (0.0000) | Acc: (97.00%) (8900/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (10156/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (11407/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (12653/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (13904/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (15153/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (16403/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (17636/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (18880/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (20123/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (21366/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (22613/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (23857/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (25102/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (26336/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (27580/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (28820/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (30066/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (31310/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (32556/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (33801/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (35044/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (36288/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (37533/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (38785/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (40034/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (41276/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (42529/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (43780/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (45015/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (46253/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (47495/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (48696/50000)
# TEST : Loss: (0.5181) | Acc: (86.00%) (8690/10000)
percent tensor([0.3740], device='cuda:0')
percent tensor([0.4881], device='cuda:0')
percent tensor([0.4057], device='cuda:0')
percent tensor([0.3518], device='cuda:0')
percent tensor([0.3537], device='cuda:0')
percent tensor([0.2902], device='cuda:0')
percent tensor([0.3727], device='cuda:0')
percent tensor([0.0683], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (1378/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (2619/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (3866/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (5119/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (6367/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (7619/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (8870/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (10120/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (11367/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (12610/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (13857/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (15110/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (16360/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (17603/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (18842/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (20091/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.0730) |  Loss2: (0.0000) | Acc: (97.00%) (21347/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (22585/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (23824/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (25077/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (26325/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (27585/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (28832/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (30078/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (31331/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (32578/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (33829/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (35080/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (36322/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (37574/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (38816/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (40065/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (41311/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (42556/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (43802/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (45050/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (46293/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (47539/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (48742/50000)
# TEST : Loss: (0.4606) | Acc: (88.00%) (8810/10000)
percent tensor([0.3734], device='cuda:0')
percent tensor([0.4887], device='cuda:0')
percent tensor([0.4046], device='cuda:0')
percent tensor([0.3498], device='cuda:0')
percent tensor([0.3532], device='cuda:0')
percent tensor([0.2894], device='cuda:0')
percent tensor([0.3718], device='cuda:0')
percent tensor([0.0678], device='cuda:0')
0 hours 58 mins 23 secs for training