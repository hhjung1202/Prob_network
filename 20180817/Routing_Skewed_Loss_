Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_models/checkpoint_080.pth.tar'
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.3967) |  Loss2: (0.0000) | Acc: (86.00%) (2325/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.3779) |  Loss2: (0.0000) | Acc: (87.00%) (3454/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.3752) |  Loss2: (0.0000) | Acc: (87.00%) (4569/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.3666) |  Loss2: (0.0000) | Acc: (87.00%) (5709/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (6834/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.3739) |  Loss2: (0.0000) | Acc: (87.00%) (7939/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (9046/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.3735) |  Loss2: (0.0000) | Acc: (87.00%) (10171/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.3729) |  Loss2: (0.0000) | Acc: (87.00%) (11288/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (12426/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.3704) |  Loss2: (0.0000) | Acc: (87.00%) (13539/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (14644/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (15781/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.3730) |  Loss2: (0.0000) | Acc: (87.00%) (16882/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.3715) |  Loss2: (0.0000) | Acc: (87.00%) (18010/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (19124/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.3698) |  Loss2: (0.0000) | Acc: (87.00%) (20249/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (21370/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (22506/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.3687) |  Loss2: (0.0000) | Acc: (87.00%) (23618/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (24731/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (25850/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (26952/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (28074/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (29172/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (30265/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (31376/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (32505/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (33636/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (34736/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (35848/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.3720) |  Loss2: (0.0000) | Acc: (87.00%) (36967/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (38102/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (39212/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (40302/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.3723) |  Loss2: (0.0000) | Acc: (87.00%) (41424/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (42531/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.3721) |  Loss2: (0.0000) | Acc: (87.00%) (43604/50000)
# TEST : Loss: (0.5134) | Acc: (83.00%) (8362/10000)
percent tensor([0.6140], device='cuda:0')
percent tensor([0.6123], device='cuda:0')
percent tensor([0.7342], device='cuda:0')
percent tensor([0.6396], device='cuda:0')
percent tensor([0.7436], device='cuda:0')
percent tensor([0.7721], device='cuda:0')
percent tensor([0.7961], device='cuda:0')
percent tensor([0.2357], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.4222) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.4197) |  Loss2: (0.0000) | Acc: (85.00%) (1207/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (2332/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (3469/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (87.00%) (4584/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (5712/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (87.00%) (6824/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.3765) |  Loss2: (0.0000) | Acc: (87.00%) (7948/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.3733) |  Loss2: (0.0000) | Acc: (87.00%) (9069/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.3736) |  Loss2: (0.0000) | Acc: (87.00%) (10194/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.3722) |  Loss2: (0.0000) | Acc: (87.00%) (11319/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (12428/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (13547/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (14676/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (15792/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (16898/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (18017/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.3686) |  Loss2: (0.0000) | Acc: (87.00%) (19136/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (20287/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (21405/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.3669) |  Loss2: (0.0000) | Acc: (87.00%) (22530/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.3649) |  Loss2: (0.0000) | Acc: (87.00%) (23677/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (24780/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.3657) |  Loss2: (0.0000) | Acc: (87.00%) (25889/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (27023/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (28149/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.3638) |  Loss2: (0.0000) | Acc: (87.00%) (29284/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (30414/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (31538/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (32676/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (33810/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.3622) |  Loss2: (0.0000) | Acc: (87.00%) (34921/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.3632) |  Loss2: (0.0000) | Acc: (87.00%) (36031/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (37147/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.3633) |  Loss2: (0.0000) | Acc: (87.00%) (38266/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (39382/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (40489/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (41599/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (87.00%) (42716/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (43807/50000)
# TEST : Loss: (0.4680) | Acc: (84.00%) (8410/10000)
percent tensor([0.6140], device='cuda:0')
percent tensor([0.6123], device='cuda:0')
percent tensor([0.7342], device='cuda:0')
percent tensor([0.6396], device='cuda:0')
percent tensor([0.7436], device='cuda:0')
percent tensor([0.7721], device='cuda:0')
percent tensor([0.7961], device='cuda:0')
percent tensor([0.2357], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.3436) |  Loss2: (0.0000) | Acc: (87.00%) (1235/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (2348/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (3472/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.3456) |  Loss2: (0.0000) | Acc: (87.00%) (5729/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (87.00%) (6867/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (87.00%) (7980/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.3485) |  Loss2: (0.0000) | Acc: (87.00%) (9103/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (10225/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.3527) |  Loss2: (0.0000) | Acc: (87.00%) (11346/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (12463/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.3539) |  Loss2: (0.0000) | Acc: (87.00%) (13594/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (14709/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.3574) |  Loss2: (0.0000) | Acc: (87.00%) (15824/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.3558) |  Loss2: (0.0000) | Acc: (87.00%) (16967/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (18075/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (19183/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (20284/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (21411/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (22545/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (23663/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.3593) |  Loss2: (0.0000) | Acc: (87.00%) (24788/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (25902/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (27020/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (28131/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.3599) |  Loss2: (0.0000) | Acc: (87.00%) (29254/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.3591) |  Loss2: (0.0000) | Acc: (87.00%) (30391/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (31520/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (32637/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (33780/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (34908/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.3582) |  Loss2: (0.0000) | Acc: (87.00%) (36024/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (37137/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (38248/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (39375/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.3586) |  Loss2: (0.0000) | Acc: (87.00%) (40498/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (41616/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (42732/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.3588) |  Loss2: (0.0000) | Acc: (87.00%) (43810/50000)
# TEST : Loss: (0.5482) | Acc: (82.00%) (8207/10000)
percent tensor([0.6140], device='cuda:0')
percent tensor([0.6123], device='cuda:0')
percent tensor([0.7342], device='cuda:0')
percent tensor([0.6396], device='cuda:0')
percent tensor([0.7436], device='cuda:0')
percent tensor([0.7721], device='cuda:0')
percent tensor([0.7961], device='cuda:0')
percent tensor([0.2357], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (1237/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (2375/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.3399) |  Loss2: (0.0000) | Acc: (88.00%) (3503/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (4629/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (5760/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (88.00%) (6885/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.3475) |  Loss2: (0.0000) | Acc: (88.00%) (8024/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (9160/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (10286/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (11414/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (12565/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (13712/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (14822/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (88.00%) (15926/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (17057/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (88.00%) (18172/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.3486) |  Loss2: (0.0000) | Acc: (88.00%) (19277/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.3487) |  Loss2: (0.0000) | Acc: (88.00%) (20401/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (88.00%) (21537/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (22652/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (23789/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (88.00%) (24920/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (26054/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (88.00%) (27182/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (28333/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (29469/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.3460) |  Loss2: (0.0000) | Acc: (88.00%) (30591/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.3458) |  Loss2: (0.0000) | Acc: (88.00%) (31717/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (32844/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.3462) |  Loss2: (0.0000) | Acc: (88.00%) (33975/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (35093/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (36224/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (88.00%) (37351/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (38497/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (39615/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (40750/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (88.00%) (41883/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (88.00%) (43002/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.3480) |  Loss2: (0.0000) | Acc: (88.00%) (44078/50000)
# TEST : Loss: (0.5133) | Acc: (82.00%) (8276/10000)
percent tensor([0.6140], device='cuda:0')
percent tensor([0.6123], device='cuda:0')
percent tensor([0.7342], device='cuda:0')
percent tensor([0.6396], device='cuda:0')
percent tensor([0.7436], device='cuda:0')
percent tensor([0.7721], device='cuda:0')
percent tensor([0.7961], device='cuda:0')
percent tensor([0.2357], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.8442) |  Loss2: (0.4462) | Acc: (87.00%) (112/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.7976) |  Loss2: (0.4461) | Acc: (87.00%) (1236/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.8436) |  Loss2: (0.4460) | Acc: (85.00%) (2305/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.8577) |  Loss2: (0.4459) | Acc: (85.00%) (3374/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.8671) |  Loss2: (0.4458) | Acc: (84.00%) (4449/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.8696) |  Loss2: (0.4458) | Acc: (84.00%) (5533/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.8687) |  Loss2: (0.4457) | Acc: (84.00%) (6636/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.8638) |  Loss2: (0.4456) | Acc: (85.00%) (7745/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.8643) |  Loss2: (0.4454) | Acc: (85.00%) (8829/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.8635) |  Loss2: (0.4453) | Acc: (85.00%) (9937/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.8650) |  Loss2: (0.4452) | Acc: (85.00%) (11027/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.8610) |  Loss2: (0.4451) | Acc: (85.00%) (12134/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.8650) |  Loss2: (0.4450) | Acc: (85.00%) (13201/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.8634) |  Loss2: (0.4449) | Acc: (85.00%) (14323/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.8630) |  Loss2: (0.4448) | Acc: (85.00%) (15431/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.8604) |  Loss2: (0.4447) | Acc: (85.00%) (16540/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.8597) |  Loss2: (0.4446) | Acc: (85.00%) (17633/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.8597) |  Loss2: (0.4445) | Acc: (85.00%) (18737/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.8545) |  Loss2: (0.4444) | Acc: (85.00%) (19869/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.8536) |  Loss2: (0.4443) | Acc: (85.00%) (20970/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.8523) |  Loss2: (0.4442) | Acc: (85.00%) (22087/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.8516) |  Loss2: (0.4442) | Acc: (85.00%) (23198/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.8514) |  Loss2: (0.4441) | Acc: (85.00%) (24295/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.8504) |  Loss2: (0.4440) | Acc: (85.00%) (25418/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.8490) |  Loss2: (0.4439) | Acc: (85.00%) (26523/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.8490) |  Loss2: (0.4438) | Acc: (85.00%) (27624/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.8469) |  Loss2: (0.4437) | Acc: (86.00%) (28738/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.8456) |  Loss2: (0.4436) | Acc: (86.00%) (29863/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.8450) |  Loss2: (0.4435) | Acc: (86.00%) (30968/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.8443) |  Loss2: (0.4434) | Acc: (86.00%) (32081/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.8426) |  Loss2: (0.4433) | Acc: (86.00%) (33201/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.8405) |  Loss2: (0.4432) | Acc: (86.00%) (34335/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.8394) |  Loss2: (0.4431) | Acc: (86.00%) (35444/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.8384) |  Loss2: (0.4430) | Acc: (86.00%) (36580/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.8369) |  Loss2: (0.4429) | Acc: (86.00%) (37709/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.8358) |  Loss2: (0.4429) | Acc: (86.00%) (38831/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.8362) |  Loss2: (0.4428) | Acc: (86.00%) (39935/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.8355) |  Loss2: (0.4427) | Acc: (86.00%) (41055/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.8352) |  Loss2: (0.4426) | Acc: (86.00%) (42163/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.8338) |  Loss2: (0.4425) | Acc: (86.00%) (43256/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_085.pth.tar'
# TEST : Loss: (0.4933) | Acc: (83.00%) (8379/10000)
percent tensor([0.6124], device='cuda:0')
percent tensor([0.6151], device='cuda:0')
percent tensor([0.7439], device='cuda:0')
percent tensor([0.6498], device='cuda:0')
percent tensor([0.7548], device='cuda:0')
percent tensor([0.7796], device='cuda:0')
percent tensor([0.8040], device='cuda:0')
percent tensor([0.2350], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.7329) |  Loss2: (0.4392) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.7677) |  Loss2: (0.4391) | Acc: (88.00%) (1251/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.7921) |  Loss2: (0.4391) | Acc: (88.00%) (2373/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.7966) |  Loss2: (0.4391) | Acc: (88.00%) (3494/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.7978) |  Loss2: (0.4390) | Acc: (87.00%) (4614/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.7959) |  Loss2: (0.4389) | Acc: (87.00%) (5743/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.7964) |  Loss2: (0.4389) | Acc: (87.00%) (6861/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.7978) |  Loss2: (0.4388) | Acc: (87.00%) (7991/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.8000) |  Loss2: (0.4387) | Acc: (87.00%) (9101/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.8011) |  Loss2: (0.4387) | Acc: (87.00%) (10223/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.8031) |  Loss2: (0.4386) | Acc: (87.00%) (11332/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.8014) |  Loss2: (0.4385) | Acc: (87.00%) (12475/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.7999) |  Loss2: (0.4385) | Acc: (87.00%) (13596/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.7992) |  Loss2: (0.4384) | Acc: (87.00%) (14719/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.8026) |  Loss2: (0.4383) | Acc: (87.00%) (15817/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.8027) |  Loss2: (0.4383) | Acc: (87.00%) (16941/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.8046) |  Loss2: (0.4382) | Acc: (87.00%) (18044/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.8028) |  Loss2: (0.4381) | Acc: (87.00%) (19179/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.8019) |  Loss2: (0.4381) | Acc: (87.00%) (20304/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.7998) |  Loss2: (0.4380) | Acc: (87.00%) (21440/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.7996) |  Loss2: (0.4380) | Acc: (87.00%) (22559/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.7988) |  Loss2: (0.4379) | Acc: (87.00%) (23686/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.7986) |  Loss2: (0.4378) | Acc: (87.00%) (24801/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.7980) |  Loss2: (0.4378) | Acc: (87.00%) (25929/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.7990) |  Loss2: (0.4377) | Acc: (87.00%) (27044/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.7985) |  Loss2: (0.4377) | Acc: (87.00%) (28176/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.7987) |  Loss2: (0.4376) | Acc: (87.00%) (29299/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.8000) |  Loss2: (0.4375) | Acc: (87.00%) (30415/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.8002) |  Loss2: (0.4375) | Acc: (87.00%) (31531/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.7992) |  Loss2: (0.4374) | Acc: (87.00%) (32675/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.7988) |  Loss2: (0.4374) | Acc: (87.00%) (33810/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.7978) |  Loss2: (0.4373) | Acc: (87.00%) (34939/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.7971) |  Loss2: (0.4373) | Acc: (87.00%) (36077/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.7972) |  Loss2: (0.4372) | Acc: (87.00%) (37201/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.7974) |  Loss2: (0.4372) | Acc: (87.00%) (38299/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.7980) |  Loss2: (0.4371) | Acc: (87.00%) (39406/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.7982) |  Loss2: (0.4371) | Acc: (87.00%) (40518/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.7985) |  Loss2: (0.4370) | Acc: (87.00%) (41635/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.7997) |  Loss2: (0.4370) | Acc: (87.00%) (42739/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.7986) |  Loss2: (0.4370) | Acc: (87.00%) (43828/50000)
# TEST : Loss: (0.4708) | Acc: (84.00%) (8407/10000)
percent tensor([0.6183], device='cuda:0')
percent tensor([0.6197], device='cuda:0')
percent tensor([0.7469], device='cuda:0')
percent tensor([0.6530], device='cuda:0')
percent tensor([0.7583], device='cuda:0')
percent tensor([0.7832], device='cuda:0')
percent tensor([0.8087], device='cuda:0')
percent tensor([0.2313], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.7634) |  Loss2: (0.4353) | Acc: (90.00%) (116/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.7959) |  Loss2: (0.4353) | Acc: (88.00%) (1241/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.7782) |  Loss2: (0.4352) | Acc: (88.00%) (2380/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.7761) |  Loss2: (0.4352) | Acc: (88.00%) (3514/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.7808) |  Loss2: (0.4352) | Acc: (88.00%) (4639/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.7810) |  Loss2: (0.4352) | Acc: (88.00%) (5765/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.7833) |  Loss2: (0.4351) | Acc: (88.00%) (6897/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.7814) |  Loss2: (0.4351) | Acc: (88.00%) (8028/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.7800) |  Loss2: (0.4351) | Acc: (88.00%) (9159/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.7846) |  Loss2: (0.4350) | Acc: (88.00%) (10263/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.7873) |  Loss2: (0.4350) | Acc: (87.00%) (11375/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.7850) |  Loss2: (0.4349) | Acc: (88.00%) (12516/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.7825) |  Loss2: (0.4349) | Acc: (88.00%) (13658/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.7849) |  Loss2: (0.4348) | Acc: (88.00%) (14765/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.7843) |  Loss2: (0.4348) | Acc: (88.00%) (15902/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.7853) |  Loss2: (0.4347) | Acc: (88.00%) (17032/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.7849) |  Loss2: (0.4347) | Acc: (88.00%) (18164/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.7835) |  Loss2: (0.4346) | Acc: (88.00%) (19305/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.7855) |  Loss2: (0.4346) | Acc: (88.00%) (20411/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.7883) |  Loss2: (0.4345) | Acc: (87.00%) (21505/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.7884) |  Loss2: (0.4345) | Acc: (87.00%) (22624/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.7877) |  Loss2: (0.4344) | Acc: (87.00%) (23766/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.7868) |  Loss2: (0.4344) | Acc: (88.00%) (24905/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.7881) |  Loss2: (0.4344) | Acc: (88.00%) (26023/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.7882) |  Loss2: (0.4343) | Acc: (88.00%) (27149/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.7883) |  Loss2: (0.4343) | Acc: (87.00%) (28270/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.7879) |  Loss2: (0.4343) | Acc: (88.00%) (29410/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.7879) |  Loss2: (0.4343) | Acc: (88.00%) (30530/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.7868) |  Loss2: (0.4342) | Acc: (88.00%) (31670/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.7854) |  Loss2: (0.4342) | Acc: (88.00%) (32808/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.7851) |  Loss2: (0.4342) | Acc: (88.00%) (33945/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.7859) |  Loss2: (0.4342) | Acc: (88.00%) (35063/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.7860) |  Loss2: (0.4342) | Acc: (88.00%) (36175/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.7854) |  Loss2: (0.4341) | Acc: (88.00%) (37313/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.7849) |  Loss2: (0.4341) | Acc: (88.00%) (38448/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.7852) |  Loss2: (0.4341) | Acc: (88.00%) (39573/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.7857) |  Loss2: (0.4341) | Acc: (88.00%) (40699/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.7862) |  Loss2: (0.4340) | Acc: (88.00%) (41833/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.7860) |  Loss2: (0.4340) | Acc: (88.00%) (42974/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.7866) |  Loss2: (0.4340) | Acc: (88.00%) (44052/50000)
# TEST : Loss: (0.4631) | Acc: (84.00%) (8456/10000)
percent tensor([0.6227], device='cuda:0')
percent tensor([0.6217], device='cuda:0')
percent tensor([0.7489], device='cuda:0')
percent tensor([0.6565], device='cuda:0')
percent tensor([0.7594], device='cuda:0')
percent tensor([0.7861], device='cuda:0')
percent tensor([0.8115], device='cuda:0')
percent tensor([0.2272], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.8321) |  Loss2: (0.4329) | Acc: (85.00%) (110/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.7996) |  Loss2: (0.4329) | Acc: (87.00%) (1232/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.8008) |  Loss2: (0.4329) | Acc: (87.00%) (2359/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.7853) |  Loss2: (0.4329) | Acc: (88.00%) (3501/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.7849) |  Loss2: (0.4329) | Acc: (88.00%) (4631/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.7841) |  Loss2: (0.4329) | Acc: (88.00%) (5761/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.7847) |  Loss2: (0.4329) | Acc: (88.00%) (6881/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.7902) |  Loss2: (0.4329) | Acc: (87.00%) (7980/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.7952) |  Loss2: (0.4328) | Acc: (87.00%) (9079/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.7962) |  Loss2: (0.4328) | Acc: (87.00%) (10192/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.7964) |  Loss2: (0.4328) | Acc: (87.00%) (11326/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.7926) |  Loss2: (0.4328) | Acc: (87.00%) (12472/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.7900) |  Loss2: (0.4327) | Acc: (87.00%) (13601/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.7886) |  Loss2: (0.4327) | Acc: (87.00%) (14736/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.7890) |  Loss2: (0.4327) | Acc: (87.00%) (15855/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.7882) |  Loss2: (0.4326) | Acc: (87.00%) (16980/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.7876) |  Loss2: (0.4326) | Acc: (87.00%) (18101/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.7892) |  Loss2: (0.4326) | Acc: (87.00%) (19217/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.7875) |  Loss2: (0.4325) | Acc: (87.00%) (20342/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.7866) |  Loss2: (0.4325) | Acc: (87.00%) (21477/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.7864) |  Loss2: (0.4325) | Acc: (87.00%) (22607/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.7842) |  Loss2: (0.4325) | Acc: (87.00%) (23761/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.7852) |  Loss2: (0.4324) | Acc: (87.00%) (24876/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.7859) |  Loss2: (0.4324) | Acc: (87.00%) (25994/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.7855) |  Loss2: (0.4324) | Acc: (87.00%) (27112/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.7845) |  Loss2: (0.4323) | Acc: (87.00%) (28244/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.7834) |  Loss2: (0.4323) | Acc: (87.00%) (29386/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.7826) |  Loss2: (0.4323) | Acc: (88.00%) (30539/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.7833) |  Loss2: (0.4323) | Acc: (87.00%) (31647/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.7829) |  Loss2: (0.4323) | Acc: (88.00%) (32782/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.7824) |  Loss2: (0.4322) | Acc: (88.00%) (33919/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.7826) |  Loss2: (0.4322) | Acc: (88.00%) (35037/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.7820) |  Loss2: (0.4322) | Acc: (88.00%) (36176/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.7821) |  Loss2: (0.4322) | Acc: (88.00%) (37313/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.7820) |  Loss2: (0.4322) | Acc: (88.00%) (38448/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.7821) |  Loss2: (0.4322) | Acc: (88.00%) (39566/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.7823) |  Loss2: (0.4321) | Acc: (88.00%) (40700/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.7826) |  Loss2: (0.4321) | Acc: (88.00%) (41830/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.7821) |  Loss2: (0.4321) | Acc: (88.00%) (42970/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.7820) |  Loss2: (0.4321) | Acc: (88.00%) (44049/50000)
# TEST : Loss: (0.4573) | Acc: (84.00%) (8487/10000)
percent tensor([0.6251], device='cuda:0')
percent tensor([0.6241], device='cuda:0')
percent tensor([0.7497], device='cuda:0')
percent tensor([0.6574], device='cuda:0')
percent tensor([0.7599], device='cuda:0')
percent tensor([0.7869], device='cuda:0')
percent tensor([0.8136], device='cuda:0')
percent tensor([0.2229], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.7121) |  Loss2: (0.4317) | Acc: (91.00%) (117/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.7368) |  Loss2: (0.4317) | Acc: (89.00%) (1265/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.7381) |  Loss2: (0.4317) | Acc: (90.00%) (2421/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.7499) |  Loss2: (0.4317) | Acc: (89.00%) (3552/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.7554) |  Loss2: (0.4317) | Acc: (89.00%) (4691/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.7598) |  Loss2: (0.4316) | Acc: (89.00%) (5827/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.7662) |  Loss2: (0.4316) | Acc: (89.00%) (6951/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.7679) |  Loss2: (0.4316) | Acc: (88.00%) (8087/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.7681) |  Loss2: (0.4316) | Acc: (88.00%) (9220/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.7727) |  Loss2: (0.4316) | Acc: (88.00%) (10326/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.7739) |  Loss2: (0.4316) | Acc: (88.00%) (11448/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.7756) |  Loss2: (0.4316) | Acc: (88.00%) (12564/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.7773) |  Loss2: (0.4316) | Acc: (88.00%) (13691/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.7775) |  Loss2: (0.4316) | Acc: (88.00%) (14815/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.7778) |  Loss2: (0.4315) | Acc: (88.00%) (15933/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.7770) |  Loss2: (0.4315) | Acc: (88.00%) (17075/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.7782) |  Loss2: (0.4315) | Acc: (88.00%) (18178/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.7769) |  Loss2: (0.4315) | Acc: (88.00%) (19320/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.7755) |  Loss2: (0.4315) | Acc: (88.00%) (20470/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.7760) |  Loss2: (0.4315) | Acc: (88.00%) (21606/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.7754) |  Loss2: (0.4315) | Acc: (88.00%) (22739/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.7757) |  Loss2: (0.4315) | Acc: (88.00%) (23865/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.7749) |  Loss2: (0.4315) | Acc: (88.00%) (24992/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.7741) |  Loss2: (0.4315) | Acc: (88.00%) (26142/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.7733) |  Loss2: (0.4315) | Acc: (88.00%) (27285/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.7729) |  Loss2: (0.4315) | Acc: (88.00%) (28435/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.7730) |  Loss2: (0.4315) | Acc: (88.00%) (29568/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.7714) |  Loss2: (0.4315) | Acc: (88.00%) (30721/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.7728) |  Loss2: (0.4315) | Acc: (88.00%) (31840/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.7736) |  Loss2: (0.4315) | Acc: (88.00%) (32968/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.7727) |  Loss2: (0.4315) | Acc: (88.00%) (34111/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.7730) |  Loss2: (0.4315) | Acc: (88.00%) (35243/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.7721) |  Loss2: (0.4315) | Acc: (88.00%) (36391/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.7725) |  Loss2: (0.4314) | Acc: (88.00%) (37514/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.7727) |  Loss2: (0.4314) | Acc: (88.00%) (38642/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.7731) |  Loss2: (0.4314) | Acc: (88.00%) (39770/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.7741) |  Loss2: (0.4314) | Acc: (88.00%) (40894/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.7743) |  Loss2: (0.4314) | Acc: (88.00%) (42020/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.7747) |  Loss2: (0.4314) | Acc: (88.00%) (43141/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.7749) |  Loss2: (0.4314) | Acc: (88.00%) (44219/50000)
# TEST : Loss: (0.4527) | Acc: (84.00%) (8497/10000)
percent tensor([0.6259], device='cuda:0')
percent tensor([0.6238], device='cuda:0')
percent tensor([0.7503], device='cuda:0')
percent tensor([0.6585], device='cuda:0')
percent tensor([0.7603], device='cuda:0')
percent tensor([0.7882], device='cuda:0')
percent tensor([0.8147], device='cuda:0')
percent tensor([0.2182], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (2368/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (88.00%) (3501/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (4625/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.3483) |  Loss2: (0.0000) | Acc: (88.00%) (5752/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (6845/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (7971/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (9091/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.3565) |  Loss2: (0.0000) | Acc: (87.00%) (10216/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.3572) |  Loss2: (0.0000) | Acc: (87.00%) (11338/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (12482/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (13598/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (87.00%) (14746/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (15873/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (16992/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (18122/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.3522) |  Loss2: (0.0000) | Acc: (87.00%) (19236/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (20354/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.3513) |  Loss2: (0.0000) | Acc: (87.00%) (21481/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (22598/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.3523) |  Loss2: (0.0000) | Acc: (87.00%) (23711/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (87.00%) (24837/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (87.00%) (25990/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (27119/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (28251/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (29361/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (30489/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.3499) |  Loss2: (0.0000) | Acc: (87.00%) (31639/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.3492) |  Loss2: (0.0000) | Acc: (88.00%) (32780/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.3501) |  Loss2: (0.0000) | Acc: (87.00%) (33885/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (35009/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.3521) |  Loss2: (0.0000) | Acc: (87.00%) (36110/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (37223/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (38360/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.3514) |  Loss2: (0.0000) | Acc: (87.00%) (39477/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (40602/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.3509) |  Loss2: (0.0000) | Acc: (87.00%) (41731/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.3504) |  Loss2: (0.0000) | Acc: (87.00%) (42850/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (43945/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_090.pth.tar'
# TEST : Loss: (0.5239) | Acc: (82.00%) (8278/10000)
percent tensor([0.6262], device='cuda:0')
percent tensor([0.6236], device='cuda:0')
percent tensor([0.7502], device='cuda:0')
percent tensor([0.6583], device='cuda:0')
percent tensor([0.7603], device='cuda:0')
percent tensor([0.7880], device='cuda:0')
percent tensor([0.8146], device='cuda:0')
percent tensor([0.2182], device='cuda:0')
Epoch: 91 | Batch_idx: 0 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.3577) |  Loss2: (0.0000) | Acc: (87.00%) (1228/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (86.00%) (2334/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (87.00%) (3472/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.3476) |  Loss2: (0.0000) | Acc: (88.00%) (4623/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (5759/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.3388) |  Loss2: (0.0000) | Acc: (88.00%) (6907/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (8019/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.3389) |  Loss2: (0.0000) | Acc: (88.00%) (9151/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.3382) |  Loss2: (0.0000) | Acc: (88.00%) (10293/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (11416/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (12531/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.3421) |  Loss2: (0.0000) | Acc: (88.00%) (13648/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (14785/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (15922/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (17048/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (18187/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (19314/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (20451/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (21581/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (22700/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.3422) |  Loss2: (0.0000) | Acc: (88.00%) (23815/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.3428) |  Loss2: (0.0000) | Acc: (88.00%) (24935/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (26078/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.3429) |  Loss2: (0.0000) | Acc: (88.00%) (27193/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (28295/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (88.00%) (29440/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (30543/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (31675/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (88.00%) (32828/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.3438) |  Loss2: (0.0000) | Acc: (88.00%) (33950/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (35078/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (36219/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (37355/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (38479/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (39612/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.3405) |  Loss2: (0.0000) | Acc: (88.00%) (40742/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (41866/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (42978/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.3410) |  Loss2: (0.0000) | Acc: (88.00%) (44088/50000)
# TEST : Loss: (0.4639) | Acc: (84.00%) (8465/10000)
percent tensor([0.6262], device='cuda:0')
percent tensor([0.6235], device='cuda:0')
percent tensor([0.7501], device='cuda:0')
percent tensor([0.6582], device='cuda:0')
percent tensor([0.7602], device='cuda:0')
percent tensor([0.7880], device='cuda:0')
percent tensor([0.8145], device='cuda:0')
percent tensor([0.2183], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (88.00%) (1244/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.3171) |  Loss2: (0.0000) | Acc: (89.00%) (2401/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (3519/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (4637/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (5790/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (6931/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (8085/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (9208/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (10340/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (11481/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (12616/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (13751/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (14900/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.3279) |  Loss2: (0.0000) | Acc: (88.00%) (16029/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (88.00%) (17158/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.3322) |  Loss2: (0.0000) | Acc: (88.00%) (18267/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (19415/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (20554/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (21677/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (22802/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (23939/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (25064/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (26205/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.3309) |  Loss2: (0.0000) | Acc: (88.00%) (27329/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (28455/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.3333) |  Loss2: (0.0000) | Acc: (88.00%) (29577/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.3339) |  Loss2: (0.0000) | Acc: (88.00%) (30708/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (31838/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (32967/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.3337) |  Loss2: (0.0000) | Acc: (88.00%) (34112/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (35248/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (36369/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (37496/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (38625/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.3346) |  Loss2: (0.0000) | Acc: (88.00%) (39753/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.3343) |  Loss2: (0.0000) | Acc: (88.00%) (40885/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (42022/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (43152/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (44219/50000)
# TEST : Loss: (0.4913) | Acc: (83.00%) (8399/10000)
percent tensor([0.6261], device='cuda:0')
percent tensor([0.6235], device='cuda:0')
percent tensor([0.7500], device='cuda:0')
percent tensor([0.6581], device='cuda:0')
percent tensor([0.7601], device='cuda:0')
percent tensor([0.7879], device='cuda:0')
percent tensor([0.8144], device='cuda:0')
percent tensor([0.2184], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2936) |  Loss2: (0.0000) | Acc: (90.00%) (1270/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2981) |  Loss2: (0.0000) | Acc: (90.00%) (2420/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2967) |  Loss2: (0.0000) | Acc: (90.00%) (3572/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (4719/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (5829/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (6969/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (8113/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.3158) |  Loss2: (0.0000) | Acc: (89.00%) (9263/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (10397/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.3172) |  Loss2: (0.0000) | Acc: (89.00%) (11539/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (12688/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (13824/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (14957/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (89.00%) (16097/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.3205) |  Loss2: (0.0000) | Acc: (89.00%) (17229/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (89.00%) (18350/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (89.00%) (19488/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.3217) |  Loss2: (0.0000) | Acc: (89.00%) (20648/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (89.00%) (21768/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (22881/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (24030/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.3239) |  Loss2: (0.0000) | Acc: (88.00%) (25175/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (88.00%) (26314/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (89.00%) (27460/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (28595/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.3256) |  Loss2: (0.0000) | Acc: (88.00%) (29718/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (30833/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (88.00%) (31996/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (33120/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (34271/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (88.00%) (35421/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.3246) |  Loss2: (0.0000) | Acc: (88.00%) (36549/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (37676/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (38807/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (39936/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (41066/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (42197/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.3259) |  Loss2: (0.0000) | Acc: (88.00%) (43333/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.3260) |  Loss2: (0.0000) | Acc: (88.00%) (44435/50000)
# TEST : Loss: (0.5869) | Acc: (81.00%) (8147/10000)
percent tensor([0.6261], device='cuda:0')
percent tensor([0.6234], device='cuda:0')
percent tensor([0.7500], device='cuda:0')
percent tensor([0.6581], device='cuda:0')
percent tensor([0.7600], device='cuda:0')
percent tensor([0.7878], device='cuda:0')
percent tensor([0.8143], device='cuda:0')
percent tensor([0.2185], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.4256) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.3440) |  Loss2: (0.0000) | Acc: (88.00%) (1250/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (2374/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (3534/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (4669/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (88.00%) (5799/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.3147) |  Loss2: (0.0000) | Acc: (88.00%) (6946/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.3150) |  Loss2: (0.0000) | Acc: (89.00%) (8097/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (9243/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (10412/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (11551/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (12682/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (13818/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (14948/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (16106/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (17229/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (18384/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (19510/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (20648/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (21778/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (22925/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (24066/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (25216/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (26370/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (27497/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (28625/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (89.00%) (29747/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.3143) |  Loss2: (0.0000) | Acc: (89.00%) (30887/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (32039/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (33158/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (88.00%) (34275/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (88.00%) (35422/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.3163) |  Loss2: (0.0000) | Acc: (88.00%) (36557/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (88.00%) (37675/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (88.00%) (38811/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.3186) |  Loss2: (0.0000) | Acc: (88.00%) (39945/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (88.00%) (41063/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (42190/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.3198) |  Loss2: (0.0000) | Acc: (88.00%) (43350/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (88.00%) (44443/50000)
# TEST : Loss: (0.4458) | Acc: (85.00%) (8516/10000)
percent tensor([0.6260], device='cuda:0')
percent tensor([0.6234], device='cuda:0')
percent tensor([0.7499], device='cuda:0')
percent tensor([0.6580], device='cuda:0')
percent tensor([0.7599], device='cuda:0')
percent tensor([0.7877], device='cuda:0')
percent tensor([0.8142], device='cuda:0')
percent tensor([0.2185], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.7244) |  Loss2: (0.4315) | Acc: (88.00%) (113/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.7460) |  Loss2: (0.4315) | Acc: (88.00%) (1243/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.8118) |  Loss2: (0.4316) | Acc: (86.00%) (2330/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.8366) |  Loss2: (0.4316) | Acc: (86.00%) (3419/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.8489) |  Loss2: (0.4316) | Acc: (85.00%) (4484/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.8492) |  Loss2: (0.4315) | Acc: (85.00%) (5571/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.8517) |  Loss2: (0.4315) | Acc: (85.00%) (6645/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.8565) |  Loss2: (0.4315) | Acc: (84.00%) (7720/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.8549) |  Loss2: (0.4314) | Acc: (84.00%) (8810/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.8576) |  Loss2: (0.4314) | Acc: (84.00%) (9883/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.8552) |  Loss2: (0.4314) | Acc: (84.00%) (10980/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.8559) |  Loss2: (0.4314) | Acc: (84.00%) (12067/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.8550) |  Loss2: (0.4314) | Acc: (84.00%) (13161/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.8517) |  Loss2: (0.4314) | Acc: (85.00%) (14261/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.8500) |  Loss2: (0.4314) | Acc: (85.00%) (15368/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.8481) |  Loss2: (0.4314) | Acc: (85.00%) (16481/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.8488) |  Loss2: (0.4314) | Acc: (85.00%) (17578/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.8468) |  Loss2: (0.4314) | Acc: (85.00%) (18673/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.8438) |  Loss2: (0.4313) | Acc: (85.00%) (19802/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.8432) |  Loss2: (0.4313) | Acc: (85.00%) (20894/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.8439) |  Loss2: (0.4313) | Acc: (85.00%) (21989/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.8419) |  Loss2: (0.4313) | Acc: (85.00%) (23105/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.8401) |  Loss2: (0.4312) | Acc: (85.00%) (24224/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.8382) |  Loss2: (0.4312) | Acc: (85.00%) (25337/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.8369) |  Loss2: (0.4312) | Acc: (85.00%) (26458/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.8359) |  Loss2: (0.4311) | Acc: (85.00%) (27563/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.8344) |  Loss2: (0.4311) | Acc: (85.00%) (28684/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.8331) |  Loss2: (0.4310) | Acc: (85.00%) (29799/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.8318) |  Loss2: (0.4310) | Acc: (85.00%) (30904/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.8318) |  Loss2: (0.4310) | Acc: (85.00%) (31985/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.8303) |  Loss2: (0.4309) | Acc: (85.00%) (33089/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.8294) |  Loss2: (0.4309) | Acc: (85.00%) (34213/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.8282) |  Loss2: (0.4308) | Acc: (86.00%) (35337/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.8265) |  Loss2: (0.4307) | Acc: (86.00%) (36460/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.8250) |  Loss2: (0.4307) | Acc: (86.00%) (37574/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.8235) |  Loss2: (0.4306) | Acc: (86.00%) (38692/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.8227) |  Loss2: (0.4306) | Acc: (86.00%) (39826/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.8206) |  Loss2: (0.4305) | Acc: (86.00%) (40972/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.8202) |  Loss2: (0.4304) | Acc: (86.00%) (42070/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.8194) |  Loss2: (0.4304) | Acc: (86.00%) (43156/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_095.pth.tar'
# TEST : Loss: (0.4745) | Acc: (84.00%) (8416/10000)
percent tensor([0.6315], device='cuda:0')
percent tensor([0.6261], device='cuda:0')
percent tensor([0.7527], device='cuda:0')
percent tensor([0.6572], device='cuda:0')
percent tensor([0.7556], device='cuda:0')
percent tensor([0.7949], device='cuda:0')
percent tensor([0.8253], device='cuda:0')
percent tensor([0.2224], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.9096) |  Loss2: (0.4277) | Acc: (82.00%) (106/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.7655) |  Loss2: (0.4276) | Acc: (88.00%) (1249/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.7682) |  Loss2: (0.4275) | Acc: (88.00%) (2371/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.7630) |  Loss2: (0.4274) | Acc: (88.00%) (3505/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.7685) |  Loss2: (0.4273) | Acc: (88.00%) (4637/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.7631) |  Loss2: (0.4273) | Acc: (88.00%) (5771/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.7675) |  Loss2: (0.4273) | Acc: (88.00%) (6893/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.7664) |  Loss2: (0.4272) | Acc: (88.00%) (8025/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.7683) |  Loss2: (0.4272) | Acc: (88.00%) (9141/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.7699) |  Loss2: (0.4272) | Acc: (88.00%) (10256/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.7730) |  Loss2: (0.4271) | Acc: (88.00%) (11378/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.7774) |  Loss2: (0.4271) | Acc: (87.00%) (12474/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.7795) |  Loss2: (0.4270) | Acc: (87.00%) (13586/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.7790) |  Loss2: (0.4270) | Acc: (87.00%) (14711/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.7771) |  Loss2: (0.4269) | Acc: (87.00%) (15853/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.7762) |  Loss2: (0.4268) | Acc: (87.00%) (16983/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.7766) |  Loss2: (0.4268) | Acc: (87.00%) (18117/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.7760) |  Loss2: (0.4267) | Acc: (87.00%) (19243/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.7754) |  Loss2: (0.4266) | Acc: (87.00%) (20374/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.7758) |  Loss2: (0.4266) | Acc: (87.00%) (21489/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.7751) |  Loss2: (0.4265) | Acc: (87.00%) (22620/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.7751) |  Loss2: (0.4264) | Acc: (87.00%) (23749/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.7751) |  Loss2: (0.4264) | Acc: (87.00%) (24893/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.7739) |  Loss2: (0.4263) | Acc: (88.00%) (26021/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.7732) |  Loss2: (0.4263) | Acc: (88.00%) (27169/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.7735) |  Loss2: (0.4262) | Acc: (88.00%) (28282/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.7726) |  Loss2: (0.4262) | Acc: (88.00%) (29414/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.7716) |  Loss2: (0.4261) | Acc: (88.00%) (30544/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.7703) |  Loss2: (0.4261) | Acc: (88.00%) (31699/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.7711) |  Loss2: (0.4260) | Acc: (88.00%) (32813/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.7700) |  Loss2: (0.4260) | Acc: (88.00%) (33959/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.7690) |  Loss2: (0.4260) | Acc: (88.00%) (35108/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.7688) |  Loss2: (0.4259) | Acc: (88.00%) (36235/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.7680) |  Loss2: (0.4259) | Acc: (88.00%) (37383/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.7693) |  Loss2: (0.4258) | Acc: (88.00%) (38486/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.7689) |  Loss2: (0.4258) | Acc: (88.00%) (39617/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.7698) |  Loss2: (0.4257) | Acc: (88.00%) (40740/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.7691) |  Loss2: (0.4257) | Acc: (88.00%) (41871/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.7690) |  Loss2: (0.4257) | Acc: (88.00%) (43000/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.7688) |  Loss2: (0.4256) | Acc: (88.00%) (44080/50000)
# TEST : Loss: (0.4549) | Acc: (84.00%) (8460/10000)
percent tensor([0.6329], device='cuda:0')
percent tensor([0.6311], device='cuda:0')
percent tensor([0.7581], device='cuda:0')
percent tensor([0.6631], device='cuda:0')
percent tensor([0.7593], device='cuda:0')
percent tensor([0.7981], device='cuda:0')
percent tensor([0.8288], device='cuda:0')
percent tensor([0.2199], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.7497) |  Loss2: (0.4238) | Acc: (86.00%) (111/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.7546) |  Loss2: (0.4237) | Acc: (88.00%) (1243/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.7553) |  Loss2: (0.4236) | Acc: (88.00%) (2375/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.7615) |  Loss2: (0.4236) | Acc: (88.00%) (3500/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.7592) |  Loss2: (0.4235) | Acc: (88.00%) (4642/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.7585) |  Loss2: (0.4235) | Acc: (88.00%) (5781/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.7589) |  Loss2: (0.4234) | Acc: (88.00%) (6912/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.7572) |  Loss2: (0.4234) | Acc: (88.00%) (8053/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.7556) |  Loss2: (0.4234) | Acc: (88.00%) (9194/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.7557) |  Loss2: (0.4234) | Acc: (88.00%) (10331/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.7608) |  Loss2: (0.4233) | Acc: (88.00%) (11437/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.7628) |  Loss2: (0.4233) | Acc: (88.00%) (12556/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.7635) |  Loss2: (0.4233) | Acc: (88.00%) (13679/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.7637) |  Loss2: (0.4232) | Acc: (88.00%) (14800/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.7588) |  Loss2: (0.4232) | Acc: (88.00%) (15967/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.7578) |  Loss2: (0.4232) | Acc: (88.00%) (17109/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.7571) |  Loss2: (0.4232) | Acc: (88.00%) (18257/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.7569) |  Loss2: (0.4231) | Acc: (88.00%) (19384/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.7543) |  Loss2: (0.4231) | Acc: (88.00%) (20536/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.7573) |  Loss2: (0.4231) | Acc: (88.00%) (21634/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.7581) |  Loss2: (0.4231) | Acc: (88.00%) (22767/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.7583) |  Loss2: (0.4231) | Acc: (88.00%) (23896/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.7573) |  Loss2: (0.4230) | Acc: (88.00%) (25042/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.7576) |  Loss2: (0.4230) | Acc: (88.00%) (26182/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.7578) |  Loss2: (0.4230) | Acc: (88.00%) (27311/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.7581) |  Loss2: (0.4230) | Acc: (88.00%) (28443/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.7583) |  Loss2: (0.4229) | Acc: (88.00%) (29568/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.7582) |  Loss2: (0.4229) | Acc: (88.00%) (30698/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.7576) |  Loss2: (0.4229) | Acc: (88.00%) (31840/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.7579) |  Loss2: (0.4229) | Acc: (88.00%) (32963/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.7567) |  Loss2: (0.4229) | Acc: (88.00%) (34110/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.7575) |  Loss2: (0.4229) | Acc: (88.00%) (35238/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.7572) |  Loss2: (0.4228) | Acc: (88.00%) (36379/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.7570) |  Loss2: (0.4228) | Acc: (88.00%) (37515/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.7563) |  Loss2: (0.4228) | Acc: (88.00%) (38672/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.7562) |  Loss2: (0.4228) | Acc: (88.00%) (39802/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.7561) |  Loss2: (0.4228) | Acc: (88.00%) (40935/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.7563) |  Loss2: (0.4227) | Acc: (88.00%) (42070/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.7561) |  Loss2: (0.4227) | Acc: (88.00%) (43208/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.7565) |  Loss2: (0.4227) | Acc: (88.00%) (44289/50000)
# TEST : Loss: (0.4428) | Acc: (85.00%) (8501/10000)
percent tensor([0.6334], device='cuda:0')
percent tensor([0.6324], device='cuda:0')
percent tensor([0.7602], device='cuda:0')
percent tensor([0.6677], device='cuda:0')
percent tensor([0.7645], device='cuda:0')
percent tensor([0.8002], device='cuda:0')
percent tensor([0.8291], device='cuda:0')
percent tensor([0.2159], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.8202) |  Loss2: (0.4216) | Acc: (85.00%) (110/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.7809) |  Loss2: (0.4216) | Acc: (87.00%) (1235/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.7761) |  Loss2: (0.4215) | Acc: (87.00%) (2365/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.7632) |  Loss2: (0.4214) | Acc: (88.00%) (3497/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.7547) |  Loss2: (0.4213) | Acc: (88.00%) (4639/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.7528) |  Loss2: (0.4212) | Acc: (88.00%) (5777/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.7479) |  Loss2: (0.4212) | Acc: (88.00%) (6927/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.7539) |  Loss2: (0.4211) | Acc: (88.00%) (8054/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.7538) |  Loss2: (0.4211) | Acc: (88.00%) (9197/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.7511) |  Loss2: (0.4211) | Acc: (88.00%) (10332/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.7550) |  Loss2: (0.4211) | Acc: (88.00%) (11459/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.7568) |  Loss2: (0.4211) | Acc: (88.00%) (12582/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.7571) |  Loss2: (0.4211) | Acc: (88.00%) (13714/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.7546) |  Loss2: (0.4211) | Acc: (88.00%) (14865/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.7541) |  Loss2: (0.4211) | Acc: (88.00%) (15995/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.7518) |  Loss2: (0.4211) | Acc: (88.00%) (17142/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.7522) |  Loss2: (0.4210) | Acc: (88.00%) (18275/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.7514) |  Loss2: (0.4210) | Acc: (88.00%) (19426/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.7518) |  Loss2: (0.4210) | Acc: (88.00%) (20545/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.7529) |  Loss2: (0.4210) | Acc: (88.00%) (21683/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.7544) |  Loss2: (0.4209) | Acc: (88.00%) (22800/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.7542) |  Loss2: (0.4209) | Acc: (88.00%) (23935/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.7522) |  Loss2: (0.4209) | Acc: (88.00%) (25101/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.7523) |  Loss2: (0.4209) | Acc: (88.00%) (26244/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.7508) |  Loss2: (0.4209) | Acc: (88.00%) (27394/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.7481) |  Loss2: (0.4209) | Acc: (88.00%) (28556/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.7477) |  Loss2: (0.4208) | Acc: (88.00%) (29704/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.7476) |  Loss2: (0.4208) | Acc: (88.00%) (30839/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.7480) |  Loss2: (0.4208) | Acc: (88.00%) (31966/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.7488) |  Loss2: (0.4208) | Acc: (88.00%) (33092/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.7494) |  Loss2: (0.4208) | Acc: (88.00%) (34224/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.7498) |  Loss2: (0.4208) | Acc: (88.00%) (35347/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.7490) |  Loss2: (0.4208) | Acc: (88.00%) (36488/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.7497) |  Loss2: (0.4207) | Acc: (88.00%) (37609/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.7498) |  Loss2: (0.4207) | Acc: (88.00%) (38742/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.7507) |  Loss2: (0.4207) | Acc: (88.00%) (39864/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.7505) |  Loss2: (0.4207) | Acc: (88.00%) (41014/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.7504) |  Loss2: (0.4207) | Acc: (88.00%) (42146/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.7505) |  Loss2: (0.4207) | Acc: (88.00%) (43275/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.7499) |  Loss2: (0.4207) | Acc: (88.00%) (44379/50000)
# TEST : Loss: (0.4391) | Acc: (85.00%) (8502/10000)
percent tensor([0.6337], device='cuda:0')
percent tensor([0.6321], device='cuda:0')
percent tensor([0.7616], device='cuda:0')
percent tensor([0.6704], device='cuda:0')
percent tensor([0.7681], device='cuda:0')
percent tensor([0.8021], device='cuda:0')
percent tensor([0.8294], device='cuda:0')
percent tensor([0.2113], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.6859) |  Loss2: (0.4203) | Acc: (90.00%) (116/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.7106) |  Loss2: (0.4203) | Acc: (90.00%) (1269/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.7390) |  Loss2: (0.4203) | Acc: (89.00%) (2396/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.7322) |  Loss2: (0.4202) | Acc: (89.00%) (3540/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.7326) |  Loss2: (0.4202) | Acc: (89.00%) (4679/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.7385) |  Loss2: (0.4202) | Acc: (88.00%) (5802/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.7416) |  Loss2: (0.4201) | Acc: (88.00%) (6935/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.7425) |  Loss2: (0.4201) | Acc: (88.00%) (8071/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.7432) |  Loss2: (0.4200) | Acc: (88.00%) (9222/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.7418) |  Loss2: (0.4200) | Acc: (89.00%) (10371/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.7423) |  Loss2: (0.4200) | Acc: (89.00%) (11508/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.7419) |  Loss2: (0.4199) | Acc: (89.00%) (12646/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.7410) |  Loss2: (0.4199) | Acc: (89.00%) (13794/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.7394) |  Loss2: (0.4198) | Acc: (89.00%) (14950/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.7386) |  Loss2: (0.4198) | Acc: (89.00%) (16100/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.7414) |  Loss2: (0.4198) | Acc: (89.00%) (17223/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.7402) |  Loss2: (0.4198) | Acc: (89.00%) (18378/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.7396) |  Loss2: (0.4197) | Acc: (89.00%) (19525/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.7405) |  Loss2: (0.4197) | Acc: (89.00%) (20672/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.7404) |  Loss2: (0.4197) | Acc: (89.00%) (21811/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.7394) |  Loss2: (0.4197) | Acc: (89.00%) (22963/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.7400) |  Loss2: (0.4197) | Acc: (89.00%) (24090/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.7389) |  Loss2: (0.4197) | Acc: (89.00%) (25239/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.7398) |  Loss2: (0.4196) | Acc: (89.00%) (26373/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.7400) |  Loss2: (0.4196) | Acc: (89.00%) (27519/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.7395) |  Loss2: (0.4196) | Acc: (89.00%) (28658/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.7397) |  Loss2: (0.4196) | Acc: (89.00%) (29796/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.7410) |  Loss2: (0.4196) | Acc: (89.00%) (30917/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.7416) |  Loss2: (0.4196) | Acc: (89.00%) (32054/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.7424) |  Loss2: (0.4195) | Acc: (89.00%) (33182/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.7428) |  Loss2: (0.4195) | Acc: (89.00%) (34316/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.7436) |  Loss2: (0.4195) | Acc: (89.00%) (35440/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.7439) |  Loss2: (0.4195) | Acc: (89.00%) (36578/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.7441) |  Loss2: (0.4195) | Acc: (89.00%) (37711/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.7443) |  Loss2: (0.4195) | Acc: (89.00%) (38850/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.7444) |  Loss2: (0.4195) | Acc: (89.00%) (39989/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.7442) |  Loss2: (0.4195) | Acc: (89.00%) (41137/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.7440) |  Loss2: (0.4195) | Acc: (89.00%) (42279/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.7438) |  Loss2: (0.4195) | Acc: (89.00%) (43427/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.7444) |  Loss2: (0.4195) | Acc: (89.00%) (44520/50000)
# TEST : Loss: (0.4417) | Acc: (85.00%) (8503/10000)
percent tensor([0.6329], device='cuda:0')
percent tensor([0.6308], device='cuda:0')
percent tensor([0.7646], device='cuda:0')
percent tensor([0.6729], device='cuda:0')
percent tensor([0.7703], device='cuda:0')
percent tensor([0.8031], device='cuda:0')
percent tensor([0.8293], device='cuda:0')
percent tensor([0.2064], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2894) |  Loss2: (0.0000) | Acc: (90.00%) (1268/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (2408/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (3571/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.3073) |  Loss2: (0.0000) | Acc: (89.00%) (4718/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (5869/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.3075) |  Loss2: (0.0000) | Acc: (89.00%) (7008/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.3059) |  Loss2: (0.0000) | Acc: (89.00%) (8166/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (9306/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (10447/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.3149) |  Loss2: (0.0000) | Acc: (89.00%) (11564/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.3162) |  Loss2: (0.0000) | Acc: (89.00%) (12702/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.3204) |  Loss2: (0.0000) | Acc: (89.00%) (13823/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (14980/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (16105/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.3219) |  Loss2: (0.0000) | Acc: (89.00%) (17251/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.3216) |  Loss2: (0.0000) | Acc: (89.00%) (18382/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.3213) |  Loss2: (0.0000) | Acc: (89.00%) (19522/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.3233) |  Loss2: (0.0000) | Acc: (89.00%) (20640/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.3242) |  Loss2: (0.0000) | Acc: (89.00%) (21762/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (88.00%) (22889/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (88.00%) (24022/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (25170/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (88.00%) (26310/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.3231) |  Loss2: (0.0000) | Acc: (89.00%) (27476/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.3236) |  Loss2: (0.0000) | Acc: (89.00%) (28606/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.3247) |  Loss2: (0.0000) | Acc: (89.00%) (29743/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (89.00%) (30874/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.3252) |  Loss2: (0.0000) | Acc: (89.00%) (32014/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.3260) |  Loss2: (0.0000) | Acc: (88.00%) (33133/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (34265/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (88.00%) (35415/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (36539/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (37660/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (38811/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.3263) |  Loss2: (0.0000) | Acc: (88.00%) (39958/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (41076/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (42220/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (43359/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (44446/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_100.pth.tar'
# TEST : Loss: (0.5180) | Acc: (83.00%) (8351/10000)
percent tensor([0.6326], device='cuda:0')
percent tensor([0.6307], device='cuda:0')
percent tensor([0.7644], device='cuda:0')
percent tensor([0.6732], device='cuda:0')
percent tensor([0.7702], device='cuda:0')
percent tensor([0.8031], device='cuda:0')
percent tensor([0.8292], device='cuda:0')
percent tensor([0.2064], device='cuda:0')
Epoch: 101 | Batch_idx: 0 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (90.00%) (1270/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (2419/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (90.00%) (3574/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (4717/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (5868/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (90.00%) (7030/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (9328/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.3023) |  Loss2: (0.0000) | Acc: (89.00%) (10461/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (11602/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (12732/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (13880/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.3050) |  Loss2: (0.0000) | Acc: (89.00%) (15027/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.3038) |  Loss2: (0.0000) | Acc: (89.00%) (16186/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (17340/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (18480/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (19634/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (20781/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (21901/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.3086) |  Loss2: (0.0000) | Acc: (89.00%) (23036/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (89.00%) (24182/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (25311/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (26464/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (27619/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (28750/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.3107) |  Loss2: (0.0000) | Acc: (89.00%) (29895/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (31026/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (32169/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (33313/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (34441/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (35559/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (36713/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.3121) |  Loss2: (0.0000) | Acc: (89.00%) (37848/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (38984/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (40139/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (41266/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (42406/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.3136) |  Loss2: (0.0000) | Acc: (89.00%) (43547/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.3144) |  Loss2: (0.0000) | Acc: (89.00%) (44631/50000)
# TEST : Loss: (0.4767) | Acc: (84.00%) (8453/10000)
percent tensor([0.6326], device='cuda:0')
percent tensor([0.6307], device='cuda:0')
percent tensor([0.7644], device='cuda:0')
percent tensor([0.6731], device='cuda:0')
percent tensor([0.7702], device='cuda:0')
percent tensor([0.8030], device='cuda:0')
percent tensor([0.8291], device='cuda:0')
percent tensor([0.2065], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (90.00%) (1272/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (2402/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (3553/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.3066) |  Loss2: (0.0000) | Acc: (89.00%) (4689/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (5816/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.3101) |  Loss2: (0.0000) | Acc: (89.00%) (6971/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (8107/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (9248/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (10394/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (11553/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.3077) |  Loss2: (0.0000) | Acc: (89.00%) (12676/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.3090) |  Loss2: (0.0000) | Acc: (89.00%) (13806/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (14949/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.3078) |  Loss2: (0.0000) | Acc: (89.00%) (16093/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (17232/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (18372/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.3099) |  Loss2: (0.0000) | Acc: (89.00%) (19511/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.3116) |  Loss2: (0.0000) | Acc: (89.00%) (20644/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.3105) |  Loss2: (0.0000) | Acc: (89.00%) (21810/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (22959/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (24110/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.3093) |  Loss2: (0.0000) | Acc: (89.00%) (25244/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (26375/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.3122) |  Loss2: (0.0000) | Acc: (89.00%) (27521/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.3123) |  Loss2: (0.0000) | Acc: (89.00%) (28658/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (29783/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (30934/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.3134) |  Loss2: (0.0000) | Acc: (89.00%) (32071/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (33199/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (34363/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (35490/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (36631/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.3135) |  Loss2: (0.0000) | Acc: (89.00%) (37775/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (38911/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.3128) |  Loss2: (0.0000) | Acc: (89.00%) (40067/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.3125) |  Loss2: (0.0000) | Acc: (89.00%) (41221/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (42362/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.3126) |  Loss2: (0.0000) | Acc: (89.00%) (43511/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (44624/50000)
# TEST : Loss: (0.4492) | Acc: (85.00%) (8559/10000)
percent tensor([0.6325], device='cuda:0')
percent tensor([0.6306], device='cuda:0')
percent tensor([0.7643], device='cuda:0')
percent tensor([0.6730], device='cuda:0')
percent tensor([0.7701], device='cuda:0')
percent tensor([0.8029], device='cuda:0')
percent tensor([0.8290], device='cuda:0')
percent tensor([0.2066], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (89.00%) (1258/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (90.00%) (2422/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (3587/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.2887) |  Loss2: (0.0000) | Acc: (90.00%) (4742/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (90.00%) (5882/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.2989) |  Loss2: (0.0000) | Acc: (89.00%) (7019/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (8173/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (9323/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (10476/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.2992) |  Loss2: (0.0000) | Acc: (89.00%) (11620/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (12773/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (13906/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (15036/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.3052) |  Loss2: (0.0000) | Acc: (89.00%) (16172/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (17321/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (89.00%) (18452/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.3040) |  Loss2: (0.0000) | Acc: (89.00%) (19612/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (20780/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (21925/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (23068/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (24218/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (25355/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (26502/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (27656/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (28790/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.3024) |  Loss2: (0.0000) | Acc: (89.00%) (29932/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.3027) |  Loss2: (0.0000) | Acc: (89.00%) (31067/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.3025) |  Loss2: (0.0000) | Acc: (89.00%) (32217/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.3021) |  Loss2: (0.0000) | Acc: (89.00%) (33373/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (34503/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (35662/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (36798/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (37943/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (39073/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.3041) |  Loss2: (0.0000) | Acc: (89.00%) (40217/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (41354/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (42498/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (43636/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.3057) |  Loss2: (0.0000) | Acc: (89.00%) (44719/50000)
# TEST : Loss: (0.4685) | Acc: (84.00%) (8485/10000)
percent tensor([0.6325], device='cuda:0')
percent tensor([0.6306], device='cuda:0')
percent tensor([0.7642], device='cuda:0')
percent tensor([0.6730], device='cuda:0')
percent tensor([0.7700], device='cuda:0')
percent tensor([0.8028], device='cuda:0')
percent tensor([0.8289], device='cuda:0')
percent tensor([0.2067], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.4047) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.3088) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (2413/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (3571/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (4742/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (5875/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (7023/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (89.00%) (8173/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (9330/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.2928) |  Loss2: (0.0000) | Acc: (89.00%) (10475/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (11616/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (89.00%) (12773/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.2956) |  Loss2: (0.0000) | Acc: (89.00%) (13923/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (15073/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (16203/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (17359/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (18510/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (19658/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (20810/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (21977/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.2968) |  Loss2: (0.0000) | Acc: (89.00%) (23135/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (24282/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (25431/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (26560/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (27699/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (28863/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (29995/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (31131/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (32295/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.3011) |  Loss2: (0.0000) | Acc: (89.00%) (33406/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.3008) |  Loss2: (0.0000) | Acc: (89.00%) (34558/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (35716/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (36858/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.3004) |  Loss2: (0.0000) | Acc: (89.00%) (37997/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (39143/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (89.00%) (40295/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (41458/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (42621/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (43777/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2983) |  Loss2: (0.0000) | Acc: (89.00%) (44874/50000)
# TEST : Loss: (0.5117) | Acc: (83.00%) (8394/10000)
percent tensor([0.6324], device='cuda:0')
percent tensor([0.6305], device='cuda:0')
percent tensor([0.7641], device='cuda:0')
percent tensor([0.6729], device='cuda:0')
percent tensor([0.7699], device='cuda:0')
percent tensor([0.8028], device='cuda:0')
percent tensor([0.8288], device='cuda:0')
percent tensor([0.2067], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.7026) |  Loss2: (0.4199) | Acc: (89.00%) (114/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.7403) |  Loss2: (0.4199) | Acc: (87.00%) (1239/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.7795) |  Loss2: (0.4198) | Acc: (87.00%) (2351/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.7892) |  Loss2: (0.4196) | Acc: (86.00%) (3441/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.8035) |  Loss2: (0.4195) | Acc: (86.00%) (4534/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.8015) |  Loss2: (0.4193) | Acc: (86.00%) (5641/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.8046) |  Loss2: (0.4191) | Acc: (86.00%) (6740/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.7997) |  Loss2: (0.4190) | Acc: (86.00%) (7871/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.8048) |  Loss2: (0.4188) | Acc: (86.00%) (8970/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.7972) |  Loss2: (0.4187) | Acc: (86.00%) (10113/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.7986) |  Loss2: (0.4185) | Acc: (86.00%) (11222/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.7967) |  Loss2: (0.4184) | Acc: (86.00%) (12345/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.7936) |  Loss2: (0.4182) | Acc: (87.00%) (13477/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.7939) |  Loss2: (0.4181) | Acc: (86.00%) (14583/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.7915) |  Loss2: (0.4180) | Acc: (87.00%) (15702/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.7903) |  Loss2: (0.4178) | Acc: (87.00%) (16833/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.7894) |  Loss2: (0.4177) | Acc: (87.00%) (17959/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.7867) |  Loss2: (0.4176) | Acc: (87.00%) (19094/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.7851) |  Loss2: (0.4175) | Acc: (87.00%) (20218/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.7826) |  Loss2: (0.4174) | Acc: (87.00%) (21348/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.7816) |  Loss2: (0.4173) | Acc: (87.00%) (22476/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.7796) |  Loss2: (0.4171) | Acc: (87.00%) (23601/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.7765) |  Loss2: (0.4170) | Acc: (87.00%) (24745/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.7740) |  Loss2: (0.4169) | Acc: (87.00%) (25896/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.7728) |  Loss2: (0.4168) | Acc: (87.00%) (27027/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.7716) |  Loss2: (0.4167) | Acc: (87.00%) (28159/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.7713) |  Loss2: (0.4165) | Acc: (87.00%) (29271/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.7697) |  Loss2: (0.4164) | Acc: (87.00%) (30399/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.7691) |  Loss2: (0.4163) | Acc: (87.00%) (31531/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.7672) |  Loss2: (0.4162) | Acc: (87.00%) (32678/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.7672) |  Loss2: (0.4161) | Acc: (87.00%) (33792/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.7661) |  Loss2: (0.4160) | Acc: (87.00%) (34931/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.7650) |  Loss2: (0.4159) | Acc: (87.00%) (36074/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.7656) |  Loss2: (0.4157) | Acc: (87.00%) (37186/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.7645) |  Loss2: (0.4156) | Acc: (87.00%) (38321/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.7623) |  Loss2: (0.4155) | Acc: (87.00%) (39482/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.7609) |  Loss2: (0.4154) | Acc: (87.00%) (40637/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.7603) |  Loss2: (0.4153) | Acc: (87.00%) (41770/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.7593) |  Loss2: (0.4152) | Acc: (87.00%) (42907/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.7577) |  Loss2: (0.4151) | Acc: (88.00%) (44019/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_105.pth.tar'
# TEST : Loss: (0.4592) | Acc: (85.00%) (8501/10000)
percent tensor([0.6461], device='cuda:0')
percent tensor([0.6367], device='cuda:0')
percent tensor([0.7744], device='cuda:0')
percent tensor([0.6964], device='cuda:0')
percent tensor([0.7786], device='cuda:0')
percent tensor([0.8016], device='cuda:0')
percent tensor([0.8318], device='cuda:0')
percent tensor([0.2080], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.7810) |  Loss2: (0.4109) | Acc: (85.00%) (110/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.7206) |  Loss2: (0.4108) | Acc: (89.00%) (1261/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.7201) |  Loss2: (0.4107) | Acc: (89.00%) (2402/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.7240) |  Loss2: (0.4106) | Acc: (89.00%) (3536/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.7209) |  Loss2: (0.4106) | Acc: (89.00%) (4683/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.7186) |  Loss2: (0.4105) | Acc: (89.00%) (5830/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.7217) |  Loss2: (0.4104) | Acc: (89.00%) (6962/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.7171) |  Loss2: (0.4104) | Acc: (89.00%) (8118/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.7203) |  Loss2: (0.4103) | Acc: (89.00%) (9253/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.7246) |  Loss2: (0.4102) | Acc: (89.00%) (10383/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.7247) |  Loss2: (0.4102) | Acc: (89.00%) (11525/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.7223) |  Loss2: (0.4101) | Acc: (89.00%) (12684/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.7205) |  Loss2: (0.4101) | Acc: (89.00%) (13837/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.7205) |  Loss2: (0.4100) | Acc: (89.00%) (14978/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.7202) |  Loss2: (0.4100) | Acc: (89.00%) (16127/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.7202) |  Loss2: (0.4099) | Acc: (89.00%) (17274/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.7198) |  Loss2: (0.4099) | Acc: (89.00%) (18420/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.7202) |  Loss2: (0.4098) | Acc: (89.00%) (19563/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.7209) |  Loss2: (0.4098) | Acc: (89.00%) (20700/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.7217) |  Loss2: (0.4098) | Acc: (89.00%) (21831/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.7217) |  Loss2: (0.4097) | Acc: (89.00%) (22971/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.7218) |  Loss2: (0.4097) | Acc: (89.00%) (24106/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.7222) |  Loss2: (0.4096) | Acc: (89.00%) (25257/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.7216) |  Loss2: (0.4096) | Acc: (89.00%) (26395/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.7230) |  Loss2: (0.4095) | Acc: (89.00%) (27511/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.7228) |  Loss2: (0.4095) | Acc: (89.00%) (28653/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.7229) |  Loss2: (0.4094) | Acc: (89.00%) (29790/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.7219) |  Loss2: (0.4094) | Acc: (89.00%) (30950/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.7211) |  Loss2: (0.4093) | Acc: (89.00%) (32107/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.7213) |  Loss2: (0.4093) | Acc: (89.00%) (33254/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.7204) |  Loss2: (0.4092) | Acc: (89.00%) (34402/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.7211) |  Loss2: (0.4092) | Acc: (89.00%) (35537/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.7202) |  Loss2: (0.4091) | Acc: (89.00%) (36691/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.7199) |  Loss2: (0.4091) | Acc: (89.00%) (37833/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.7185) |  Loss2: (0.4091) | Acc: (89.00%) (39000/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.7197) |  Loss2: (0.4090) | Acc: (89.00%) (40127/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.7195) |  Loss2: (0.4090) | Acc: (89.00%) (41267/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.7191) |  Loss2: (0.4089) | Acc: (89.00%) (42429/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.7182) |  Loss2: (0.4089) | Acc: (89.00%) (43583/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.7170) |  Loss2: (0.4088) | Acc: (89.00%) (44712/50000)
# TEST : Loss: (0.4384) | Acc: (85.00%) (8568/10000)
percent tensor([0.6504], device='cuda:0')
percent tensor([0.6391], device='cuda:0')
percent tensor([0.7782], device='cuda:0')
percent tensor([0.7012], device='cuda:0')
percent tensor([0.7831], device='cuda:0')
percent tensor([0.8042], device='cuda:0')
percent tensor([0.8343], device='cuda:0')
percent tensor([0.2059], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.7089) |  Loss2: (0.4073) | Acc: (89.00%) (115/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.7447) |  Loss2: (0.4073) | Acc: (88.00%) (1251/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.7324) |  Loss2: (0.4073) | Acc: (89.00%) (2393/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.7256) |  Loss2: (0.4073) | Acc: (89.00%) (3536/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.7182) |  Loss2: (0.4072) | Acc: (89.00%) (4676/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.7180) |  Loss2: (0.4072) | Acc: (89.00%) (5827/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.7170) |  Loss2: (0.4072) | Acc: (89.00%) (6977/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.7134) |  Loss2: (0.4071) | Acc: (89.00%) (8123/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.7135) |  Loss2: (0.4071) | Acc: (89.00%) (9258/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.7142) |  Loss2: (0.4070) | Acc: (89.00%) (10401/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.7134) |  Loss2: (0.4070) | Acc: (89.00%) (11546/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.7138) |  Loss2: (0.4070) | Acc: (89.00%) (12681/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.7119) |  Loss2: (0.4070) | Acc: (89.00%) (13832/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.7111) |  Loss2: (0.4070) | Acc: (89.00%) (14985/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.7098) |  Loss2: (0.4069) | Acc: (89.00%) (16130/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.7105) |  Loss2: (0.4069) | Acc: (89.00%) (17269/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.7102) |  Loss2: (0.4069) | Acc: (89.00%) (18418/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.7101) |  Loss2: (0.4069) | Acc: (89.00%) (19567/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.7095) |  Loss2: (0.4068) | Acc: (89.00%) (20720/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.7103) |  Loss2: (0.4068) | Acc: (89.00%) (21863/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.7102) |  Loss2: (0.4068) | Acc: (89.00%) (23019/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.7093) |  Loss2: (0.4067) | Acc: (89.00%) (24177/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.7089) |  Loss2: (0.4067) | Acc: (89.00%) (25321/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.7093) |  Loss2: (0.4067) | Acc: (89.00%) (26458/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.7089) |  Loss2: (0.4066) | Acc: (89.00%) (27610/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.7093) |  Loss2: (0.4066) | Acc: (89.00%) (28750/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.7098) |  Loss2: (0.4066) | Acc: (89.00%) (29895/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.7101) |  Loss2: (0.4065) | Acc: (89.00%) (31049/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.7095) |  Loss2: (0.4065) | Acc: (89.00%) (32208/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.7084) |  Loss2: (0.4065) | Acc: (89.00%) (33373/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.7077) |  Loss2: (0.4065) | Acc: (89.00%) (34525/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.7070) |  Loss2: (0.4064) | Acc: (89.00%) (35680/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.7074) |  Loss2: (0.4064) | Acc: (89.00%) (36824/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.7078) |  Loss2: (0.4064) | Acc: (89.00%) (37962/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.7083) |  Loss2: (0.4064) | Acc: (89.00%) (39099/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.7085) |  Loss2: (0.4064) | Acc: (89.00%) (40246/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.7086) |  Loss2: (0.4063) | Acc: (89.00%) (41399/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.7080) |  Loss2: (0.4063) | Acc: (89.00%) (42563/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.7089) |  Loss2: (0.4063) | Acc: (89.00%) (43694/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.7092) |  Loss2: (0.4063) | Acc: (89.00%) (44794/50000)
# TEST : Loss: (0.4302) | Acc: (85.00%) (8579/10000)
percent tensor([0.6533], device='cuda:0')
percent tensor([0.6397], device='cuda:0')
percent tensor([0.7806], device='cuda:0')
percent tensor([0.6996], device='cuda:0')
percent tensor([0.7858], device='cuda:0')
percent tensor([0.8072], device='cuda:0')
percent tensor([0.8367], device='cuda:0')
percent tensor([0.2029], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.6455) |  Loss2: (0.4055) | Acc: (91.00%) (117/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.6867) |  Loss2: (0.4054) | Acc: (90.00%) (1279/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.7039) |  Loss2: (0.4054) | Acc: (89.00%) (2418/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.6984) |  Loss2: (0.4054) | Acc: (90.00%) (3580/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.6917) |  Loss2: (0.4054) | Acc: (90.00%) (4743/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.6856) |  Loss2: (0.4053) | Acc: (90.00%) (5909/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.6866) |  Loss2: (0.4053) | Acc: (90.00%) (7056/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.6859) |  Loss2: (0.4053) | Acc: (90.00%) (8204/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.6908) |  Loss2: (0.4052) | Acc: (90.00%) (9350/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.6920) |  Loss2: (0.4052) | Acc: (90.00%) (10499/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.6944) |  Loss2: (0.4052) | Acc: (90.00%) (11648/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.6949) |  Loss2: (0.4051) | Acc: (90.00%) (12795/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.6940) |  Loss2: (0.4051) | Acc: (90.00%) (13953/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.6944) |  Loss2: (0.4051) | Acc: (90.00%) (15107/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.6951) |  Loss2: (0.4051) | Acc: (89.00%) (16237/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.6959) |  Loss2: (0.4050) | Acc: (89.00%) (17388/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.6959) |  Loss2: (0.4050) | Acc: (89.00%) (18546/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.6981) |  Loss2: (0.4050) | Acc: (89.00%) (19688/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.6985) |  Loss2: (0.4050) | Acc: (89.00%) (20848/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.6979) |  Loss2: (0.4050) | Acc: (90.00%) (22010/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.6986) |  Loss2: (0.4050) | Acc: (89.00%) (23152/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.6991) |  Loss2: (0.4049) | Acc: (89.00%) (24290/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.6983) |  Loss2: (0.4049) | Acc: (89.00%) (25451/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.6974) |  Loss2: (0.4049) | Acc: (90.00%) (26621/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.6964) |  Loss2: (0.4049) | Acc: (90.00%) (27785/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.6969) |  Loss2: (0.4049) | Acc: (90.00%) (28931/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.6975) |  Loss2: (0.4048) | Acc: (89.00%) (30067/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.6976) |  Loss2: (0.4048) | Acc: (90.00%) (31223/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.6986) |  Loss2: (0.4048) | Acc: (89.00%) (32351/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.6985) |  Loss2: (0.4048) | Acc: (89.00%) (33503/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.6985) |  Loss2: (0.4047) | Acc: (89.00%) (34655/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.6991) |  Loss2: (0.4047) | Acc: (89.00%) (35796/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.6985) |  Loss2: (0.4047) | Acc: (89.00%) (36962/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.6992) |  Loss2: (0.4047) | Acc: (89.00%) (38102/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.6992) |  Loss2: (0.4047) | Acc: (89.00%) (39251/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.6997) |  Loss2: (0.4046) | Acc: (89.00%) (40388/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.6998) |  Loss2: (0.4046) | Acc: (89.00%) (41540/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.7002) |  Loss2: (0.4046) | Acc: (89.00%) (42682/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.6998) |  Loss2: (0.4046) | Acc: (89.00%) (43841/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.7004) |  Loss2: (0.4046) | Acc: (89.00%) (44936/50000)
# TEST : Loss: (0.4304) | Acc: (85.00%) (8592/10000)
percent tensor([0.6537], device='cuda:0')
percent tensor([0.6394], device='cuda:0')
percent tensor([0.7829], device='cuda:0')
percent tensor([0.7017], device='cuda:0')
percent tensor([0.7871], device='cuda:0')
percent tensor([0.8094], device='cuda:0')
percent tensor([0.8385], device='cuda:0')
percent tensor([0.1992], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.6660) |  Loss2: (0.4041) | Acc: (91.00%) (117/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.6956) |  Loss2: (0.4041) | Acc: (90.00%) (1270/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.7019) |  Loss2: (0.4041) | Acc: (90.00%) (2422/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.6997) |  Loss2: (0.4040) | Acc: (90.00%) (3579/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.6900) |  Loss2: (0.4040) | Acc: (90.00%) (4739/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.6907) |  Loss2: (0.4039) | Acc: (90.00%) (5892/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.6975) |  Loss2: (0.4039) | Acc: (89.00%) (7025/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.6972) |  Loss2: (0.4038) | Acc: (90.00%) (8181/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.6969) |  Loss2: (0.4038) | Acc: (89.00%) (9331/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.6958) |  Loss2: (0.4037) | Acc: (90.00%) (10485/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.6940) |  Loss2: (0.4037) | Acc: (90.00%) (11648/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.6959) |  Loss2: (0.4037) | Acc: (89.00%) (12781/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.6985) |  Loss2: (0.4036) | Acc: (89.00%) (13902/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.6990) |  Loss2: (0.4036) | Acc: (89.00%) (15042/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.7006) |  Loss2: (0.4036) | Acc: (89.00%) (16178/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.7005) |  Loss2: (0.4036) | Acc: (89.00%) (17316/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.7002) |  Loss2: (0.4035) | Acc: (89.00%) (18470/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.7017) |  Loss2: (0.4035) | Acc: (89.00%) (19611/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.7030) |  Loss2: (0.4035) | Acc: (89.00%) (20749/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.7027) |  Loss2: (0.4035) | Acc: (89.00%) (21909/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.7033) |  Loss2: (0.4034) | Acc: (89.00%) (23051/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.7035) |  Loss2: (0.4034) | Acc: (89.00%) (24200/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.7045) |  Loss2: (0.4034) | Acc: (89.00%) (25334/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.7027) |  Loss2: (0.4034) | Acc: (89.00%) (26499/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.7026) |  Loss2: (0.4034) | Acc: (89.00%) (27649/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.7028) |  Loss2: (0.4034) | Acc: (89.00%) (28788/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.7025) |  Loss2: (0.4033) | Acc: (89.00%) (29925/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.7022) |  Loss2: (0.4033) | Acc: (89.00%) (31086/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.7018) |  Loss2: (0.4033) | Acc: (89.00%) (32245/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.7017) |  Loss2: (0.4033) | Acc: (89.00%) (33390/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.7011) |  Loss2: (0.4033) | Acc: (89.00%) (34547/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.7015) |  Loss2: (0.4032) | Acc: (89.00%) (35684/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.7014) |  Loss2: (0.4032) | Acc: (89.00%) (36828/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.7007) |  Loss2: (0.4032) | Acc: (89.00%) (37984/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.6995) |  Loss2: (0.4032) | Acc: (89.00%) (39146/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.6993) |  Loss2: (0.4032) | Acc: (89.00%) (40309/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.6996) |  Loss2: (0.4032) | Acc: (89.00%) (41462/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.6995) |  Loss2: (0.4032) | Acc: (89.00%) (42612/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.6999) |  Loss2: (0.4032) | Acc: (89.00%) (43755/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.7004) |  Loss2: (0.4032) | Acc: (89.00%) (44859/50000)
# TEST : Loss: (0.4296) | Acc: (85.00%) (8584/10000)
percent tensor([0.6545], device='cuda:0')
percent tensor([0.6392], device='cuda:0')
percent tensor([0.7849], device='cuda:0')
percent tensor([0.7021], device='cuda:0')
percent tensor([0.7897], device='cuda:0')
percent tensor([0.8112], device='cuda:0')
percent tensor([0.8400], device='cuda:0')
percent tensor([0.1956], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.3032) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (90.00%) (1273/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.2884) |  Loss2: (0.0000) | Acc: (90.00%) (2427/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.2976) |  Loss2: (0.0000) | Acc: (89.00%) (3565/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (4705/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (5855/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.2977) |  Loss2: (0.0000) | Acc: (89.00%) (7019/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (9319/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.2979) |  Loss2: (0.0000) | Acc: (89.00%) (10454/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (11596/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.2996) |  Loss2: (0.0000) | Acc: (89.00%) (12756/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (13888/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.3003) |  Loss2: (0.0000) | Acc: (89.00%) (15049/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (16206/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (17351/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (18503/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.2995) |  Loss2: (0.0000) | Acc: (89.00%) (19642/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (20777/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.3018) |  Loss2: (0.0000) | Acc: (89.00%) (21906/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.3015) |  Loss2: (0.0000) | Acc: (89.00%) (23064/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (24206/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.3026) |  Loss2: (0.0000) | Acc: (89.00%) (25355/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (26510/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.3030) |  Loss2: (0.0000) | Acc: (89.00%) (27668/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (28805/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.3036) |  Loss2: (0.0000) | Acc: (89.00%) (29943/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (89.00%) (31059/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.3048) |  Loss2: (0.0000) | Acc: (89.00%) (32199/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.3046) |  Loss2: (0.0000) | Acc: (89.00%) (33344/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.3034) |  Loss2: (0.0000) | Acc: (89.00%) (34515/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (35658/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.3042) |  Loss2: (0.0000) | Acc: (89.00%) (36784/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (37915/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (39066/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.3053) |  Loss2: (0.0000) | Acc: (89.00%) (40211/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.3054) |  Loss2: (0.0000) | Acc: (89.00%) (41361/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (42500/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.3069) |  Loss2: (0.0000) | Acc: (89.00%) (43626/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.3072) |  Loss2: (0.0000) | Acc: (89.00%) (44723/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_110.pth.tar'
# TEST : Loss: (0.5174) | Acc: (83.00%) (8322/10000)
percent tensor([0.6545], device='cuda:0')
percent tensor([0.6389], device='cuda:0')
percent tensor([0.7848], device='cuda:0')
percent tensor([0.7017], device='cuda:0')
percent tensor([0.7896], device='cuda:0')
percent tensor([0.8112], device='cuda:0')
percent tensor([0.8400], device='cuda:0')
percent tensor([0.1956], device='cuda:0')
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (1274/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (2412/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (3563/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.2960) |  Loss2: (0.0000) | Acc: (89.00%) (4705/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (5858/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (6983/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.3028) |  Loss2: (0.0000) | Acc: (89.00%) (8109/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.3017) |  Loss2: (0.0000) | Acc: (89.00%) (9260/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.2990) |  Loss2: (0.0000) | Acc: (89.00%) (10429/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (11579/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (12737/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (13882/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.2985) |  Loss2: (0.0000) | Acc: (89.00%) (15031/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (16171/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (17315/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (18472/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.2975) |  Loss2: (0.0000) | Acc: (89.00%) (19617/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (20748/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (21893/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.2974) |  Loss2: (0.0000) | Acc: (89.00%) (23038/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (24189/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (25331/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (26484/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (27634/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (28795/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (29947/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.2965) |  Loss2: (0.0000) | Acc: (89.00%) (31103/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.2964) |  Loss2: (0.0000) | Acc: (89.00%) (32249/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.2958) |  Loss2: (0.0000) | Acc: (89.00%) (33397/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (34539/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.2962) |  Loss2: (0.0000) | Acc: (89.00%) (35700/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (89.00%) (36857/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.2952) |  Loss2: (0.0000) | Acc: (89.00%) (38021/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.2949) |  Loss2: (0.0000) | Acc: (89.00%) (39177/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.2959) |  Loss2: (0.0000) | Acc: (89.00%) (40314/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.2966) |  Loss2: (0.0000) | Acc: (89.00%) (41455/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (42603/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (43743/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (44849/50000)
# TEST : Loss: (0.4416) | Acc: (85.00%) (8571/10000)
percent tensor([0.6544], device='cuda:0')
percent tensor([0.6388], device='cuda:0')
percent tensor([0.7847], device='cuda:0')
percent tensor([0.7017], device='cuda:0')
percent tensor([0.7895], device='cuda:0')
percent tensor([0.8112], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.1957], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (89.00%) (1260/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.3005) |  Loss2: (0.0000) | Acc: (89.00%) (2400/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.2973) |  Loss2: (0.0000) | Acc: (89.00%) (3556/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (4719/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.2858) |  Loss2: (0.0000) | Acc: (90.00%) (5878/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (7031/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (8176/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (89.00%) (9320/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (89.00%) (10474/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (89.00%) (11626/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (12797/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (13943/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.2831) |  Loss2: (0.0000) | Acc: (89.00%) (15088/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (89.00%) (16240/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (17382/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (89.00%) (18529/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (89.00%) (19680/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.2865) |  Loss2: (0.0000) | Acc: (89.00%) (20834/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (90.00%) (22006/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (23166/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (89.00%) (24301/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (89.00%) (25456/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (26630/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (27790/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (28954/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (30104/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (31267/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (32409/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (90.00%) (33552/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (34684/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (35822/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (89.00%) (36958/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (38125/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (39272/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.2904) |  Loss2: (0.0000) | Acc: (89.00%) (40392/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.2903) |  Loss2: (0.0000) | Acc: (89.00%) (41538/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (42691/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (89.00%) (43855/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.2905) |  Loss2: (0.0000) | Acc: (89.00%) (44959/50000)
# TEST : Loss: (0.4750) | Acc: (85.00%) (8500/10000)
percent tensor([0.6543], device='cuda:0')
percent tensor([0.6388], device='cuda:0')
percent tensor([0.7847], device='cuda:0')
percent tensor([0.7016], device='cuda:0')
percent tensor([0.7894], device='cuda:0')
percent tensor([0.8111], device='cuda:0')
percent tensor([0.8398], device='cuda:0')
percent tensor([0.1958], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (2433/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (3591/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (4740/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (5890/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (7057/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (8238/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (9382/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (10541/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (11708/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (12879/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (14044/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (15206/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (16351/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (17513/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (18664/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (19826/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (20983/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (22128/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (23272/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (24434/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (90.00%) (25581/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (26716/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (27847/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (29011/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (30173/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (31341/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (32497/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (33649/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (34799/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (35961/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (37099/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (38263/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (39413/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (40563/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.2838) |  Loss2: (0.0000) | Acc: (90.00%) (41708/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (42845/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (43987/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.2860) |  Loss2: (0.0000) | Acc: (90.00%) (45083/50000)
# TEST : Loss: (0.5369) | Acc: (83.00%) (8340/10000)
percent tensor([0.6543], device='cuda:0')
percent tensor([0.6387], device='cuda:0')
percent tensor([0.7846], device='cuda:0')
percent tensor([0.7015], device='cuda:0')
percent tensor([0.7893], device='cuda:0')
percent tensor([0.8110], device='cuda:0')
percent tensor([0.8397], device='cuda:0')
percent tensor([0.1959], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.2705) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.2681) |  Loss2: (0.0000) | Acc: (90.00%) (2442/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (3597/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (4748/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (5927/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (7075/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (8241/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (9386/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (10533/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (11688/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (12857/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (14023/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (15190/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (16339/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.2806) |  Loss2: (0.0000) | Acc: (90.00%) (17482/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (18648/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (19820/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (20980/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (22113/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (23278/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (24425/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (25582/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (26738/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (27874/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (29060/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (30219/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (31384/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (32553/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (33713/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (34873/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (36047/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.2752) |  Loss2: (0.0000) | Acc: (90.00%) (37227/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (38385/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (39541/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (40715/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (41858/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.2760) |  Loss2: (0.0000) | Acc: (90.00%) (43037/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (44179/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (45281/50000)
# TEST : Loss: (0.4411) | Acc: (86.00%) (8622/10000)
percent tensor([0.6542], device='cuda:0')
percent tensor([0.6386], device='cuda:0')
percent tensor([0.7845], device='cuda:0')
percent tensor([0.7015], device='cuda:0')
percent tensor([0.7893], device='cuda:0')
percent tensor([0.8109], device='cuda:0')
percent tensor([0.8396], device='cuda:0')
percent tensor([0.1960], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.7221) |  Loss2: (0.4033) | Acc: (88.00%) (113/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.7325) |  Loss2: (0.4033) | Acc: (88.00%) (1240/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.7385) |  Loss2: (0.4032) | Acc: (88.00%) (2373/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.7408) |  Loss2: (0.4032) | Acc: (88.00%) (3505/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.7361) |  Loss2: (0.4031) | Acc: (88.00%) (4638/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.7343) |  Loss2: (0.4030) | Acc: (88.00%) (5767/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.7403) |  Loss2: (0.4030) | Acc: (88.00%) (6880/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.7377) |  Loss2: (0.4029) | Acc: (88.00%) (8023/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.7322) |  Loss2: (0.4029) | Acc: (88.00%) (9185/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.7290) |  Loss2: (0.4028) | Acc: (88.00%) (10325/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.7280) |  Loss2: (0.4028) | Acc: (88.00%) (11458/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.7275) |  Loss2: (0.4028) | Acc: (88.00%) (12590/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.7273) |  Loss2: (0.4027) | Acc: (88.00%) (13727/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.7236) |  Loss2: (0.4027) | Acc: (88.00%) (14880/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.7233) |  Loss2: (0.4026) | Acc: (88.00%) (16024/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.7232) |  Loss2: (0.4026) | Acc: (88.00%) (17167/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.7176) |  Loss2: (0.4025) | Acc: (89.00%) (18346/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.7174) |  Loss2: (0.4025) | Acc: (89.00%) (19493/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.7165) |  Loss2: (0.4024) | Acc: (89.00%) (20647/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.7159) |  Loss2: (0.4024) | Acc: (89.00%) (21801/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.7160) |  Loss2: (0.4023) | Acc: (89.00%) (22941/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.7154) |  Loss2: (0.4023) | Acc: (89.00%) (24096/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.7144) |  Loss2: (0.4022) | Acc: (89.00%) (25249/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.7141) |  Loss2: (0.4022) | Acc: (89.00%) (26393/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.7129) |  Loss2: (0.4021) | Acc: (89.00%) (27537/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.7110) |  Loss2: (0.4021) | Acc: (89.00%) (28696/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.7104) |  Loss2: (0.4020) | Acc: (89.00%) (29847/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.7090) |  Loss2: (0.4020) | Acc: (89.00%) (31008/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.7090) |  Loss2: (0.4019) | Acc: (89.00%) (32150/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.7077) |  Loss2: (0.4019) | Acc: (89.00%) (33310/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.7077) |  Loss2: (0.4018) | Acc: (89.00%) (34456/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.7064) |  Loss2: (0.4017) | Acc: (89.00%) (35614/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.7062) |  Loss2: (0.4017) | Acc: (89.00%) (36763/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.7058) |  Loss2: (0.4016) | Acc: (89.00%) (37921/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.7055) |  Loss2: (0.4016) | Acc: (89.00%) (39055/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.7049) |  Loss2: (0.4015) | Acc: (89.00%) (40216/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.7040) |  Loss2: (0.4015) | Acc: (89.00%) (41375/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.7026) |  Loss2: (0.4014) | Acc: (89.00%) (42546/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.7020) |  Loss2: (0.4014) | Acc: (89.00%) (43704/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.7016) |  Loss2: (0.4013) | Acc: (89.00%) (44813/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_115.pth.tar'
# TEST : Loss: (0.4307) | Acc: (86.00%) (8617/10000)
percent tensor([0.6695], device='cuda:0')
percent tensor([0.6341], device='cuda:0')
percent tensor([0.7891], device='cuda:0')
percent tensor([0.6980], device='cuda:0')
percent tensor([0.7908], device='cuda:0')
percent tensor([0.8184], device='cuda:0')
percent tensor([0.8449], device='cuda:0')
percent tensor([0.1955], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.6331) |  Loss2: (0.3993) | Acc: (90.00%) (116/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.7073) |  Loss2: (0.3992) | Acc: (89.00%) (1262/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.7047) |  Loss2: (0.3991) | Acc: (89.00%) (2401/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.6923) |  Loss2: (0.3991) | Acc: (89.00%) (3571/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.6864) |  Loss2: (0.3990) | Acc: (90.00%) (4736/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.6802) |  Loss2: (0.3989) | Acc: (90.00%) (5908/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.6813) |  Loss2: (0.3988) | Acc: (90.00%) (7060/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.6797) |  Loss2: (0.3988) | Acc: (90.00%) (8228/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.6825) |  Loss2: (0.3987) | Acc: (90.00%) (9371/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.6833) |  Loss2: (0.3986) | Acc: (90.00%) (10514/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.6811) |  Loss2: (0.3986) | Acc: (90.00%) (11682/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.6802) |  Loss2: (0.3985) | Acc: (90.00%) (12847/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.6814) |  Loss2: (0.3984) | Acc: (90.00%) (13993/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.6827) |  Loss2: (0.3984) | Acc: (90.00%) (15148/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.6815) |  Loss2: (0.3984) | Acc: (90.00%) (16314/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.6795) |  Loss2: (0.3983) | Acc: (90.00%) (17479/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.6789) |  Loss2: (0.3983) | Acc: (90.00%) (18626/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.6781) |  Loss2: (0.3982) | Acc: (90.00%) (19800/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.6781) |  Loss2: (0.3982) | Acc: (90.00%) (20965/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.6764) |  Loss2: (0.3982) | Acc: (90.00%) (22138/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.6750) |  Loss2: (0.3981) | Acc: (90.00%) (23304/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.6742) |  Loss2: (0.3981) | Acc: (90.00%) (24472/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.6731) |  Loss2: (0.3980) | Acc: (90.00%) (25633/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.6726) |  Loss2: (0.3980) | Acc: (90.00%) (26794/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.6726) |  Loss2: (0.3979) | Acc: (90.00%) (27952/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.6721) |  Loss2: (0.3979) | Acc: (90.00%) (29122/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.6723) |  Loss2: (0.3978) | Acc: (90.00%) (30269/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.6735) |  Loss2: (0.3977) | Acc: (90.00%) (31412/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.6737) |  Loss2: (0.3977) | Acc: (90.00%) (32576/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.6742) |  Loss2: (0.3976) | Acc: (90.00%) (33724/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.6737) |  Loss2: (0.3976) | Acc: (90.00%) (34887/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.6743) |  Loss2: (0.3975) | Acc: (90.00%) (36033/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.6741) |  Loss2: (0.3975) | Acc: (90.00%) (37194/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.6740) |  Loss2: (0.3974) | Acc: (90.00%) (38351/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.6742) |  Loss2: (0.3974) | Acc: (90.00%) (39514/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.6742) |  Loss2: (0.3973) | Acc: (90.00%) (40676/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.6741) |  Loss2: (0.3973) | Acc: (90.00%) (41830/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.6743) |  Loss2: (0.3972) | Acc: (90.00%) (42984/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.6751) |  Loss2: (0.3972) | Acc: (90.00%) (44124/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.6749) |  Loss2: (0.3971) | Acc: (90.00%) (45243/50000)
# TEST : Loss: (0.4298) | Acc: (85.00%) (8584/10000)
percent tensor([0.6739], device='cuda:0')
percent tensor([0.6384], device='cuda:0')
percent tensor([0.7945], device='cuda:0')
percent tensor([0.7038], device='cuda:0')
percent tensor([0.7952], device='cuda:0')
percent tensor([0.8202], device='cuda:0')
percent tensor([0.8470], device='cuda:0')
percent tensor([0.1928], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.7948) |  Loss2: (0.3952) | Acc: (86.00%) (111/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.7078) |  Loss2: (0.3952) | Acc: (88.00%) (1253/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.6946) |  Loss2: (0.3952) | Acc: (89.00%) (2411/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.6986) |  Loss2: (0.3952) | Acc: (89.00%) (3553/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.6864) |  Loss2: (0.3952) | Acc: (89.00%) (4719/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.6814) |  Loss2: (0.3952) | Acc: (90.00%) (5877/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.6783) |  Loss2: (0.3952) | Acc: (90.00%) (7060/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.6794) |  Loss2: (0.3951) | Acc: (90.00%) (8202/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.6798) |  Loss2: (0.3951) | Acc: (90.00%) (9358/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.6795) |  Loss2: (0.3951) | Acc: (90.00%) (10511/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.6804) |  Loss2: (0.3950) | Acc: (90.00%) (11665/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.6786) |  Loss2: (0.3950) | Acc: (90.00%) (12830/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.6792) |  Loss2: (0.3949) | Acc: (90.00%) (13991/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.6788) |  Loss2: (0.3949) | Acc: (90.00%) (15149/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.6767) |  Loss2: (0.3949) | Acc: (90.00%) (16321/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.6736) |  Loss2: (0.3948) | Acc: (90.00%) (17489/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.6717) |  Loss2: (0.3948) | Acc: (90.00%) (18662/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.6723) |  Loss2: (0.3947) | Acc: (90.00%) (19815/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.6723) |  Loss2: (0.3947) | Acc: (90.00%) (20975/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.6726) |  Loss2: (0.3946) | Acc: (90.00%) (22145/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.6715) |  Loss2: (0.3946) | Acc: (90.00%) (23323/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.6702) |  Loss2: (0.3945) | Acc: (90.00%) (24494/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.6696) |  Loss2: (0.3945) | Acc: (90.00%) (25658/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.6703) |  Loss2: (0.3945) | Acc: (90.00%) (26817/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.6706) |  Loss2: (0.3945) | Acc: (90.00%) (27981/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.6706) |  Loss2: (0.3944) | Acc: (90.00%) (29144/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.6696) |  Loss2: (0.3944) | Acc: (90.00%) (30312/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.6690) |  Loss2: (0.3944) | Acc: (90.00%) (31481/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.6682) |  Loss2: (0.3943) | Acc: (90.00%) (32653/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.6682) |  Loss2: (0.3943) | Acc: (90.00%) (33817/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.6685) |  Loss2: (0.3942) | Acc: (90.00%) (34976/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.6690) |  Loss2: (0.3942) | Acc: (90.00%) (36131/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.6686) |  Loss2: (0.3942) | Acc: (90.00%) (37302/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.6691) |  Loss2: (0.3941) | Acc: (90.00%) (38458/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.6692) |  Loss2: (0.3941) | Acc: (90.00%) (39626/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.6691) |  Loss2: (0.3940) | Acc: (90.00%) (40778/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.6693) |  Loss2: (0.3940) | Acc: (90.00%) (41944/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.6690) |  Loss2: (0.3940) | Acc: (90.00%) (43113/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.6688) |  Loss2: (0.3939) | Acc: (90.00%) (44272/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.6685) |  Loss2: (0.3939) | Acc: (90.00%) (45390/50000)
# TEST : Loss: (0.4323) | Acc: (85.00%) (8592/10000)
percent tensor([0.6747], device='cuda:0')
percent tensor([0.6441], device='cuda:0')
percent tensor([0.7959], device='cuda:0')
percent tensor([0.7090], device='cuda:0')
percent tensor([0.7980], device='cuda:0')
percent tensor([0.8222], device='cuda:0')
percent tensor([0.8480], device='cuda:0')
percent tensor([0.1898], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.5849) |  Loss2: (0.3926) | Acc: (94.00%) (121/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.6546) |  Loss2: (0.3926) | Acc: (91.00%) (1283/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.6588) |  Loss2: (0.3926) | Acc: (90.00%) (2445/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.6610) |  Loss2: (0.3925) | Acc: (90.00%) (3606/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.6644) |  Loss2: (0.3925) | Acc: (90.00%) (4768/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.6630) |  Loss2: (0.3924) | Acc: (90.00%) (5932/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.6616) |  Loss2: (0.3924) | Acc: (90.00%) (7103/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.6594) |  Loss2: (0.3924) | Acc: (91.00%) (8271/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.6637) |  Loss2: (0.3924) | Acc: (90.00%) (9421/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.6666) |  Loss2: (0.3923) | Acc: (90.00%) (10567/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.6694) |  Loss2: (0.3923) | Acc: (90.00%) (11719/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.6677) |  Loss2: (0.3923) | Acc: (90.00%) (12882/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.6675) |  Loss2: (0.3923) | Acc: (90.00%) (14042/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.6668) |  Loss2: (0.3923) | Acc: (90.00%) (15218/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.6678) |  Loss2: (0.3923) | Acc: (90.00%) (16372/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.6665) |  Loss2: (0.3922) | Acc: (90.00%) (17546/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.6654) |  Loss2: (0.3922) | Acc: (90.00%) (18722/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.6645) |  Loss2: (0.3922) | Acc: (90.00%) (19897/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.6652) |  Loss2: (0.3922) | Acc: (90.00%) (21054/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.6652) |  Loss2: (0.3921) | Acc: (90.00%) (22221/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.6658) |  Loss2: (0.3921) | Acc: (90.00%) (23379/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.6655) |  Loss2: (0.3921) | Acc: (90.00%) (24555/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.6655) |  Loss2: (0.3921) | Acc: (90.00%) (25705/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.6650) |  Loss2: (0.3921) | Acc: (90.00%) (26874/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.6654) |  Loss2: (0.3921) | Acc: (90.00%) (28025/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.6637) |  Loss2: (0.3920) | Acc: (90.00%) (29203/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.6635) |  Loss2: (0.3920) | Acc: (90.00%) (30367/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.6632) |  Loss2: (0.3920) | Acc: (90.00%) (31535/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.6638) |  Loss2: (0.3920) | Acc: (90.00%) (32677/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.6630) |  Loss2: (0.3920) | Acc: (90.00%) (33851/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.6628) |  Loss2: (0.3919) | Acc: (90.00%) (35013/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.6624) |  Loss2: (0.3919) | Acc: (90.00%) (36170/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.6627) |  Loss2: (0.3919) | Acc: (90.00%) (37325/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.6631) |  Loss2: (0.3919) | Acc: (90.00%) (38488/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.6633) |  Loss2: (0.3919) | Acc: (90.00%) (39652/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.6633) |  Loss2: (0.3918) | Acc: (90.00%) (40811/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.6634) |  Loss2: (0.3918) | Acc: (90.00%) (41973/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.6637) |  Loss2: (0.3918) | Acc: (90.00%) (43114/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.6636) |  Loss2: (0.3918) | Acc: (90.00%) (44274/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.6637) |  Loss2: (0.3918) | Acc: (90.00%) (45401/50000)
# TEST : Loss: (0.4223) | Acc: (86.00%) (8623/10000)
percent tensor([0.6759], device='cuda:0')
percent tensor([0.6463], device='cuda:0')
percent tensor([0.7954], device='cuda:0')
percent tensor([0.7120], device='cuda:0')
percent tensor([0.8004], device='cuda:0')
percent tensor([0.8231], device='cuda:0')
percent tensor([0.8498], device='cuda:0')
percent tensor([0.1865], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.6488) |  Loss2: (0.3911) | Acc: (93.00%) (120/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.6561) |  Loss2: (0.3910) | Acc: (91.00%) (1294/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.6709) |  Loss2: (0.3910) | Acc: (90.00%) (2445/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.6561) |  Loss2: (0.3910) | Acc: (91.00%) (3629/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.6598) |  Loss2: (0.3910) | Acc: (91.00%) (4796/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.6632) |  Loss2: (0.3910) | Acc: (91.00%) (5942/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.6586) |  Loss2: (0.3910) | Acc: (91.00%) (7112/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.6624) |  Loss2: (0.3910) | Acc: (90.00%) (8265/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.6618) |  Loss2: (0.3910) | Acc: (90.00%) (9434/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.6615) |  Loss2: (0.3910) | Acc: (90.00%) (10595/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.6613) |  Loss2: (0.3909) | Acc: (90.00%) (11748/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.6614) |  Loss2: (0.3909) | Acc: (90.00%) (12906/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.6605) |  Loss2: (0.3909) | Acc: (90.00%) (14067/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.6621) |  Loss2: (0.3908) | Acc: (90.00%) (15227/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.6616) |  Loss2: (0.3908) | Acc: (90.00%) (16390/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.6621) |  Loss2: (0.3908) | Acc: (90.00%) (17554/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.6641) |  Loss2: (0.3907) | Acc: (90.00%) (18698/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.6639) |  Loss2: (0.3907) | Acc: (90.00%) (19864/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.6630) |  Loss2: (0.3907) | Acc: (90.00%) (21033/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.6630) |  Loss2: (0.3906) | Acc: (90.00%) (22186/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.6627) |  Loss2: (0.3906) | Acc: (90.00%) (23352/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.6627) |  Loss2: (0.3906) | Acc: (90.00%) (24508/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.6618) |  Loss2: (0.3905) | Acc: (90.00%) (25675/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.6624) |  Loss2: (0.3905) | Acc: (90.00%) (26834/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.6621) |  Loss2: (0.3905) | Acc: (90.00%) (27998/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.6620) |  Loss2: (0.3905) | Acc: (90.00%) (29157/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.6616) |  Loss2: (0.3905) | Acc: (90.00%) (30329/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.6615) |  Loss2: (0.3904) | Acc: (90.00%) (31498/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.6610) |  Loss2: (0.3904) | Acc: (90.00%) (32669/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.6614) |  Loss2: (0.3904) | Acc: (90.00%) (33834/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.6616) |  Loss2: (0.3904) | Acc: (90.00%) (34987/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.6609) |  Loss2: (0.3903) | Acc: (90.00%) (36148/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.6601) |  Loss2: (0.3903) | Acc: (90.00%) (37326/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.6599) |  Loss2: (0.3903) | Acc: (90.00%) (38480/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.6595) |  Loss2: (0.3903) | Acc: (90.00%) (39655/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.6597) |  Loss2: (0.3902) | Acc: (90.00%) (40820/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.6598) |  Loss2: (0.3902) | Acc: (90.00%) (41971/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.6594) |  Loss2: (0.3902) | Acc: (90.00%) (43139/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.6600) |  Loss2: (0.3902) | Acc: (90.00%) (44284/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.6600) |  Loss2: (0.3902) | Acc: (90.00%) (45416/50000)
# TEST : Loss: (0.4216) | Acc: (86.00%) (8632/10000)
percent tensor([0.6761], device='cuda:0')
percent tensor([0.6473], device='cuda:0')
percent tensor([0.7979], device='cuda:0')
percent tensor([0.7152], device='cuda:0')
percent tensor([0.8034], device='cuda:0')
percent tensor([0.8245], device='cuda:0')
percent tensor([0.8505], device='cuda:0')
percent tensor([0.1835], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.2819) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (1288/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (3613/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (90.00%) (4771/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (90.00%) (5926/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (7068/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (8229/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (9378/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.2747) |  Loss2: (0.0000) | Acc: (90.00%) (10521/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (11663/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (12829/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (13985/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (15133/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (16267/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (17433/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (18595/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (19757/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (90.00%) (20911/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (22066/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (23208/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (24350/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.2843) |  Loss2: (0.0000) | Acc: (90.00%) (25506/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (26670/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (27826/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (28982/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (30132/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (31270/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (32410/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.2845) |  Loss2: (0.0000) | Acc: (90.00%) (33568/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (34717/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (35876/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (37042/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (38170/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (39335/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (90.00%) (40479/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (41624/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.2867) |  Loss2: (0.0000) | Acc: (90.00%) (42779/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.2872) |  Loss2: (0.0000) | Acc: (90.00%) (43920/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.2876) |  Loss2: (0.0000) | Acc: (90.00%) (45020/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_120.pth.tar'
# TEST : Loss: (0.5104) | Acc: (84.00%) (8406/10000)
percent tensor([0.6760], device='cuda:0')
percent tensor([0.6474], device='cuda:0')
percent tensor([0.7980], device='cuda:0')
percent tensor([0.7153], device='cuda:0')
percent tensor([0.8036], device='cuda:0')
percent tensor([0.8244], device='cuda:0')
percent tensor([0.8504], device='cuda:0')
percent tensor([0.1835], device='cuda:0')
Epoch: 121 | Batch_idx: 0 |  Loss: (0.3155) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (89.00%) (1267/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (3585/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (4725/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (89.00%) (5875/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.2796) |  Loss2: (0.0000) | Acc: (90.00%) (7044/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (89.00%) (8178/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (9341/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (90.00%) (10514/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.2802) |  Loss2: (0.0000) | Acc: (90.00%) (11669/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.2810) |  Loss2: (0.0000) | Acc: (90.00%) (12812/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (13978/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (15150/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (16314/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (17483/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (18658/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.2743) |  Loss2: (0.0000) | Acc: (90.00%) (19800/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (20950/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (22087/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (23233/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (24388/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (25561/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (26725/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (27892/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (29034/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.2775) |  Loss2: (0.0000) | Acc: (90.00%) (30193/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (31353/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (32504/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (33658/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (34813/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (35971/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (37116/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (38266/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (39418/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (40569/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (41746/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (42902/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (44057/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (45174/50000)
# TEST : Loss: (0.4554) | Acc: (85.00%) (8569/10000)
percent tensor([0.6759], device='cuda:0')
percent tensor([0.6473], device='cuda:0')
percent tensor([0.7979], device='cuda:0')
percent tensor([0.7153], device='cuda:0')
percent tensor([0.8035], device='cuda:0')
percent tensor([0.8243], device='cuda:0')
percent tensor([0.8503], device='cuda:0')
percent tensor([0.1836], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (1279/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (2448/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (3612/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (4769/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (5931/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (7091/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (8266/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (9429/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (90.00%) (10594/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (91.00%) (11767/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (12920/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (90.00%) (14081/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (15227/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (16400/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.2678) |  Loss2: (0.0000) | Acc: (90.00%) (17547/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (18708/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (19855/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.2695) |  Loss2: (0.0000) | Acc: (90.00%) (21009/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (22169/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.2700) |  Loss2: (0.0000) | Acc: (90.00%) (23331/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.2694) |  Loss2: (0.0000) | Acc: (90.00%) (24498/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (25654/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (26821/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.2689) |  Loss2: (0.0000) | Acc: (90.00%) (27976/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.2690) |  Loss2: (0.0000) | Acc: (90.00%) (29126/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.2691) |  Loss2: (0.0000) | Acc: (90.00%) (30286/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (31443/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (32609/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (33754/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.2709) |  Loss2: (0.0000) | Acc: (90.00%) (34899/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (36053/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (37216/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (38337/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (39495/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (40664/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.2737) |  Loss2: (0.0000) | Acc: (90.00%) (41824/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (42978/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.2732) |  Loss2: (0.0000) | Acc: (90.00%) (44158/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (45280/50000)
# TEST : Loss: (0.4641) | Acc: (85.00%) (8559/10000)
percent tensor([0.6759], device='cuda:0')
percent tensor([0.6473], device='cuda:0')
percent tensor([0.7978], device='cuda:0')
percent tensor([0.7152], device='cuda:0')
percent tensor([0.8034], device='cuda:0')
percent tensor([0.8242], device='cuda:0')
percent tensor([0.8502], device='cuda:0')
percent tensor([0.1837], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (2499/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (3679/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (4849/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (92.00%) (6010/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (7161/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (8335/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (9488/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.2529) |  Loss2: (0.0000) | Acc: (91.00%) (10647/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (11794/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (12964/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (14138/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (91.00%) (16457/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (17625/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (18764/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (19903/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (21064/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.2662) |  Loss2: (0.0000) | Acc: (90.00%) (22208/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (23372/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (24533/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (25693/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (26861/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (28016/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (29159/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (30340/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (31508/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (32659/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (33819/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (34960/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.2684) |  Loss2: (0.0000) | Acc: (90.00%) (36126/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (37298/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (90.00%) (38462/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (39608/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (40785/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.2669) |  Loss2: (0.0000) | Acc: (90.00%) (41942/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (43106/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.2661) |  Loss2: (0.0000) | Acc: (90.00%) (44282/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.2670) |  Loss2: (0.0000) | Acc: (90.00%) (45394/50000)
# TEST : Loss: (0.4577) | Acc: (86.00%) (8604/10000)
percent tensor([0.6758], device='cuda:0')
percent tensor([0.6472], device='cuda:0')
percent tensor([0.7977], device='cuda:0')
percent tensor([0.7151], device='cuda:0')
percent tensor([0.8033], device='cuda:0')
percent tensor([0.8242], device='cuda:0')
percent tensor([0.8501], device='cuda:0')
percent tensor([0.1838], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (2454/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.2535) |  Loss2: (0.0000) | Acc: (90.00%) (3604/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (4780/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (5962/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (7130/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.2514) |  Loss2: (0.0000) | Acc: (91.00%) (8277/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.2507) |  Loss2: (0.0000) | Acc: (91.00%) (9442/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (10602/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (11766/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.2521) |  Loss2: (0.0000) | Acc: (91.00%) (12934/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.2511) |  Loss2: (0.0000) | Acc: (91.00%) (14108/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (15284/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (16448/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (17619/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (18791/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (19965/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (21121/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (22280/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (23445/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (24598/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (25764/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.2533) |  Loss2: (0.0000) | Acc: (91.00%) (26934/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (28115/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (29280/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (30422/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (31580/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (32736/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (33901/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (91.00%) (35066/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (90.00%) (36214/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (90.00%) (37381/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.2581) |  Loss2: (0.0000) | Acc: (90.00%) (38537/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (90.00%) (39672/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (90.00%) (40835/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (90.00%) (41982/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (43140/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (44302/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (45420/50000)
# TEST : Loss: (0.5604) | Acc: (83.00%) (8372/10000)
percent tensor([0.6757], device='cuda:0')
percent tensor([0.6472], device='cuda:0')
percent tensor([0.7976], device='cuda:0')
percent tensor([0.7151], device='cuda:0')
percent tensor([0.8032], device='cuda:0')
percent tensor([0.8241], device='cuda:0')
percent tensor([0.8501], device='cuda:0')
percent tensor([0.1839], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.6749) |  Loss2: (0.3896) | Acc: (90.00%) (116/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.6693) |  Loss2: (0.3896) | Acc: (90.00%) (1281/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.6878) |  Loss2: (0.3896) | Acc: (90.00%) (2421/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.6950) |  Loss2: (0.3896) | Acc: (89.00%) (3563/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.7047) |  Loss2: (0.3896) | Acc: (89.00%) (4693/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.7060) |  Loss2: (0.3896) | Acc: (89.00%) (5841/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.7059) |  Loss2: (0.3897) | Acc: (89.00%) (6981/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.7054) |  Loss2: (0.3897) | Acc: (89.00%) (8127/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.7049) |  Loss2: (0.3897) | Acc: (89.00%) (9267/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.7077) |  Loss2: (0.3897) | Acc: (89.00%) (10386/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.7031) |  Loss2: (0.3897) | Acc: (89.00%) (11555/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.7037) |  Loss2: (0.3897) | Acc: (89.00%) (12690/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.6993) |  Loss2: (0.3897) | Acc: (89.00%) (13849/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.6957) |  Loss2: (0.3897) | Acc: (89.00%) (15002/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.6963) |  Loss2: (0.3896) | Acc: (89.00%) (16138/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.6952) |  Loss2: (0.3896) | Acc: (89.00%) (17288/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.6942) |  Loss2: (0.3896) | Acc: (89.00%) (18432/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.6937) |  Loss2: (0.3896) | Acc: (89.00%) (19592/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.6931) |  Loss2: (0.3895) | Acc: (89.00%) (20741/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.6927) |  Loss2: (0.3895) | Acc: (89.00%) (21892/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.6930) |  Loss2: (0.3894) | Acc: (89.00%) (23031/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.6914) |  Loss2: (0.3894) | Acc: (89.00%) (24185/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.6904) |  Loss2: (0.3893) | Acc: (89.00%) (25343/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.6902) |  Loss2: (0.3893) | Acc: (89.00%) (26486/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.6895) |  Loss2: (0.3893) | Acc: (89.00%) (27626/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.6880) |  Loss2: (0.3892) | Acc: (89.00%) (28780/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.6879) |  Loss2: (0.3892) | Acc: (89.00%) (29931/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.6871) |  Loss2: (0.3891) | Acc: (89.00%) (31081/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.6854) |  Loss2: (0.3891) | Acc: (89.00%) (32254/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.6848) |  Loss2: (0.3890) | Acc: (89.00%) (33414/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.6840) |  Loss2: (0.3890) | Acc: (89.00%) (34566/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.6831) |  Loss2: (0.3889) | Acc: (89.00%) (35735/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.6816) |  Loss2: (0.3888) | Acc: (89.00%) (36905/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.6810) |  Loss2: (0.3888) | Acc: (89.00%) (38063/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.6801) |  Loss2: (0.3887) | Acc: (89.00%) (39232/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.6788) |  Loss2: (0.3887) | Acc: (89.00%) (40400/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.6788) |  Loss2: (0.3886) | Acc: (89.00%) (41548/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.6779) |  Loss2: (0.3885) | Acc: (89.00%) (42731/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.6774) |  Loss2: (0.3885) | Acc: (89.00%) (43876/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.6755) |  Loss2: (0.3884) | Acc: (90.00%) (45025/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_125.pth.tar'
# TEST : Loss: (0.4394) | Acc: (86.00%) (8605/10000)
percent tensor([0.6875], device='cuda:0')
percent tensor([0.6444], device='cuda:0')
percent tensor([0.8053], device='cuda:0')
percent tensor([0.7207], device='cuda:0')
percent tensor([0.7967], device='cuda:0')
percent tensor([0.8296], device='cuda:0')
percent tensor([0.8518], device='cuda:0')
percent tensor([0.1811], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.8173) |  Loss2: (0.3863) | Acc: (85.00%) (110/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.6722) |  Loss2: (0.3862) | Acc: (90.00%) (1269/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.6442) |  Loss2: (0.3862) | Acc: (91.00%) (2453/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.6616) |  Loss2: (0.3861) | Acc: (90.00%) (3595/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.6616) |  Loss2: (0.3861) | Acc: (90.00%) (4749/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.6533) |  Loss2: (0.3860) | Acc: (90.00%) (5919/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.6535) |  Loss2: (0.3860) | Acc: (90.00%) (7078/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.6547) |  Loss2: (0.3859) | Acc: (90.00%) (8244/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.6517) |  Loss2: (0.3859) | Acc: (90.00%) (9420/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.6525) |  Loss2: (0.3858) | Acc: (90.00%) (10584/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.6533) |  Loss2: (0.3858) | Acc: (90.00%) (11741/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.6519) |  Loss2: (0.3857) | Acc: (90.00%) (12910/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.6539) |  Loss2: (0.3857) | Acc: (90.00%) (14060/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.6538) |  Loss2: (0.3856) | Acc: (90.00%) (15220/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.6566) |  Loss2: (0.3856) | Acc: (90.00%) (16365/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.6570) |  Loss2: (0.3855) | Acc: (90.00%) (17526/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.6556) |  Loss2: (0.3854) | Acc: (90.00%) (18701/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.6529) |  Loss2: (0.3854) | Acc: (90.00%) (19893/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.6519) |  Loss2: (0.3853) | Acc: (90.00%) (21059/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.6504) |  Loss2: (0.3853) | Acc: (90.00%) (22234/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.6486) |  Loss2: (0.3852) | Acc: (91.00%) (23419/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.6479) |  Loss2: (0.3852) | Acc: (91.00%) (24594/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.6480) |  Loss2: (0.3851) | Acc: (91.00%) (25750/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.6462) |  Loss2: (0.3851) | Acc: (91.00%) (26931/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.6468) |  Loss2: (0.3850) | Acc: (91.00%) (28099/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.6463) |  Loss2: (0.3850) | Acc: (91.00%) (29267/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.6472) |  Loss2: (0.3849) | Acc: (91.00%) (30418/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.6469) |  Loss2: (0.3849) | Acc: (91.00%) (31594/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.6474) |  Loss2: (0.3849) | Acc: (91.00%) (32745/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.6484) |  Loss2: (0.3848) | Acc: (90.00%) (33887/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.6490) |  Loss2: (0.3848) | Acc: (90.00%) (35049/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.6490) |  Loss2: (0.3847) | Acc: (90.00%) (36211/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.6493) |  Loss2: (0.3847) | Acc: (90.00%) (37382/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.6492) |  Loss2: (0.3847) | Acc: (90.00%) (38554/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.6489) |  Loss2: (0.3846) | Acc: (90.00%) (39710/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.6491) |  Loss2: (0.3846) | Acc: (90.00%) (40869/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.6491) |  Loss2: (0.3846) | Acc: (90.00%) (42040/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.6487) |  Loss2: (0.3845) | Acc: (91.00%) (43215/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.6487) |  Loss2: (0.3845) | Acc: (91.00%) (44388/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.6477) |  Loss2: (0.3845) | Acc: (91.00%) (45531/50000)
# TEST : Loss: (0.4295) | Acc: (86.00%) (8638/10000)
percent tensor([0.6910], device='cuda:0')
percent tensor([0.6477], device='cuda:0')
percent tensor([0.8073], device='cuda:0')
percent tensor([0.7246], device='cuda:0')
percent tensor([0.7988], device='cuda:0')
percent tensor([0.8350], device='cuda:0')
percent tensor([0.8537], device='cuda:0')
percent tensor([0.1779], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.6208) |  Loss2: (0.3830) | Acc: (92.00%) (118/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.6465) |  Loss2: (0.3830) | Acc: (90.00%) (1278/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.6395) |  Loss2: (0.3829) | Acc: (91.00%) (2460/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.6411) |  Loss2: (0.3829) | Acc: (91.00%) (3633/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.6416) |  Loss2: (0.3828) | Acc: (91.00%) (4806/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.6437) |  Loss2: (0.3828) | Acc: (91.00%) (5976/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.6402) |  Loss2: (0.3828) | Acc: (91.00%) (7141/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.6399) |  Loss2: (0.3827) | Acc: (91.00%) (8300/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.6452) |  Loss2: (0.3827) | Acc: (91.00%) (9437/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.6435) |  Loss2: (0.3827) | Acc: (91.00%) (10610/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.6433) |  Loss2: (0.3827) | Acc: (91.00%) (11786/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.6440) |  Loss2: (0.3826) | Acc: (91.00%) (12948/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.6414) |  Loss2: (0.3826) | Acc: (91.00%) (14129/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.6399) |  Loss2: (0.3826) | Acc: (91.00%) (15305/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.6390) |  Loss2: (0.3825) | Acc: (91.00%) (16469/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.6383) |  Loss2: (0.3825) | Acc: (91.00%) (17643/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.6372) |  Loss2: (0.3825) | Acc: (91.00%) (18802/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.6376) |  Loss2: (0.3825) | Acc: (91.00%) (19964/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.6375) |  Loss2: (0.3824) | Acc: (91.00%) (21126/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.6398) |  Loss2: (0.3824) | Acc: (91.00%) (22280/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.6393) |  Loss2: (0.3823) | Acc: (91.00%) (23453/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.6386) |  Loss2: (0.3823) | Acc: (91.00%) (24620/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.6383) |  Loss2: (0.3823) | Acc: (91.00%) (25790/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.6380) |  Loss2: (0.3822) | Acc: (91.00%) (26967/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.6374) |  Loss2: (0.3822) | Acc: (91.00%) (28146/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.6371) |  Loss2: (0.3821) | Acc: (91.00%) (29308/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.6368) |  Loss2: (0.3821) | Acc: (91.00%) (30474/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.6362) |  Loss2: (0.3821) | Acc: (91.00%) (31654/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.6357) |  Loss2: (0.3820) | Acc: (91.00%) (32824/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.6349) |  Loss2: (0.3820) | Acc: (91.00%) (34009/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.6349) |  Loss2: (0.3820) | Acc: (91.00%) (35181/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.6346) |  Loss2: (0.3820) | Acc: (91.00%) (36353/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.6343) |  Loss2: (0.3819) | Acc: (91.00%) (37529/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.6341) |  Loss2: (0.3819) | Acc: (91.00%) (38700/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.6346) |  Loss2: (0.3819) | Acc: (91.00%) (39859/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.6340) |  Loss2: (0.3818) | Acc: (91.00%) (41050/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.6338) |  Loss2: (0.3818) | Acc: (91.00%) (42226/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.6337) |  Loss2: (0.3818) | Acc: (91.00%) (43392/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.6338) |  Loss2: (0.3817) | Acc: (91.00%) (44559/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.6340) |  Loss2: (0.3817) | Acc: (91.00%) (45681/50000)
# TEST : Loss: (0.4263) | Acc: (86.00%) (8659/10000)
percent tensor([0.6941], device='cuda:0')
percent tensor([0.6503], device='cuda:0')
percent tensor([0.8102], device='cuda:0')
percent tensor([0.7293], device='cuda:0')
percent tensor([0.8009], device='cuda:0')
percent tensor([0.8372], device='cuda:0')
percent tensor([0.8550], device='cuda:0')
percent tensor([0.1741], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.5474) |  Loss2: (0.3804) | Acc: (96.00%) (123/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.5992) |  Loss2: (0.3803) | Acc: (92.00%) (1306/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.6153) |  Loss2: (0.3803) | Acc: (91.00%) (2471/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.6228) |  Loss2: (0.3803) | Acc: (91.00%) (3637/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.6284) |  Loss2: (0.3803) | Acc: (91.00%) (4804/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.6258) |  Loss2: (0.3802) | Acc: (91.00%) (5976/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.6240) |  Loss2: (0.3802) | Acc: (91.00%) (7159/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.6237) |  Loss2: (0.3802) | Acc: (91.00%) (8337/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.6213) |  Loss2: (0.3801) | Acc: (91.00%) (9529/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.6251) |  Loss2: (0.3801) | Acc: (91.00%) (10687/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.6262) |  Loss2: (0.3801) | Acc: (91.00%) (11852/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.6260) |  Loss2: (0.3801) | Acc: (91.00%) (13019/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.6287) |  Loss2: (0.3800) | Acc: (91.00%) (14180/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.6271) |  Loss2: (0.3800) | Acc: (91.00%) (15354/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.6255) |  Loss2: (0.3800) | Acc: (91.00%) (16541/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.6263) |  Loss2: (0.3800) | Acc: (91.00%) (17708/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.6260) |  Loss2: (0.3799) | Acc: (91.00%) (18875/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.6274) |  Loss2: (0.3799) | Acc: (91.00%) (20034/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.6276) |  Loss2: (0.3799) | Acc: (91.00%) (21216/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.6270) |  Loss2: (0.3798) | Acc: (91.00%) (22389/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.6263) |  Loss2: (0.3798) | Acc: (91.00%) (23567/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.6271) |  Loss2: (0.3798) | Acc: (91.00%) (24742/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.6287) |  Loss2: (0.3798) | Acc: (91.00%) (25902/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.6299) |  Loss2: (0.3797) | Acc: (91.00%) (27060/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.6289) |  Loss2: (0.3797) | Acc: (91.00%) (28230/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.6284) |  Loss2: (0.3797) | Acc: (91.00%) (29406/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.6283) |  Loss2: (0.3796) | Acc: (91.00%) (30583/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.6279) |  Loss2: (0.3796) | Acc: (91.00%) (31763/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.6281) |  Loss2: (0.3796) | Acc: (91.00%) (32927/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.6283) |  Loss2: (0.3796) | Acc: (91.00%) (34093/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.6282) |  Loss2: (0.3795) | Acc: (91.00%) (35277/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.6270) |  Loss2: (0.3795) | Acc: (91.00%) (36464/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.6271) |  Loss2: (0.3795) | Acc: (91.00%) (37630/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.6265) |  Loss2: (0.3795) | Acc: (91.00%) (38807/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.6265) |  Loss2: (0.3794) | Acc: (91.00%) (39977/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.6263) |  Loss2: (0.3794) | Acc: (91.00%) (41151/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.6258) |  Loss2: (0.3794) | Acc: (91.00%) (42337/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.6256) |  Loss2: (0.3794) | Acc: (91.00%) (43511/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.6257) |  Loss2: (0.3793) | Acc: (91.00%) (44690/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.6253) |  Loss2: (0.3793) | Acc: (91.00%) (45821/50000)
# TEST : Loss: (0.4213) | Acc: (86.00%) (8667/10000)
percent tensor([0.6951], device='cuda:0')
percent tensor([0.6533], device='cuda:0')
percent tensor([0.8115], device='cuda:0')
percent tensor([0.7324], device='cuda:0')
percent tensor([0.8027], device='cuda:0')
percent tensor([0.8393], device='cuda:0')
percent tensor([0.8566], device='cuda:0')
percent tensor([0.1702], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.6621) |  Loss2: (0.3784) | Acc: (90.00%) (116/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.6501) |  Loss2: (0.3784) | Acc: (90.00%) (1278/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.6428) |  Loss2: (0.3784) | Acc: (90.00%) (2445/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.6338) |  Loss2: (0.3785) | Acc: (91.00%) (3625/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.6312) |  Loss2: (0.3784) | Acc: (91.00%) (4798/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.6334) |  Loss2: (0.3784) | Acc: (91.00%) (5968/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.6286) |  Loss2: (0.3784) | Acc: (91.00%) (7141/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.6264) |  Loss2: (0.3784) | Acc: (91.00%) (8321/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.6277) |  Loss2: (0.3783) | Acc: (91.00%) (9480/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.6257) |  Loss2: (0.3783) | Acc: (91.00%) (10658/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.6247) |  Loss2: (0.3783) | Acc: (91.00%) (11838/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.6267) |  Loss2: (0.3783) | Acc: (91.00%) (13003/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.6252) |  Loss2: (0.3783) | Acc: (91.00%) (14189/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.6260) |  Loss2: (0.3782) | Acc: (91.00%) (15360/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.6256) |  Loss2: (0.3782) | Acc: (91.00%) (16544/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.6245) |  Loss2: (0.3782) | Acc: (91.00%) (17724/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.6246) |  Loss2: (0.3782) | Acc: (91.00%) (18884/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.6237) |  Loss2: (0.3781) | Acc: (91.00%) (20065/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.6241) |  Loss2: (0.3781) | Acc: (91.00%) (21234/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.6234) |  Loss2: (0.3781) | Acc: (91.00%) (22417/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.6238) |  Loss2: (0.3781) | Acc: (91.00%) (23586/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.6237) |  Loss2: (0.3781) | Acc: (91.00%) (24764/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.6238) |  Loss2: (0.3781) | Acc: (91.00%) (25933/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.6232) |  Loss2: (0.3781) | Acc: (91.00%) (27113/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.6242) |  Loss2: (0.3781) | Acc: (91.00%) (28268/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.6239) |  Loss2: (0.3781) | Acc: (91.00%) (29440/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.6232) |  Loss2: (0.3780) | Acc: (91.00%) (30625/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.6238) |  Loss2: (0.3780) | Acc: (91.00%) (31787/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.6232) |  Loss2: (0.3780) | Acc: (91.00%) (32969/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.6233) |  Loss2: (0.3780) | Acc: (91.00%) (34155/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.6244) |  Loss2: (0.3780) | Acc: (91.00%) (35302/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.6240) |  Loss2: (0.3779) | Acc: (91.00%) (36482/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.6236) |  Loss2: (0.3779) | Acc: (91.00%) (37654/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.6232) |  Loss2: (0.3779) | Acc: (91.00%) (38828/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.6241) |  Loss2: (0.3779) | Acc: (91.00%) (39992/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.6241) |  Loss2: (0.3779) | Acc: (91.00%) (41163/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.6244) |  Loss2: (0.3778) | Acc: (91.00%) (42332/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.6243) |  Loss2: (0.3778) | Acc: (91.00%) (43508/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.6249) |  Loss2: (0.3778) | Acc: (91.00%) (44666/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.6248) |  Loss2: (0.3778) | Acc: (91.00%) (45793/50000)
# TEST : Loss: (0.4199) | Acc: (86.00%) (8654/10000)
percent tensor([0.6951], device='cuda:0')
percent tensor([0.6524], device='cuda:0')
percent tensor([0.8123], device='cuda:0')
percent tensor([0.7351], device='cuda:0')
percent tensor([0.8062], device='cuda:0')
percent tensor([0.8406], device='cuda:0')
percent tensor([0.8572], device='cuda:0')
percent tensor([0.1665], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (2455/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (3625/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (4792/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (5951/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (7110/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (8273/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.2610) |  Loss2: (0.0000) | Acc: (91.00%) (9438/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.2606) |  Loss2: (0.0000) | Acc: (91.00%) (10607/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.2585) |  Loss2: (0.0000) | Acc: (91.00%) (11792/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (12933/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (14088/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (15262/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (91.00%) (16439/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (91.00%) (17601/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (18763/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (19931/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (21102/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.2608) |  Loss2: (0.0000) | Acc: (91.00%) (22277/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (91.00%) (23436/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (24584/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (91.00%) (25747/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (26902/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (91.00%) (28076/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (29241/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (91.00%) (30409/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (90.00%) (31565/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (32728/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.2643) |  Loss2: (0.0000) | Acc: (91.00%) (33897/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (35056/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.2649) |  Loss2: (0.0000) | Acc: (90.00%) (36214/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.2657) |  Loss2: (0.0000) | Acc: (90.00%) (37369/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (38546/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.2647) |  Loss2: (0.0000) | Acc: (90.00%) (39715/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (40865/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (42025/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (43176/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (44333/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (90.00%) (45438/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_130.pth.tar'
# TEST : Loss: (0.5409) | Acc: (83.00%) (8326/10000)
percent tensor([0.6949], device='cuda:0')
percent tensor([0.6521], device='cuda:0')
percent tensor([0.8120], device='cuda:0')
percent tensor([0.7348], device='cuda:0')
percent tensor([0.8061], device='cuda:0')
percent tensor([0.8405], device='cuda:0')
percent tensor([0.8572], device='cuda:0')
percent tensor([0.1665], device='cuda:0')
Epoch: 131 | Batch_idx: 0 |  Loss: (0.3434) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (2465/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (3646/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.2385) |  Loss2: (0.0000) | Acc: (91.00%) (4819/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (5985/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (7160/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (8354/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (9515/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (10699/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (11855/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (13022/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (14168/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (15332/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (16495/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (17650/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (18822/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (19990/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (21164/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (22319/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (23500/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.2542) |  Loss2: (0.0000) | Acc: (91.00%) (24657/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (25799/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (91.00%) (26964/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (28122/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (29278/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.2583) |  Loss2: (0.0000) | Acc: (91.00%) (30431/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (31588/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.2592) |  Loss2: (0.0000) | Acc: (91.00%) (32745/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.2595) |  Loss2: (0.0000) | Acc: (91.00%) (33903/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (35072/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (91.00%) (36252/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (37423/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.2582) |  Loss2: (0.0000) | Acc: (91.00%) (38584/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (39760/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (40908/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.2598) |  Loss2: (0.0000) | Acc: (91.00%) (42055/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (43243/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (91.00%) (44390/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (45491/50000)
# TEST : Loss: (0.4907) | Acc: (84.00%) (8453/10000)
percent tensor([0.6948], device='cuda:0')
percent tensor([0.6521], device='cuda:0')
percent tensor([0.8119], device='cuda:0')
percent tensor([0.7347], device='cuda:0')
percent tensor([0.8060], device='cuda:0')
percent tensor([0.8405], device='cuda:0')
percent tensor([0.8571], device='cuda:0')
percent tensor([0.1666], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (90.00%) (2441/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (90.00%) (3599/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (90.00%) (4750/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (90.00%) (5926/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (90.00%) (7105/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (8282/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (91.00%) (9453/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (10634/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (91.00%) (11799/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (12963/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (14130/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (16485/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (91.00%) (17646/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.2580) |  Loss2: (0.0000) | Acc: (91.00%) (18813/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (19997/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (21154/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (22343/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.2548) |  Loss2: (0.0000) | Acc: (91.00%) (23512/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (24673/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (25834/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (27017/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (28190/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (29354/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.2545) |  Loss2: (0.0000) | Acc: (91.00%) (30526/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.2549) |  Loss2: (0.0000) | Acc: (91.00%) (31690/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (32851/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.2565) |  Loss2: (0.0000) | Acc: (91.00%) (34003/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (35181/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.2556) |  Loss2: (0.0000) | Acc: (91.00%) (36343/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (37512/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (38673/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (39849/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (41011/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (42184/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (43356/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.2540) |  Loss2: (0.0000) | Acc: (91.00%) (44519/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (45632/50000)
# TEST : Loss: (0.4281) | Acc: (86.00%) (8653/10000)
percent tensor([0.6948], device='cuda:0')
percent tensor([0.6520], device='cuda:0')
percent tensor([0.8118], device='cuda:0')
percent tensor([0.7346], device='cuda:0')
percent tensor([0.8059], device='cuda:0')
percent tensor([0.8404], device='cuda:0')
percent tensor([0.8570], device='cuda:0')
percent tensor([0.1667], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (1290/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (2494/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (4837/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (92.00%) (6009/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (92.00%) (7184/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (8366/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (92.00%) (9547/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (10701/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (13058/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (14235/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (15403/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (16585/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (17742/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (18908/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.2429) |  Loss2: (0.0000) | Acc: (91.00%) (20069/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (21237/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (22387/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (23547/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (24707/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (25897/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (27076/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (28257/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (29444/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (30618/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (31798/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (32949/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (34108/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (35280/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (36437/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (37617/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.2466) |  Loss2: (0.0000) | Acc: (91.00%) (38780/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (39931/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (41073/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (42236/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (43399/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (44555/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (45690/50000)
# TEST : Loss: (0.4585) | Acc: (86.00%) (8608/10000)
percent tensor([0.6947], device='cuda:0')
percent tensor([0.6519], device='cuda:0')
percent tensor([0.8117], device='cuda:0')
percent tensor([0.7345], device='cuda:0')
percent tensor([0.8058], device='cuda:0')
percent tensor([0.8403], device='cuda:0')
percent tensor([0.8569], device='cuda:0')
percent tensor([0.1668], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (2472/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (3631/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.2539) |  Loss2: (0.0000) | Acc: (91.00%) (4787/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (5963/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (7140/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (8301/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (9469/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (11829/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (13002/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.2445) |  Loss2: (0.0000) | Acc: (91.00%) (14172/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (15335/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (16511/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (17674/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (18853/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (20034/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.2441) |  Loss2: (0.0000) | Acc: (91.00%) (21202/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (22368/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (23555/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (24718/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (25910/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (27087/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (28251/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (29418/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (30592/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (31749/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (32925/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (34091/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (35265/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (36430/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (37596/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (38763/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (39939/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (41096/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (42268/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (43434/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (44599/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (45707/50000)
# TEST : Loss: (0.5164) | Acc: (84.00%) (8417/10000)
percent tensor([0.6946], device='cuda:0')
percent tensor([0.6519], device='cuda:0')
percent tensor([0.8117], device='cuda:0')
percent tensor([0.7345], device='cuda:0')
percent tensor([0.8057], device='cuda:0')
percent tensor([0.8402], device='cuda:0')
percent tensor([0.8568], device='cuda:0')
percent tensor([0.1668], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.5785) |  Loss2: (0.3777) | Acc: (92.00%) (119/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.6532) |  Loss2: (0.3777) | Acc: (90.00%) (1269/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.6625) |  Loss2: (0.3777) | Acc: (89.00%) (2419/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.6684) |  Loss2: (0.3776) | Acc: (89.00%) (3558/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.6713) |  Loss2: (0.3775) | Acc: (89.00%) (4697/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.6784) |  Loss2: (0.3774) | Acc: (89.00%) (5818/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.6766) |  Loss2: (0.3773) | Acc: (89.00%) (6978/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.6711) |  Loss2: (0.3772) | Acc: (89.00%) (8147/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.6679) |  Loss2: (0.3771) | Acc: (89.00%) (9301/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.6683) |  Loss2: (0.3770) | Acc: (89.00%) (10446/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.6695) |  Loss2: (0.3770) | Acc: (89.00%) (11597/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.6689) |  Loss2: (0.3769) | Acc: (89.00%) (12744/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.6672) |  Loss2: (0.3768) | Acc: (89.00%) (13906/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.6638) |  Loss2: (0.3767) | Acc: (89.00%) (15082/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.6626) |  Loss2: (0.3767) | Acc: (89.00%) (16238/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.6630) |  Loss2: (0.3766) | Acc: (89.00%) (17394/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.6604) |  Loss2: (0.3765) | Acc: (90.00%) (18556/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.6592) |  Loss2: (0.3765) | Acc: (90.00%) (19713/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.6590) |  Loss2: (0.3764) | Acc: (90.00%) (20876/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.6587) |  Loss2: (0.3763) | Acc: (90.00%) (22032/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.6577) |  Loss2: (0.3762) | Acc: (90.00%) (23197/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.6562) |  Loss2: (0.3762) | Acc: (90.00%) (24365/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.6547) |  Loss2: (0.3761) | Acc: (90.00%) (25543/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.6530) |  Loss2: (0.3760) | Acc: (90.00%) (26706/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.6524) |  Loss2: (0.3760) | Acc: (90.00%) (27853/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.6520) |  Loss2: (0.3759) | Acc: (90.00%) (29017/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.6514) |  Loss2: (0.3759) | Acc: (90.00%) (30189/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.6501) |  Loss2: (0.3758) | Acc: (90.00%) (31357/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.6488) |  Loss2: (0.3757) | Acc: (90.00%) (32528/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.6470) |  Loss2: (0.3757) | Acc: (90.00%) (33706/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.6463) |  Loss2: (0.3756) | Acc: (90.00%) (34873/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.6457) |  Loss2: (0.3755) | Acc: (90.00%) (36036/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.6451) |  Loss2: (0.3755) | Acc: (90.00%) (37205/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.6445) |  Loss2: (0.3754) | Acc: (90.00%) (38370/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.6444) |  Loss2: (0.3754) | Acc: (90.00%) (39538/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.6439) |  Loss2: (0.3753) | Acc: (90.00%) (40712/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.6432) |  Loss2: (0.3753) | Acc: (90.00%) (41884/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.6424) |  Loss2: (0.3752) | Acc: (90.00%) (43066/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.6423) |  Loss2: (0.3752) | Acc: (90.00%) (44224/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.6416) |  Loss2: (0.3751) | Acc: (90.00%) (45345/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_135.pth.tar'
# TEST : Loss: (0.4235) | Acc: (86.00%) (8644/10000)
percent tensor([0.7034], device='cuda:0')
percent tensor([0.6600], device='cuda:0')
percent tensor([0.8178], device='cuda:0')
percent tensor([0.7346], device='cuda:0')
percent tensor([0.8142], device='cuda:0')
percent tensor([0.8399], device='cuda:0')
percent tensor([0.8585], device='cuda:0')
percent tensor([0.1655], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.6074) |  Loss2: (0.3728) | Acc: (91.00%) (117/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.6225) |  Loss2: (0.3728) | Acc: (91.00%) (1283/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.6156) |  Loss2: (0.3727) | Acc: (91.00%) (2451/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.6112) |  Loss2: (0.3727) | Acc: (91.00%) (3640/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.6159) |  Loss2: (0.3726) | Acc: (91.00%) (4812/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.6169) |  Loss2: (0.3726) | Acc: (91.00%) (5983/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.6131) |  Loss2: (0.3726) | Acc: (91.00%) (7156/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.6130) |  Loss2: (0.3725) | Acc: (91.00%) (8323/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.6136) |  Loss2: (0.3725) | Acc: (91.00%) (9496/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.6168) |  Loss2: (0.3724) | Acc: (91.00%) (10649/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.6163) |  Loss2: (0.3724) | Acc: (91.00%) (11836/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.6163) |  Loss2: (0.3724) | Acc: (91.00%) (13002/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.6168) |  Loss2: (0.3723) | Acc: (91.00%) (14162/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.6181) |  Loss2: (0.3723) | Acc: (91.00%) (15329/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.6182) |  Loss2: (0.3723) | Acc: (91.00%) (16506/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.6184) |  Loss2: (0.3722) | Acc: (91.00%) (17681/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.6166) |  Loss2: (0.3722) | Acc: (91.00%) (18860/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.6168) |  Loss2: (0.3721) | Acc: (91.00%) (20023/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.6161) |  Loss2: (0.3721) | Acc: (91.00%) (21202/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.6161) |  Loss2: (0.3721) | Acc: (91.00%) (22376/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.6147) |  Loss2: (0.3720) | Acc: (91.00%) (23550/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.6154) |  Loss2: (0.3720) | Acc: (91.00%) (24725/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.6148) |  Loss2: (0.3719) | Acc: (91.00%) (25909/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.6136) |  Loss2: (0.3719) | Acc: (91.00%) (27091/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.6135) |  Loss2: (0.3718) | Acc: (91.00%) (28271/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.6131) |  Loss2: (0.3718) | Acc: (91.00%) (29449/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.6134) |  Loss2: (0.3718) | Acc: (91.00%) (30617/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.6137) |  Loss2: (0.3717) | Acc: (91.00%) (31796/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.6135) |  Loss2: (0.3717) | Acc: (91.00%) (32965/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.6127) |  Loss2: (0.3717) | Acc: (91.00%) (34149/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.6142) |  Loss2: (0.3716) | Acc: (91.00%) (35313/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.6133) |  Loss2: (0.3716) | Acc: (91.00%) (36500/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.6135) |  Loss2: (0.3716) | Acc: (91.00%) (37664/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.6123) |  Loss2: (0.3715) | Acc: (91.00%) (38847/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.6125) |  Loss2: (0.3715) | Acc: (91.00%) (40011/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.6124) |  Loss2: (0.3714) | Acc: (91.00%) (41195/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.6120) |  Loss2: (0.3714) | Acc: (91.00%) (42375/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.6115) |  Loss2: (0.3714) | Acc: (91.00%) (43562/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.6117) |  Loss2: (0.3713) | Acc: (91.00%) (44727/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.6121) |  Loss2: (0.3713) | Acc: (91.00%) (45852/50000)
# TEST : Loss: (0.4142) | Acc: (86.00%) (8686/10000)
percent tensor([0.7059], device='cuda:0')
percent tensor([0.6641], device='cuda:0')
percent tensor([0.8221], device='cuda:0')
percent tensor([0.7391], device='cuda:0')
percent tensor([0.8186], device='cuda:0')
percent tensor([0.8405], device='cuda:0')
percent tensor([0.8585], device='cuda:0')
percent tensor([0.1627], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.5799) |  Loss2: (0.3699) | Acc: (92.00%) (118/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.5961) |  Loss2: (0.3699) | Acc: (91.00%) (1295/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.6054) |  Loss2: (0.3699) | Acc: (91.00%) (2457/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.6112) |  Loss2: (0.3699) | Acc: (91.00%) (3624/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.6054) |  Loss2: (0.3699) | Acc: (91.00%) (4813/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.6105) |  Loss2: (0.3699) | Acc: (91.00%) (5973/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.6104) |  Loss2: (0.3698) | Acc: (91.00%) (7153/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.6117) |  Loss2: (0.3698) | Acc: (91.00%) (8325/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.6132) |  Loss2: (0.3698) | Acc: (91.00%) (9493/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.6125) |  Loss2: (0.3698) | Acc: (91.00%) (10666/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.6114) |  Loss2: (0.3698) | Acc: (91.00%) (11839/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.6102) |  Loss2: (0.3697) | Acc: (91.00%) (13015/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.6108) |  Loss2: (0.3697) | Acc: (91.00%) (14190/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.6106) |  Loss2: (0.3696) | Acc: (91.00%) (15368/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.6105) |  Loss2: (0.3696) | Acc: (91.00%) (16550/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.6100) |  Loss2: (0.3696) | Acc: (91.00%) (17732/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.6105) |  Loss2: (0.3695) | Acc: (91.00%) (18910/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.6101) |  Loss2: (0.3695) | Acc: (91.00%) (20083/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.6091) |  Loss2: (0.3694) | Acc: (91.00%) (21265/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.6076) |  Loss2: (0.3694) | Acc: (91.00%) (22448/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.6071) |  Loss2: (0.3694) | Acc: (91.00%) (23623/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.6080) |  Loss2: (0.3694) | Acc: (91.00%) (24794/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.6079) |  Loss2: (0.3693) | Acc: (91.00%) (25974/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.6087) |  Loss2: (0.3693) | Acc: (91.00%) (27144/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.6086) |  Loss2: (0.3693) | Acc: (91.00%) (28333/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.6088) |  Loss2: (0.3692) | Acc: (91.00%) (29503/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.6087) |  Loss2: (0.3692) | Acc: (91.00%) (30673/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.6080) |  Loss2: (0.3691) | Acc: (91.00%) (31860/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.6089) |  Loss2: (0.3691) | Acc: (91.00%) (33021/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.6082) |  Loss2: (0.3691) | Acc: (91.00%) (34209/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.6080) |  Loss2: (0.3690) | Acc: (91.00%) (35393/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.6064) |  Loss2: (0.3690) | Acc: (91.00%) (36594/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.6063) |  Loss2: (0.3689) | Acc: (91.00%) (37781/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.6065) |  Loss2: (0.3689) | Acc: (91.00%) (38948/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.6068) |  Loss2: (0.3688) | Acc: (91.00%) (40126/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.6056) |  Loss2: (0.3688) | Acc: (91.00%) (41320/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.6053) |  Loss2: (0.3688) | Acc: (91.00%) (42498/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.6049) |  Loss2: (0.3687) | Acc: (91.00%) (43685/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.6041) |  Loss2: (0.3687) | Acc: (92.00%) (44871/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.6049) |  Loss2: (0.3687) | Acc: (91.00%) (45991/50000)
# TEST : Loss: (0.4131) | Acc: (86.00%) (8683/10000)
percent tensor([0.7082], device='cuda:0')
percent tensor([0.6653], device='cuda:0')
percent tensor([0.8241], device='cuda:0')
percent tensor([0.7443], device='cuda:0')
percent tensor([0.8204], device='cuda:0')
percent tensor([0.8428], device='cuda:0')
percent tensor([0.8590], device='cuda:0')
percent tensor([0.1601], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.6069) |  Loss2: (0.3676) | Acc: (90.00%) (116/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.5868) |  Loss2: (0.3676) | Acc: (92.00%) (1303/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.5980) |  Loss2: (0.3676) | Acc: (92.00%) (2482/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.5958) |  Loss2: (0.3676) | Acc: (92.00%) (3653/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.5938) |  Loss2: (0.3676) | Acc: (92.00%) (4831/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.5947) |  Loss2: (0.3676) | Acc: (92.00%) (6011/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.5937) |  Loss2: (0.3676) | Acc: (92.00%) (7198/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.5935) |  Loss2: (0.3676) | Acc: (92.00%) (8384/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.5925) |  Loss2: (0.3675) | Acc: (92.00%) (9568/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.5950) |  Loss2: (0.3675) | Acc: (92.00%) (10737/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.5936) |  Loss2: (0.3675) | Acc: (92.00%) (11922/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.5945) |  Loss2: (0.3674) | Acc: (92.00%) (13094/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.5958) |  Loss2: (0.3674) | Acc: (92.00%) (14272/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.5946) |  Loss2: (0.3674) | Acc: (92.00%) (15457/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.5961) |  Loss2: (0.3674) | Acc: (92.00%) (16632/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.5973) |  Loss2: (0.3673) | Acc: (92.00%) (17806/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.5977) |  Loss2: (0.3673) | Acc: (92.00%) (18980/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.5987) |  Loss2: (0.3673) | Acc: (92.00%) (20141/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.5991) |  Loss2: (0.3673) | Acc: (92.00%) (21323/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.5966) |  Loss2: (0.3673) | Acc: (92.00%) (22520/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.5968) |  Loss2: (0.3673) | Acc: (92.00%) (23694/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.5965) |  Loss2: (0.3672) | Acc: (92.00%) (24872/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.5971) |  Loss2: (0.3672) | Acc: (92.00%) (26040/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.5969) |  Loss2: (0.3672) | Acc: (92.00%) (27228/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.5963) |  Loss2: (0.3672) | Acc: (92.00%) (28414/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.5956) |  Loss2: (0.3671) | Acc: (92.00%) (29612/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.5950) |  Loss2: (0.3671) | Acc: (92.00%) (30810/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.5954) |  Loss2: (0.3671) | Acc: (92.00%) (31982/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.5960) |  Loss2: (0.3671) | Acc: (92.00%) (33157/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.5970) |  Loss2: (0.3670) | Acc: (92.00%) (34334/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.5961) |  Loss2: (0.3670) | Acc: (92.00%) (35532/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.5962) |  Loss2: (0.3670) | Acc: (92.00%) (36718/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.5965) |  Loss2: (0.3670) | Acc: (92.00%) (37889/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.5969) |  Loss2: (0.3669) | Acc: (92.00%) (39067/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.5959) |  Loss2: (0.3669) | Acc: (92.00%) (40257/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.5963) |  Loss2: (0.3669) | Acc: (92.00%) (41419/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.5966) |  Loss2: (0.3669) | Acc: (92.00%) (42598/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.5966) |  Loss2: (0.3669) | Acc: (92.00%) (43773/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.5963) |  Loss2: (0.3668) | Acc: (92.00%) (44954/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.5967) |  Loss2: (0.3668) | Acc: (92.00%) (46080/50000)
# TEST : Loss: (0.4089) | Acc: (87.00%) (8715/10000)
percent tensor([0.7070], device='cuda:0')
percent tensor([0.6661], device='cuda:0')
percent tensor([0.8256], device='cuda:0')
percent tensor([0.7478], device='cuda:0')
percent tensor([0.8231], device='cuda:0')
percent tensor([0.8441], device='cuda:0')
percent tensor([0.8589], device='cuda:0')
percent tensor([0.1572], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.6133) |  Loss2: (0.3663) | Acc: (92.00%) (119/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.5819) |  Loss2: (0.3664) | Acc: (92.00%) (1304/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.5848) |  Loss2: (0.3664) | Acc: (92.00%) (2489/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.5864) |  Loss2: (0.3664) | Acc: (92.00%) (3667/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.5902) |  Loss2: (0.3664) | Acc: (92.00%) (4853/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.5845) |  Loss2: (0.3664) | Acc: (92.00%) (6047/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.5870) |  Loss2: (0.3664) | Acc: (92.00%) (7222/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.5851) |  Loss2: (0.3664) | Acc: (92.00%) (8412/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.5833) |  Loss2: (0.3663) | Acc: (92.00%) (9604/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.5842) |  Loss2: (0.3663) | Acc: (92.00%) (10793/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.5859) |  Loss2: (0.3663) | Acc: (92.00%) (11980/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.5893) |  Loss2: (0.3663) | Acc: (92.00%) (13155/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.5918) |  Loss2: (0.3663) | Acc: (92.00%) (14318/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.5924) |  Loss2: (0.3663) | Acc: (92.00%) (15491/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.5932) |  Loss2: (0.3663) | Acc: (92.00%) (16664/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.5930) |  Loss2: (0.3663) | Acc: (92.00%) (17847/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.5934) |  Loss2: (0.3663) | Acc: (92.00%) (19030/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.5943) |  Loss2: (0.3663) | Acc: (92.00%) (20206/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.5939) |  Loss2: (0.3663) | Acc: (92.00%) (21388/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.5934) |  Loss2: (0.3663) | Acc: (92.00%) (22561/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.5938) |  Loss2: (0.3663) | Acc: (92.00%) (23748/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.5949) |  Loss2: (0.3663) | Acc: (92.00%) (24930/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.5945) |  Loss2: (0.3663) | Acc: (92.00%) (26118/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.5942) |  Loss2: (0.3662) | Acc: (92.00%) (27307/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.5953) |  Loss2: (0.3662) | Acc: (92.00%) (28478/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.5970) |  Loss2: (0.3662) | Acc: (92.00%) (29641/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.5971) |  Loss2: (0.3662) | Acc: (92.00%) (30816/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.5975) |  Loss2: (0.3662) | Acc: (92.00%) (31995/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.5966) |  Loss2: (0.3662) | Acc: (92.00%) (33193/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.5973) |  Loss2: (0.3662) | Acc: (92.00%) (34365/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.5978) |  Loss2: (0.3662) | Acc: (92.00%) (35531/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.5974) |  Loss2: (0.3661) | Acc: (92.00%) (36713/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.5972) |  Loss2: (0.3661) | Acc: (92.00%) (37897/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.5970) |  Loss2: (0.3661) | Acc: (92.00%) (39093/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.5974) |  Loss2: (0.3661) | Acc: (92.00%) (40263/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.5973) |  Loss2: (0.3660) | Acc: (92.00%) (41448/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.5973) |  Loss2: (0.3660) | Acc: (92.00%) (42619/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.5980) |  Loss2: (0.3660) | Acc: (92.00%) (43784/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.5976) |  Loss2: (0.3660) | Acc: (92.00%) (44974/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.5980) |  Loss2: (0.3660) | Acc: (92.00%) (46101/50000)
# TEST : Loss: (0.4083) | Acc: (87.00%) (8722/10000)
percent tensor([0.7094], device='cuda:0')
percent tensor([0.6655], device='cuda:0')
percent tensor([0.8261], device='cuda:0')
percent tensor([0.7485], device='cuda:0')
percent tensor([0.8240], device='cuda:0')
percent tensor([0.8457], device='cuda:0')
percent tensor([0.8593], device='cuda:0')
percent tensor([0.1545], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (1302/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (92.00%) (2473/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (3632/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.2588) |  Loss2: (0.0000) | Acc: (91.00%) (4776/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (5963/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (7117/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.2570) |  Loss2: (0.0000) | Acc: (90.00%) (8268/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (9439/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (90.00%) (10596/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (90.00%) (11760/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.2569) |  Loss2: (0.0000) | Acc: (90.00%) (12925/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (14100/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (15276/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (16444/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (17615/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.2575) |  Loss2: (0.0000) | Acc: (90.00%) (18752/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.2579) |  Loss2: (0.0000) | Acc: (90.00%) (19910/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.2587) |  Loss2: (0.0000) | Acc: (90.00%) (21069/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (90.00%) (22239/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (90.00%) (23406/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (90.00%) (24576/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (25737/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.2564) |  Loss2: (0.0000) | Acc: (90.00%) (26906/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.2577) |  Loss2: (0.0000) | Acc: (90.00%) (28048/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (29225/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.2586) |  Loss2: (0.0000) | Acc: (90.00%) (30377/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (90.00%) (31563/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.2571) |  Loss2: (0.0000) | Acc: (90.00%) (32724/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.2576) |  Loss2: (0.0000) | Acc: (90.00%) (33892/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (35071/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (36241/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (37404/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (38579/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (39755/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.2554) |  Loss2: (0.0000) | Acc: (91.00%) (40921/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (42075/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (43263/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.2553) |  Loss2: (0.0000) | Acc: (91.00%) (44421/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (45544/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_140.pth.tar'
# TEST : Loss: (0.4754) | Acc: (85.00%) (8539/10000)
percent tensor([0.7095], device='cuda:0')
percent tensor([0.6652], device='cuda:0')
percent tensor([0.8260], device='cuda:0')
percent tensor([0.7481], device='cuda:0')
percent tensor([0.8238], device='cuda:0')
percent tensor([0.8456], device='cuda:0')
percent tensor([0.8593], device='cuda:0')
percent tensor([0.1545], device='cuda:0')
Epoch: 141 | Batch_idx: 0 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.2710) |  Loss2: (0.0000) | Acc: (90.00%) (1277/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.2596) |  Loss2: (0.0000) | Acc: (91.00%) (2448/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (3616/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (4785/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (5949/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (7120/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (8295/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (9456/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (10642/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (11803/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (12970/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (14150/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (15328/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (16513/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (17681/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (18861/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (20021/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.2456) |  Loss2: (0.0000) | Acc: (91.00%) (21190/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (22347/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (23514/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (24681/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (25856/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (27018/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (28189/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (29365/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (30522/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (31697/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (32860/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (34035/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (35203/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (36371/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (91.00%) (37529/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (38702/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (39863/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.2502) |  Loss2: (0.0000) | Acc: (91.00%) (41019/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (42175/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (43335/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (44511/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (45630/50000)
# TEST : Loss: (0.4577) | Acc: (85.00%) (8596/10000)
percent tensor([0.7095], device='cuda:0')
percent tensor([0.6651], device='cuda:0')
percent tensor([0.8259], device='cuda:0')
percent tensor([0.7480], device='cuda:0')
percent tensor([0.8237], device='cuda:0')
percent tensor([0.8455], device='cuda:0')
percent tensor([0.8592], device='cuda:0')
percent tensor([0.1546], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.2510) |  Loss2: (0.0000) | Acc: (90.00%) (1279/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (2450/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (3634/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (4813/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (5962/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (7120/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (8301/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (9466/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (10627/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (11809/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (12983/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (14140/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (15310/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.2488) |  Loss2: (0.0000) | Acc: (91.00%) (16471/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (17642/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (18826/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (19997/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (21169/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (22336/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (23509/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.2437) |  Loss2: (0.0000) | Acc: (91.00%) (24687/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (25858/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (27046/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (28233/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (29420/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (30601/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (31770/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (32932/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (34103/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (35283/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (36447/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (37620/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (38788/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (39956/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (41117/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (42292/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (43466/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.2430) |  Loss2: (0.0000) | Acc: (91.00%) (44626/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (45733/50000)
# TEST : Loss: (0.4617) | Acc: (86.00%) (8615/10000)
percent tensor([0.7094], device='cuda:0')
percent tensor([0.6651], device='cuda:0')
percent tensor([0.8258], device='cuda:0')
percent tensor([0.7480], device='cuda:0')
percent tensor([0.8237], device='cuda:0')
percent tensor([0.8455], device='cuda:0')
percent tensor([0.8591], device='cuda:0')
percent tensor([0.1547], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (3668/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.2229) |  Loss2: (0.0000) | Acc: (92.00%) (4858/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (6039/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (7210/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (8387/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (9571/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (10742/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (11923/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (92.00%) (13078/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.2330) |  Loss2: (0.0000) | Acc: (91.00%) (14242/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (91.00%) (15418/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.2361) |  Loss2: (0.0000) | Acc: (91.00%) (16579/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (17761/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (18927/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.2358) |  Loss2: (0.0000) | Acc: (91.00%) (20098/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (21296/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (22461/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (23651/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (24835/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (26003/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (27170/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (28354/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (29521/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (30689/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.2360) |  Loss2: (0.0000) | Acc: (91.00%) (31852/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (33020/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (34190/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (35353/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (36528/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (37699/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (38867/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (40048/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (41216/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (42385/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.2377) |  Loss2: (0.0000) | Acc: (91.00%) (43562/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (44728/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (45842/50000)
# TEST : Loss: (0.4310) | Acc: (86.00%) (8664/10000)
percent tensor([0.7093], device='cuda:0')
percent tensor([0.6650], device='cuda:0')
percent tensor([0.8258], device='cuda:0')
percent tensor([0.7479], device='cuda:0')
percent tensor([0.8236], device='cuda:0')
percent tensor([0.8454], device='cuda:0')
percent tensor([0.8590], device='cuda:0')
percent tensor([0.1548], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (1296/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (2470/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (3638/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (4829/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (6022/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (7210/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (8371/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (91.00%) (9537/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (10706/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (11881/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (13048/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (14227/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (15413/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (16597/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (91.00%) (17771/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (18955/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (20132/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (21312/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (22491/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (23661/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (24831/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.2347) |  Loss2: (0.0000) | Acc: (91.00%) (26002/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (27191/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (28367/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (29540/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (30717/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (31881/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (33057/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.2351) |  Loss2: (0.0000) | Acc: (91.00%) (34216/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (91.00%) (35393/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (36579/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (37752/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (38908/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (40094/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (41260/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (42421/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (43575/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (91.00%) (44751/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (45895/50000)
# TEST : Loss: (0.4375) | Acc: (86.00%) (8682/10000)
percent tensor([0.7092], device='cuda:0')
percent tensor([0.6649], device='cuda:0')
percent tensor([0.8257], device='cuda:0')
percent tensor([0.7478], device='cuda:0')
percent tensor([0.8235], device='cuda:0')
percent tensor([0.8453], device='cuda:0')
percent tensor([0.8590], device='cuda:0')
percent tensor([0.1549], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.5191) |  Loss2: (0.3659) | Acc: (96.00%) (124/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.6094) |  Loss2: (0.3659) | Acc: (92.00%) (1303/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.6317) |  Loss2: (0.3659) | Acc: (91.00%) (2449/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.6349) |  Loss2: (0.3659) | Acc: (90.00%) (3606/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.6472) |  Loss2: (0.3659) | Acc: (90.00%) (4745/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.6450) |  Loss2: (0.3658) | Acc: (90.00%) (5914/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.6487) |  Loss2: (0.3658) | Acc: (90.00%) (7076/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.6492) |  Loss2: (0.3658) | Acc: (90.00%) (8232/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.6515) |  Loss2: (0.3658) | Acc: (90.00%) (9381/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.6469) |  Loss2: (0.3658) | Acc: (90.00%) (10553/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.6438) |  Loss2: (0.3658) | Acc: (90.00%) (11727/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.6420) |  Loss2: (0.3657) | Acc: (90.00%) (12883/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.6403) |  Loss2: (0.3657) | Acc: (90.00%) (14051/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.6381) |  Loss2: (0.3656) | Acc: (90.00%) (15233/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.6369) |  Loss2: (0.3656) | Acc: (90.00%) (16395/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.6367) |  Loss2: (0.3655) | Acc: (90.00%) (17560/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.6367) |  Loss2: (0.3654) | Acc: (90.00%) (18726/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.6354) |  Loss2: (0.3654) | Acc: (90.00%) (19897/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.6336) |  Loss2: (0.3653) | Acc: (90.00%) (21070/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.6330) |  Loss2: (0.3652) | Acc: (90.00%) (22236/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.6323) |  Loss2: (0.3652) | Acc: (90.00%) (23400/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.6326) |  Loss2: (0.3651) | Acc: (90.00%) (24563/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.6308) |  Loss2: (0.3651) | Acc: (90.00%) (25735/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.6296) |  Loss2: (0.3650) | Acc: (91.00%) (26916/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.6277) |  Loss2: (0.3649) | Acc: (91.00%) (28106/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.6263) |  Loss2: (0.3649) | Acc: (91.00%) (29285/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.6259) |  Loss2: (0.3648) | Acc: (91.00%) (30458/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.6254) |  Loss2: (0.3647) | Acc: (91.00%) (31633/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.6241) |  Loss2: (0.3647) | Acc: (91.00%) (32814/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.6230) |  Loss2: (0.3646) | Acc: (91.00%) (34001/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.6213) |  Loss2: (0.3645) | Acc: (91.00%) (35184/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.6212) |  Loss2: (0.3645) | Acc: (91.00%) (36351/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.6198) |  Loss2: (0.3644) | Acc: (91.00%) (37534/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.6192) |  Loss2: (0.3643) | Acc: (91.00%) (38707/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.6190) |  Loss2: (0.3643) | Acc: (91.00%) (39883/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.6177) |  Loss2: (0.3642) | Acc: (91.00%) (41064/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.6165) |  Loss2: (0.3642) | Acc: (91.00%) (42236/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.6157) |  Loss2: (0.3641) | Acc: (91.00%) (43415/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.6155) |  Loss2: (0.3640) | Acc: (91.00%) (44579/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.6147) |  Loss2: (0.3640) | Acc: (91.00%) (45719/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_145.pth.tar'
# TEST : Loss: (0.4216) | Acc: (86.00%) (8676/10000)
percent tensor([0.7130], device='cuda:0')
percent tensor([0.6702], device='cuda:0')
percent tensor([0.8324], device='cuda:0')
percent tensor([0.7527], device='cuda:0')
percent tensor([0.8245], device='cuda:0')
percent tensor([0.8453], device='cuda:0')
percent tensor([0.8623], device='cuda:0')
percent tensor([0.1549], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.5369) |  Loss2: (0.3621) | Acc: (93.00%) (120/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.5922) |  Loss2: (0.3621) | Acc: (91.00%) (1295/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.5833) |  Loss2: (0.3621) | Acc: (92.00%) (2483/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.5761) |  Loss2: (0.3620) | Acc: (92.00%) (3677/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.5716) |  Loss2: (0.3619) | Acc: (92.00%) (4868/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.5766) |  Loss2: (0.3619) | Acc: (92.00%) (6038/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.5807) |  Loss2: (0.3618) | Acc: (92.00%) (7209/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.5816) |  Loss2: (0.3617) | Acc: (92.00%) (8393/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.5865) |  Loss2: (0.3616) | Acc: (92.00%) (9564/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.5860) |  Loss2: (0.3616) | Acc: (92.00%) (10749/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.5876) |  Loss2: (0.3615) | Acc: (92.00%) (11927/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.5878) |  Loss2: (0.3614) | Acc: (92.00%) (13102/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.5891) |  Loss2: (0.3614) | Acc: (92.00%) (14283/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.5897) |  Loss2: (0.3613) | Acc: (92.00%) (15463/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.5899) |  Loss2: (0.3613) | Acc: (92.00%) (16642/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.5908) |  Loss2: (0.3613) | Acc: (92.00%) (17817/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.5910) |  Loss2: (0.3612) | Acc: (92.00%) (18992/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.5920) |  Loss2: (0.3612) | Acc: (92.00%) (20161/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.5932) |  Loss2: (0.3612) | Acc: (92.00%) (21333/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.5912) |  Loss2: (0.3611) | Acc: (92.00%) (22525/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.5906) |  Loss2: (0.3611) | Acc: (92.00%) (23699/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.5910) |  Loss2: (0.3611) | Acc: (92.00%) (24883/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.5908) |  Loss2: (0.3610) | Acc: (92.00%) (26072/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.5901) |  Loss2: (0.3610) | Acc: (92.00%) (27249/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.5914) |  Loss2: (0.3609) | Acc: (92.00%) (28410/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.5907) |  Loss2: (0.3609) | Acc: (92.00%) (29598/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.5893) |  Loss2: (0.3609) | Acc: (92.00%) (30796/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.5893) |  Loss2: (0.3608) | Acc: (92.00%) (31983/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.5893) |  Loss2: (0.3608) | Acc: (92.00%) (33161/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.5893) |  Loss2: (0.3608) | Acc: (92.00%) (34337/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.5896) |  Loss2: (0.3607) | Acc: (92.00%) (35525/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.5895) |  Loss2: (0.3607) | Acc: (92.00%) (36703/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.5894) |  Loss2: (0.3607) | Acc: (92.00%) (37878/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.5900) |  Loss2: (0.3607) | Acc: (92.00%) (39055/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.5895) |  Loss2: (0.3606) | Acc: (92.00%) (40236/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.5890) |  Loss2: (0.3606) | Acc: (92.00%) (41418/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.5887) |  Loss2: (0.3606) | Acc: (92.00%) (42605/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.5883) |  Loss2: (0.3605) | Acc: (92.00%) (43788/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.5884) |  Loss2: (0.3605) | Acc: (92.00%) (44960/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.5889) |  Loss2: (0.3605) | Acc: (92.00%) (46097/50000)
# TEST : Loss: (0.4128) | Acc: (87.00%) (8712/10000)
percent tensor([0.7167], device='cuda:0')
percent tensor([0.6755], device='cuda:0')
percent tensor([0.8338], device='cuda:0')
percent tensor([0.7564], device='cuda:0')
percent tensor([0.8264], device='cuda:0')
percent tensor([0.8462], device='cuda:0')
percent tensor([0.8643], device='cuda:0')
percent tensor([0.1536], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.5925) |  Loss2: (0.3593) | Acc: (91.00%) (117/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.5875) |  Loss2: (0.3593) | Acc: (91.00%) (1291/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.5931) |  Loss2: (0.3593) | Acc: (91.00%) (2468/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.5899) |  Loss2: (0.3592) | Acc: (92.00%) (3653/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.5899) |  Loss2: (0.3592) | Acc: (92.00%) (4837/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.5903) |  Loss2: (0.3592) | Acc: (92.00%) (6011/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.5894) |  Loss2: (0.3592) | Acc: (92.00%) (7185/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.5918) |  Loss2: (0.3592) | Acc: (91.00%) (8352/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.5900) |  Loss2: (0.3592) | Acc: (91.00%) (9527/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.5885) |  Loss2: (0.3592) | Acc: (92.00%) (10724/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.5879) |  Loss2: (0.3592) | Acc: (92.00%) (11901/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.5877) |  Loss2: (0.3591) | Acc: (92.00%) (13090/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.5898) |  Loss2: (0.3591) | Acc: (92.00%) (14260/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.5891) |  Loss2: (0.3591) | Acc: (92.00%) (15446/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.5877) |  Loss2: (0.3591) | Acc: (92.00%) (16638/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.5886) |  Loss2: (0.3591) | Acc: (92.00%) (17819/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.5893) |  Loss2: (0.3590) | Acc: (92.00%) (18990/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.5898) |  Loss2: (0.3590) | Acc: (92.00%) (20160/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.5892) |  Loss2: (0.3590) | Acc: (92.00%) (21341/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.5884) |  Loss2: (0.3590) | Acc: (92.00%) (22513/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.5881) |  Loss2: (0.3589) | Acc: (92.00%) (23697/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.5868) |  Loss2: (0.3589) | Acc: (92.00%) (24880/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.5866) |  Loss2: (0.3589) | Acc: (92.00%) (26057/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.5867) |  Loss2: (0.3589) | Acc: (92.00%) (27241/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.5875) |  Loss2: (0.3588) | Acc: (92.00%) (28410/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.5882) |  Loss2: (0.3588) | Acc: (92.00%) (29580/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.5875) |  Loss2: (0.3588) | Acc: (92.00%) (30767/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.5880) |  Loss2: (0.3587) | Acc: (92.00%) (31936/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.5881) |  Loss2: (0.3587) | Acc: (92.00%) (33114/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.5877) |  Loss2: (0.3587) | Acc: (92.00%) (34301/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.5872) |  Loss2: (0.3587) | Acc: (92.00%) (35491/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.5867) |  Loss2: (0.3586) | Acc: (92.00%) (36667/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.5854) |  Loss2: (0.3586) | Acc: (92.00%) (37863/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.5846) |  Loss2: (0.3586) | Acc: (92.00%) (39057/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.5855) |  Loss2: (0.3586) | Acc: (92.00%) (40226/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.5855) |  Loss2: (0.3585) | Acc: (92.00%) (41401/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.5851) |  Loss2: (0.3585) | Acc: (92.00%) (42588/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.5847) |  Loss2: (0.3585) | Acc: (92.00%) (43784/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.5848) |  Loss2: (0.3585) | Acc: (92.00%) (44964/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.5843) |  Loss2: (0.3585) | Acc: (92.00%) (46120/50000)
# TEST : Loss: (0.4092) | Acc: (87.00%) (8712/10000)
percent tensor([0.7190], device='cuda:0')
percent tensor([0.6759], device='cuda:0')
percent tensor([0.8346], device='cuda:0')
percent tensor([0.7587], device='cuda:0')
percent tensor([0.8288], device='cuda:0')
percent tensor([0.8481], device='cuda:0')
percent tensor([0.8646], device='cuda:0')
percent tensor([0.1520], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.6149) |  Loss2: (0.3577) | Acc: (93.00%) (120/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.5712) |  Loss2: (0.3577) | Acc: (92.00%) (1308/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.5826) |  Loss2: (0.3576) | Acc: (92.00%) (2484/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.5784) |  Loss2: (0.3576) | Acc: (92.00%) (3671/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.5818) |  Loss2: (0.3576) | Acc: (92.00%) (4846/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.5846) |  Loss2: (0.3576) | Acc: (92.00%) (6024/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.5847) |  Loss2: (0.3576) | Acc: (92.00%) (7203/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.5860) |  Loss2: (0.3575) | Acc: (92.00%) (8371/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.5820) |  Loss2: (0.3575) | Acc: (92.00%) (9561/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.5830) |  Loss2: (0.3575) | Acc: (92.00%) (10748/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.5820) |  Loss2: (0.3575) | Acc: (92.00%) (11939/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.5830) |  Loss2: (0.3575) | Acc: (92.00%) (13116/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.5828) |  Loss2: (0.3575) | Acc: (92.00%) (14300/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.5842) |  Loss2: (0.3575) | Acc: (92.00%) (15462/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.5832) |  Loss2: (0.3575) | Acc: (92.00%) (16643/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.5820) |  Loss2: (0.3575) | Acc: (92.00%) (17829/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.5828) |  Loss2: (0.3575) | Acc: (92.00%) (19010/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.5826) |  Loss2: (0.3575) | Acc: (92.00%) (20193/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.5817) |  Loss2: (0.3575) | Acc: (92.00%) (21380/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.5811) |  Loss2: (0.3575) | Acc: (92.00%) (22565/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.5816) |  Loss2: (0.3575) | Acc: (92.00%) (23743/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.5817) |  Loss2: (0.3575) | Acc: (92.00%) (24933/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.5826) |  Loss2: (0.3574) | Acc: (92.00%) (26107/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.5822) |  Loss2: (0.3574) | Acc: (92.00%) (27283/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.5825) |  Loss2: (0.3574) | Acc: (92.00%) (28466/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.5825) |  Loss2: (0.3574) | Acc: (92.00%) (29656/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.5812) |  Loss2: (0.3574) | Acc: (92.00%) (30843/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.5805) |  Loss2: (0.3574) | Acc: (92.00%) (32033/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.5810) |  Loss2: (0.3574) | Acc: (92.00%) (33208/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.5809) |  Loss2: (0.3573) | Acc: (92.00%) (34396/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.5813) |  Loss2: (0.3573) | Acc: (92.00%) (35585/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.5811) |  Loss2: (0.3573) | Acc: (92.00%) (36765/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.5810) |  Loss2: (0.3573) | Acc: (92.00%) (37946/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.5809) |  Loss2: (0.3573) | Acc: (92.00%) (39130/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.5804) |  Loss2: (0.3572) | Acc: (92.00%) (40319/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.5796) |  Loss2: (0.3572) | Acc: (92.00%) (41511/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.5795) |  Loss2: (0.3572) | Acc: (92.00%) (42698/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.5789) |  Loss2: (0.3572) | Acc: (92.00%) (43901/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.5786) |  Loss2: (0.3571) | Acc: (92.00%) (45080/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.5788) |  Loss2: (0.3571) | Acc: (92.00%) (46217/50000)
# TEST : Loss: (0.4111) | Acc: (87.00%) (8710/10000)
percent tensor([0.7197], device='cuda:0')
percent tensor([0.6787], device='cuda:0')
percent tensor([0.8357], device='cuda:0')
percent tensor([0.7611], device='cuda:0')
percent tensor([0.8305], device='cuda:0')
percent tensor([0.8492], device='cuda:0')
percent tensor([0.8655], device='cuda:0')
percent tensor([0.1502], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.6092) |  Loss2: (0.3561) | Acc: (90.00%) (116/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.5947) |  Loss2: (0.3561) | Acc: (91.00%) (1292/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.5877) |  Loss2: (0.3561) | Acc: (92.00%) (2475/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.5853) |  Loss2: (0.3561) | Acc: (92.00%) (3664/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.5896) |  Loss2: (0.3561) | Acc: (92.00%) (4833/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.5889) |  Loss2: (0.3561) | Acc: (92.00%) (6013/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.5896) |  Loss2: (0.3561) | Acc: (91.00%) (7181/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.5874) |  Loss2: (0.3561) | Acc: (91.00%) (8360/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.5848) |  Loss2: (0.3561) | Acc: (92.00%) (9540/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.5845) |  Loss2: (0.3561) | Acc: (92.00%) (10721/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.5823) |  Loss2: (0.3561) | Acc: (92.00%) (11913/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.5796) |  Loss2: (0.3561) | Acc: (92.00%) (13105/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.5779) |  Loss2: (0.3561) | Acc: (92.00%) (14300/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.5774) |  Loss2: (0.3561) | Acc: (92.00%) (15497/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.5774) |  Loss2: (0.3560) | Acc: (92.00%) (16685/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.5775) |  Loss2: (0.3560) | Acc: (92.00%) (17875/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.5774) |  Loss2: (0.3560) | Acc: (92.00%) (19057/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.5779) |  Loss2: (0.3560) | Acc: (92.00%) (20243/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.5800) |  Loss2: (0.3559) | Acc: (92.00%) (21398/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.5795) |  Loss2: (0.3559) | Acc: (92.00%) (22592/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.5800) |  Loss2: (0.3559) | Acc: (92.00%) (23771/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.5822) |  Loss2: (0.3558) | Acc: (92.00%) (24929/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.5817) |  Loss2: (0.3558) | Acc: (92.00%) (26120/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.5796) |  Loss2: (0.3558) | Acc: (92.00%) (27320/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.5798) |  Loss2: (0.3557) | Acc: (92.00%) (28490/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.5790) |  Loss2: (0.3557) | Acc: (92.00%) (29687/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.5796) |  Loss2: (0.3557) | Acc: (92.00%) (30868/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.5793) |  Loss2: (0.3556) | Acc: (92.00%) (32055/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.5794) |  Loss2: (0.3556) | Acc: (92.00%) (33235/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.5797) |  Loss2: (0.3556) | Acc: (92.00%) (34404/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.5787) |  Loss2: (0.3555) | Acc: (92.00%) (35596/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.5786) |  Loss2: (0.3555) | Acc: (92.00%) (36775/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.5789) |  Loss2: (0.3555) | Acc: (92.00%) (37959/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.5785) |  Loss2: (0.3555) | Acc: (92.00%) (39152/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.5784) |  Loss2: (0.3554) | Acc: (92.00%) (40325/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.5787) |  Loss2: (0.3554) | Acc: (92.00%) (41506/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.5779) |  Loss2: (0.3554) | Acc: (92.00%) (42702/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.5770) |  Loss2: (0.3554) | Acc: (92.00%) (43897/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.5767) |  Loss2: (0.3554) | Acc: (92.00%) (45080/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.5776) |  Loss2: (0.3553) | Acc: (92.00%) (46201/50000)
# TEST : Loss: (0.4038) | Acc: (87.00%) (8739/10000)
percent tensor([0.7205], device='cuda:0')
percent tensor([0.6808], device='cuda:0')
percent tensor([0.8367], device='cuda:0')
percent tensor([0.7632], device='cuda:0')
percent tensor([0.8312], device='cuda:0')
percent tensor([0.8503], device='cuda:0')
percent tensor([0.8668], device='cuda:0')
percent tensor([0.1486], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.2808) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (1284/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (3623/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (4787/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (5943/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (7133/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (8291/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (9467/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (10635/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (11807/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (12966/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (14138/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (15286/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (91.00%) (16470/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.2384) |  Loss2: (0.0000) | Acc: (91.00%) (17655/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (18838/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.2386) |  Loss2: (0.0000) | Acc: (91.00%) (20002/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.2380) |  Loss2: (0.0000) | Acc: (91.00%) (21188/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (22352/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (23533/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.2378) |  Loss2: (0.0000) | Acc: (91.00%) (24694/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.2383) |  Loss2: (0.0000) | Acc: (91.00%) (25856/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (27015/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (28184/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.2392) |  Loss2: (0.0000) | Acc: (91.00%) (29364/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (30530/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.2382) |  Loss2: (0.0000) | Acc: (91.00%) (31720/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (32902/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.2401) |  Loss2: (0.0000) | Acc: (91.00%) (34065/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.2406) |  Loss2: (0.0000) | Acc: (91.00%) (35234/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (36407/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.2407) |  Loss2: (0.0000) | Acc: (91.00%) (37571/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.2402) |  Loss2: (0.0000) | Acc: (91.00%) (38754/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.2409) |  Loss2: (0.0000) | Acc: (91.00%) (39916/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (41093/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (42259/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (43421/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (91.00%) (44596/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (45718/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_150.pth.tar'
# TEST : Loss: (0.4398) | Acc: (86.00%) (8621/10000)
percent tensor([0.7205], device='cuda:0')
percent tensor([0.6806], device='cuda:0')
percent tensor([0.8366], device='cuda:0')
percent tensor([0.7631], device='cuda:0')
percent tensor([0.8311], device='cuda:0')
percent tensor([0.8502], device='cuda:0')
percent tensor([0.8668], device='cuda:0')
percent tensor([0.1486], device='cuda:0')
Epoch: 151 | Batch_idx: 0 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (3690/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.2226) |  Loss2: (0.0000) | Acc: (92.00%) (4870/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (6037/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (7233/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (8397/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (9587/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (10769/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (11967/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (13132/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (14313/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (92.00%) (15479/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (16644/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (17806/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (18972/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.2354) |  Loss2: (0.0000) | Acc: (92.00%) (20150/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (92.00%) (21333/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (92.00%) (22511/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (92.00%) (23693/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (24854/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (91.00%) (26017/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (92.00%) (27204/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (28387/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.2334) |  Loss2: (0.0000) | Acc: (92.00%) (29578/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (30776/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.2317) |  Loss2: (0.0000) | Acc: (92.00%) (31952/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (92.00%) (33127/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (34308/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (35484/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (36658/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (37835/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (38997/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (92.00%) (40191/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (41372/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (42562/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (92.00%) (43731/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (92.00%) (44905/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (46034/50000)
# TEST : Loss: (0.4553) | Acc: (86.00%) (8639/10000)
percent tensor([0.7205], device='cuda:0')
percent tensor([0.6805], device='cuda:0')
percent tensor([0.8365], device='cuda:0')
percent tensor([0.7630], device='cuda:0')
percent tensor([0.8310], device='cuda:0')
percent tensor([0.8501], device='cuda:0')
percent tensor([0.8668], device='cuda:0')
percent tensor([0.1487], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (2479/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (3676/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (4859/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (6046/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (7225/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (8403/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.2182) |  Loss2: (0.0000) | Acc: (92.00%) (9582/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (10758/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (11938/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (13126/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.2180) |  Loss2: (0.0000) | Acc: (92.00%) (14305/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.2189) |  Loss2: (0.0000) | Acc: (92.00%) (15484/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.2212) |  Loss2: (0.0000) | Acc: (92.00%) (16647/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (17826/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (18997/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (20174/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (21337/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (22507/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (23685/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (24875/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.2268) |  Loss2: (0.0000) | Acc: (92.00%) (26056/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (27218/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (28397/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (29585/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (30763/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (31944/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (33132/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (34306/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (35483/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (36644/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.2288) |  Loss2: (0.0000) | Acc: (91.00%) (37794/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (38988/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (91.00%) (40147/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (41304/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (91.00%) (42482/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (43655/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (44839/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (91.00%) (45987/50000)
# TEST : Loss: (0.4613) | Acc: (86.00%) (8605/10000)
percent tensor([0.7204], device='cuda:0')
percent tensor([0.6804], device='cuda:0')
percent tensor([0.8364], device='cuda:0')
percent tensor([0.7630], device='cuda:0')
percent tensor([0.8309], device='cuda:0')
percent tensor([0.8500], device='cuda:0')
percent tensor([0.8667], device='cuda:0')
percent tensor([0.1488], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (91.00%) (2468/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.2165) |  Loss2: (0.0000) | Acc: (92.00%) (3665/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (4863/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (6033/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (7195/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (8383/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (9569/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (10753/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (11933/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (13119/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (14322/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (15505/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (16688/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (17858/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (19036/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (20225/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (21394/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (22575/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (23768/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (24940/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.2178) |  Loss2: (0.0000) | Acc: (92.00%) (26132/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.2175) |  Loss2: (0.0000) | Acc: (92.00%) (27316/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.2161) |  Loss2: (0.0000) | Acc: (92.00%) (28517/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (29706/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.2159) |  Loss2: (0.0000) | Acc: (92.00%) (30891/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (32080/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (33249/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (34423/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (35605/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (36787/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (37967/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (39142/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (40330/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.2191) |  Loss2: (0.0000) | Acc: (92.00%) (41500/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (42675/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (43837/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (45005/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.2206) |  Loss2: (0.0000) | Acc: (92.00%) (46151/50000)
# TEST : Loss: (0.4449) | Acc: (86.00%) (8626/10000)
percent tensor([0.7203], device='cuda:0')
percent tensor([0.6804], device='cuda:0')
percent tensor([0.8363], device='cuda:0')
percent tensor([0.7629], device='cuda:0')
percent tensor([0.8308], device='cuda:0')
percent tensor([0.8499], device='cuda:0')
percent tensor([0.8666], device='cuda:0')
percent tensor([0.1489], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (2510/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (3695/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.2042) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (6053/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (7233/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (8420/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (9610/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (10790/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (11964/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (13144/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (14333/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (15521/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (16706/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.2128) |  Loss2: (0.0000) | Acc: (92.00%) (17885/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.2127) |  Loss2: (0.0000) | Acc: (92.00%) (19066/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (20245/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (21432/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (22617/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (23812/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (25010/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (26199/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (27391/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (28551/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (29747/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (30934/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (32127/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (33297/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (34477/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.2158) |  Loss2: (0.0000) | Acc: (92.00%) (35645/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.2162) |  Loss2: (0.0000) | Acc: (92.00%) (36817/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (37997/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (39173/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.2176) |  Loss2: (0.0000) | Acc: (92.00%) (40349/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (41532/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.2186) |  Loss2: (0.0000) | Acc: (92.00%) (42703/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.2196) |  Loss2: (0.0000) | Acc: (92.00%) (43860/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (45046/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (46185/50000)
# TEST : Loss: (0.4288) | Acc: (86.00%) (8678/10000)
percent tensor([0.7202], device='cuda:0')
percent tensor([0.6803], device='cuda:0')
percent tensor([0.8363], device='cuda:0')
percent tensor([0.7628], device='cuda:0')
percent tensor([0.8307], device='cuda:0')
percent tensor([0.8498], device='cuda:0')
percent tensor([0.8665], device='cuda:0')
percent tensor([0.1490], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.5789) |  Loss2: (0.3552) | Acc: (92.00%) (118/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.5579) |  Loss2: (0.3552) | Acc: (93.00%) (1314/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.5780) |  Loss2: (0.3552) | Acc: (92.00%) (2491/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.5830) |  Loss2: (0.3551) | Acc: (92.00%) (3654/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.5900) |  Loss2: (0.3551) | Acc: (91.00%) (4827/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.6006) |  Loss2: (0.3551) | Acc: (91.00%) (5971/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.5996) |  Loss2: (0.3551) | Acc: (91.00%) (7149/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.5998) |  Loss2: (0.3550) | Acc: (91.00%) (8325/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.5997) |  Loss2: (0.3550) | Acc: (91.00%) (9499/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.5988) |  Loss2: (0.3550) | Acc: (91.00%) (10673/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.6002) |  Loss2: (0.3549) | Acc: (91.00%) (11850/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.5969) |  Loss2: (0.3549) | Acc: (91.00%) (13032/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.5979) |  Loss2: (0.3548) | Acc: (91.00%) (14196/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.5989) |  Loss2: (0.3547) | Acc: (91.00%) (15368/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.6003) |  Loss2: (0.3546) | Acc: (91.00%) (16522/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.5978) |  Loss2: (0.3545) | Acc: (91.00%) (17708/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.5969) |  Loss2: (0.3545) | Acc: (91.00%) (18891/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.5958) |  Loss2: (0.3544) | Acc: (91.00%) (20063/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.5951) |  Loss2: (0.3543) | Acc: (91.00%) (21252/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.5957) |  Loss2: (0.3542) | Acc: (91.00%) (22423/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.5950) |  Loss2: (0.3541) | Acc: (91.00%) (23596/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.5947) |  Loss2: (0.3541) | Acc: (91.00%) (24767/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.5941) |  Loss2: (0.3540) | Acc: (91.00%) (25951/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.5933) |  Loss2: (0.3539) | Acc: (91.00%) (27133/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.5925) |  Loss2: (0.3539) | Acc: (91.00%) (28317/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.5916) |  Loss2: (0.3538) | Acc: (91.00%) (29497/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.5902) |  Loss2: (0.3537) | Acc: (91.00%) (30693/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.5895) |  Loss2: (0.3537) | Acc: (91.00%) (31886/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.5887) |  Loss2: (0.3536) | Acc: (91.00%) (33075/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.5879) |  Loss2: (0.3536) | Acc: (91.00%) (34263/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.5883) |  Loss2: (0.3535) | Acc: (91.00%) (35432/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.5869) |  Loss2: (0.3534) | Acc: (92.00%) (36625/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.5869) |  Loss2: (0.3534) | Acc: (91.00%) (37796/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.5865) |  Loss2: (0.3533) | Acc: (91.00%) (38973/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.5859) |  Loss2: (0.3532) | Acc: (92.00%) (40159/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.5861) |  Loss2: (0.3532) | Acc: (91.00%) (41330/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.5861) |  Loss2: (0.3531) | Acc: (91.00%) (42504/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.5857) |  Loss2: (0.3531) | Acc: (91.00%) (43682/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.5847) |  Loss2: (0.3530) | Acc: (92.00%) (44876/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.5846) |  Loss2: (0.3529) | Acc: (92.00%) (46012/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_155.pth.tar'
# TEST : Loss: (0.4209) | Acc: (86.00%) (8695/10000)
percent tensor([0.7282], device='cuda:0')
percent tensor([0.6904], device='cuda:0')
percent tensor([0.8393], device='cuda:0')
percent tensor([0.7663], device='cuda:0')
percent tensor([0.8394], device='cuda:0')
percent tensor([0.8454], device='cuda:0')
percent tensor([0.8683], device='cuda:0')
percent tensor([0.1477], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.5829) |  Loss2: (0.3506) | Acc: (91.00%) (117/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.5577) |  Loss2: (0.3505) | Acc: (92.00%) (1307/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.5612) |  Loss2: (0.3505) | Acc: (92.00%) (2493/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.5601) |  Loss2: (0.3504) | Acc: (93.00%) (3695/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.5601) |  Loss2: (0.3503) | Acc: (92.00%) (4868/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.5571) |  Loss2: (0.3502) | Acc: (92.00%) (6064/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.5566) |  Loss2: (0.3501) | Acc: (92.00%) (7251/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.5617) |  Loss2: (0.3500) | Acc: (92.00%) (8427/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.5614) |  Loss2: (0.3500) | Acc: (92.00%) (9615/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.5652) |  Loss2: (0.3499) | Acc: (92.00%) (10780/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.5673) |  Loss2: (0.3498) | Acc: (92.00%) (11956/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.5663) |  Loss2: (0.3498) | Acc: (92.00%) (13145/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.5672) |  Loss2: (0.3497) | Acc: (92.00%) (14316/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.5656) |  Loss2: (0.3497) | Acc: (92.00%) (15511/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.5655) |  Loss2: (0.3497) | Acc: (92.00%) (16702/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.5657) |  Loss2: (0.3497) | Acc: (92.00%) (17882/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.5667) |  Loss2: (0.3496) | Acc: (92.00%) (19064/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.5680) |  Loss2: (0.3496) | Acc: (92.00%) (20248/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.5677) |  Loss2: (0.3496) | Acc: (92.00%) (21434/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.5691) |  Loss2: (0.3495) | Acc: (92.00%) (22607/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.5688) |  Loss2: (0.3495) | Acc: (92.00%) (23791/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.5683) |  Loss2: (0.3495) | Acc: (92.00%) (24987/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.5687) |  Loss2: (0.3494) | Acc: (92.00%) (26166/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.5686) |  Loss2: (0.3494) | Acc: (92.00%) (27341/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.5678) |  Loss2: (0.3494) | Acc: (92.00%) (28539/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.5685) |  Loss2: (0.3494) | Acc: (92.00%) (29724/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.5677) |  Loss2: (0.3493) | Acc: (92.00%) (30916/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.5674) |  Loss2: (0.3493) | Acc: (92.00%) (32105/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.5675) |  Loss2: (0.3493) | Acc: (92.00%) (33294/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.5667) |  Loss2: (0.3493) | Acc: (92.00%) (34481/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.5675) |  Loss2: (0.3493) | Acc: (92.00%) (35654/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.5677) |  Loss2: (0.3493) | Acc: (92.00%) (36831/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.5675) |  Loss2: (0.3493) | Acc: (92.00%) (38023/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.5667) |  Loss2: (0.3493) | Acc: (92.00%) (39205/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.5673) |  Loss2: (0.3493) | Acc: (92.00%) (40382/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.5670) |  Loss2: (0.3492) | Acc: (92.00%) (41579/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.5675) |  Loss2: (0.3492) | Acc: (92.00%) (42758/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.5680) |  Loss2: (0.3492) | Acc: (92.00%) (43937/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.5684) |  Loss2: (0.3492) | Acc: (92.00%) (45111/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.5685) |  Loss2: (0.3492) | Acc: (92.00%) (46250/50000)
# TEST : Loss: (0.4197) | Acc: (87.00%) (8702/10000)
percent tensor([0.7320], device='cuda:0')
percent tensor([0.6932], device='cuda:0')
percent tensor([0.8398], device='cuda:0')
percent tensor([0.7704], device='cuda:0')
percent tensor([0.8420], device='cuda:0')
percent tensor([0.8459], device='cuda:0')
percent tensor([0.8698], device='cuda:0')
percent tensor([0.1461], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.5911) |  Loss2: (0.3483) | Acc: (89.00%) (114/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.5483) |  Loss2: (0.3482) | Acc: (92.00%) (1307/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.5397) |  Loss2: (0.3482) | Acc: (93.00%) (2509/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.5404) |  Loss2: (0.3482) | Acc: (93.00%) (3703/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.5442) |  Loss2: (0.3482) | Acc: (93.00%) (4895/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.5497) |  Loss2: (0.3482) | Acc: (93.00%) (6080/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.5541) |  Loss2: (0.3482) | Acc: (93.00%) (7263/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.5552) |  Loss2: (0.3482) | Acc: (92.00%) (8444/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.5545) |  Loss2: (0.3482) | Acc: (92.00%) (9638/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.5569) |  Loss2: (0.3482) | Acc: (92.00%) (10819/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.5584) |  Loss2: (0.3481) | Acc: (92.00%) (11994/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.5603) |  Loss2: (0.3481) | Acc: (92.00%) (13180/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.5597) |  Loss2: (0.3481) | Acc: (92.00%) (14375/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.5594) |  Loss2: (0.3480) | Acc: (92.00%) (15570/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.5596) |  Loss2: (0.3480) | Acc: (92.00%) (16749/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.5601) |  Loss2: (0.3480) | Acc: (92.00%) (17924/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.5606) |  Loss2: (0.3480) | Acc: (92.00%) (19109/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.5589) |  Loss2: (0.3479) | Acc: (92.00%) (20301/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.5581) |  Loss2: (0.3479) | Acc: (92.00%) (21494/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.5587) |  Loss2: (0.3479) | Acc: (92.00%) (22675/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.5579) |  Loss2: (0.3479) | Acc: (92.00%) (23880/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.5588) |  Loss2: (0.3478) | Acc: (92.00%) (25053/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.5593) |  Loss2: (0.3478) | Acc: (92.00%) (26230/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.5589) |  Loss2: (0.3478) | Acc: (92.00%) (27425/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.5589) |  Loss2: (0.3478) | Acc: (92.00%) (28617/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.5582) |  Loss2: (0.3477) | Acc: (92.00%) (29817/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.5579) |  Loss2: (0.3477) | Acc: (92.00%) (31011/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.5578) |  Loss2: (0.3477) | Acc: (92.00%) (32199/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.5571) |  Loss2: (0.3477) | Acc: (92.00%) (33399/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.5572) |  Loss2: (0.3477) | Acc: (92.00%) (34585/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.5568) |  Loss2: (0.3476) | Acc: (92.00%) (35784/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.5573) |  Loss2: (0.3476) | Acc: (92.00%) (36963/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.5566) |  Loss2: (0.3476) | Acc: (92.00%) (38163/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.5574) |  Loss2: (0.3476) | Acc: (92.00%) (39342/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.5574) |  Loss2: (0.3475) | Acc: (92.00%) (40535/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.5570) |  Loss2: (0.3475) | Acc: (92.00%) (41732/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.5572) |  Loss2: (0.3475) | Acc: (92.00%) (42918/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.5574) |  Loss2: (0.3475) | Acc: (92.00%) (44103/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.5583) |  Loss2: (0.3475) | Acc: (92.00%) (45274/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.5585) |  Loss2: (0.3474) | Acc: (92.00%) (46416/50000)
# TEST : Loss: (0.4117) | Acc: (87.00%) (8739/10000)
percent tensor([0.7352], device='cuda:0')
percent tensor([0.6946], device='cuda:0')
percent tensor([0.8410], device='cuda:0')
percent tensor([0.7728], device='cuda:0')
percent tensor([0.8432], device='cuda:0')
percent tensor([0.8466], device='cuda:0')
percent tensor([0.8710], device='cuda:0')
percent tensor([0.1441], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.5048) |  Loss2: (0.3465) | Acc: (95.00%) (122/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.5523) |  Loss2: (0.3465) | Acc: (93.00%) (1314/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.5471) |  Loss2: (0.3465) | Acc: (93.00%) (2509/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.5461) |  Loss2: (0.3465) | Acc: (93.00%) (3702/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.5564) |  Loss2: (0.3465) | Acc: (92.00%) (4878/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.5528) |  Loss2: (0.3464) | Acc: (93.00%) (6078/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.5581) |  Loss2: (0.3464) | Acc: (92.00%) (7251/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.5570) |  Loss2: (0.3464) | Acc: (92.00%) (8437/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.5581) |  Loss2: (0.3464) | Acc: (92.00%) (9623/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.5568) |  Loss2: (0.3464) | Acc: (92.00%) (10823/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.5564) |  Loss2: (0.3464) | Acc: (92.00%) (12007/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.5536) |  Loss2: (0.3464) | Acc: (92.00%) (13211/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.5541) |  Loss2: (0.3463) | Acc: (92.00%) (14394/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.5547) |  Loss2: (0.3463) | Acc: (92.00%) (15577/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.5548) |  Loss2: (0.3463) | Acc: (92.00%) (16760/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.5564) |  Loss2: (0.3463) | Acc: (92.00%) (17943/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.5562) |  Loss2: (0.3463) | Acc: (92.00%) (19136/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.5549) |  Loss2: (0.3463) | Acc: (92.00%) (20331/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.5548) |  Loss2: (0.3463) | Acc: (92.00%) (21513/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.5567) |  Loss2: (0.3463) | Acc: (92.00%) (22691/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.5570) |  Loss2: (0.3463) | Acc: (92.00%) (23872/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.5574) |  Loss2: (0.3463) | Acc: (92.00%) (25060/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.5570) |  Loss2: (0.3463) | Acc: (92.00%) (26249/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.5578) |  Loss2: (0.3463) | Acc: (92.00%) (27434/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.5578) |  Loss2: (0.3463) | Acc: (92.00%) (28621/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.5573) |  Loss2: (0.3463) | Acc: (92.00%) (29812/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.5570) |  Loss2: (0.3463) | Acc: (92.00%) (30996/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.5565) |  Loss2: (0.3463) | Acc: (92.00%) (32190/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.5568) |  Loss2: (0.3463) | Acc: (92.00%) (33378/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.5566) |  Loss2: (0.3463) | Acc: (92.00%) (34571/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.5565) |  Loss2: (0.3463) | Acc: (92.00%) (35760/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.5561) |  Loss2: (0.3462) | Acc: (92.00%) (36948/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.5559) |  Loss2: (0.3462) | Acc: (92.00%) (38141/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.5560) |  Loss2: (0.3462) | Acc: (92.00%) (39324/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.5560) |  Loss2: (0.3462) | Acc: (92.00%) (40513/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.5557) |  Loss2: (0.3462) | Acc: (92.00%) (41701/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.5560) |  Loss2: (0.3461) | Acc: (92.00%) (42881/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.5567) |  Loss2: (0.3461) | Acc: (92.00%) (44052/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.5567) |  Loss2: (0.3461) | Acc: (92.00%) (45238/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.5568) |  Loss2: (0.3461) | Acc: (92.00%) (46377/50000)
# TEST : Loss: (0.4102) | Acc: (87.00%) (8749/10000)
percent tensor([0.7358], device='cuda:0')
percent tensor([0.6952], device='cuda:0')
percent tensor([0.8428], device='cuda:0')
percent tensor([0.7736], device='cuda:0')
percent tensor([0.8441], device='cuda:0')
percent tensor([0.8484], device='cuda:0')
percent tensor([0.8717], device='cuda:0')
percent tensor([0.1421], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.5688) |  Loss2: (0.3454) | Acc: (92.00%) (118/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.5629) |  Loss2: (0.3454) | Acc: (92.00%) (1307/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.5564) |  Loss2: (0.3454) | Acc: (92.00%) (2498/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.5560) |  Loss2: (0.3454) | Acc: (93.00%) (3692/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.5569) |  Loss2: (0.3454) | Acc: (93.00%) (4882/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.5549) |  Loss2: (0.3454) | Acc: (92.00%) (6070/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.5563) |  Loss2: (0.3454) | Acc: (92.00%) (7261/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.5559) |  Loss2: (0.3454) | Acc: (92.00%) (8444/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.5530) |  Loss2: (0.3454) | Acc: (92.00%) (9642/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.5537) |  Loss2: (0.3454) | Acc: (92.00%) (10828/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.5523) |  Loss2: (0.3454) | Acc: (93.00%) (12031/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.5531) |  Loss2: (0.3453) | Acc: (93.00%) (13220/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.5536) |  Loss2: (0.3453) | Acc: (93.00%) (14409/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.5543) |  Loss2: (0.3453) | Acc: (93.00%) (15597/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.5563) |  Loss2: (0.3453) | Acc: (92.00%) (16772/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.5562) |  Loss2: (0.3453) | Acc: (92.00%) (17959/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.5554) |  Loss2: (0.3453) | Acc: (92.00%) (19145/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.5564) |  Loss2: (0.3453) | Acc: (92.00%) (20328/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.5556) |  Loss2: (0.3453) | Acc: (92.00%) (21528/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.5541) |  Loss2: (0.3453) | Acc: (92.00%) (22733/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.5532) |  Loss2: (0.3453) | Acc: (92.00%) (23925/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.5528) |  Loss2: (0.3453) | Acc: (93.00%) (25119/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.5532) |  Loss2: (0.3453) | Acc: (92.00%) (26300/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.5534) |  Loss2: (0.3453) | Acc: (92.00%) (27485/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.5531) |  Loss2: (0.3453) | Acc: (92.00%) (28676/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.5530) |  Loss2: (0.3453) | Acc: (92.00%) (29869/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.5525) |  Loss2: (0.3453) | Acc: (92.00%) (31064/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.5528) |  Loss2: (0.3453) | Acc: (93.00%) (32264/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.5538) |  Loss2: (0.3453) | Acc: (92.00%) (33446/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.5533) |  Loss2: (0.3453) | Acc: (92.00%) (34638/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.5548) |  Loss2: (0.3453) | Acc: (92.00%) (35807/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.5552) |  Loss2: (0.3453) | Acc: (92.00%) (36987/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.5551) |  Loss2: (0.3452) | Acc: (92.00%) (38178/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.5549) |  Loss2: (0.3452) | Acc: (92.00%) (39366/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.5552) |  Loss2: (0.3452) | Acc: (92.00%) (40559/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.5550) |  Loss2: (0.3452) | Acc: (92.00%) (41744/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.5557) |  Loss2: (0.3452) | Acc: (92.00%) (42929/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.5559) |  Loss2: (0.3452) | Acc: (92.00%) (44118/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.5556) |  Loss2: (0.3452) | Acc: (92.00%) (45305/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.5561) |  Loss2: (0.3452) | Acc: (92.00%) (46434/50000)
# TEST : Loss: (0.4129) | Acc: (87.00%) (8738/10000)
percent tensor([0.7363], device='cuda:0')
percent tensor([0.6957], device='cuda:0')
percent tensor([0.8423], device='cuda:0')
percent tensor([0.7747], device='cuda:0')
percent tensor([0.8447], device='cuda:0')
percent tensor([0.8496], device='cuda:0')
percent tensor([0.8723], device='cuda:0')
percent tensor([0.1402], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1, out_features=1, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.2043) |  Loss2: (0.0000) | Acc: (92.00%) (2487/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.2169) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (91.00%) (4820/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (91.00%) (5980/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (7142/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (91.00%) (8334/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.2301) |  Loss2: (0.0000) | Acc: (91.00%) (9511/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (91.00%) (10708/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (91.00%) (11881/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (13051/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (14220/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (91.00%) (15407/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (16586/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.2303) |  Loss2: (0.0000) | Acc: (91.00%) (17775/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (18942/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (91.00%) (20126/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (21301/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (22483/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (91.00%) (23651/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (24831/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (26014/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (27181/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (28356/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (29537/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.2308) |  Loss2: (0.0000) | Acc: (91.00%) (30715/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (31907/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (33094/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (34270/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.2283) |  Loss2: (0.0000) | Acc: (92.00%) (35449/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (36629/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (37820/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.2277) |  Loss2: (0.0000) | Acc: (92.00%) (39003/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (40175/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (41352/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (42519/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (43697/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.2282) |  Loss2: (0.0000) | Acc: (92.00%) (44881/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (46016/50000)
=> saving checkpoint 'drive/app/torch/save_models/checkpoint_160.pth.tar'
# TEST : Loss: (0.4345) | Acc: (86.00%) (8688/10000)
percent tensor([0.7361], device='cuda:0')
percent tensor([0.6956], device='cuda:0')
percent tensor([0.8422], device='cuda:0')
percent tensor([0.7749], device='cuda:0')
percent tensor([0.8445], device='cuda:0')
percent tensor([0.8495], device='cuda:0')
percent tensor([0.8722], device='cuda:0')
percent tensor([0.1403], device='cuda:0')
Epoch: 161 | Batch_idx: 0 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.2135) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (4863/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (6054/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.2137) |  Loss2: (0.0000) | Acc: (92.00%) (7234/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (8428/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (9617/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (10791/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (11964/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (13122/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (14299/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (15478/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (16662/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (17844/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (19016/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (20196/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.2213) |  Loss2: (0.0000) | Acc: (92.00%) (21402/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.2219) |  Loss2: (0.0000) | Acc: (92.00%) (22579/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (23779/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (24966/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (26144/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (27321/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (28511/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (29686/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (30873/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (32044/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (33232/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (34407/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (35584/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (36762/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.2198) |  Loss2: (0.0000) | Acc: (92.00%) (37953/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (39149/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (40330/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.2187) |  Loss2: (0.0000) | Acc: (92.00%) (41511/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (42698/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (43884/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.2188) |  Loss2: (0.0000) | Acc: (92.00%) (45068/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.2195) |  Loss2: (0.0000) | Acc: (92.00%) (46185/50000)
# TEST : Loss: (0.4423) | Acc: (86.00%) (8657/10000)
percent tensor([0.7360], device='cuda:0')
percent tensor([0.6955], device='cuda:0')
percent tensor([0.8421], device='cuda:0')
percent tensor([0.7748], device='cuda:0')
percent tensor([0.8444], device='cuda:0')
percent tensor([0.8494], device='cuda:0')
percent tensor([0.8721], device='cuda:0')
percent tensor([0.1403], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.2022) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (4876/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (6061/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (7255/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (8445/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (9625/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (10812/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (12013/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (13193/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (14373/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (15552/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (16742/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (17943/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (19134/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.2116) |  Loss2: (0.0000) | Acc: (92.00%) (20321/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (21510/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (22695/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (23879/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (25051/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (26239/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.2111) |  Loss2: (0.0000) | Acc: (92.00%) (27436/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (28629/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (29819/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.2118) |  Loss2: (0.0000) | Acc: (92.00%) (30991/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (32182/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (33371/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (34547/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (35719/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (36917/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (38098/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (39281/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (40445/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (41628/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (42826/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (43998/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (45187/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.2120) |  Loss2: (0.0000) | Acc: (92.00%) (46336/50000)
# TEST : Loss: (0.4329) | Acc: (86.00%) (8699/10000)
percent tensor([0.7360], device='cuda:0')
percent tensor([0.6954], device='cuda:0')
percent tensor([0.8420], device='cuda:0')
percent tensor([0.7747], device='cuda:0')
percent tensor([0.8444], device='cuda:0')
percent tensor([0.8493], device='cuda:0')
percent tensor([0.8720], device='cuda:0')
percent tensor([0.1404], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (2492/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (3681/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (4870/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (6048/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (7226/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (8414/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (9604/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (10807/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (12010/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.2041) |  Loss2: (0.0000) | Acc: (92.00%) (13191/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (14370/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (15562/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (16749/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (17940/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (19120/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.2098) |  Loss2: (0.0000) | Acc: (92.00%) (20283/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (21472/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (22662/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (23857/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (25030/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (26220/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (27392/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (28581/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (29761/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (30949/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.2105) |  Loss2: (0.0000) | Acc: (92.00%) (32133/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (33327/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (34516/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (35709/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (36883/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (38067/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (39256/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (40455/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (41638/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.2106) |  Loss2: (0.0000) | Acc: (92.00%) (42807/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (43990/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.2112) |  Loss2: (0.0000) | Acc: (92.00%) (45171/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (46323/50000)
# TEST : Loss: (0.4962) | Acc: (85.00%) (8546/10000)
percent tensor([0.7359], device='cuda:0')
percent tensor([0.6954], device='cuda:0')
percent tensor([0.8419], device='cuda:0')
percent tensor([0.7746], device='cuda:0')
percent tensor([0.8443], device='cuda:0')
percent tensor([0.8492], device='cuda:0')
percent tensor([0.8719], device='cuda:0')
percent tensor([0.1405], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (1331/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (2517/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (4921/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (7305/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (8487/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (9672/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (10862/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1958) |  Loss2: (0.0000) | Acc: (93.00%) (12046/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (13230/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (14411/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (15591/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (16774/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (17956/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (19152/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (92.00%) (20339/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (21526/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (22725/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (23905/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (25105/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (26293/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (27475/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (28666/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (29862/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (31058/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.2032) |  Loss2: (0.0000) | Acc: (92.00%) (32228/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.2030) |  Loss2: (0.0000) | Acc: (92.00%) (33421/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (92.00%) (34613/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (92.00%) (35822/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (37015/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (38201/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (39380/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (40570/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.2035) |  Loss2: (0.0000) | Acc: (92.00%) (41748/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.2040) |  Loss2: (0.0000) | Acc: (92.00%) (42927/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.2047) |  Loss2: (0.0000) | Acc: (92.00%) (44107/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (92.00%) (45305/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.2048) |  Loss2: (0.0000) | Acc: (92.00%) (46432/50000)
# TEST : Loss: (0.4645) | Acc: (86.00%) (8647/10000)
percent tensor([0.7358], device='cuda:0')
percent tensor([0.6953], device='cuda:0')
percent tensor([0.8418], device='cuda:0')
percent tensor([0.7746], device='cuda:0')
percent tensor([0.8442], device='cuda:0')
percent tensor([0.8491], device='cuda:0')
percent tensor([0.8719], device='cuda:0')
percent tensor([0.1406], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1, out_features=1, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
1 hours 23 mins 53 secs for training