Files already downloaded and verified
USE 1 GPUs!
Epoch: 0 | Batch_idx: 0 |  Loss: (2.3256) |  Loss2: (0.0000) | Acc: (6.00%) (8/128)
Epoch: 0 | Batch_idx: 10 |  Loss: (2.2984) |  Loss2: (0.0000) | Acc: (12.00%) (180/1408)
Epoch: 0 | Batch_idx: 20 |  Loss: (2.2937) |  Loss2: (0.0000) | Acc: (13.00%) (372/2688)
Epoch: 0 | Batch_idx: 30 |  Loss: (2.2873) |  Loss2: (0.0000) | Acc: (14.00%) (571/3968)
Epoch: 0 | Batch_idx: 40 |  Loss: (2.2805) |  Loss2: (0.0000) | Acc: (15.00%) (817/5248)
Epoch: 0 | Batch_idx: 50 |  Loss: (2.2703) |  Loss2: (0.0000) | Acc: (16.00%) (1064/6528)
Epoch: 0 | Batch_idx: 60 |  Loss: (2.2618) |  Loss2: (0.0000) | Acc: (17.00%) (1331/7808)
Epoch: 0 | Batch_idx: 70 |  Loss: (2.2523) |  Loss2: (0.0000) | Acc: (17.00%) (1605/9088)
Epoch: 0 | Batch_idx: 80 |  Loss: (2.2414) |  Loss2: (0.0000) | Acc: (18.00%) (1890/10368)
Epoch: 0 | Batch_idx: 90 |  Loss: (2.2325) |  Loss2: (0.0000) | Acc: (18.00%) (2179/11648)
Epoch: 0 | Batch_idx: 100 |  Loss: (2.2231) |  Loss2: (0.0000) | Acc: (19.00%) (2474/12928)
Epoch: 0 | Batch_idx: 110 |  Loss: (2.2132) |  Loss2: (0.0000) | Acc: (19.00%) (2808/14208)
Epoch: 0 | Batch_idx: 120 |  Loss: (2.2044) |  Loss2: (0.0000) | Acc: (20.00%) (3109/15488)
Epoch: 0 | Batch_idx: 130 |  Loss: (2.1955) |  Loss2: (0.0000) | Acc: (20.00%) (3422/16768)
Epoch: 0 | Batch_idx: 140 |  Loss: (2.1870) |  Loss2: (0.0000) | Acc: (20.00%) (3732/18048)
Epoch: 0 | Batch_idx: 150 |  Loss: (2.1783) |  Loss2: (0.0000) | Acc: (21.00%) (4062/19328)
Epoch: 0 | Batch_idx: 160 |  Loss: (2.1702) |  Loss2: (0.0000) | Acc: (21.00%) (4386/20608)
Epoch: 0 | Batch_idx: 170 |  Loss: (2.1607) |  Loss2: (0.0000) | Acc: (21.00%) (4765/21888)
Epoch: 0 | Batch_idx: 180 |  Loss: (2.1530) |  Loss2: (0.0000) | Acc: (21.00%) (5081/23168)
Epoch: 0 | Batch_idx: 190 |  Loss: (2.1459) |  Loss2: (0.0000) | Acc: (22.00%) (5412/24448)
Epoch: 0 | Batch_idx: 200 |  Loss: (2.1385) |  Loss2: (0.0000) | Acc: (22.00%) (5766/25728)
Epoch: 0 | Batch_idx: 210 |  Loss: (2.1317) |  Loss2: (0.0000) | Acc: (22.00%) (6130/27008)
Epoch: 0 | Batch_idx: 220 |  Loss: (2.1241) |  Loss2: (0.0000) | Acc: (22.00%) (6494/28288)
Epoch: 0 | Batch_idx: 230 |  Loss: (2.1167) |  Loss2: (0.0000) | Acc: (23.00%) (6861/29568)
Epoch: 0 | Batch_idx: 240 |  Loss: (2.1103) |  Loss2: (0.0000) | Acc: (23.00%) (7225/30848)
Epoch: 0 | Batch_idx: 250 |  Loss: (2.1022) |  Loss2: (0.0000) | Acc: (23.00%) (7611/32128)
Epoch: 0 | Batch_idx: 260 |  Loss: (2.0961) |  Loss2: (0.0000) | Acc: (23.00%) (8009/33408)
Epoch: 0 | Batch_idx: 270 |  Loss: (2.0892) |  Loss2: (0.0000) | Acc: (24.00%) (8406/34688)
Epoch: 0 | Batch_idx: 280 |  Loss: (2.0826) |  Loss2: (0.0000) | Acc: (24.00%) (8826/35968)
Epoch: 0 | Batch_idx: 290 |  Loss: (2.0762) |  Loss2: (0.0000) | Acc: (24.00%) (9230/37248)
Epoch: 0 | Batch_idx: 300 |  Loss: (2.0707) |  Loss2: (0.0000) | Acc: (24.00%) (9606/38528)
Epoch: 0 | Batch_idx: 310 |  Loss: (2.0648) |  Loss2: (0.0000) | Acc: (25.00%) (9994/39808)
Epoch: 0 | Batch_idx: 320 |  Loss: (2.0592) |  Loss2: (0.0000) | Acc: (25.00%) (10408/41088)
Epoch: 0 | Batch_idx: 330 |  Loss: (2.0536) |  Loss2: (0.0000) | Acc: (25.00%) (10798/42368)
Epoch: 0 | Batch_idx: 340 |  Loss: (2.0487) |  Loss2: (0.0000) | Acc: (25.00%) (11196/43648)
Epoch: 0 | Batch_idx: 350 |  Loss: (2.0437) |  Loss2: (0.0000) | Acc: (25.00%) (11619/44928)
Epoch: 0 | Batch_idx: 360 |  Loss: (2.0387) |  Loss2: (0.0000) | Acc: (25.00%) (12010/46208)
Epoch: 0 | Batch_idx: 370 |  Loss: (2.0335) |  Loss2: (0.0000) | Acc: (26.00%) (12426/47488)
Epoch: 0 | Batch_idx: 380 |  Loss: (2.0289) |  Loss2: (0.0000) | Acc: (26.00%) (12853/48768)
Epoch: 0 | Batch_idx: 390 |  Loss: (2.0237) |  Loss2: (0.0000) | Acc: (26.00%) (13263/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_000.pth.tar'
# TEST : Loss: (1.7932) | Acc: (34.00%) (3435/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(167.9001, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(766.9728, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(764.3212, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1534.8134, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(509.8291, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2169.7302, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4346.4194, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1445.2970, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6137.2163, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12287.4521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4087.8960, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17369.8574, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 1 | Batch_idx: 0 |  Loss: (1.8120) |  Loss2: (0.0000) | Acc: (30.00%) (39/128)
Epoch: 1 | Batch_idx: 10 |  Loss: (1.8499) |  Loss2: (0.0000) | Acc: (32.00%) (456/1408)
Epoch: 1 | Batch_idx: 20 |  Loss: (1.8372) |  Loss2: (0.0000) | Acc: (32.00%) (876/2688)
Epoch: 1 | Batch_idx: 30 |  Loss: (1.8329) |  Loss2: (0.0000) | Acc: (33.00%) (1314/3968)
Epoch: 1 | Batch_idx: 40 |  Loss: (1.8244) |  Loss2: (0.0000) | Acc: (33.00%) (1747/5248)
Epoch: 1 | Batch_idx: 50 |  Loss: (1.8217) |  Loss2: (0.0000) | Acc: (33.00%) (2195/6528)
Epoch: 1 | Batch_idx: 60 |  Loss: (1.8143) |  Loss2: (0.0000) | Acc: (33.00%) (2636/7808)
Epoch: 1 | Batch_idx: 70 |  Loss: (1.8129) |  Loss2: (0.0000) | Acc: (33.00%) (3053/9088)
Epoch: 1 | Batch_idx: 80 |  Loss: (1.8077) |  Loss2: (0.0000) | Acc: (33.00%) (3506/10368)
Epoch: 1 | Batch_idx: 90 |  Loss: (1.8021) |  Loss2: (0.0000) | Acc: (34.00%) (3961/11648)
Epoch: 1 | Batch_idx: 100 |  Loss: (1.7963) |  Loss2: (0.0000) | Acc: (34.00%) (4460/12928)
Epoch: 1 | Batch_idx: 110 |  Loss: (1.7933) |  Loss2: (0.0000) | Acc: (34.00%) (4901/14208)
Epoch: 1 | Batch_idx: 120 |  Loss: (1.7913) |  Loss2: (0.0000) | Acc: (34.00%) (5340/15488)
Epoch: 1 | Batch_idx: 130 |  Loss: (1.7885) |  Loss2: (0.0000) | Acc: (34.00%) (5816/16768)
Epoch: 1 | Batch_idx: 140 |  Loss: (1.7850) |  Loss2: (0.0000) | Acc: (34.00%) (6278/18048)
Epoch: 1 | Batch_idx: 150 |  Loss: (1.7800) |  Loss2: (0.0000) | Acc: (34.00%) (6756/19328)
Epoch: 1 | Batch_idx: 160 |  Loss: (1.7758) |  Loss2: (0.0000) | Acc: (35.00%) (7237/20608)
Epoch: 1 | Batch_idx: 170 |  Loss: (1.7725) |  Loss2: (0.0000) | Acc: (35.00%) (7689/21888)
Epoch: 1 | Batch_idx: 180 |  Loss: (1.7705) |  Loss2: (0.0000) | Acc: (35.00%) (8114/23168)
Epoch: 1 | Batch_idx: 190 |  Loss: (1.7677) |  Loss2: (0.0000) | Acc: (35.00%) (8576/24448)
Epoch: 1 | Batch_idx: 200 |  Loss: (1.7638) |  Loss2: (0.0000) | Acc: (35.00%) (9073/25728)
Epoch: 1 | Batch_idx: 210 |  Loss: (1.7602) |  Loss2: (0.0000) | Acc: (35.00%) (9546/27008)
Epoch: 1 | Batch_idx: 220 |  Loss: (1.7572) |  Loss2: (0.0000) | Acc: (35.00%) (10003/28288)
Epoch: 1 | Batch_idx: 230 |  Loss: (1.7538) |  Loss2: (0.0000) | Acc: (35.00%) (10486/29568)
Epoch: 1 | Batch_idx: 240 |  Loss: (1.7499) |  Loss2: (0.0000) | Acc: (35.00%) (10993/30848)
Epoch: 1 | Batch_idx: 250 |  Loss: (1.7466) |  Loss2: (0.0000) | Acc: (35.00%) (11485/32128)
Epoch: 1 | Batch_idx: 260 |  Loss: (1.7432) |  Loss2: (0.0000) | Acc: (35.00%) (11970/33408)
Epoch: 1 | Batch_idx: 270 |  Loss: (1.7403) |  Loss2: (0.0000) | Acc: (35.00%) (12469/34688)
Epoch: 1 | Batch_idx: 280 |  Loss: (1.7379) |  Loss2: (0.0000) | Acc: (35.00%) (12945/35968)
Epoch: 1 | Batch_idx: 290 |  Loss: (1.7348) |  Loss2: (0.0000) | Acc: (36.00%) (13457/37248)
Epoch: 1 | Batch_idx: 300 |  Loss: (1.7315) |  Loss2: (0.0000) | Acc: (36.00%) (13952/38528)
Epoch: 1 | Batch_idx: 310 |  Loss: (1.7277) |  Loss2: (0.0000) | Acc: (36.00%) (14496/39808)
Epoch: 1 | Batch_idx: 320 |  Loss: (1.7255) |  Loss2: (0.0000) | Acc: (36.00%) (15010/41088)
Epoch: 1 | Batch_idx: 330 |  Loss: (1.7218) |  Loss2: (0.0000) | Acc: (36.00%) (15523/42368)
Epoch: 1 | Batch_idx: 340 |  Loss: (1.7194) |  Loss2: (0.0000) | Acc: (36.00%) (16008/43648)
Epoch: 1 | Batch_idx: 350 |  Loss: (1.7168) |  Loss2: (0.0000) | Acc: (36.00%) (16511/44928)
Epoch: 1 | Batch_idx: 360 |  Loss: (1.7138) |  Loss2: (0.0000) | Acc: (36.00%) (17059/46208)
Epoch: 1 | Batch_idx: 370 |  Loss: (1.7109) |  Loss2: (0.0000) | Acc: (37.00%) (17580/47488)
Epoch: 1 | Batch_idx: 380 |  Loss: (1.7076) |  Loss2: (0.0000) | Acc: (37.00%) (18107/48768)
Epoch: 1 | Batch_idx: 390 |  Loss: (1.7054) |  Loss2: (0.0000) | Acc: (37.00%) (18586/50000)
# TEST : Loss: (1.7550) | Acc: (34.00%) (3490/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 2 | Batch_idx: 0 |  Loss: (1.5348) |  Loss2: (0.0000) | Acc: (42.00%) (54/128)
Epoch: 2 | Batch_idx: 10 |  Loss: (1.5794) |  Loss2: (0.0000) | Acc: (41.00%) (579/1408)
Epoch: 2 | Batch_idx: 20 |  Loss: (1.5737) |  Loss2: (0.0000) | Acc: (42.00%) (1131/2688)
Epoch: 2 | Batch_idx: 30 |  Loss: (1.5767) |  Loss2: (0.0000) | Acc: (42.00%) (1677/3968)
Epoch: 2 | Batch_idx: 40 |  Loss: (1.5837) |  Loss2: (0.0000) | Acc: (41.00%) (2192/5248)
Epoch: 2 | Batch_idx: 50 |  Loss: (1.5896) |  Loss2: (0.0000) | Acc: (41.00%) (2692/6528)
Epoch: 2 | Batch_idx: 60 |  Loss: (1.5876) |  Loss2: (0.0000) | Acc: (41.00%) (3236/7808)
Epoch: 2 | Batch_idx: 70 |  Loss: (1.5830) |  Loss2: (0.0000) | Acc: (41.00%) (3775/9088)
Epoch: 2 | Batch_idx: 80 |  Loss: (1.5836) |  Loss2: (0.0000) | Acc: (41.00%) (4289/10368)
Epoch: 2 | Batch_idx: 90 |  Loss: (1.5838) |  Loss2: (0.0000) | Acc: (41.00%) (4806/11648)
Epoch: 2 | Batch_idx: 100 |  Loss: (1.5829) |  Loss2: (0.0000) | Acc: (41.00%) (5323/12928)
Epoch: 2 | Batch_idx: 110 |  Loss: (1.5821) |  Loss2: (0.0000) | Acc: (41.00%) (5861/14208)
Epoch: 2 | Batch_idx: 120 |  Loss: (1.5793) |  Loss2: (0.0000) | Acc: (41.00%) (6407/15488)
Epoch: 2 | Batch_idx: 130 |  Loss: (1.5788) |  Loss2: (0.0000) | Acc: (41.00%) (6911/16768)
Epoch: 2 | Batch_idx: 140 |  Loss: (1.5771) |  Loss2: (0.0000) | Acc: (41.00%) (7440/18048)
Epoch: 2 | Batch_idx: 150 |  Loss: (1.5758) |  Loss2: (0.0000) | Acc: (41.00%) (7970/19328)
Epoch: 2 | Batch_idx: 160 |  Loss: (1.5764) |  Loss2: (0.0000) | Acc: (41.00%) (8494/20608)
Epoch: 2 | Batch_idx: 170 |  Loss: (1.5746) |  Loss2: (0.0000) | Acc: (41.00%) (9048/21888)
Epoch: 2 | Batch_idx: 180 |  Loss: (1.5720) |  Loss2: (0.0000) | Acc: (41.00%) (9592/23168)
Epoch: 2 | Batch_idx: 190 |  Loss: (1.5700) |  Loss2: (0.0000) | Acc: (41.00%) (10162/24448)
Epoch: 2 | Batch_idx: 200 |  Loss: (1.5670) |  Loss2: (0.0000) | Acc: (41.00%) (10742/25728)
Epoch: 2 | Batch_idx: 210 |  Loss: (1.5650) |  Loss2: (0.0000) | Acc: (41.00%) (11299/27008)
Epoch: 2 | Batch_idx: 220 |  Loss: (1.5636) |  Loss2: (0.0000) | Acc: (41.00%) (11849/28288)
Epoch: 2 | Batch_idx: 230 |  Loss: (1.5614) |  Loss2: (0.0000) | Acc: (41.00%) (12413/29568)
Epoch: 2 | Batch_idx: 240 |  Loss: (1.5584) |  Loss2: (0.0000) | Acc: (42.00%) (13008/30848)
Epoch: 2 | Batch_idx: 250 |  Loss: (1.5568) |  Loss2: (0.0000) | Acc: (42.00%) (13561/32128)
Epoch: 2 | Batch_idx: 260 |  Loss: (1.5555) |  Loss2: (0.0000) | Acc: (42.00%) (14120/33408)
Epoch: 2 | Batch_idx: 270 |  Loss: (1.5525) |  Loss2: (0.0000) | Acc: (42.00%) (14723/34688)
Epoch: 2 | Batch_idx: 280 |  Loss: (1.5509) |  Loss2: (0.0000) | Acc: (42.00%) (15280/35968)
Epoch: 2 | Batch_idx: 290 |  Loss: (1.5476) |  Loss2: (0.0000) | Acc: (42.00%) (15890/37248)
Epoch: 2 | Batch_idx: 300 |  Loss: (1.5469) |  Loss2: (0.0000) | Acc: (42.00%) (16439/38528)
Epoch: 2 | Batch_idx: 310 |  Loss: (1.5464) |  Loss2: (0.0000) | Acc: (42.00%) (17008/39808)
Epoch: 2 | Batch_idx: 320 |  Loss: (1.5447) |  Loss2: (0.0000) | Acc: (42.00%) (17595/41088)
Epoch: 2 | Batch_idx: 330 |  Loss: (1.5428) |  Loss2: (0.0000) | Acc: (42.00%) (18175/42368)
Epoch: 2 | Batch_idx: 340 |  Loss: (1.5406) |  Loss2: (0.0000) | Acc: (43.00%) (18770/43648)
Epoch: 2 | Batch_idx: 350 |  Loss: (1.5395) |  Loss2: (0.0000) | Acc: (43.00%) (19330/44928)
Epoch: 2 | Batch_idx: 360 |  Loss: (1.5375) |  Loss2: (0.0000) | Acc: (43.00%) (19912/46208)
Epoch: 2 | Batch_idx: 370 |  Loss: (1.5359) |  Loss2: (0.0000) | Acc: (43.00%) (20499/47488)
Epoch: 2 | Batch_idx: 380 |  Loss: (1.5345) |  Loss2: (0.0000) | Acc: (43.00%) (21034/48768)
Epoch: 2 | Batch_idx: 390 |  Loss: (1.5327) |  Loss2: (0.0000) | Acc: (43.00%) (21600/50000)
# TEST : Loss: (1.5264) | Acc: (43.00%) (4384/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 3 | Batch_idx: 0 |  Loss: (1.5118) |  Loss2: (0.0000) | Acc: (47.00%) (61/128)
Epoch: 3 | Batch_idx: 10 |  Loss: (1.4761) |  Loss2: (0.0000) | Acc: (44.00%) (632/1408)
Epoch: 3 | Batch_idx: 20 |  Loss: (1.4587) |  Loss2: (0.0000) | Acc: (46.00%) (1241/2688)
Epoch: 3 | Batch_idx: 30 |  Loss: (1.4593) |  Loss2: (0.0000) | Acc: (46.00%) (1826/3968)
Epoch: 3 | Batch_idx: 40 |  Loss: (1.4553) |  Loss2: (0.0000) | Acc: (46.00%) (2432/5248)
Epoch: 3 | Batch_idx: 50 |  Loss: (1.4515) |  Loss2: (0.0000) | Acc: (46.00%) (3030/6528)
Epoch: 3 | Batch_idx: 60 |  Loss: (1.4471) |  Loss2: (0.0000) | Acc: (46.00%) (3643/7808)
Epoch: 3 | Batch_idx: 70 |  Loss: (1.4488) |  Loss2: (0.0000) | Acc: (46.00%) (4262/9088)
Epoch: 3 | Batch_idx: 80 |  Loss: (1.4455) |  Loss2: (0.0000) | Acc: (47.00%) (4893/10368)
Epoch: 3 | Batch_idx: 90 |  Loss: (1.4465) |  Loss2: (0.0000) | Acc: (47.00%) (5477/11648)
Epoch: 3 | Batch_idx: 100 |  Loss: (1.4457) |  Loss2: (0.0000) | Acc: (46.00%) (6047/12928)
Epoch: 3 | Batch_idx: 110 |  Loss: (1.4446) |  Loss2: (0.0000) | Acc: (46.00%) (6660/14208)
Epoch: 3 | Batch_idx: 120 |  Loss: (1.4416) |  Loss2: (0.0000) | Acc: (47.00%) (7287/15488)
Epoch: 3 | Batch_idx: 130 |  Loss: (1.4396) |  Loss2: (0.0000) | Acc: (47.00%) (7888/16768)
Epoch: 3 | Batch_idx: 140 |  Loss: (1.4347) |  Loss2: (0.0000) | Acc: (47.00%) (8531/18048)
Epoch: 3 | Batch_idx: 150 |  Loss: (1.4358) |  Loss2: (0.0000) | Acc: (47.00%) (9117/19328)
Epoch: 3 | Batch_idx: 160 |  Loss: (1.4322) |  Loss2: (0.0000) | Acc: (47.00%) (9751/20608)
Epoch: 3 | Batch_idx: 170 |  Loss: (1.4307) |  Loss2: (0.0000) | Acc: (47.00%) (10386/21888)
Epoch: 3 | Batch_idx: 180 |  Loss: (1.4271) |  Loss2: (0.0000) | Acc: (47.00%) (11043/23168)
Epoch: 3 | Batch_idx: 190 |  Loss: (1.4254) |  Loss2: (0.0000) | Acc: (47.00%) (11683/24448)
Epoch: 3 | Batch_idx: 200 |  Loss: (1.4214) |  Loss2: (0.0000) | Acc: (47.00%) (12321/25728)
Epoch: 3 | Batch_idx: 210 |  Loss: (1.4184) |  Loss2: (0.0000) | Acc: (48.00%) (12992/27008)
Epoch: 3 | Batch_idx: 220 |  Loss: (1.4169) |  Loss2: (0.0000) | Acc: (48.00%) (13660/28288)
Epoch: 3 | Batch_idx: 230 |  Loss: (1.4152) |  Loss2: (0.0000) | Acc: (48.00%) (14279/29568)
Epoch: 3 | Batch_idx: 240 |  Loss: (1.4135) |  Loss2: (0.0000) | Acc: (48.00%) (14919/30848)
Epoch: 3 | Batch_idx: 250 |  Loss: (1.4128) |  Loss2: (0.0000) | Acc: (48.00%) (15528/32128)
Epoch: 3 | Batch_idx: 260 |  Loss: (1.4108) |  Loss2: (0.0000) | Acc: (48.00%) (16172/33408)
Epoch: 3 | Batch_idx: 270 |  Loss: (1.4100) |  Loss2: (0.0000) | Acc: (48.00%) (16818/34688)
Epoch: 3 | Batch_idx: 280 |  Loss: (1.4104) |  Loss2: (0.0000) | Acc: (48.00%) (17429/35968)
Epoch: 3 | Batch_idx: 290 |  Loss: (1.4085) |  Loss2: (0.0000) | Acc: (48.00%) (18081/37248)
Epoch: 3 | Batch_idx: 300 |  Loss: (1.4067) |  Loss2: (0.0000) | Acc: (48.00%) (18718/38528)
Epoch: 3 | Batch_idx: 310 |  Loss: (1.4059) |  Loss2: (0.0000) | Acc: (48.00%) (19384/39808)
Epoch: 3 | Batch_idx: 320 |  Loss: (1.4034) |  Loss2: (0.0000) | Acc: (48.00%) (20046/41088)
Epoch: 3 | Batch_idx: 330 |  Loss: (1.4021) |  Loss2: (0.0000) | Acc: (48.00%) (20695/42368)
Epoch: 3 | Batch_idx: 340 |  Loss: (1.4004) |  Loss2: (0.0000) | Acc: (48.00%) (21354/43648)
Epoch: 3 | Batch_idx: 350 |  Loss: (1.3989) |  Loss2: (0.0000) | Acc: (48.00%) (22013/44928)
Epoch: 3 | Batch_idx: 360 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (49.00%) (22692/46208)
Epoch: 3 | Batch_idx: 370 |  Loss: (1.3957) |  Loss2: (0.0000) | Acc: (49.00%) (23365/47488)
Epoch: 3 | Batch_idx: 380 |  Loss: (1.3939) |  Loss2: (0.0000) | Acc: (49.00%) (24058/48768)
Epoch: 3 | Batch_idx: 390 |  Loss: (1.3926) |  Loss2: (0.0000) | Acc: (49.00%) (24681/50000)
# TEST : Loss: (1.3975) | Acc: (49.00%) (4940/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
Epoch: 4 | Batch_idx: 0 |  Loss: (1.2493) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 4 | Batch_idx: 10 |  Loss: (1.3323) |  Loss2: (0.0000) | Acc: (52.00%) (739/1408)
Epoch: 4 | Batch_idx: 20 |  Loss: (1.3292) |  Loss2: (0.0000) | Acc: (52.00%) (1406/2688)
Epoch: 4 | Batch_idx: 30 |  Loss: (1.3304) |  Loss2: (0.0000) | Acc: (52.00%) (2066/3968)
Epoch: 4 | Batch_idx: 40 |  Loss: (1.3209) |  Loss2: (0.0000) | Acc: (52.00%) (2737/5248)
Epoch: 4 | Batch_idx: 50 |  Loss: (1.3164) |  Loss2: (0.0000) | Acc: (52.00%) (3410/6528)
Epoch: 4 | Batch_idx: 60 |  Loss: (1.3158) |  Loss2: (0.0000) | Acc: (52.00%) (4071/7808)
Epoch: 4 | Batch_idx: 70 |  Loss: (1.3145) |  Loss2: (0.0000) | Acc: (52.00%) (4754/9088)
Epoch: 4 | Batch_idx: 80 |  Loss: (1.3118) |  Loss2: (0.0000) | Acc: (52.00%) (5436/10368)
Epoch: 4 | Batch_idx: 90 |  Loss: (1.3074) |  Loss2: (0.0000) | Acc: (52.00%) (6142/11648)
Epoch: 4 | Batch_idx: 100 |  Loss: (1.3076) |  Loss2: (0.0000) | Acc: (52.00%) (6823/12928)
Epoch: 4 | Batch_idx: 110 |  Loss: (1.3045) |  Loss2: (0.0000) | Acc: (52.00%) (7528/14208)
Epoch: 4 | Batch_idx: 120 |  Loss: (1.3035) |  Loss2: (0.0000) | Acc: (53.00%) (8225/15488)
Epoch: 4 | Batch_idx: 130 |  Loss: (1.3011) |  Loss2: (0.0000) | Acc: (53.00%) (8950/16768)
Epoch: 4 | Batch_idx: 140 |  Loss: (1.3019) |  Loss2: (0.0000) | Acc: (53.00%) (9616/18048)
Epoch: 4 | Batch_idx: 150 |  Loss: (1.2970) |  Loss2: (0.0000) | Acc: (53.00%) (10343/19328)
Epoch: 4 | Batch_idx: 160 |  Loss: (1.2952) |  Loss2: (0.0000) | Acc: (53.00%) (11031/20608)
Epoch: 4 | Batch_idx: 170 |  Loss: (1.2943) |  Loss2: (0.0000) | Acc: (53.00%) (11713/21888)
Epoch: 4 | Batch_idx: 180 |  Loss: (1.2931) |  Loss2: (0.0000) | Acc: (53.00%) (12441/23168)
Epoch: 4 | Batch_idx: 190 |  Loss: (1.2943) |  Loss2: (0.0000) | Acc: (53.00%) (13125/24448)
Epoch: 4 | Batch_idx: 200 |  Loss: (1.2948) |  Loss2: (0.0000) | Acc: (53.00%) (13806/25728)
Epoch: 4 | Batch_idx: 210 |  Loss: (1.2960) |  Loss2: (0.0000) | Acc: (53.00%) (14471/27008)
Epoch: 4 | Batch_idx: 220 |  Loss: (1.2935) |  Loss2: (0.0000) | Acc: (53.00%) (15225/28288)
Epoch: 4 | Batch_idx: 230 |  Loss: (1.2921) |  Loss2: (0.0000) | Acc: (53.00%) (15939/29568)
Epoch: 4 | Batch_idx: 240 |  Loss: (1.2886) |  Loss2: (0.0000) | Acc: (54.00%) (16677/30848)
Epoch: 4 | Batch_idx: 250 |  Loss: (1.2861) |  Loss2: (0.0000) | Acc: (54.00%) (17371/32128)
Epoch: 4 | Batch_idx: 260 |  Loss: (1.2840) |  Loss2: (0.0000) | Acc: (54.00%) (18095/33408)
Epoch: 4 | Batch_idx: 270 |  Loss: (1.2827) |  Loss2: (0.0000) | Acc: (54.00%) (18787/34688)
Epoch: 4 | Batch_idx: 280 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (54.00%) (19489/35968)
Epoch: 4 | Batch_idx: 290 |  Loss: (1.2809) |  Loss2: (0.0000) | Acc: (54.00%) (20217/37248)
Epoch: 4 | Batch_idx: 300 |  Loss: (1.2780) |  Loss2: (0.0000) | Acc: (54.00%) (20956/38528)
Epoch: 4 | Batch_idx: 310 |  Loss: (1.2767) |  Loss2: (0.0000) | Acc: (54.00%) (21654/39808)
Epoch: 4 | Batch_idx: 320 |  Loss: (1.2759) |  Loss2: (0.0000) | Acc: (54.00%) (22361/41088)
Epoch: 4 | Batch_idx: 330 |  Loss: (1.2746) |  Loss2: (0.0000) | Acc: (54.00%) (23089/42368)
Epoch: 4 | Batch_idx: 340 |  Loss: (1.2725) |  Loss2: (0.0000) | Acc: (54.00%) (23816/43648)
Epoch: 4 | Batch_idx: 350 |  Loss: (1.2697) |  Loss2: (0.0000) | Acc: (54.00%) (24570/44928)
Epoch: 4 | Batch_idx: 360 |  Loss: (1.2690) |  Loss2: (0.0000) | Acc: (54.00%) (25247/46208)
Epoch: 4 | Batch_idx: 370 |  Loss: (1.2668) |  Loss2: (0.0000) | Acc: (54.00%) (25988/47488)
Epoch: 4 | Batch_idx: 380 |  Loss: (1.2650) |  Loss2: (0.0000) | Acc: (54.00%) (26726/48768)
Epoch: 4 | Batch_idx: 390 |  Loss: (1.2637) |  Loss2: (0.0000) | Acc: (54.00%) (27423/50000)
# TEST : Loss: (1.2452) | Acc: (55.00%) (5547/10000)
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
percent tensor([0.5000, 0.5000], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 5 | Batch_idx: 0 |  Loss: (1.0700) |  Loss2: (0.0000) | Acc: (69.00%) (89/128)
Epoch: 5 | Batch_idx: 10 |  Loss: (1.2717) |  Loss2: (0.0000) | Acc: (55.00%) (784/1408)
Epoch: 5 | Batch_idx: 20 |  Loss: (1.3472) |  Loss2: (0.0000) | Acc: (52.00%) (1420/2688)
Epoch: 5 | Batch_idx: 30 |  Loss: (1.3787) |  Loss2: (0.0000) | Acc: (51.00%) (2035/3968)
Epoch: 5 | Batch_idx: 40 |  Loss: (1.4048) |  Loss2: (0.0000) | Acc: (50.00%) (2637/5248)
Epoch: 5 | Batch_idx: 50 |  Loss: (1.4161) |  Loss2: (0.0000) | Acc: (50.00%) (3270/6528)
Epoch: 5 | Batch_idx: 60 |  Loss: (1.4312) |  Loss2: (0.0000) | Acc: (49.00%) (3863/7808)
Epoch: 5 | Batch_idx: 70 |  Loss: (1.4332) |  Loss2: (0.0000) | Acc: (49.00%) (4478/9088)
Epoch: 5 | Batch_idx: 80 |  Loss: (1.4370) |  Loss2: (0.0000) | Acc: (48.00%) (5071/10368)
Epoch: 5 | Batch_idx: 90 |  Loss: (1.4352) |  Loss2: (0.0000) | Acc: (49.00%) (5716/11648)
Epoch: 5 | Batch_idx: 100 |  Loss: (1.4343) |  Loss2: (0.0000) | Acc: (49.00%) (6344/12928)
Epoch: 5 | Batch_idx: 110 |  Loss: (1.4331) |  Loss2: (0.0000) | Acc: (49.00%) (6967/14208)
Epoch: 5 | Batch_idx: 120 |  Loss: (1.4287) |  Loss2: (0.0000) | Acc: (49.00%) (7625/15488)
Epoch: 5 | Batch_idx: 130 |  Loss: (1.4239) |  Loss2: (0.0000) | Acc: (49.00%) (8278/16768)
Epoch: 5 | Batch_idx: 140 |  Loss: (1.4233) |  Loss2: (0.0000) | Acc: (49.00%) (8900/18048)
Epoch: 5 | Batch_idx: 150 |  Loss: (1.4246) |  Loss2: (0.0000) | Acc: (49.00%) (9534/19328)
Epoch: 5 | Batch_idx: 160 |  Loss: (1.4226) |  Loss2: (0.0000) | Acc: (49.00%) (10178/20608)
Epoch: 5 | Batch_idx: 170 |  Loss: (1.4215) |  Loss2: (0.0000) | Acc: (49.00%) (10806/21888)
Epoch: 5 | Batch_idx: 180 |  Loss: (1.4195) |  Loss2: (0.0000) | Acc: (49.00%) (11451/23168)
Epoch: 5 | Batch_idx: 190 |  Loss: (1.4168) |  Loss2: (0.0000) | Acc: (49.00%) (12095/24448)
Epoch: 5 | Batch_idx: 200 |  Loss: (1.4145) |  Loss2: (0.0000) | Acc: (49.00%) (12734/25728)
Epoch: 5 | Batch_idx: 210 |  Loss: (1.4139) |  Loss2: (0.0000) | Acc: (49.00%) (13394/27008)
Epoch: 5 | Batch_idx: 220 |  Loss: (1.4109) |  Loss2: (0.0000) | Acc: (49.00%) (14064/28288)
Epoch: 5 | Batch_idx: 230 |  Loss: (1.4107) |  Loss2: (0.0000) | Acc: (49.00%) (14681/29568)
Epoch: 5 | Batch_idx: 240 |  Loss: (1.4086) |  Loss2: (0.0000) | Acc: (49.00%) (15315/30848)
Epoch: 5 | Batch_idx: 250 |  Loss: (1.4072) |  Loss2: (0.0000) | Acc: (49.00%) (15965/32128)
Epoch: 5 | Batch_idx: 260 |  Loss: (1.4029) |  Loss2: (0.0000) | Acc: (49.00%) (16665/33408)
Epoch: 5 | Batch_idx: 270 |  Loss: (1.4000) |  Loss2: (0.0000) | Acc: (50.00%) (17367/34688)
Epoch: 5 | Batch_idx: 280 |  Loss: (1.3972) |  Loss2: (0.0000) | Acc: (50.00%) (18017/35968)
Epoch: 5 | Batch_idx: 290 |  Loss: (1.3948) |  Loss2: (0.0000) | Acc: (50.00%) (18676/37248)
Epoch: 5 | Batch_idx: 300 |  Loss: (1.3928) |  Loss2: (0.0000) | Acc: (50.00%) (19341/38528)
Epoch: 5 | Batch_idx: 310 |  Loss: (1.3907) |  Loss2: (0.0000) | Acc: (50.00%) (19984/39808)
Epoch: 5 | Batch_idx: 320 |  Loss: (1.3882) |  Loss2: (0.0000) | Acc: (50.00%) (20675/41088)
Epoch: 5 | Batch_idx: 330 |  Loss: (1.3857) |  Loss2: (0.0000) | Acc: (50.00%) (21361/42368)
Epoch: 5 | Batch_idx: 340 |  Loss: (1.3833) |  Loss2: (0.0000) | Acc: (50.00%) (22064/43648)
Epoch: 5 | Batch_idx: 350 |  Loss: (1.3817) |  Loss2: (0.0000) | Acc: (50.00%) (22731/44928)
Epoch: 5 | Batch_idx: 360 |  Loss: (1.3803) |  Loss2: (0.0000) | Acc: (50.00%) (23385/46208)
Epoch: 5 | Batch_idx: 370 |  Loss: (1.3776) |  Loss2: (0.0000) | Acc: (50.00%) (24071/47488)
Epoch: 5 | Batch_idx: 380 |  Loss: (1.3762) |  Loss2: (0.0000) | Acc: (50.00%) (24744/48768)
Epoch: 5 | Batch_idx: 390 |  Loss: (1.3732) |  Loss2: (0.0000) | Acc: (50.00%) (25395/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_005.pth.tar'
# TEST : Loss: (1.2769) | Acc: (53.00%) (5350/10000)
percent tensor([0.5069, 0.4931], device='cuda:0')
percent tensor([0.4941, 0.5059], device='cuda:0')
percent tensor([0.4929, 0.5071], device='cuda:0')
percent tensor([0.5026, 0.4974], device='cuda:0')
percent tensor([0.5028, 0.4972], device='cuda:0')
percent tensor([0.5071, 0.4929], device='cuda:0')
percent tensor([0.5090, 0.4910], device='cuda:0')
percent tensor([0.5393, 0.4607], device='cuda:0')
Epoch: 6 | Batch_idx: 0 |  Loss: (1.2735) |  Loss2: (0.0000) | Acc: (53.00%) (68/128)
Epoch: 6 | Batch_idx: 10 |  Loss: (1.2976) |  Loss2: (0.0000) | Acc: (51.00%) (731/1408)
Epoch: 6 | Batch_idx: 20 |  Loss: (1.3048) |  Loss2: (0.0000) | Acc: (51.00%) (1391/2688)
Epoch: 6 | Batch_idx: 30 |  Loss: (1.3097) |  Loss2: (0.0000) | Acc: (52.00%) (2071/3968)
Epoch: 6 | Batch_idx: 40 |  Loss: (1.3056) |  Loss2: (0.0000) | Acc: (52.00%) (2744/5248)
Epoch: 6 | Batch_idx: 50 |  Loss: (1.3057) |  Loss2: (0.0000) | Acc: (52.00%) (3424/6528)
Epoch: 6 | Batch_idx: 60 |  Loss: (1.2959) |  Loss2: (0.0000) | Acc: (53.00%) (4149/7808)
Epoch: 6 | Batch_idx: 70 |  Loss: (1.2956) |  Loss2: (0.0000) | Acc: (53.00%) (4820/9088)
Epoch: 6 | Batch_idx: 80 |  Loss: (1.2930) |  Loss2: (0.0000) | Acc: (53.00%) (5526/10368)
Epoch: 6 | Batch_idx: 90 |  Loss: (1.2918) |  Loss2: (0.0000) | Acc: (53.00%) (6185/11648)
Epoch: 6 | Batch_idx: 100 |  Loss: (1.2869) |  Loss2: (0.0000) | Acc: (53.00%) (6908/12928)
Epoch: 6 | Batch_idx: 110 |  Loss: (1.2873) |  Loss2: (0.0000) | Acc: (53.00%) (7600/14208)
Epoch: 6 | Batch_idx: 120 |  Loss: (1.2884) |  Loss2: (0.0000) | Acc: (53.00%) (8269/15488)
Epoch: 6 | Batch_idx: 130 |  Loss: (1.2850) |  Loss2: (0.0000) | Acc: (53.00%) (8984/16768)
Epoch: 6 | Batch_idx: 140 |  Loss: (1.2845) |  Loss2: (0.0000) | Acc: (53.00%) (9669/18048)
Epoch: 6 | Batch_idx: 150 |  Loss: (1.2831) |  Loss2: (0.0000) | Acc: (53.00%) (10347/19328)
Epoch: 6 | Batch_idx: 160 |  Loss: (1.2823) |  Loss2: (0.0000) | Acc: (53.00%) (11042/20608)
Epoch: 6 | Batch_idx: 170 |  Loss: (1.2829) |  Loss2: (0.0000) | Acc: (53.00%) (11721/21888)
Epoch: 6 | Batch_idx: 180 |  Loss: (1.2786) |  Loss2: (0.0000) | Acc: (53.00%) (12435/23168)
Epoch: 6 | Batch_idx: 190 |  Loss: (1.2768) |  Loss2: (0.0000) | Acc: (53.00%) (13139/24448)
Epoch: 6 | Batch_idx: 200 |  Loss: (1.2783) |  Loss2: (0.0000) | Acc: (53.00%) (13823/25728)
Epoch: 6 | Batch_idx: 210 |  Loss: (1.2768) |  Loss2: (0.0000) | Acc: (53.00%) (14531/27008)
Epoch: 6 | Batch_idx: 220 |  Loss: (1.2762) |  Loss2: (0.0000) | Acc: (53.00%) (15214/28288)
Epoch: 6 | Batch_idx: 230 |  Loss: (1.2732) |  Loss2: (0.0000) | Acc: (53.00%) (15956/29568)
Epoch: 6 | Batch_idx: 240 |  Loss: (1.2700) |  Loss2: (0.0000) | Acc: (54.00%) (16687/30848)
Epoch: 6 | Batch_idx: 250 |  Loss: (1.2692) |  Loss2: (0.0000) | Acc: (54.00%) (17378/32128)
Epoch: 6 | Batch_idx: 260 |  Loss: (1.2691) |  Loss2: (0.0000) | Acc: (54.00%) (18072/33408)
Epoch: 6 | Batch_idx: 270 |  Loss: (1.2674) |  Loss2: (0.0000) | Acc: (54.00%) (18801/34688)
Epoch: 6 | Batch_idx: 280 |  Loss: (1.2658) |  Loss2: (0.0000) | Acc: (54.00%) (19512/35968)
Epoch: 6 | Batch_idx: 290 |  Loss: (1.2661) |  Loss2: (0.0000) | Acc: (54.00%) (20198/37248)
Epoch: 6 | Batch_idx: 300 |  Loss: (1.2670) |  Loss2: (0.0000) | Acc: (54.00%) (20886/38528)
Epoch: 6 | Batch_idx: 310 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (21612/39808)
Epoch: 6 | Batch_idx: 320 |  Loss: (1.2646) |  Loss2: (0.0000) | Acc: (54.00%) (22332/41088)
Epoch: 6 | Batch_idx: 330 |  Loss: (1.2651) |  Loss2: (0.0000) | Acc: (54.00%) (23014/42368)
Epoch: 6 | Batch_idx: 340 |  Loss: (1.2655) |  Loss2: (0.0000) | Acc: (54.00%) (23689/43648)
Epoch: 6 | Batch_idx: 350 |  Loss: (1.2648) |  Loss2: (0.0000) | Acc: (54.00%) (24402/44928)
Epoch: 6 | Batch_idx: 360 |  Loss: (1.2643) |  Loss2: (0.0000) | Acc: (54.00%) (25083/46208)
Epoch: 6 | Batch_idx: 370 |  Loss: (1.2637) |  Loss2: (0.0000) | Acc: (54.00%) (25784/47488)
Epoch: 6 | Batch_idx: 380 |  Loss: (1.2629) |  Loss2: (0.0000) | Acc: (54.00%) (26497/48768)
Epoch: 6 | Batch_idx: 390 |  Loss: (1.2616) |  Loss2: (0.0000) | Acc: (54.00%) (27200/50000)
# TEST : Loss: (1.2319) | Acc: (55.00%) (5551/10000)
percent tensor([0.5121, 0.4879], device='cuda:0')
percent tensor([0.4966, 0.5034], device='cuda:0')
percent tensor([0.4869, 0.5131], device='cuda:0')
percent tensor([0.5093, 0.4907], device='cuda:0')
percent tensor([0.5138, 0.4862], device='cuda:0')
percent tensor([0.5142, 0.4858], device='cuda:0')
percent tensor([0.5224, 0.4776], device='cuda:0')
percent tensor([0.6587, 0.3413], device='cuda:0')
Epoch: 7 | Batch_idx: 0 |  Loss: (1.2048) |  Loss2: (0.0000) | Acc: (57.00%) (73/128)
Epoch: 7 | Batch_idx: 10 |  Loss: (1.2344) |  Loss2: (0.0000) | Acc: (56.00%) (790/1408)
Epoch: 7 | Batch_idx: 20 |  Loss: (1.2379) |  Loss2: (0.0000) | Acc: (55.00%) (1502/2688)
Epoch: 7 | Batch_idx: 30 |  Loss: (1.2374) |  Loss2: (0.0000) | Acc: (55.00%) (2205/3968)
Epoch: 7 | Batch_idx: 40 |  Loss: (1.2442) |  Loss2: (0.0000) | Acc: (54.00%) (2880/5248)
Epoch: 7 | Batch_idx: 50 |  Loss: (1.2371) |  Loss2: (0.0000) | Acc: (55.00%) (3599/6528)
Epoch: 7 | Batch_idx: 60 |  Loss: (1.2411) |  Loss2: (0.0000) | Acc: (55.00%) (4296/7808)
Epoch: 7 | Batch_idx: 70 |  Loss: (1.2452) |  Loss2: (0.0000) | Acc: (54.00%) (4977/9088)
Epoch: 7 | Batch_idx: 80 |  Loss: (1.2453) |  Loss2: (0.0000) | Acc: (54.00%) (5677/10368)
Epoch: 7 | Batch_idx: 90 |  Loss: (1.2441) |  Loss2: (0.0000) | Acc: (54.00%) (6377/11648)
Epoch: 7 | Batch_idx: 100 |  Loss: (1.2465) |  Loss2: (0.0000) | Acc: (54.00%) (7072/12928)
Epoch: 7 | Batch_idx: 110 |  Loss: (1.2459) |  Loss2: (0.0000) | Acc: (54.00%) (7759/14208)
Epoch: 7 | Batch_idx: 120 |  Loss: (1.2469) |  Loss2: (0.0000) | Acc: (54.00%) (8450/15488)
Epoch: 7 | Batch_idx: 130 |  Loss: (1.2414) |  Loss2: (0.0000) | Acc: (54.00%) (9164/16768)
Epoch: 7 | Batch_idx: 140 |  Loss: (1.2400) |  Loss2: (0.0000) | Acc: (54.00%) (9873/18048)
Epoch: 7 | Batch_idx: 150 |  Loss: (1.2374) |  Loss2: (0.0000) | Acc: (54.00%) (10593/19328)
Epoch: 7 | Batch_idx: 160 |  Loss: (1.2387) |  Loss2: (0.0000) | Acc: (54.00%) (11270/20608)
Epoch: 7 | Batch_idx: 170 |  Loss: (1.2382) |  Loss2: (0.0000) | Acc: (54.00%) (11980/21888)
Epoch: 7 | Batch_idx: 180 |  Loss: (1.2380) |  Loss2: (0.0000) | Acc: (54.00%) (12671/23168)
Epoch: 7 | Batch_idx: 190 |  Loss: (1.2394) |  Loss2: (0.0000) | Acc: (54.00%) (13347/24448)
Epoch: 7 | Batch_idx: 200 |  Loss: (1.2360) |  Loss2: (0.0000) | Acc: (54.00%) (14086/25728)
Epoch: 7 | Batch_idx: 210 |  Loss: (1.2333) |  Loss2: (0.0000) | Acc: (54.00%) (14794/27008)
Epoch: 7 | Batch_idx: 220 |  Loss: (1.2337) |  Loss2: (0.0000) | Acc: (54.00%) (15479/28288)
Epoch: 7 | Batch_idx: 230 |  Loss: (1.2364) |  Loss2: (0.0000) | Acc: (54.00%) (16138/29568)
Epoch: 7 | Batch_idx: 240 |  Loss: (1.2350) |  Loss2: (0.0000) | Acc: (54.00%) (16859/30848)
Epoch: 7 | Batch_idx: 250 |  Loss: (1.2345) |  Loss2: (0.0000) | Acc: (54.00%) (17581/32128)
Epoch: 7 | Batch_idx: 260 |  Loss: (1.2321) |  Loss2: (0.0000) | Acc: (54.00%) (18322/33408)
Epoch: 7 | Batch_idx: 270 |  Loss: (1.2319) |  Loss2: (0.0000) | Acc: (54.00%) (19050/34688)
Epoch: 7 | Batch_idx: 280 |  Loss: (1.2309) |  Loss2: (0.0000) | Acc: (54.00%) (19782/35968)
Epoch: 7 | Batch_idx: 290 |  Loss: (1.2301) |  Loss2: (0.0000) | Acc: (55.00%) (20512/37248)
Epoch: 7 | Batch_idx: 300 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (21257/38528)
Epoch: 7 | Batch_idx: 310 |  Loss: (1.2282) |  Loss2: (0.0000) | Acc: (55.00%) (21961/39808)
Epoch: 7 | Batch_idx: 320 |  Loss: (1.2294) |  Loss2: (0.0000) | Acc: (55.00%) (22666/41088)
Epoch: 7 | Batch_idx: 330 |  Loss: (1.2293) |  Loss2: (0.0000) | Acc: (55.00%) (23367/42368)
Epoch: 7 | Batch_idx: 340 |  Loss: (1.2280) |  Loss2: (0.0000) | Acc: (55.00%) (24067/43648)
Epoch: 7 | Batch_idx: 350 |  Loss: (1.2287) |  Loss2: (0.0000) | Acc: (55.00%) (24764/44928)
Epoch: 7 | Batch_idx: 360 |  Loss: (1.2287) |  Loss2: (0.0000) | Acc: (55.00%) (25470/46208)
Epoch: 7 | Batch_idx: 370 |  Loss: (1.2293) |  Loss2: (0.0000) | Acc: (55.00%) (26155/47488)
Epoch: 7 | Batch_idx: 380 |  Loss: (1.2281) |  Loss2: (0.0000) | Acc: (55.00%) (26900/48768)
Epoch: 7 | Batch_idx: 390 |  Loss: (1.2280) |  Loss2: (0.0000) | Acc: (55.00%) (27566/50000)
# TEST : Loss: (1.2189) | Acc: (55.00%) (5583/10000)
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.4996, 0.5004], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5160, 0.4840], device='cuda:0')
percent tensor([0.5260, 0.4740], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.5269, 0.4731], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
Epoch: 8 | Batch_idx: 0 |  Loss: (1.1083) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 8 | Batch_idx: 10 |  Loss: (1.2195) |  Loss2: (0.0000) | Acc: (56.00%) (796/1408)
Epoch: 8 | Batch_idx: 20 |  Loss: (1.2222) |  Loss2: (0.0000) | Acc: (56.00%) (1522/2688)
Epoch: 8 | Batch_idx: 30 |  Loss: (1.2300) |  Loss2: (0.0000) | Acc: (56.00%) (2228/3968)
Epoch: 8 | Batch_idx: 40 |  Loss: (1.2260) |  Loss2: (0.0000) | Acc: (55.00%) (2938/5248)
Epoch: 8 | Batch_idx: 50 |  Loss: (1.2289) |  Loss2: (0.0000) | Acc: (56.00%) (3672/6528)
Epoch: 8 | Batch_idx: 60 |  Loss: (1.2202) |  Loss2: (0.0000) | Acc: (56.00%) (4404/7808)
Epoch: 8 | Batch_idx: 70 |  Loss: (1.2225) |  Loss2: (0.0000) | Acc: (56.00%) (5094/9088)
Epoch: 8 | Batch_idx: 80 |  Loss: (1.2167) |  Loss2: (0.0000) | Acc: (56.00%) (5819/10368)
Epoch: 8 | Batch_idx: 90 |  Loss: (1.2211) |  Loss2: (0.0000) | Acc: (55.00%) (6511/11648)
Epoch: 8 | Batch_idx: 100 |  Loss: (1.2211) |  Loss2: (0.0000) | Acc: (55.00%) (7210/12928)
Epoch: 8 | Batch_idx: 110 |  Loss: (1.2146) |  Loss2: (0.0000) | Acc: (55.00%) (7950/14208)
Epoch: 8 | Batch_idx: 120 |  Loss: (1.2116) |  Loss2: (0.0000) | Acc: (56.00%) (8675/15488)
Epoch: 8 | Batch_idx: 130 |  Loss: (1.2128) |  Loss2: (0.0000) | Acc: (55.00%) (9372/16768)
Epoch: 8 | Batch_idx: 140 |  Loss: (1.2151) |  Loss2: (0.0000) | Acc: (55.00%) (10060/18048)
Epoch: 8 | Batch_idx: 150 |  Loss: (1.2135) |  Loss2: (0.0000) | Acc: (55.00%) (10777/19328)
Epoch: 8 | Batch_idx: 160 |  Loss: (1.2109) |  Loss2: (0.0000) | Acc: (55.00%) (11494/20608)
Epoch: 8 | Batch_idx: 170 |  Loss: (1.2099) |  Loss2: (0.0000) | Acc: (55.00%) (12203/21888)
Epoch: 8 | Batch_idx: 180 |  Loss: (1.2102) |  Loss2: (0.0000) | Acc: (55.00%) (12932/23168)
Epoch: 8 | Batch_idx: 190 |  Loss: (1.2103) |  Loss2: (0.0000) | Acc: (55.00%) (13656/24448)
Epoch: 8 | Batch_idx: 200 |  Loss: (1.2101) |  Loss2: (0.0000) | Acc: (55.00%) (14388/25728)
Epoch: 8 | Batch_idx: 210 |  Loss: (1.2120) |  Loss2: (0.0000) | Acc: (55.00%) (15085/27008)
Epoch: 8 | Batch_idx: 220 |  Loss: (1.2110) |  Loss2: (0.0000) | Acc: (55.00%) (15815/28288)
Epoch: 8 | Batch_idx: 230 |  Loss: (1.2107) |  Loss2: (0.0000) | Acc: (55.00%) (16542/29568)
Epoch: 8 | Batch_idx: 240 |  Loss: (1.2121) |  Loss2: (0.0000) | Acc: (55.00%) (17245/30848)
Epoch: 8 | Batch_idx: 250 |  Loss: (1.2126) |  Loss2: (0.0000) | Acc: (55.00%) (17956/32128)
Epoch: 8 | Batch_idx: 260 |  Loss: (1.2140) |  Loss2: (0.0000) | Acc: (55.00%) (18647/33408)
Epoch: 8 | Batch_idx: 270 |  Loss: (1.2138) |  Loss2: (0.0000) | Acc: (55.00%) (19349/34688)
Epoch: 8 | Batch_idx: 280 |  Loss: (1.2138) |  Loss2: (0.0000) | Acc: (55.00%) (20053/35968)
Epoch: 8 | Batch_idx: 290 |  Loss: (1.2140) |  Loss2: (0.0000) | Acc: (55.00%) (20755/37248)
Epoch: 8 | Batch_idx: 300 |  Loss: (1.2141) |  Loss2: (0.0000) | Acc: (55.00%) (21465/38528)
Epoch: 8 | Batch_idx: 310 |  Loss: (1.2129) |  Loss2: (0.0000) | Acc: (55.00%) (22209/39808)
Epoch: 8 | Batch_idx: 320 |  Loss: (1.2107) |  Loss2: (0.0000) | Acc: (55.00%) (22946/41088)
Epoch: 8 | Batch_idx: 330 |  Loss: (1.2116) |  Loss2: (0.0000) | Acc: (55.00%) (23659/42368)
Epoch: 8 | Batch_idx: 340 |  Loss: (1.2127) |  Loss2: (0.0000) | Acc: (55.00%) (24368/43648)
Epoch: 8 | Batch_idx: 350 |  Loss: (1.2124) |  Loss2: (0.0000) | Acc: (55.00%) (25103/44928)
Epoch: 8 | Batch_idx: 360 |  Loss: (1.2127) |  Loss2: (0.0000) | Acc: (55.00%) (25798/46208)
Epoch: 8 | Batch_idx: 370 |  Loss: (1.2128) |  Loss2: (0.0000) | Acc: (55.00%) (26518/47488)
Epoch: 8 | Batch_idx: 380 |  Loss: (1.2117) |  Loss2: (0.0000) | Acc: (55.00%) (27244/48768)
Epoch: 8 | Batch_idx: 390 |  Loss: (1.2121) |  Loss2: (0.0000) | Acc: (55.00%) (27931/50000)
# TEST : Loss: (1.2099) | Acc: (56.00%) (5639/10000)
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.5014, 0.4986], device='cuda:0')
percent tensor([0.4785, 0.5215], device='cuda:0')
percent tensor([0.5229, 0.4771], device='cuda:0')
percent tensor([0.5394, 0.4606], device='cuda:0')
percent tensor([0.5178, 0.4822], device='cuda:0')
percent tensor([0.5268, 0.4732], device='cuda:0')
percent tensor([0.7588, 0.2412], device='cuda:0')
Epoch: 9 | Batch_idx: 0 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (53.00%) (69/128)
Epoch: 9 | Batch_idx: 10 |  Loss: (1.2070) |  Loss2: (0.0000) | Acc: (54.00%) (763/1408)
Epoch: 9 | Batch_idx: 20 |  Loss: (1.1663) |  Loss2: (0.0000) | Acc: (56.00%) (1517/2688)
Epoch: 9 | Batch_idx: 30 |  Loss: (1.1784) |  Loss2: (0.0000) | Acc: (56.00%) (2243/3968)
Epoch: 9 | Batch_idx: 40 |  Loss: (1.1912) |  Loss2: (0.0000) | Acc: (56.00%) (2945/5248)
Epoch: 9 | Batch_idx: 50 |  Loss: (1.1863) |  Loss2: (0.0000) | Acc: (55.00%) (3655/6528)
Epoch: 9 | Batch_idx: 60 |  Loss: (1.1884) |  Loss2: (0.0000) | Acc: (56.00%) (4375/7808)
Epoch: 9 | Batch_idx: 70 |  Loss: (1.1869) |  Loss2: (0.0000) | Acc: (56.00%) (5095/9088)
Epoch: 9 | Batch_idx: 80 |  Loss: (1.1894) |  Loss2: (0.0000) | Acc: (55.00%) (5788/10368)
Epoch: 9 | Batch_idx: 90 |  Loss: (1.1975) |  Loss2: (0.0000) | Acc: (55.00%) (6483/11648)
Epoch: 9 | Batch_idx: 100 |  Loss: (1.1960) |  Loss2: (0.0000) | Acc: (55.00%) (7221/12928)
Epoch: 9 | Batch_idx: 110 |  Loss: (1.1968) |  Loss2: (0.0000) | Acc: (55.00%) (7939/14208)
Epoch: 9 | Batch_idx: 120 |  Loss: (1.1929) |  Loss2: (0.0000) | Acc: (55.00%) (8666/15488)
Epoch: 9 | Batch_idx: 130 |  Loss: (1.1903) |  Loss2: (0.0000) | Acc: (56.00%) (9424/16768)
Epoch: 9 | Batch_idx: 140 |  Loss: (1.1888) |  Loss2: (0.0000) | Acc: (56.00%) (10166/18048)
Epoch: 9 | Batch_idx: 150 |  Loss: (1.1936) |  Loss2: (0.0000) | Acc: (56.00%) (10869/19328)
Epoch: 9 | Batch_idx: 160 |  Loss: (1.1966) |  Loss2: (0.0000) | Acc: (56.00%) (11574/20608)
Epoch: 9 | Batch_idx: 170 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (12301/21888)
Epoch: 9 | Batch_idx: 180 |  Loss: (1.1976) |  Loss2: (0.0000) | Acc: (56.00%) (13034/23168)
Epoch: 9 | Batch_idx: 190 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (13783/24448)
Epoch: 9 | Batch_idx: 200 |  Loss: (1.1979) |  Loss2: (0.0000) | Acc: (56.00%) (14492/25728)
Epoch: 9 | Batch_idx: 210 |  Loss: (1.1976) |  Loss2: (0.0000) | Acc: (56.00%) (15222/27008)
Epoch: 9 | Batch_idx: 220 |  Loss: (1.1965) |  Loss2: (0.0000) | Acc: (56.00%) (15974/28288)
Epoch: 9 | Batch_idx: 230 |  Loss: (1.1978) |  Loss2: (0.0000) | Acc: (56.00%) (16674/29568)
Epoch: 9 | Batch_idx: 240 |  Loss: (1.1962) |  Loss2: (0.0000) | Acc: (56.00%) (17433/30848)
Epoch: 9 | Batch_idx: 250 |  Loss: (1.1961) |  Loss2: (0.0000) | Acc: (56.00%) (18171/32128)
Epoch: 9 | Batch_idx: 260 |  Loss: (1.1972) |  Loss2: (0.0000) | Acc: (56.00%) (18871/33408)
Epoch: 9 | Batch_idx: 270 |  Loss: (1.1990) |  Loss2: (0.0000) | Acc: (56.00%) (19546/34688)
Epoch: 9 | Batch_idx: 280 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (20279/35968)
Epoch: 9 | Batch_idx: 290 |  Loss: (1.1987) |  Loss2: (0.0000) | Acc: (56.00%) (21016/37248)
Epoch: 9 | Batch_idx: 300 |  Loss: (1.1989) |  Loss2: (0.0000) | Acc: (56.00%) (21723/38528)
Epoch: 9 | Batch_idx: 310 |  Loss: (1.1977) |  Loss2: (0.0000) | Acc: (56.00%) (22474/39808)
Epoch: 9 | Batch_idx: 320 |  Loss: (1.1993) |  Loss2: (0.0000) | Acc: (56.00%) (23169/41088)
Epoch: 9 | Batch_idx: 330 |  Loss: (1.1994) |  Loss2: (0.0000) | Acc: (56.00%) (23893/42368)
Epoch: 9 | Batch_idx: 340 |  Loss: (1.2006) |  Loss2: (0.0000) | Acc: (56.00%) (24594/43648)
Epoch: 9 | Batch_idx: 350 |  Loss: (1.2016) |  Loss2: (0.0000) | Acc: (56.00%) (25307/44928)
Epoch: 9 | Batch_idx: 360 |  Loss: (1.2021) |  Loss2: (0.0000) | Acc: (56.00%) (26025/46208)
Epoch: 9 | Batch_idx: 370 |  Loss: (1.2027) |  Loss2: (0.0000) | Acc: (56.00%) (26729/47488)
Epoch: 9 | Batch_idx: 380 |  Loss: (1.2026) |  Loss2: (0.0000) | Acc: (56.00%) (27445/48768)
Epoch: 9 | Batch_idx: 390 |  Loss: (1.2018) |  Loss2: (0.0000) | Acc: (56.00%) (28147/50000)
# TEST : Loss: (1.1964) | Acc: (56.00%) (5665/10000)
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.5036, 0.4964], device='cuda:0')
percent tensor([0.4764, 0.5236], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7744, 0.2256], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 10 | Batch_idx: 0 |  Loss: (1.1443) |  Loss2: (0.0000) | Acc: (57.00%) (74/128)
Epoch: 10 | Batch_idx: 10 |  Loss: (1.1957) |  Loss2: (0.0000) | Acc: (56.00%) (789/1408)
Epoch: 10 | Batch_idx: 20 |  Loss: (1.2033) |  Loss2: (0.0000) | Acc: (55.00%) (1497/2688)
Epoch: 10 | Batch_idx: 30 |  Loss: (1.1859) |  Loss2: (0.0000) | Acc: (56.00%) (2225/3968)
Epoch: 10 | Batch_idx: 40 |  Loss: (1.1816) |  Loss2: (0.0000) | Acc: (56.00%) (2968/5248)
Epoch: 10 | Batch_idx: 50 |  Loss: (1.1747) |  Loss2: (0.0000) | Acc: (56.00%) (3719/6528)
Epoch: 10 | Batch_idx: 60 |  Loss: (1.1745) |  Loss2: (0.0000) | Acc: (57.00%) (4469/7808)
Epoch: 10 | Batch_idx: 70 |  Loss: (1.1760) |  Loss2: (0.0000) | Acc: (57.00%) (5202/9088)
Epoch: 10 | Batch_idx: 80 |  Loss: (1.1691) |  Loss2: (0.0000) | Acc: (57.00%) (5967/10368)
Epoch: 10 | Batch_idx: 90 |  Loss: (1.1658) |  Loss2: (0.0000) | Acc: (57.00%) (6718/11648)
Epoch: 10 | Batch_idx: 100 |  Loss: (1.1645) |  Loss2: (0.0000) | Acc: (57.00%) (7443/12928)
Epoch: 10 | Batch_idx: 110 |  Loss: (1.1653) |  Loss2: (0.0000) | Acc: (57.00%) (8173/14208)
Epoch: 10 | Batch_idx: 120 |  Loss: (1.1634) |  Loss2: (0.0000) | Acc: (57.00%) (8932/15488)
Epoch: 10 | Batch_idx: 130 |  Loss: (1.1625) |  Loss2: (0.0000) | Acc: (57.00%) (9685/16768)
Epoch: 10 | Batch_idx: 140 |  Loss: (1.1616) |  Loss2: (0.0000) | Acc: (57.00%) (10438/18048)
Epoch: 10 | Batch_idx: 150 |  Loss: (1.1627) |  Loss2: (0.0000) | Acc: (57.00%) (11165/19328)
Epoch: 10 | Batch_idx: 160 |  Loss: (1.1591) |  Loss2: (0.0000) | Acc: (57.00%) (11931/20608)
Epoch: 10 | Batch_idx: 170 |  Loss: (1.1566) |  Loss2: (0.0000) | Acc: (57.00%) (12690/21888)
Epoch: 10 | Batch_idx: 180 |  Loss: (1.1540) |  Loss2: (0.0000) | Acc: (58.00%) (13455/23168)
Epoch: 10 | Batch_idx: 190 |  Loss: (1.1533) |  Loss2: (0.0000) | Acc: (58.00%) (14214/24448)
Epoch: 10 | Batch_idx: 200 |  Loss: (1.1520) |  Loss2: (0.0000) | Acc: (58.00%) (14993/25728)
Epoch: 10 | Batch_idx: 210 |  Loss: (1.1500) |  Loss2: (0.0000) | Acc: (58.00%) (15770/27008)
Epoch: 10 | Batch_idx: 220 |  Loss: (1.1496) |  Loss2: (0.0000) | Acc: (58.00%) (16517/28288)
Epoch: 10 | Batch_idx: 230 |  Loss: (1.1476) |  Loss2: (0.0000) | Acc: (58.00%) (17278/29568)
Epoch: 10 | Batch_idx: 240 |  Loss: (1.1471) |  Loss2: (0.0000) | Acc: (58.00%) (18029/30848)
Epoch: 10 | Batch_idx: 250 |  Loss: (1.1460) |  Loss2: (0.0000) | Acc: (58.00%) (18802/32128)
Epoch: 10 | Batch_idx: 260 |  Loss: (1.1456) |  Loss2: (0.0000) | Acc: (58.00%) (19550/33408)
Epoch: 10 | Batch_idx: 270 |  Loss: (1.1444) |  Loss2: (0.0000) | Acc: (58.00%) (20314/34688)
Epoch: 10 | Batch_idx: 280 |  Loss: (1.1438) |  Loss2: (0.0000) | Acc: (58.00%) (21084/35968)
Epoch: 10 | Batch_idx: 290 |  Loss: (1.1432) |  Loss2: (0.0000) | Acc: (58.00%) (21850/37248)
Epoch: 10 | Batch_idx: 300 |  Loss: (1.1422) |  Loss2: (0.0000) | Acc: (58.00%) (22624/38528)
Epoch: 10 | Batch_idx: 310 |  Loss: (1.1416) |  Loss2: (0.0000) | Acc: (58.00%) (23397/39808)
Epoch: 10 | Batch_idx: 320 |  Loss: (1.1388) |  Loss2: (0.0000) | Acc: (58.00%) (24197/41088)
Epoch: 10 | Batch_idx: 330 |  Loss: (1.1372) |  Loss2: (0.0000) | Acc: (58.00%) (24988/42368)
Epoch: 10 | Batch_idx: 340 |  Loss: (1.1349) |  Loss2: (0.0000) | Acc: (59.00%) (25800/43648)
Epoch: 10 | Batch_idx: 350 |  Loss: (1.1336) |  Loss2: (0.0000) | Acc: (59.00%) (26573/44928)
Epoch: 10 | Batch_idx: 360 |  Loss: (1.1332) |  Loss2: (0.0000) | Acc: (59.00%) (27339/46208)
Epoch: 10 | Batch_idx: 370 |  Loss: (1.1314) |  Loss2: (0.0000) | Acc: (59.00%) (28140/47488)
Epoch: 10 | Batch_idx: 380 |  Loss: (1.1303) |  Loss2: (0.0000) | Acc: (59.00%) (28917/48768)
Epoch: 10 | Batch_idx: 390 |  Loss: (1.1290) |  Loss2: (0.0000) | Acc: (59.00%) (29681/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_010.pth.tar'
# TEST : Loss: (1.1054) | Acc: (59.00%) (5997/10000)
percent tensor([0.5175, 0.4825], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.4768, 0.5232], device='cuda:0')
percent tensor([0.5319, 0.4681], device='cuda:0')
percent tensor([0.5495, 0.4505], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7771, 0.2229], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(168.4986, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(772.9026, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(768.8468, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1530.8716, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(507.8289, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2165.1082, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4329.1650, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1439.6720, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6107.7583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12232.9238, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4071.2241, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17281.3105, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 11 | Batch_idx: 0 |  Loss: (1.0827) |  Loss2: (0.0000) | Acc: (60.00%) (78/128)
Epoch: 11 | Batch_idx: 10 |  Loss: (1.0592) |  Loss2: (0.0000) | Acc: (61.00%) (859/1408)
Epoch: 11 | Batch_idx: 20 |  Loss: (1.0678) |  Loss2: (0.0000) | Acc: (61.00%) (1646/2688)
Epoch: 11 | Batch_idx: 30 |  Loss: (1.0621) |  Loss2: (0.0000) | Acc: (62.00%) (2463/3968)
Epoch: 11 | Batch_idx: 40 |  Loss: (1.0666) |  Loss2: (0.0000) | Acc: (61.00%) (3248/5248)
Epoch: 11 | Batch_idx: 50 |  Loss: (1.0681) |  Loss2: (0.0000) | Acc: (61.00%) (4019/6528)
Epoch: 11 | Batch_idx: 60 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (61.00%) (4816/7808)
Epoch: 11 | Batch_idx: 70 |  Loss: (1.0633) |  Loss2: (0.0000) | Acc: (61.00%) (5601/9088)
Epoch: 11 | Batch_idx: 80 |  Loss: (1.0595) |  Loss2: (0.0000) | Acc: (61.00%) (6410/10368)
Epoch: 11 | Batch_idx: 90 |  Loss: (1.0623) |  Loss2: (0.0000) | Acc: (61.00%) (7193/11648)
Epoch: 11 | Batch_idx: 100 |  Loss: (1.0619) |  Loss2: (0.0000) | Acc: (61.00%) (8006/12928)
Epoch: 11 | Batch_idx: 110 |  Loss: (1.0589) |  Loss2: (0.0000) | Acc: (61.00%) (8796/14208)
Epoch: 11 | Batch_idx: 120 |  Loss: (1.0605) |  Loss2: (0.0000) | Acc: (61.00%) (9575/15488)
Epoch: 11 | Batch_idx: 130 |  Loss: (1.0572) |  Loss2: (0.0000) | Acc: (61.00%) (10393/16768)
Epoch: 11 | Batch_idx: 140 |  Loss: (1.0553) |  Loss2: (0.0000) | Acc: (62.00%) (11229/18048)
Epoch: 11 | Batch_idx: 150 |  Loss: (1.0536) |  Loss2: (0.0000) | Acc: (62.00%) (12038/19328)
Epoch: 11 | Batch_idx: 160 |  Loss: (1.0516) |  Loss2: (0.0000) | Acc: (62.00%) (12847/20608)
Epoch: 11 | Batch_idx: 170 |  Loss: (1.0470) |  Loss2: (0.0000) | Acc: (62.00%) (13692/21888)
Epoch: 11 | Batch_idx: 180 |  Loss: (1.0456) |  Loss2: (0.0000) | Acc: (62.00%) (14503/23168)
Epoch: 11 | Batch_idx: 190 |  Loss: (1.0424) |  Loss2: (0.0000) | Acc: (62.00%) (15331/24448)
Epoch: 11 | Batch_idx: 200 |  Loss: (1.0402) |  Loss2: (0.0000) | Acc: (62.00%) (16145/25728)
Epoch: 11 | Batch_idx: 210 |  Loss: (1.0378) |  Loss2: (0.0000) | Acc: (62.00%) (16966/27008)
Epoch: 11 | Batch_idx: 220 |  Loss: (1.0371) |  Loss2: (0.0000) | Acc: (62.00%) (17777/28288)
Epoch: 11 | Batch_idx: 230 |  Loss: (1.0337) |  Loss2: (0.0000) | Acc: (63.00%) (18643/29568)
Epoch: 11 | Batch_idx: 240 |  Loss: (1.0328) |  Loss2: (0.0000) | Acc: (63.00%) (19454/30848)
Epoch: 11 | Batch_idx: 250 |  Loss: (1.0310) |  Loss2: (0.0000) | Acc: (63.00%) (20303/32128)
Epoch: 11 | Batch_idx: 260 |  Loss: (1.0298) |  Loss2: (0.0000) | Acc: (63.00%) (21119/33408)
Epoch: 11 | Batch_idx: 270 |  Loss: (1.0292) |  Loss2: (0.0000) | Acc: (63.00%) (21939/34688)
Epoch: 11 | Batch_idx: 280 |  Loss: (1.0294) |  Loss2: (0.0000) | Acc: (63.00%) (22749/35968)
Epoch: 11 | Batch_idx: 290 |  Loss: (1.0285) |  Loss2: (0.0000) | Acc: (63.00%) (23558/37248)
Epoch: 11 | Batch_idx: 300 |  Loss: (1.0267) |  Loss2: (0.0000) | Acc: (63.00%) (24380/38528)
Epoch: 11 | Batch_idx: 310 |  Loss: (1.0258) |  Loss2: (0.0000) | Acc: (63.00%) (25197/39808)
Epoch: 11 | Batch_idx: 320 |  Loss: (1.0252) |  Loss2: (0.0000) | Acc: (63.00%) (26021/41088)
Epoch: 11 | Batch_idx: 330 |  Loss: (1.0232) |  Loss2: (0.0000) | Acc: (63.00%) (26853/42368)
Epoch: 11 | Batch_idx: 340 |  Loss: (1.0210) |  Loss2: (0.0000) | Acc: (63.00%) (27720/43648)
Epoch: 11 | Batch_idx: 350 |  Loss: (1.0199) |  Loss2: (0.0000) | Acc: (63.00%) (28545/44928)
Epoch: 11 | Batch_idx: 360 |  Loss: (1.0184) |  Loss2: (0.0000) | Acc: (63.00%) (29397/46208)
Epoch: 11 | Batch_idx: 370 |  Loss: (1.0180) |  Loss2: (0.0000) | Acc: (63.00%) (30221/47488)
Epoch: 11 | Batch_idx: 380 |  Loss: (1.0168) |  Loss2: (0.0000) | Acc: (63.00%) (31058/48768)
Epoch: 11 | Batch_idx: 390 |  Loss: (1.0153) |  Loss2: (0.0000) | Acc: (63.00%) (31880/50000)
# TEST : Loss: (1.0851) | Acc: (61.00%) (6132/10000)
percent tensor([0.5172, 0.4828], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4766, 0.5234], device='cuda:0')
percent tensor([0.5317, 0.4683], device='cuda:0')
percent tensor([0.5525, 0.4475], device='cuda:0')
percent tensor([0.5197, 0.4803], device='cuda:0')
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.7967, 0.2033], device='cuda:0')
Epoch: 12 | Batch_idx: 0 |  Loss: (0.9216) |  Loss2: (0.0000) | Acc: (61.00%) (79/128)
Epoch: 12 | Batch_idx: 10 |  Loss: (0.9952) |  Loss2: (0.0000) | Acc: (64.00%) (907/1408)
Epoch: 12 | Batch_idx: 20 |  Loss: (0.9885) |  Loss2: (0.0000) | Acc: (64.00%) (1737/2688)
Epoch: 12 | Batch_idx: 30 |  Loss: (0.9840) |  Loss2: (0.0000) | Acc: (64.00%) (2557/3968)
Epoch: 12 | Batch_idx: 40 |  Loss: (0.9856) |  Loss2: (0.0000) | Acc: (64.00%) (3399/5248)
Epoch: 12 | Batch_idx: 50 |  Loss: (0.9745) |  Loss2: (0.0000) | Acc: (65.00%) (4252/6528)
Epoch: 12 | Batch_idx: 60 |  Loss: (0.9699) |  Loss2: (0.0000) | Acc: (65.00%) (5123/7808)
Epoch: 12 | Batch_idx: 70 |  Loss: (0.9726) |  Loss2: (0.0000) | Acc: (65.00%) (5951/9088)
Epoch: 12 | Batch_idx: 80 |  Loss: (0.9709) |  Loss2: (0.0000) | Acc: (65.00%) (6782/10368)
Epoch: 12 | Batch_idx: 90 |  Loss: (0.9675) |  Loss2: (0.0000) | Acc: (65.00%) (7625/11648)
Epoch: 12 | Batch_idx: 100 |  Loss: (0.9634) |  Loss2: (0.0000) | Acc: (65.00%) (8477/12928)
Epoch: 12 | Batch_idx: 110 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (9360/14208)
Epoch: 12 | Batch_idx: 120 |  Loss: (0.9545) |  Loss2: (0.0000) | Acc: (65.00%) (10215/15488)
Epoch: 12 | Batch_idx: 130 |  Loss: (0.9589) |  Loss2: (0.0000) | Acc: (65.00%) (11045/16768)
Epoch: 12 | Batch_idx: 140 |  Loss: (0.9559) |  Loss2: (0.0000) | Acc: (65.00%) (11898/18048)
Epoch: 12 | Batch_idx: 150 |  Loss: (0.9562) |  Loss2: (0.0000) | Acc: (65.00%) (12743/19328)
Epoch: 12 | Batch_idx: 160 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (13557/20608)
Epoch: 12 | Batch_idx: 170 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (14378/21888)
Epoch: 12 | Batch_idx: 180 |  Loss: (0.9613) |  Loss2: (0.0000) | Acc: (65.00%) (15220/23168)
Epoch: 12 | Batch_idx: 190 |  Loss: (0.9612) |  Loss2: (0.0000) | Acc: (65.00%) (16065/24448)
Epoch: 12 | Batch_idx: 200 |  Loss: (0.9602) |  Loss2: (0.0000) | Acc: (65.00%) (16903/25728)
Epoch: 12 | Batch_idx: 210 |  Loss: (0.9591) |  Loss2: (0.0000) | Acc: (65.00%) (17767/27008)
Epoch: 12 | Batch_idx: 220 |  Loss: (0.9574) |  Loss2: (0.0000) | Acc: (65.00%) (18617/28288)
Epoch: 12 | Batch_idx: 230 |  Loss: (0.9567) |  Loss2: (0.0000) | Acc: (65.00%) (19480/29568)
Epoch: 12 | Batch_idx: 240 |  Loss: (0.9565) |  Loss2: (0.0000) | Acc: (65.00%) (20328/30848)
Epoch: 12 | Batch_idx: 250 |  Loss: (0.9550) |  Loss2: (0.0000) | Acc: (65.00%) (21183/32128)
Epoch: 12 | Batch_idx: 260 |  Loss: (0.9542) |  Loss2: (0.0000) | Acc: (65.00%) (22037/33408)
Epoch: 12 | Batch_idx: 270 |  Loss: (0.9542) |  Loss2: (0.0000) | Acc: (65.00%) (22881/34688)
Epoch: 12 | Batch_idx: 280 |  Loss: (0.9536) |  Loss2: (0.0000) | Acc: (65.00%) (23732/35968)
Epoch: 12 | Batch_idx: 290 |  Loss: (0.9549) |  Loss2: (0.0000) | Acc: (65.00%) (24568/37248)
Epoch: 12 | Batch_idx: 300 |  Loss: (0.9541) |  Loss2: (0.0000) | Acc: (66.00%) (25436/38528)
Epoch: 12 | Batch_idx: 310 |  Loss: (0.9523) |  Loss2: (0.0000) | Acc: (66.00%) (26303/39808)
Epoch: 12 | Batch_idx: 320 |  Loss: (0.9510) |  Loss2: (0.0000) | Acc: (66.00%) (27171/41088)
Epoch: 12 | Batch_idx: 330 |  Loss: (0.9489) |  Loss2: (0.0000) | Acc: (66.00%) (28063/42368)
Epoch: 12 | Batch_idx: 340 |  Loss: (0.9480) |  Loss2: (0.0000) | Acc: (66.00%) (28929/43648)
Epoch: 12 | Batch_idx: 350 |  Loss: (0.9471) |  Loss2: (0.0000) | Acc: (66.00%) (29787/44928)
Epoch: 12 | Batch_idx: 360 |  Loss: (0.9461) |  Loss2: (0.0000) | Acc: (66.00%) (30645/46208)
Epoch: 12 | Batch_idx: 370 |  Loss: (0.9456) |  Loss2: (0.0000) | Acc: (66.00%) (31518/47488)
Epoch: 12 | Batch_idx: 380 |  Loss: (0.9447) |  Loss2: (0.0000) | Acc: (66.00%) (32392/48768)
Epoch: 12 | Batch_idx: 390 |  Loss: (0.9437) |  Loss2: (0.0000) | Acc: (66.00%) (33226/50000)
# TEST : Loss: (0.9457) | Acc: (66.00%) (6659/10000)
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.5037, 0.4963], device='cuda:0')
percent tensor([0.4769, 0.5231], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5189, 0.4811], device='cuda:0')
percent tensor([0.5258, 0.4742], device='cuda:0')
percent tensor([0.7881, 0.2119], device='cuda:0')
Epoch: 13 | Batch_idx: 0 |  Loss: (0.8239) |  Loss2: (0.0000) | Acc: (70.00%) (90/128)
Epoch: 13 | Batch_idx: 10 |  Loss: (0.9286) |  Loss2: (0.0000) | Acc: (66.00%) (938/1408)
Epoch: 13 | Batch_idx: 20 |  Loss: (0.9015) |  Loss2: (0.0000) | Acc: (68.00%) (1832/2688)
Epoch: 13 | Batch_idx: 30 |  Loss: (0.8938) |  Loss2: (0.0000) | Acc: (68.00%) (2700/3968)
Epoch: 13 | Batch_idx: 40 |  Loss: (0.8980) |  Loss2: (0.0000) | Acc: (67.00%) (3566/5248)
Epoch: 13 | Batch_idx: 50 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (4451/6528)
Epoch: 13 | Batch_idx: 60 |  Loss: (0.8900) |  Loss2: (0.0000) | Acc: (68.00%) (5326/7808)
Epoch: 13 | Batch_idx: 70 |  Loss: (0.8910) |  Loss2: (0.0000) | Acc: (68.00%) (6198/9088)
Epoch: 13 | Batch_idx: 80 |  Loss: (0.8894) |  Loss2: (0.0000) | Acc: (68.00%) (7080/10368)
Epoch: 13 | Batch_idx: 90 |  Loss: (0.8815) |  Loss2: (0.0000) | Acc: (68.00%) (7985/11648)
Epoch: 13 | Batch_idx: 100 |  Loss: (0.8879) |  Loss2: (0.0000) | Acc: (68.00%) (8830/12928)
Epoch: 13 | Batch_idx: 110 |  Loss: (0.8903) |  Loss2: (0.0000) | Acc: (68.00%) (9685/14208)
Epoch: 13 | Batch_idx: 120 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (10540/15488)
Epoch: 13 | Batch_idx: 130 |  Loss: (0.8912) |  Loss2: (0.0000) | Acc: (68.00%) (11440/16768)
Epoch: 13 | Batch_idx: 140 |  Loss: (0.8957) |  Loss2: (0.0000) | Acc: (68.00%) (12287/18048)
Epoch: 13 | Batch_idx: 150 |  Loss: (0.8937) |  Loss2: (0.0000) | Acc: (68.00%) (13178/19328)
Epoch: 13 | Batch_idx: 160 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (14051/20608)
Epoch: 13 | Batch_idx: 170 |  Loss: (0.8950) |  Loss2: (0.0000) | Acc: (68.00%) (14928/21888)
Epoch: 13 | Batch_idx: 180 |  Loss: (0.8960) |  Loss2: (0.0000) | Acc: (68.00%) (15787/23168)
Epoch: 13 | Batch_idx: 190 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (16652/24448)
Epoch: 13 | Batch_idx: 200 |  Loss: (0.8965) |  Loss2: (0.0000) | Acc: (68.00%) (17532/25728)
Epoch: 13 | Batch_idx: 210 |  Loss: (0.8970) |  Loss2: (0.0000) | Acc: (68.00%) (18395/27008)
Epoch: 13 | Batch_idx: 220 |  Loss: (0.8954) |  Loss2: (0.0000) | Acc: (68.00%) (19281/28288)
Epoch: 13 | Batch_idx: 230 |  Loss: (0.8949) |  Loss2: (0.0000) | Acc: (68.00%) (20155/29568)
Epoch: 13 | Batch_idx: 240 |  Loss: (0.8939) |  Loss2: (0.0000) | Acc: (68.00%) (21035/30848)
Epoch: 13 | Batch_idx: 250 |  Loss: (0.8941) |  Loss2: (0.0000) | Acc: (68.00%) (21916/32128)
Epoch: 13 | Batch_idx: 260 |  Loss: (0.8953) |  Loss2: (0.0000) | Acc: (68.00%) (22793/33408)
Epoch: 13 | Batch_idx: 270 |  Loss: (0.8944) |  Loss2: (0.0000) | Acc: (68.00%) (23685/34688)
Epoch: 13 | Batch_idx: 280 |  Loss: (0.8935) |  Loss2: (0.0000) | Acc: (68.00%) (24583/35968)
Epoch: 13 | Batch_idx: 290 |  Loss: (0.8916) |  Loss2: (0.0000) | Acc: (68.00%) (25491/37248)
Epoch: 13 | Batch_idx: 300 |  Loss: (0.8914) |  Loss2: (0.0000) | Acc: (68.00%) (26375/38528)
Epoch: 13 | Batch_idx: 310 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (27237/39808)
Epoch: 13 | Batch_idx: 320 |  Loss: (0.8924) |  Loss2: (0.0000) | Acc: (68.00%) (28109/41088)
Epoch: 13 | Batch_idx: 330 |  Loss: (0.8927) |  Loss2: (0.0000) | Acc: (68.00%) (28987/42368)
Epoch: 13 | Batch_idx: 340 |  Loss: (0.8921) |  Loss2: (0.0000) | Acc: (68.00%) (29882/43648)
Epoch: 13 | Batch_idx: 350 |  Loss: (0.8915) |  Loss2: (0.0000) | Acc: (68.00%) (30776/44928)
Epoch: 13 | Batch_idx: 360 |  Loss: (0.8902) |  Loss2: (0.0000) | Acc: (68.00%) (31681/46208)
Epoch: 13 | Batch_idx: 370 |  Loss: (0.8893) |  Loss2: (0.0000) | Acc: (68.00%) (32576/47488)
Epoch: 13 | Batch_idx: 380 |  Loss: (0.8883) |  Loss2: (0.0000) | Acc: (68.00%) (33485/48768)
Epoch: 13 | Batch_idx: 390 |  Loss: (0.8873) |  Loss2: (0.0000) | Acc: (68.00%) (34352/50000)
# TEST : Loss: (0.9341) | Acc: (67.00%) (6725/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4765, 0.5235], device='cuda:0')
percent tensor([0.5310, 0.4690], device='cuda:0')
percent tensor([0.5519, 0.4481], device='cuda:0')
percent tensor([0.5194, 0.4806], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.7998, 0.2002], device='cuda:0')
Epoch: 14 | Batch_idx: 0 |  Loss: (0.7329) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 14 | Batch_idx: 10 |  Loss: (0.8267) |  Loss2: (0.0000) | Acc: (71.00%) (1002/1408)
Epoch: 14 | Batch_idx: 20 |  Loss: (0.8389) |  Loss2: (0.0000) | Acc: (71.00%) (1914/2688)
Epoch: 14 | Batch_idx: 30 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (71.00%) (2819/3968)
Epoch: 14 | Batch_idx: 40 |  Loss: (0.8438) |  Loss2: (0.0000) | Acc: (70.00%) (3694/5248)
Epoch: 14 | Batch_idx: 50 |  Loss: (0.8470) |  Loss2: (0.0000) | Acc: (70.00%) (4587/6528)
Epoch: 14 | Batch_idx: 60 |  Loss: (0.8465) |  Loss2: (0.0000) | Acc: (70.00%) (5498/7808)
Epoch: 14 | Batch_idx: 70 |  Loss: (0.8425) |  Loss2: (0.0000) | Acc: (70.00%) (6396/9088)
Epoch: 14 | Batch_idx: 80 |  Loss: (0.8492) |  Loss2: (0.0000) | Acc: (70.00%) (7284/10368)
Epoch: 14 | Batch_idx: 90 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (70.00%) (8196/11648)
Epoch: 14 | Batch_idx: 100 |  Loss: (0.8395) |  Loss2: (0.0000) | Acc: (70.00%) (9112/12928)
Epoch: 14 | Batch_idx: 110 |  Loss: (0.8426) |  Loss2: (0.0000) | Acc: (70.00%) (9991/14208)
Epoch: 14 | Batch_idx: 120 |  Loss: (0.8426) |  Loss2: (0.0000) | Acc: (70.00%) (10896/15488)
Epoch: 14 | Batch_idx: 130 |  Loss: (0.8412) |  Loss2: (0.0000) | Acc: (70.00%) (11797/16768)
Epoch: 14 | Batch_idx: 140 |  Loss: (0.8431) |  Loss2: (0.0000) | Acc: (70.00%) (12678/18048)
Epoch: 14 | Batch_idx: 150 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (13579/19328)
Epoch: 14 | Batch_idx: 160 |  Loss: (0.8429) |  Loss2: (0.0000) | Acc: (70.00%) (14470/20608)
Epoch: 14 | Batch_idx: 170 |  Loss: (0.8432) |  Loss2: (0.0000) | Acc: (70.00%) (15359/21888)
Epoch: 14 | Batch_idx: 180 |  Loss: (0.8424) |  Loss2: (0.0000) | Acc: (70.00%) (16267/23168)
Epoch: 14 | Batch_idx: 190 |  Loss: (0.8430) |  Loss2: (0.0000) | Acc: (70.00%) (17152/24448)
Epoch: 14 | Batch_idx: 200 |  Loss: (0.8414) |  Loss2: (0.0000) | Acc: (70.00%) (18066/25728)
Epoch: 14 | Batch_idx: 210 |  Loss: (0.8417) |  Loss2: (0.0000) | Acc: (70.00%) (18968/27008)
Epoch: 14 | Batch_idx: 220 |  Loss: (0.8399) |  Loss2: (0.0000) | Acc: (70.00%) (19881/28288)
Epoch: 14 | Batch_idx: 230 |  Loss: (0.8398) |  Loss2: (0.0000) | Acc: (70.00%) (20764/29568)
Epoch: 14 | Batch_idx: 240 |  Loss: (0.8398) |  Loss2: (0.0000) | Acc: (70.00%) (21653/30848)
Epoch: 14 | Batch_idx: 250 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (22557/32128)
Epoch: 14 | Batch_idx: 260 |  Loss: (0.8385) |  Loss2: (0.0000) | Acc: (70.00%) (23467/33408)
Epoch: 14 | Batch_idx: 270 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (24391/34688)
Epoch: 14 | Batch_idx: 280 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (25294/35968)
Epoch: 14 | Batch_idx: 290 |  Loss: (0.8372) |  Loss2: (0.0000) | Acc: (70.00%) (26197/37248)
Epoch: 14 | Batch_idx: 300 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (27136/38528)
Epoch: 14 | Batch_idx: 310 |  Loss: (0.8367) |  Loss2: (0.0000) | Acc: (70.00%) (28029/39808)
Epoch: 14 | Batch_idx: 320 |  Loss: (0.8369) |  Loss2: (0.0000) | Acc: (70.00%) (28921/41088)
Epoch: 14 | Batch_idx: 330 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (29827/42368)
Epoch: 14 | Batch_idx: 340 |  Loss: (0.8357) |  Loss2: (0.0000) | Acc: (70.00%) (30747/43648)
Epoch: 14 | Batch_idx: 350 |  Loss: (0.8341) |  Loss2: (0.0000) | Acc: (70.00%) (31692/44928)
Epoch: 14 | Batch_idx: 360 |  Loss: (0.8348) |  Loss2: (0.0000) | Acc: (70.00%) (32577/46208)
Epoch: 14 | Batch_idx: 370 |  Loss: (0.8347) |  Loss2: (0.0000) | Acc: (70.00%) (33482/47488)
Epoch: 14 | Batch_idx: 380 |  Loss: (0.8338) |  Loss2: (0.0000) | Acc: (70.00%) (34401/48768)
Epoch: 14 | Batch_idx: 390 |  Loss: (0.8324) |  Loss2: (0.0000) | Acc: (70.00%) (35291/50000)
# TEST : Loss: (0.8621) | Acc: (69.00%) (6972/10000)
percent tensor([0.5168, 0.4832], device='cuda:0')
percent tensor([0.5038, 0.4962], device='cuda:0')
percent tensor([0.4773, 0.5227], device='cuda:0')
percent tensor([0.5305, 0.4695], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5186, 0.4814], device='cuda:0')
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.7888, 0.2112], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 15 | Batch_idx: 0 |  Loss: (0.6573) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 15 | Batch_idx: 10 |  Loss: (0.8436) |  Loss2: (0.0000) | Acc: (70.00%) (994/1408)
Epoch: 15 | Batch_idx: 20 |  Loss: (0.9084) |  Loss2: (0.0000) | Acc: (69.00%) (1856/2688)
Epoch: 15 | Batch_idx: 30 |  Loss: (0.9269) |  Loss2: (0.0000) | Acc: (68.00%) (2700/3968)
Epoch: 15 | Batch_idx: 40 |  Loss: (0.9299) |  Loss2: (0.0000) | Acc: (67.00%) (3552/5248)
Epoch: 15 | Batch_idx: 50 |  Loss: (0.9394) |  Loss2: (0.0000) | Acc: (67.00%) (4392/6528)
Epoch: 15 | Batch_idx: 60 |  Loss: (0.9404) |  Loss2: (0.0000) | Acc: (66.00%) (5231/7808)
Epoch: 15 | Batch_idx: 70 |  Loss: (0.9427) |  Loss2: (0.0000) | Acc: (67.00%) (6100/9088)
Epoch: 15 | Batch_idx: 80 |  Loss: (0.9419) |  Loss2: (0.0000) | Acc: (67.00%) (6964/10368)
Epoch: 15 | Batch_idx: 90 |  Loss: (0.9444) |  Loss2: (0.0000) | Acc: (66.00%) (7796/11648)
Epoch: 15 | Batch_idx: 100 |  Loss: (0.9444) |  Loss2: (0.0000) | Acc: (66.00%) (8657/12928)
Epoch: 15 | Batch_idx: 110 |  Loss: (0.9416) |  Loss2: (0.0000) | Acc: (66.00%) (9516/14208)
Epoch: 15 | Batch_idx: 120 |  Loss: (0.9349) |  Loss2: (0.0000) | Acc: (67.00%) (10399/15488)
Epoch: 15 | Batch_idx: 130 |  Loss: (0.9307) |  Loss2: (0.0000) | Acc: (67.00%) (11286/16768)
Epoch: 15 | Batch_idx: 140 |  Loss: (0.9261) |  Loss2: (0.0000) | Acc: (67.00%) (12184/18048)
Epoch: 15 | Batch_idx: 150 |  Loss: (0.9245) |  Loss2: (0.0000) | Acc: (67.00%) (13065/19328)
Epoch: 15 | Batch_idx: 160 |  Loss: (0.9248) |  Loss2: (0.0000) | Acc: (67.00%) (13927/20608)
Epoch: 15 | Batch_idx: 170 |  Loss: (0.9225) |  Loss2: (0.0000) | Acc: (67.00%) (14812/21888)
Epoch: 15 | Batch_idx: 180 |  Loss: (0.9200) |  Loss2: (0.0000) | Acc: (67.00%) (15697/23168)
Epoch: 15 | Batch_idx: 190 |  Loss: (0.9187) |  Loss2: (0.0000) | Acc: (67.00%) (16551/24448)
Epoch: 15 | Batch_idx: 200 |  Loss: (0.9170) |  Loss2: (0.0000) | Acc: (67.00%) (17417/25728)
Epoch: 15 | Batch_idx: 210 |  Loss: (0.9151) |  Loss2: (0.0000) | Acc: (67.00%) (18292/27008)
Epoch: 15 | Batch_idx: 220 |  Loss: (0.9117) |  Loss2: (0.0000) | Acc: (67.00%) (19197/28288)
Epoch: 15 | Batch_idx: 230 |  Loss: (0.9091) |  Loss2: (0.0000) | Acc: (67.00%) (20094/29568)
Epoch: 15 | Batch_idx: 240 |  Loss: (0.9067) |  Loss2: (0.0000) | Acc: (68.00%) (20996/30848)
Epoch: 15 | Batch_idx: 250 |  Loss: (0.9068) |  Loss2: (0.0000) | Acc: (68.00%) (21855/32128)
Epoch: 15 | Batch_idx: 260 |  Loss: (0.9043) |  Loss2: (0.0000) | Acc: (68.00%) (22746/33408)
Epoch: 15 | Batch_idx: 270 |  Loss: (0.9052) |  Loss2: (0.0000) | Acc: (68.00%) (23611/34688)
Epoch: 15 | Batch_idx: 280 |  Loss: (0.9027) |  Loss2: (0.0000) | Acc: (68.00%) (24510/35968)
Epoch: 15 | Batch_idx: 290 |  Loss: (0.9013) |  Loss2: (0.0000) | Acc: (68.00%) (25384/37248)
Epoch: 15 | Batch_idx: 300 |  Loss: (0.9002) |  Loss2: (0.0000) | Acc: (68.00%) (26278/38528)
Epoch: 15 | Batch_idx: 310 |  Loss: (0.8973) |  Loss2: (0.0000) | Acc: (68.00%) (27168/39808)
Epoch: 15 | Batch_idx: 320 |  Loss: (0.8948) |  Loss2: (0.0000) | Acc: (68.00%) (28078/41088)
Epoch: 15 | Batch_idx: 330 |  Loss: (0.8941) |  Loss2: (0.0000) | Acc: (68.00%) (28971/42368)
Epoch: 15 | Batch_idx: 340 |  Loss: (0.8920) |  Loss2: (0.0000) | Acc: (68.00%) (29890/43648)
Epoch: 15 | Batch_idx: 350 |  Loss: (0.8904) |  Loss2: (0.0000) | Acc: (68.00%) (30792/44928)
Epoch: 15 | Batch_idx: 360 |  Loss: (0.8895) |  Loss2: (0.0000) | Acc: (68.00%) (31697/46208)
Epoch: 15 | Batch_idx: 370 |  Loss: (0.8892) |  Loss2: (0.0000) | Acc: (68.00%) (32578/47488)
Epoch: 15 | Batch_idx: 380 |  Loss: (0.8894) |  Loss2: (0.0000) | Acc: (68.00%) (33457/48768)
Epoch: 15 | Batch_idx: 390 |  Loss: (0.8885) |  Loss2: (0.0000) | Acc: (68.00%) (34314/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_015.pth.tar'
# TEST : Loss: (0.8692) | Acc: (69.00%) (6913/10000)
percent tensor([0.5132, 0.4868], device='cuda:0')
percent tensor([0.5084, 0.4916], device='cuda:0')
percent tensor([0.4777, 0.5223], device='cuda:0')
percent tensor([0.5271, 0.4729], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5145, 0.4855], device='cuda:0')
percent tensor([0.5217, 0.4783], device='cuda:0')
percent tensor([0.8609, 0.1391], device='cuda:0')
Epoch: 16 | Batch_idx: 0 |  Loss: (0.7284) |  Loss2: (0.0000) | Acc: (71.00%) (91/128)
Epoch: 16 | Batch_idx: 10 |  Loss: (0.8272) |  Loss2: (0.0000) | Acc: (71.00%) (1000/1408)
Epoch: 16 | Batch_idx: 20 |  Loss: (0.8459) |  Loss2: (0.0000) | Acc: (70.00%) (1888/2688)
Epoch: 16 | Batch_idx: 30 |  Loss: (0.8611) |  Loss2: (0.0000) | Acc: (69.00%) (2763/3968)
Epoch: 16 | Batch_idx: 40 |  Loss: (0.8558) |  Loss2: (0.0000) | Acc: (70.00%) (3689/5248)
Epoch: 16 | Batch_idx: 50 |  Loss: (0.8499) |  Loss2: (0.0000) | Acc: (70.00%) (4592/6528)
Epoch: 16 | Batch_idx: 60 |  Loss: (0.8497) |  Loss2: (0.0000) | Acc: (69.00%) (5464/7808)
Epoch: 16 | Batch_idx: 70 |  Loss: (0.8439) |  Loss2: (0.0000) | Acc: (70.00%) (6378/9088)
Epoch: 16 | Batch_idx: 80 |  Loss: (0.8409) |  Loss2: (0.0000) | Acc: (70.00%) (7287/10368)
Epoch: 16 | Batch_idx: 90 |  Loss: (0.8401) |  Loss2: (0.0000) | Acc: (70.00%) (8194/11648)
Epoch: 16 | Batch_idx: 100 |  Loss: (0.8421) |  Loss2: (0.0000) | Acc: (70.00%) (9074/12928)
Epoch: 16 | Batch_idx: 110 |  Loss: (0.8423) |  Loss2: (0.0000) | Acc: (70.00%) (9981/14208)
Epoch: 16 | Batch_idx: 120 |  Loss: (0.8387) |  Loss2: (0.0000) | Acc: (70.00%) (10903/15488)
Epoch: 16 | Batch_idx: 130 |  Loss: (0.8370) |  Loss2: (0.0000) | Acc: (70.00%) (11811/16768)
Epoch: 16 | Batch_idx: 140 |  Loss: (0.8394) |  Loss2: (0.0000) | Acc: (70.00%) (12694/18048)
Epoch: 16 | Batch_idx: 150 |  Loss: (0.8393) |  Loss2: (0.0000) | Acc: (70.00%) (13607/19328)
Epoch: 16 | Batch_idx: 160 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (70.00%) (14529/20608)
Epoch: 16 | Batch_idx: 170 |  Loss: (0.8381) |  Loss2: (0.0000) | Acc: (70.00%) (15447/21888)
Epoch: 16 | Batch_idx: 180 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (16351/23168)
Epoch: 16 | Batch_idx: 190 |  Loss: (0.8360) |  Loss2: (0.0000) | Acc: (70.00%) (17244/24448)
Epoch: 16 | Batch_idx: 200 |  Loss: (0.8374) |  Loss2: (0.0000) | Acc: (70.00%) (18120/25728)
Epoch: 16 | Batch_idx: 210 |  Loss: (0.8374) |  Loss2: (0.0000) | Acc: (70.00%) (19026/27008)
Epoch: 16 | Batch_idx: 220 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (19918/28288)
Epoch: 16 | Batch_idx: 230 |  Loss: (0.8373) |  Loss2: (0.0000) | Acc: (70.00%) (20809/29568)
Epoch: 16 | Batch_idx: 240 |  Loss: (0.8362) |  Loss2: (0.0000) | Acc: (70.00%) (21707/30848)
Epoch: 16 | Batch_idx: 250 |  Loss: (0.8364) |  Loss2: (0.0000) | Acc: (70.00%) (22604/32128)
Epoch: 16 | Batch_idx: 260 |  Loss: (0.8379) |  Loss2: (0.0000) | Acc: (70.00%) (23474/33408)
Epoch: 16 | Batch_idx: 270 |  Loss: (0.8371) |  Loss2: (0.0000) | Acc: (70.00%) (24388/34688)
Epoch: 16 | Batch_idx: 280 |  Loss: (0.8355) |  Loss2: (0.0000) | Acc: (70.00%) (25307/35968)
Epoch: 16 | Batch_idx: 290 |  Loss: (0.8353) |  Loss2: (0.0000) | Acc: (70.00%) (26206/37248)
Epoch: 16 | Batch_idx: 300 |  Loss: (0.8344) |  Loss2: (0.0000) | Acc: (70.00%) (27124/38528)
Epoch: 16 | Batch_idx: 310 |  Loss: (0.8335) |  Loss2: (0.0000) | Acc: (70.00%) (28058/39808)
Epoch: 16 | Batch_idx: 320 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (28940/41088)
Epoch: 16 | Batch_idx: 330 |  Loss: (0.8328) |  Loss2: (0.0000) | Acc: (70.00%) (29856/42368)
Epoch: 16 | Batch_idx: 340 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (30749/43648)
Epoch: 16 | Batch_idx: 350 |  Loss: (0.8345) |  Loss2: (0.0000) | Acc: (70.00%) (31628/44928)
Epoch: 16 | Batch_idx: 360 |  Loss: (0.8332) |  Loss2: (0.0000) | Acc: (70.00%) (32544/46208)
Epoch: 16 | Batch_idx: 370 |  Loss: (0.8325) |  Loss2: (0.0000) | Acc: (70.00%) (33477/47488)
Epoch: 16 | Batch_idx: 380 |  Loss: (0.8323) |  Loss2: (0.0000) | Acc: (70.00%) (34377/48768)
Epoch: 16 | Batch_idx: 390 |  Loss: (0.8316) |  Loss2: (0.0000) | Acc: (70.00%) (35282/50000)
# TEST : Loss: (0.8483) | Acc: (70.00%) (7027/10000)
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.4787, 0.5213], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5128, 0.4872], device='cuda:0')
percent tensor([0.5238, 0.4762], device='cuda:0')
percent tensor([0.9042, 0.0958], device='cuda:0')
Epoch: 17 | Batch_idx: 0 |  Loss: (0.8239) |  Loss2: (0.0000) | Acc: (74.00%) (95/128)
Epoch: 17 | Batch_idx: 10 |  Loss: (0.8024) |  Loss2: (0.0000) | Acc: (72.00%) (1018/1408)
Epoch: 17 | Batch_idx: 20 |  Loss: (0.8038) |  Loss2: (0.0000) | Acc: (71.00%) (1932/2688)
Epoch: 17 | Batch_idx: 30 |  Loss: (0.8101) |  Loss2: (0.0000) | Acc: (71.00%) (2839/3968)
Epoch: 17 | Batch_idx: 40 |  Loss: (0.8093) |  Loss2: (0.0000) | Acc: (71.00%) (3758/5248)
Epoch: 17 | Batch_idx: 50 |  Loss: (0.8193) |  Loss2: (0.0000) | Acc: (71.00%) (4650/6528)
Epoch: 17 | Batch_idx: 60 |  Loss: (0.8246) |  Loss2: (0.0000) | Acc: (71.00%) (5553/7808)
Epoch: 17 | Batch_idx: 70 |  Loss: (0.8285) |  Loss2: (0.0000) | Acc: (71.00%) (6457/9088)
Epoch: 17 | Batch_idx: 80 |  Loss: (0.8279) |  Loss2: (0.0000) | Acc: (71.00%) (7367/10368)
Epoch: 17 | Batch_idx: 90 |  Loss: (0.8305) |  Loss2: (0.0000) | Acc: (70.00%) (8262/11648)
Epoch: 17 | Batch_idx: 100 |  Loss: (0.8236) |  Loss2: (0.0000) | Acc: (71.00%) (9199/12928)
Epoch: 17 | Batch_idx: 110 |  Loss: (0.8228) |  Loss2: (0.0000) | Acc: (71.00%) (10105/14208)
Epoch: 17 | Batch_idx: 120 |  Loss: (0.8194) |  Loss2: (0.0000) | Acc: (71.00%) (11021/15488)
Epoch: 17 | Batch_idx: 130 |  Loss: (0.8201) |  Loss2: (0.0000) | Acc: (71.00%) (11934/16768)
Epoch: 17 | Batch_idx: 140 |  Loss: (0.8182) |  Loss2: (0.0000) | Acc: (71.00%) (12855/18048)
Epoch: 17 | Batch_idx: 150 |  Loss: (0.8175) |  Loss2: (0.0000) | Acc: (71.00%) (13764/19328)
Epoch: 17 | Batch_idx: 160 |  Loss: (0.8156) |  Loss2: (0.0000) | Acc: (71.00%) (14669/20608)
Epoch: 17 | Batch_idx: 170 |  Loss: (0.8160) |  Loss2: (0.0000) | Acc: (71.00%) (15591/21888)
Epoch: 17 | Batch_idx: 180 |  Loss: (0.8164) |  Loss2: (0.0000) | Acc: (71.00%) (16482/23168)
Epoch: 17 | Batch_idx: 190 |  Loss: (0.8158) |  Loss2: (0.0000) | Acc: (71.00%) (17398/24448)
Epoch: 17 | Batch_idx: 200 |  Loss: (0.8191) |  Loss2: (0.0000) | Acc: (71.00%) (18280/25728)
Epoch: 17 | Batch_idx: 210 |  Loss: (0.8167) |  Loss2: (0.0000) | Acc: (71.00%) (19213/27008)
Epoch: 17 | Batch_idx: 220 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (20134/28288)
Epoch: 17 | Batch_idx: 230 |  Loss: (0.8170) |  Loss2: (0.0000) | Acc: (71.00%) (21043/29568)
Epoch: 17 | Batch_idx: 240 |  Loss: (0.8152) |  Loss2: (0.0000) | Acc: (71.00%) (21978/30848)
Epoch: 17 | Batch_idx: 250 |  Loss: (0.8145) |  Loss2: (0.0000) | Acc: (71.00%) (22893/32128)
Epoch: 17 | Batch_idx: 260 |  Loss: (0.8146) |  Loss2: (0.0000) | Acc: (71.00%) (23802/33408)
Epoch: 17 | Batch_idx: 270 |  Loss: (0.8127) |  Loss2: (0.0000) | Acc: (71.00%) (24750/34688)
Epoch: 17 | Batch_idx: 280 |  Loss: (0.8103) |  Loss2: (0.0000) | Acc: (71.00%) (25687/35968)
Epoch: 17 | Batch_idx: 290 |  Loss: (0.8096) |  Loss2: (0.0000) | Acc: (71.00%) (26606/37248)
Epoch: 17 | Batch_idx: 300 |  Loss: (0.8105) |  Loss2: (0.0000) | Acc: (71.00%) (27504/38528)
Epoch: 17 | Batch_idx: 310 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (28430/39808)
Epoch: 17 | Batch_idx: 320 |  Loss: (0.8105) |  Loss2: (0.0000) | Acc: (71.00%) (29346/41088)
Epoch: 17 | Batch_idx: 330 |  Loss: (0.8114) |  Loss2: (0.0000) | Acc: (71.00%) (30237/42368)
Epoch: 17 | Batch_idx: 340 |  Loss: (0.8104) |  Loss2: (0.0000) | Acc: (71.00%) (31168/43648)
Epoch: 17 | Batch_idx: 350 |  Loss: (0.8107) |  Loss2: (0.0000) | Acc: (71.00%) (32074/44928)
Epoch: 17 | Batch_idx: 360 |  Loss: (0.8098) |  Loss2: (0.0000) | Acc: (71.00%) (32997/46208)
Epoch: 17 | Batch_idx: 370 |  Loss: (0.8102) |  Loss2: (0.0000) | Acc: (71.00%) (33897/47488)
Epoch: 17 | Batch_idx: 380 |  Loss: (0.8100) |  Loss2: (0.0000) | Acc: (71.00%) (34829/48768)
Epoch: 17 | Batch_idx: 390 |  Loss: (0.8109) |  Loss2: (0.0000) | Acc: (71.00%) (35674/50000)
# TEST : Loss: (0.8386) | Acc: (70.00%) (7048/10000)
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5113, 0.4887], device='cuda:0')
percent tensor([0.4786, 0.5214], device='cuda:0')
percent tensor([0.5373, 0.4627], device='cuda:0')
percent tensor([0.5678, 0.4322], device='cuda:0')
percent tensor([0.5127, 0.4873], device='cuda:0')
percent tensor([0.5249, 0.4751], device='cuda:0')
percent tensor([0.9282, 0.0718], device='cuda:0')
Epoch: 18 | Batch_idx: 0 |  Loss: (0.9898) |  Loss2: (0.0000) | Acc: (64.00%) (82/128)
Epoch: 18 | Batch_idx: 10 |  Loss: (0.7518) |  Loss2: (0.0000) | Acc: (73.00%) (1040/1408)
Epoch: 18 | Batch_idx: 20 |  Loss: (0.7782) |  Loss2: (0.0000) | Acc: (72.00%) (1943/2688)
Epoch: 18 | Batch_idx: 30 |  Loss: (0.7898) |  Loss2: (0.0000) | Acc: (71.00%) (2850/3968)
Epoch: 18 | Batch_idx: 40 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (71.00%) (3766/5248)
Epoch: 18 | Batch_idx: 50 |  Loss: (0.7983) |  Loss2: (0.0000) | Acc: (71.00%) (4682/6528)
Epoch: 18 | Batch_idx: 60 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (5580/7808)
Epoch: 18 | Batch_idx: 70 |  Loss: (0.8046) |  Loss2: (0.0000) | Acc: (71.00%) (6490/9088)
Epoch: 18 | Batch_idx: 80 |  Loss: (0.8126) |  Loss2: (0.0000) | Acc: (71.00%) (7366/10368)
Epoch: 18 | Batch_idx: 90 |  Loss: (0.8089) |  Loss2: (0.0000) | Acc: (71.00%) (8300/11648)
Epoch: 18 | Batch_idx: 100 |  Loss: (0.8069) |  Loss2: (0.0000) | Acc: (71.00%) (9226/12928)
Epoch: 18 | Batch_idx: 110 |  Loss: (0.8081) |  Loss2: (0.0000) | Acc: (71.00%) (10125/14208)
Epoch: 18 | Batch_idx: 120 |  Loss: (0.8083) |  Loss2: (0.0000) | Acc: (71.00%) (11023/15488)
Epoch: 18 | Batch_idx: 130 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (11923/16768)
Epoch: 18 | Batch_idx: 140 |  Loss: (0.8078) |  Loss2: (0.0000) | Acc: (71.00%) (12837/18048)
Epoch: 18 | Batch_idx: 150 |  Loss: (0.8049) |  Loss2: (0.0000) | Acc: (71.00%) (13766/19328)
Epoch: 18 | Batch_idx: 160 |  Loss: (0.8041) |  Loss2: (0.0000) | Acc: (71.00%) (14706/20608)
Epoch: 18 | Batch_idx: 170 |  Loss: (0.8033) |  Loss2: (0.0000) | Acc: (71.00%) (15631/21888)
Epoch: 18 | Batch_idx: 180 |  Loss: (0.8046) |  Loss2: (0.0000) | Acc: (71.00%) (16532/23168)
Epoch: 18 | Batch_idx: 190 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (17446/24448)
Epoch: 18 | Batch_idx: 200 |  Loss: (0.8034) |  Loss2: (0.0000) | Acc: (71.00%) (18373/25728)
Epoch: 18 | Batch_idx: 210 |  Loss: (0.8043) |  Loss2: (0.0000) | Acc: (71.00%) (19281/27008)
Epoch: 18 | Batch_idx: 220 |  Loss: (0.8040) |  Loss2: (0.0000) | Acc: (71.00%) (20216/28288)
Epoch: 18 | Batch_idx: 230 |  Loss: (0.8044) |  Loss2: (0.0000) | Acc: (71.00%) (21137/29568)
Epoch: 18 | Batch_idx: 240 |  Loss: (0.8027) |  Loss2: (0.0000) | Acc: (71.00%) (22068/30848)
Epoch: 18 | Batch_idx: 250 |  Loss: (0.8016) |  Loss2: (0.0000) | Acc: (71.00%) (22998/32128)
Epoch: 18 | Batch_idx: 260 |  Loss: (0.8004) |  Loss2: (0.0000) | Acc: (71.00%) (23910/33408)
Epoch: 18 | Batch_idx: 270 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (24827/34688)
Epoch: 18 | Batch_idx: 280 |  Loss: (0.8000) |  Loss2: (0.0000) | Acc: (71.00%) (25747/35968)
Epoch: 18 | Batch_idx: 290 |  Loss: (0.7985) |  Loss2: (0.0000) | Acc: (71.00%) (26682/37248)
Epoch: 18 | Batch_idx: 300 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (27606/38528)
Epoch: 18 | Batch_idx: 310 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (28539/39808)
Epoch: 18 | Batch_idx: 320 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (29435/41088)
Epoch: 18 | Batch_idx: 330 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (30331/42368)
Epoch: 18 | Batch_idx: 340 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (71.00%) (31236/43648)
Epoch: 18 | Batch_idx: 350 |  Loss: (0.7972) |  Loss2: (0.0000) | Acc: (71.00%) (32144/44928)
Epoch: 18 | Batch_idx: 360 |  Loss: (0.7966) |  Loss2: (0.0000) | Acc: (71.00%) (33075/46208)
Epoch: 18 | Batch_idx: 370 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (33950/47488)
Epoch: 18 | Batch_idx: 380 |  Loss: (0.7980) |  Loss2: (0.0000) | Acc: (71.00%) (34873/48768)
Epoch: 18 | Batch_idx: 390 |  Loss: (0.7990) |  Loss2: (0.0000) | Acc: (71.00%) (35739/50000)
# TEST : Loss: (0.8336) | Acc: (70.00%) (7073/10000)
percent tensor([0.5173, 0.4827], device='cuda:0')
percent tensor([0.5114, 0.4886], device='cuda:0')
percent tensor([0.4800, 0.5200], device='cuda:0')
percent tensor([0.5412, 0.4588], device='cuda:0')
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.5131, 0.4869], device='cuda:0')
percent tensor([0.5290, 0.4710], device='cuda:0')
percent tensor([0.9427, 0.0573], device='cuda:0')
Epoch: 19 | Batch_idx: 0 |  Loss: (0.9866) |  Loss2: (0.0000) | Acc: (63.00%) (81/128)
Epoch: 19 | Batch_idx: 10 |  Loss: (0.7946) |  Loss2: (0.0000) | Acc: (72.00%) (1014/1408)
Epoch: 19 | Batch_idx: 20 |  Loss: (0.7809) |  Loss2: (0.0000) | Acc: (71.00%) (1928/2688)
Epoch: 19 | Batch_idx: 30 |  Loss: (0.7802) |  Loss2: (0.0000) | Acc: (71.00%) (2849/3968)
Epoch: 19 | Batch_idx: 40 |  Loss: (0.7768) |  Loss2: (0.0000) | Acc: (71.00%) (3772/5248)
Epoch: 19 | Batch_idx: 50 |  Loss: (0.7677) |  Loss2: (0.0000) | Acc: (72.00%) (4712/6528)
Epoch: 19 | Batch_idx: 60 |  Loss: (0.7738) |  Loss2: (0.0000) | Acc: (71.00%) (5621/7808)
Epoch: 19 | Batch_idx: 70 |  Loss: (0.7762) |  Loss2: (0.0000) | Acc: (71.00%) (6536/9088)
Epoch: 19 | Batch_idx: 80 |  Loss: (0.7826) |  Loss2: (0.0000) | Acc: (71.00%) (7433/10368)
Epoch: 19 | Batch_idx: 90 |  Loss: (0.7811) |  Loss2: (0.0000) | Acc: (71.00%) (8361/11648)
Epoch: 19 | Batch_idx: 100 |  Loss: (0.7837) |  Loss2: (0.0000) | Acc: (71.00%) (9279/12928)
Epoch: 19 | Batch_idx: 110 |  Loss: (0.7828) |  Loss2: (0.0000) | Acc: (71.00%) (10191/14208)
Epoch: 19 | Batch_idx: 120 |  Loss: (0.7900) |  Loss2: (0.0000) | Acc: (71.00%) (11080/15488)
Epoch: 19 | Batch_idx: 130 |  Loss: (0.7916) |  Loss2: (0.0000) | Acc: (71.00%) (11991/16768)
Epoch: 19 | Batch_idx: 140 |  Loss: (0.7935) |  Loss2: (0.0000) | Acc: (71.00%) (12912/18048)
Epoch: 19 | Batch_idx: 150 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (13814/19328)
Epoch: 19 | Batch_idx: 160 |  Loss: (0.7986) |  Loss2: (0.0000) | Acc: (71.00%) (14708/20608)
Epoch: 19 | Batch_idx: 170 |  Loss: (0.7962) |  Loss2: (0.0000) | Acc: (71.00%) (15645/21888)
Epoch: 19 | Batch_idx: 180 |  Loss: (0.7944) |  Loss2: (0.0000) | Acc: (71.00%) (16584/23168)
Epoch: 19 | Batch_idx: 190 |  Loss: (0.7948) |  Loss2: (0.0000) | Acc: (71.00%) (17506/24448)
Epoch: 19 | Batch_idx: 200 |  Loss: (0.7958) |  Loss2: (0.0000) | Acc: (71.00%) (18413/25728)
Epoch: 19 | Batch_idx: 210 |  Loss: (0.7969) |  Loss2: (0.0000) | Acc: (71.00%) (19291/27008)
Epoch: 19 | Batch_idx: 220 |  Loss: (0.7955) |  Loss2: (0.0000) | Acc: (71.00%) (20217/28288)
Epoch: 19 | Batch_idx: 230 |  Loss: (0.7956) |  Loss2: (0.0000) | Acc: (71.00%) (21124/29568)
Epoch: 19 | Batch_idx: 240 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (22015/30848)
Epoch: 19 | Batch_idx: 250 |  Loss: (0.7965) |  Loss2: (0.0000) | Acc: (71.00%) (22946/32128)
Epoch: 19 | Batch_idx: 260 |  Loss: (0.7978) |  Loss2: (0.0000) | Acc: (71.00%) (23855/33408)
Epoch: 19 | Batch_idx: 270 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (24785/34688)
Epoch: 19 | Batch_idx: 280 |  Loss: (0.7975) |  Loss2: (0.0000) | Acc: (71.00%) (25699/35968)
Epoch: 19 | Batch_idx: 290 |  Loss: (0.7968) |  Loss2: (0.0000) | Acc: (71.00%) (26637/37248)
Epoch: 19 | Batch_idx: 300 |  Loss: (0.7966) |  Loss2: (0.0000) | Acc: (71.00%) (27570/38528)
Epoch: 19 | Batch_idx: 310 |  Loss: (0.7981) |  Loss2: (0.0000) | Acc: (71.00%) (28463/39808)
Epoch: 19 | Batch_idx: 320 |  Loss: (0.8002) |  Loss2: (0.0000) | Acc: (71.00%) (29320/41088)
Epoch: 19 | Batch_idx: 330 |  Loss: (0.7995) |  Loss2: (0.0000) | Acc: (71.00%) (30227/42368)
Epoch: 19 | Batch_idx: 340 |  Loss: (0.7992) |  Loss2: (0.0000) | Acc: (71.00%) (31152/43648)
Epoch: 19 | Batch_idx: 350 |  Loss: (0.7996) |  Loss2: (0.0000) | Acc: (71.00%) (32064/44928)
Epoch: 19 | Batch_idx: 360 |  Loss: (0.8001) |  Loss2: (0.0000) | Acc: (71.00%) (32983/46208)
Epoch: 19 | Batch_idx: 370 |  Loss: (0.7987) |  Loss2: (0.0000) | Acc: (71.00%) (33908/47488)
Epoch: 19 | Batch_idx: 380 |  Loss: (0.7988) |  Loss2: (0.0000) | Acc: (71.00%) (34828/48768)
Epoch: 19 | Batch_idx: 390 |  Loss: (0.7979) |  Loss2: (0.0000) | Acc: (71.00%) (35721/50000)
# TEST : Loss: (0.8276) | Acc: (70.00%) (7097/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5111, 0.4889], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5141, 0.4859], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.9501, 0.0499], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 20 | Batch_idx: 0 |  Loss: (0.9664) |  Loss2: (0.0000) | Acc: (67.00%) (87/128)
Epoch: 20 | Batch_idx: 10 |  Loss: (0.8039) |  Loss2: (0.0000) | Acc: (70.00%) (997/1408)
Epoch: 20 | Batch_idx: 20 |  Loss: (0.8090) |  Loss2: (0.0000) | Acc: (71.00%) (1919/2688)
Epoch: 20 | Batch_idx: 30 |  Loss: (0.8052) |  Loss2: (0.0000) | Acc: (71.00%) (2830/3968)
Epoch: 20 | Batch_idx: 40 |  Loss: (0.8032) |  Loss2: (0.0000) | Acc: (71.00%) (3766/5248)
Epoch: 20 | Batch_idx: 50 |  Loss: (0.7930) |  Loss2: (0.0000) | Acc: (72.00%) (4728/6528)
Epoch: 20 | Batch_idx: 60 |  Loss: (0.7911) |  Loss2: (0.0000) | Acc: (72.00%) (5653/7808)
Epoch: 20 | Batch_idx: 70 |  Loss: (0.7945) |  Loss2: (0.0000) | Acc: (72.00%) (6556/9088)
Epoch: 20 | Batch_idx: 80 |  Loss: (0.7906) |  Loss2: (0.0000) | Acc: (72.00%) (7488/10368)
Epoch: 20 | Batch_idx: 90 |  Loss: (0.7911) |  Loss2: (0.0000) | Acc: (72.00%) (8410/11648)
Epoch: 20 | Batch_idx: 100 |  Loss: (0.7919) |  Loss2: (0.0000) | Acc: (72.00%) (9330/12928)
Epoch: 20 | Batch_idx: 110 |  Loss: (0.7928) |  Loss2: (0.0000) | Acc: (72.00%) (10251/14208)
Epoch: 20 | Batch_idx: 120 |  Loss: (0.7932) |  Loss2: (0.0000) | Acc: (71.00%) (11146/15488)
Epoch: 20 | Batch_idx: 130 |  Loss: (0.7916) |  Loss2: (0.0000) | Acc: (71.00%) (12072/16768)
Epoch: 20 | Batch_idx: 140 |  Loss: (0.7850) |  Loss2: (0.0000) | Acc: (72.00%) (13033/18048)
Epoch: 20 | Batch_idx: 150 |  Loss: (0.7860) |  Loss2: (0.0000) | Acc: (72.00%) (13943/19328)
Epoch: 20 | Batch_idx: 160 |  Loss: (0.7873) |  Loss2: (0.0000) | Acc: (72.00%) (14876/20608)
Epoch: 20 | Batch_idx: 170 |  Loss: (0.7887) |  Loss2: (0.0000) | Acc: (72.00%) (15787/21888)
Epoch: 20 | Batch_idx: 180 |  Loss: (0.7903) |  Loss2: (0.0000) | Acc: (72.00%) (16705/23168)
Epoch: 20 | Batch_idx: 190 |  Loss: (0.7905) |  Loss2: (0.0000) | Acc: (72.00%) (17624/24448)
Epoch: 20 | Batch_idx: 200 |  Loss: (0.7912) |  Loss2: (0.0000) | Acc: (72.00%) (18554/25728)
Epoch: 20 | Batch_idx: 210 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (72.00%) (19482/27008)
Epoch: 20 | Batch_idx: 220 |  Loss: (0.7907) |  Loss2: (0.0000) | Acc: (72.00%) (20419/28288)
Epoch: 20 | Batch_idx: 230 |  Loss: (0.7886) |  Loss2: (0.0000) | Acc: (72.00%) (21375/29568)
Epoch: 20 | Batch_idx: 240 |  Loss: (0.7882) |  Loss2: (0.0000) | Acc: (72.00%) (22302/30848)
Epoch: 20 | Batch_idx: 250 |  Loss: (0.7901) |  Loss2: (0.0000) | Acc: (72.00%) (23209/32128)
Epoch: 20 | Batch_idx: 260 |  Loss: (0.7915) |  Loss2: (0.0000) | Acc: (72.00%) (24104/33408)
Epoch: 20 | Batch_idx: 270 |  Loss: (0.7922) |  Loss2: (0.0000) | Acc: (72.00%) (24997/34688)
Epoch: 20 | Batch_idx: 280 |  Loss: (0.7924) |  Loss2: (0.0000) | Acc: (72.00%) (25911/35968)
Epoch: 20 | Batch_idx: 290 |  Loss: (0.7914) |  Loss2: (0.0000) | Acc: (72.00%) (26833/37248)
Epoch: 20 | Batch_idx: 300 |  Loss: (0.7917) |  Loss2: (0.0000) | Acc: (72.00%) (27747/38528)
Epoch: 20 | Batch_idx: 310 |  Loss: (0.7908) |  Loss2: (0.0000) | Acc: (72.00%) (28692/39808)
Epoch: 20 | Batch_idx: 320 |  Loss: (0.7896) |  Loss2: (0.0000) | Acc: (72.00%) (29630/41088)
Epoch: 20 | Batch_idx: 330 |  Loss: (0.7884) |  Loss2: (0.0000) | Acc: (72.00%) (30579/42368)
Epoch: 20 | Batch_idx: 340 |  Loss: (0.7867) |  Loss2: (0.0000) | Acc: (72.00%) (31529/43648)
Epoch: 20 | Batch_idx: 350 |  Loss: (0.7872) |  Loss2: (0.0000) | Acc: (72.00%) (32456/44928)
Epoch: 20 | Batch_idx: 360 |  Loss: (0.7877) |  Loss2: (0.0000) | Acc: (72.00%) (33370/46208)
Epoch: 20 | Batch_idx: 370 |  Loss: (0.7874) |  Loss2: (0.0000) | Acc: (72.00%) (34296/47488)
Epoch: 20 | Batch_idx: 380 |  Loss: (0.7875) |  Loss2: (0.0000) | Acc: (72.00%) (35225/48768)
Epoch: 20 | Batch_idx: 390 |  Loss: (0.7863) |  Loss2: (0.0000) | Acc: (72.00%) (36136/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_020.pth.tar'
# TEST : Loss: (0.8718) | Acc: (70.00%) (7041/10000)
percent tensor([0.5202, 0.4798], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4818, 0.5182], device='cuda:0')
percent tensor([0.5424, 0.4576], device='cuda:0')
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5143, 0.4857], device='cuda:0')
percent tensor([0.5346, 0.4654], device='cuda:0')
percent tensor([0.9492, 0.0508], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(170.0464, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(781.2100, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(776.6609, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.9296, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(506.3695, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2172.0833, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4317.7002, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1434.2269, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6090.0811, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12182.5391, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4055.2380, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17200.2910, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 21 | Batch_idx: 0 |  Loss: (0.7584) |  Loss2: (0.0000) | Acc: (73.00%) (94/128)
Epoch: 21 | Batch_idx: 10 |  Loss: (0.7366) |  Loss2: (0.0000) | Acc: (73.00%) (1028/1408)
Epoch: 21 | Batch_idx: 20 |  Loss: (0.7222) |  Loss2: (0.0000) | Acc: (74.00%) (1991/2688)
Epoch: 21 | Batch_idx: 30 |  Loss: (0.7383) |  Loss2: (0.0000) | Acc: (73.00%) (2921/3968)
Epoch: 21 | Batch_idx: 40 |  Loss: (0.7545) |  Loss2: (0.0000) | Acc: (72.00%) (3830/5248)
Epoch: 21 | Batch_idx: 50 |  Loss: (0.7461) |  Loss2: (0.0000) | Acc: (73.00%) (4788/6528)
Epoch: 21 | Batch_idx: 60 |  Loss: (0.7403) |  Loss2: (0.0000) | Acc: (73.00%) (5755/7808)
Epoch: 21 | Batch_idx: 70 |  Loss: (0.7429) |  Loss2: (0.0000) | Acc: (73.00%) (6698/9088)
Epoch: 21 | Batch_idx: 80 |  Loss: (0.7450) |  Loss2: (0.0000) | Acc: (73.00%) (7630/10368)
Epoch: 21 | Batch_idx: 90 |  Loss: (0.7424) |  Loss2: (0.0000) | Acc: (73.00%) (8587/11648)
Epoch: 21 | Batch_idx: 100 |  Loss: (0.7423) |  Loss2: (0.0000) | Acc: (73.00%) (9538/12928)
Epoch: 21 | Batch_idx: 110 |  Loss: (0.7432) |  Loss2: (0.0000) | Acc: (73.00%) (10491/14208)
Epoch: 21 | Batch_idx: 120 |  Loss: (0.7454) |  Loss2: (0.0000) | Acc: (73.00%) (11422/15488)
Epoch: 21 | Batch_idx: 130 |  Loss: (0.7495) |  Loss2: (0.0000) | Acc: (73.00%) (12344/16768)
Epoch: 21 | Batch_idx: 140 |  Loss: (0.7486) |  Loss2: (0.0000) | Acc: (73.00%) (13300/18048)
Epoch: 21 | Batch_idx: 150 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (14243/19328)
Epoch: 21 | Batch_idx: 160 |  Loss: (0.7459) |  Loss2: (0.0000) | Acc: (73.00%) (15209/20608)
Epoch: 21 | Batch_idx: 170 |  Loss: (0.7453) |  Loss2: (0.0000) | Acc: (73.00%) (16132/21888)
Epoch: 21 | Batch_idx: 180 |  Loss: (0.7441) |  Loss2: (0.0000) | Acc: (73.00%) (17101/23168)
Epoch: 21 | Batch_idx: 190 |  Loss: (0.7457) |  Loss2: (0.0000) | Acc: (73.00%) (18020/24448)
Epoch: 21 | Batch_idx: 200 |  Loss: (0.7464) |  Loss2: (0.0000) | Acc: (73.00%) (18954/25728)
Epoch: 21 | Batch_idx: 210 |  Loss: (0.7478) |  Loss2: (0.0000) | Acc: (73.00%) (19891/27008)
Epoch: 21 | Batch_idx: 220 |  Loss: (0.7496) |  Loss2: (0.0000) | Acc: (73.00%) (20820/28288)
Epoch: 21 | Batch_idx: 230 |  Loss: (0.7489) |  Loss2: (0.0000) | Acc: (73.00%) (21780/29568)
Epoch: 21 | Batch_idx: 240 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (22717/30848)
Epoch: 21 | Batch_idx: 250 |  Loss: (0.7466) |  Loss2: (0.0000) | Acc: (73.00%) (23651/32128)
Epoch: 21 | Batch_idx: 260 |  Loss: (0.7467) |  Loss2: (0.0000) | Acc: (73.00%) (24590/33408)
Epoch: 21 | Batch_idx: 270 |  Loss: (0.7485) |  Loss2: (0.0000) | Acc: (73.00%) (25521/34688)
Epoch: 21 | Batch_idx: 280 |  Loss: (0.7487) |  Loss2: (0.0000) | Acc: (73.00%) (26455/35968)
Epoch: 21 | Batch_idx: 290 |  Loss: (0.7481) |  Loss2: (0.0000) | Acc: (73.00%) (27398/37248)
Epoch: 21 | Batch_idx: 300 |  Loss: (0.7473) |  Loss2: (0.0000) | Acc: (73.00%) (28351/38528)
Epoch: 21 | Batch_idx: 310 |  Loss: (0.7471) |  Loss2: (0.0000) | Acc: (73.00%) (29296/39808)
Epoch: 21 | Batch_idx: 320 |  Loss: (0.7456) |  Loss2: (0.0000) | Acc: (73.00%) (30275/41088)
Epoch: 21 | Batch_idx: 330 |  Loss: (0.7453) |  Loss2: (0.0000) | Acc: (73.00%) (31226/42368)
Epoch: 21 | Batch_idx: 340 |  Loss: (0.7451) |  Loss2: (0.0000) | Acc: (73.00%) (32178/43648)
Epoch: 21 | Batch_idx: 350 |  Loss: (0.7445) |  Loss2: (0.0000) | Acc: (73.00%) (33141/44928)
Epoch: 21 | Batch_idx: 360 |  Loss: (0.7438) |  Loss2: (0.0000) | Acc: (73.00%) (34103/46208)
Epoch: 21 | Batch_idx: 370 |  Loss: (0.7430) |  Loss2: (0.0000) | Acc: (73.00%) (35056/47488)
Epoch: 21 | Batch_idx: 380 |  Loss: (0.7418) |  Loss2: (0.0000) | Acc: (73.00%) (36032/48768)
Epoch: 21 | Batch_idx: 390 |  Loss: (0.7402) |  Loss2: (0.0000) | Acc: (73.00%) (36969/50000)
# TEST : Loss: (0.8035) | Acc: (72.00%) (7216/10000)
percent tensor([0.5201, 0.4799], device='cuda:0')
percent tensor([0.5107, 0.4893], device='cuda:0')
percent tensor([0.4824, 0.5176], device='cuda:0')
percent tensor([0.5420, 0.4580], device='cuda:0')
percent tensor([0.5761, 0.4239], device='cuda:0')
percent tensor([0.5139, 0.4861], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.9519, 0.0481], device='cuda:0')
Epoch: 22 | Batch_idx: 0 |  Loss: (0.6565) |  Loss2: (0.0000) | Acc: (76.00%) (98/128)
Epoch: 22 | Batch_idx: 10 |  Loss: (0.6683) |  Loss2: (0.0000) | Acc: (76.00%) (1083/1408)
Epoch: 22 | Batch_idx: 20 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (2031/2688)
Epoch: 22 | Batch_idx: 30 |  Loss: (0.6920) |  Loss2: (0.0000) | Acc: (75.00%) (3010/3968)
Epoch: 22 | Batch_idx: 40 |  Loss: (0.6935) |  Loss2: (0.0000) | Acc: (75.00%) (3964/5248)
Epoch: 22 | Batch_idx: 50 |  Loss: (0.6908) |  Loss2: (0.0000) | Acc: (75.00%) (4952/6528)
Epoch: 22 | Batch_idx: 60 |  Loss: (0.6928) |  Loss2: (0.0000) | Acc: (75.00%) (5925/7808)
Epoch: 22 | Batch_idx: 70 |  Loss: (0.6881) |  Loss2: (0.0000) | Acc: (76.00%) (6920/9088)
Epoch: 22 | Batch_idx: 80 |  Loss: (0.6918) |  Loss2: (0.0000) | Acc: (76.00%) (7887/10368)
Epoch: 22 | Batch_idx: 90 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (8814/11648)
Epoch: 22 | Batch_idx: 100 |  Loss: (0.6967) |  Loss2: (0.0000) | Acc: (75.00%) (9802/12928)
Epoch: 22 | Batch_idx: 110 |  Loss: (0.6975) |  Loss2: (0.0000) | Acc: (75.00%) (10761/14208)
Epoch: 22 | Batch_idx: 120 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (11737/15488)
Epoch: 22 | Batch_idx: 130 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (75.00%) (12686/16768)
Epoch: 22 | Batch_idx: 140 |  Loss: (0.7016) |  Loss2: (0.0000) | Acc: (75.00%) (13642/18048)
Epoch: 22 | Batch_idx: 150 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (14632/19328)
Epoch: 22 | Batch_idx: 160 |  Loss: (0.7017) |  Loss2: (0.0000) | Acc: (75.00%) (15596/20608)
Epoch: 22 | Batch_idx: 170 |  Loss: (0.7060) |  Loss2: (0.0000) | Acc: (75.00%) (16529/21888)
Epoch: 22 | Batch_idx: 180 |  Loss: (0.7044) |  Loss2: (0.0000) | Acc: (75.00%) (17500/23168)
Epoch: 22 | Batch_idx: 190 |  Loss: (0.7030) |  Loss2: (0.0000) | Acc: (75.00%) (18481/24448)
Epoch: 22 | Batch_idx: 200 |  Loss: (0.7028) |  Loss2: (0.0000) | Acc: (75.00%) (19433/25728)
Epoch: 22 | Batch_idx: 210 |  Loss: (0.7027) |  Loss2: (0.0000) | Acc: (75.00%) (20384/27008)
Epoch: 22 | Batch_idx: 220 |  Loss: (0.7028) |  Loss2: (0.0000) | Acc: (75.00%) (21339/28288)
Epoch: 22 | Batch_idx: 230 |  Loss: (0.7014) |  Loss2: (0.0000) | Acc: (75.00%) (22317/29568)
Epoch: 22 | Batch_idx: 240 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (23283/30848)
Epoch: 22 | Batch_idx: 250 |  Loss: (0.6994) |  Loss2: (0.0000) | Acc: (75.00%) (24254/32128)
Epoch: 22 | Batch_idx: 260 |  Loss: (0.6982) |  Loss2: (0.0000) | Acc: (75.00%) (25243/33408)
Epoch: 22 | Batch_idx: 270 |  Loss: (0.6984) |  Loss2: (0.0000) | Acc: (75.00%) (26219/34688)
Epoch: 22 | Batch_idx: 280 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (27191/35968)
Epoch: 22 | Batch_idx: 290 |  Loss: (0.7000) |  Loss2: (0.0000) | Acc: (75.00%) (28148/37248)
Epoch: 22 | Batch_idx: 300 |  Loss: (0.6994) |  Loss2: (0.0000) | Acc: (75.00%) (29121/38528)
Epoch: 22 | Batch_idx: 310 |  Loss: (0.6992) |  Loss2: (0.0000) | Acc: (75.00%) (30103/39808)
Epoch: 22 | Batch_idx: 320 |  Loss: (0.6995) |  Loss2: (0.0000) | Acc: (75.00%) (31061/41088)
Epoch: 22 | Batch_idx: 330 |  Loss: (0.6995) |  Loss2: (0.0000) | Acc: (75.00%) (32024/42368)
Epoch: 22 | Batch_idx: 340 |  Loss: (0.6997) |  Loss2: (0.0000) | Acc: (75.00%) (32988/43648)
Epoch: 22 | Batch_idx: 350 |  Loss: (0.6988) |  Loss2: (0.0000) | Acc: (75.00%) (33946/44928)
Epoch: 22 | Batch_idx: 360 |  Loss: (0.6975) |  Loss2: (0.0000) | Acc: (75.00%) (34932/46208)
Epoch: 22 | Batch_idx: 370 |  Loss: (0.6964) |  Loss2: (0.0000) | Acc: (75.00%) (35923/47488)
Epoch: 22 | Batch_idx: 380 |  Loss: (0.6954) |  Loss2: (0.0000) | Acc: (75.00%) (36908/48768)
Epoch: 22 | Batch_idx: 390 |  Loss: (0.6943) |  Loss2: (0.0000) | Acc: (75.00%) (37840/50000)
# TEST : Loss: (0.7787) | Acc: (73.00%) (7314/10000)
percent tensor([0.5199, 0.4801], device='cuda:0')
percent tensor([0.5109, 0.4891], device='cuda:0')
percent tensor([0.4814, 0.5186], device='cuda:0')
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5778, 0.4222], device='cuda:0')
percent tensor([0.5151, 0.4849], device='cuda:0')
percent tensor([0.5371, 0.4629], device='cuda:0')
percent tensor([0.9601, 0.0399], device='cuda:0')
Epoch: 23 | Batch_idx: 0 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 23 | Batch_idx: 10 |  Loss: (0.6052) |  Loss2: (0.0000) | Acc: (78.00%) (1101/1408)
Epoch: 23 | Batch_idx: 20 |  Loss: (0.6387) |  Loss2: (0.0000) | Acc: (77.00%) (2089/2688)
Epoch: 23 | Batch_idx: 30 |  Loss: (0.6345) |  Loss2: (0.0000) | Acc: (77.00%) (3080/3968)
Epoch: 23 | Batch_idx: 40 |  Loss: (0.6425) |  Loss2: (0.0000) | Acc: (77.00%) (4063/5248)
Epoch: 23 | Batch_idx: 50 |  Loss: (0.6421) |  Loss2: (0.0000) | Acc: (77.00%) (5056/6528)
Epoch: 23 | Batch_idx: 60 |  Loss: (0.6399) |  Loss2: (0.0000) | Acc: (77.00%) (6055/7808)
Epoch: 23 | Batch_idx: 70 |  Loss: (0.6446) |  Loss2: (0.0000) | Acc: (77.00%) (7034/9088)
Epoch: 23 | Batch_idx: 80 |  Loss: (0.6427) |  Loss2: (0.0000) | Acc: (77.00%) (8039/10368)
Epoch: 23 | Batch_idx: 90 |  Loss: (0.6460) |  Loss2: (0.0000) | Acc: (77.00%) (9006/11648)
Epoch: 23 | Batch_idx: 100 |  Loss: (0.6459) |  Loss2: (0.0000) | Acc: (77.00%) (10009/12928)
Epoch: 23 | Batch_idx: 110 |  Loss: (0.6472) |  Loss2: (0.0000) | Acc: (77.00%) (10995/14208)
Epoch: 23 | Batch_idx: 120 |  Loss: (0.6476) |  Loss2: (0.0000) | Acc: (77.00%) (11986/15488)
Epoch: 23 | Batch_idx: 130 |  Loss: (0.6498) |  Loss2: (0.0000) | Acc: (77.00%) (12953/16768)
Epoch: 23 | Batch_idx: 140 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (13919/18048)
Epoch: 23 | Batch_idx: 150 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (14906/19328)
Epoch: 23 | Batch_idx: 160 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (15891/20608)
Epoch: 23 | Batch_idx: 170 |  Loss: (0.6540) |  Loss2: (0.0000) | Acc: (77.00%) (16881/21888)
Epoch: 23 | Batch_idx: 180 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (17858/23168)
Epoch: 23 | Batch_idx: 190 |  Loss: (0.6550) |  Loss2: (0.0000) | Acc: (77.00%) (18850/24448)
Epoch: 23 | Batch_idx: 200 |  Loss: (0.6562) |  Loss2: (0.0000) | Acc: (77.00%) (19832/25728)
Epoch: 23 | Batch_idx: 210 |  Loss: (0.6562) |  Loss2: (0.0000) | Acc: (77.00%) (20821/27008)
Epoch: 23 | Batch_idx: 220 |  Loss: (0.6545) |  Loss2: (0.0000) | Acc: (77.00%) (21826/28288)
Epoch: 23 | Batch_idx: 230 |  Loss: (0.6530) |  Loss2: (0.0000) | Acc: (77.00%) (22842/29568)
Epoch: 23 | Batch_idx: 240 |  Loss: (0.6542) |  Loss2: (0.0000) | Acc: (77.00%) (23807/30848)
Epoch: 23 | Batch_idx: 250 |  Loss: (0.6549) |  Loss2: (0.0000) | Acc: (77.00%) (24786/32128)
Epoch: 23 | Batch_idx: 260 |  Loss: (0.6541) |  Loss2: (0.0000) | Acc: (77.00%) (25784/33408)
Epoch: 23 | Batch_idx: 270 |  Loss: (0.6530) |  Loss2: (0.0000) | Acc: (77.00%) (26787/34688)
Epoch: 23 | Batch_idx: 280 |  Loss: (0.6528) |  Loss2: (0.0000) | Acc: (77.00%) (27774/35968)
Epoch: 23 | Batch_idx: 290 |  Loss: (0.6516) |  Loss2: (0.0000) | Acc: (77.00%) (28783/37248)
Epoch: 23 | Batch_idx: 300 |  Loss: (0.6532) |  Loss2: (0.0000) | Acc: (77.00%) (29762/38528)
Epoch: 23 | Batch_idx: 310 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (30753/39808)
Epoch: 23 | Batch_idx: 320 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (31754/41088)
Epoch: 23 | Batch_idx: 330 |  Loss: (0.6527) |  Loss2: (0.0000) | Acc: (77.00%) (32739/42368)
Epoch: 23 | Batch_idx: 340 |  Loss: (0.6534) |  Loss2: (0.0000) | Acc: (77.00%) (33713/43648)
Epoch: 23 | Batch_idx: 350 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (34695/44928)
Epoch: 23 | Batch_idx: 360 |  Loss: (0.6553) |  Loss2: (0.0000) | Acc: (77.00%) (35666/46208)
Epoch: 23 | Batch_idx: 370 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (36668/47488)
Epoch: 23 | Batch_idx: 380 |  Loss: (0.6536) |  Loss2: (0.0000) | Acc: (77.00%) (37690/48768)
Epoch: 23 | Batch_idx: 390 |  Loss: (0.6539) |  Loss2: (0.0000) | Acc: (77.00%) (38645/50000)
# TEST : Loss: (0.7714) | Acc: (74.00%) (7431/10000)
percent tensor([0.5196, 0.4804], device='cuda:0')
percent tensor([0.5110, 0.4890], device='cuda:0')
percent tensor([0.4823, 0.5177], device='cuda:0')
percent tensor([0.5425, 0.4575], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5146, 0.4854], device='cuda:0')
percent tensor([0.5372, 0.4628], device='cuda:0')
percent tensor([0.9588, 0.0412], device='cuda:0')
Epoch: 24 | Batch_idx: 0 |  Loss: (0.6985) |  Loss2: (0.0000) | Acc: (77.00%) (99/128)
Epoch: 24 | Batch_idx: 10 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (1110/1408)
Epoch: 24 | Batch_idx: 20 |  Loss: (0.6223) |  Loss2: (0.0000) | Acc: (78.00%) (2119/2688)
Epoch: 24 | Batch_idx: 30 |  Loss: (0.6277) |  Loss2: (0.0000) | Acc: (78.00%) (3120/3968)
Epoch: 24 | Batch_idx: 40 |  Loss: (0.6243) |  Loss2: (0.0000) | Acc: (78.00%) (4136/5248)
Epoch: 24 | Batch_idx: 50 |  Loss: (0.6173) |  Loss2: (0.0000) | Acc: (79.00%) (5159/6528)
Epoch: 24 | Batch_idx: 60 |  Loss: (0.6233) |  Loss2: (0.0000) | Acc: (78.00%) (6149/7808)
Epoch: 24 | Batch_idx: 70 |  Loss: (0.6278) |  Loss2: (0.0000) | Acc: (78.00%) (7140/9088)
Epoch: 24 | Batch_idx: 80 |  Loss: (0.6327) |  Loss2: (0.0000) | Acc: (78.00%) (8126/10368)
Epoch: 24 | Batch_idx: 90 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (78.00%) (9145/11648)
Epoch: 24 | Batch_idx: 100 |  Loss: (0.6293) |  Loss2: (0.0000) | Acc: (78.00%) (10151/12928)
Epoch: 24 | Batch_idx: 110 |  Loss: (0.6248) |  Loss2: (0.0000) | Acc: (78.00%) (11170/14208)
Epoch: 24 | Batch_idx: 120 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (12152/15488)
Epoch: 24 | Batch_idx: 130 |  Loss: (0.6255) |  Loss2: (0.0000) | Acc: (78.00%) (13149/16768)
Epoch: 24 | Batch_idx: 140 |  Loss: (0.6258) |  Loss2: (0.0000) | Acc: (78.00%) (14157/18048)
Epoch: 24 | Batch_idx: 150 |  Loss: (0.6271) |  Loss2: (0.0000) | Acc: (78.00%) (15151/19328)
Epoch: 24 | Batch_idx: 160 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (16153/20608)
Epoch: 24 | Batch_idx: 170 |  Loss: (0.6262) |  Loss2: (0.0000) | Acc: (78.00%) (17149/21888)
Epoch: 24 | Batch_idx: 180 |  Loss: (0.6245) |  Loss2: (0.0000) | Acc: (78.00%) (18175/23168)
Epoch: 24 | Batch_idx: 190 |  Loss: (0.6242) |  Loss2: (0.0000) | Acc: (78.00%) (19179/24448)
Epoch: 24 | Batch_idx: 200 |  Loss: (0.6235) |  Loss2: (0.0000) | Acc: (78.00%) (20189/25728)
Epoch: 24 | Batch_idx: 210 |  Loss: (0.6200) |  Loss2: (0.0000) | Acc: (78.00%) (21234/27008)
Epoch: 24 | Batch_idx: 220 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (22227/28288)
Epoch: 24 | Batch_idx: 230 |  Loss: (0.6199) |  Loss2: (0.0000) | Acc: (78.00%) (23232/29568)
Epoch: 24 | Batch_idx: 240 |  Loss: (0.6215) |  Loss2: (0.0000) | Acc: (78.00%) (24222/30848)
Epoch: 24 | Batch_idx: 250 |  Loss: (0.6214) |  Loss2: (0.0000) | Acc: (78.00%) (25221/32128)
Epoch: 24 | Batch_idx: 260 |  Loss: (0.6217) |  Loss2: (0.0000) | Acc: (78.00%) (26225/33408)
Epoch: 24 | Batch_idx: 270 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (78.00%) (27244/34688)
Epoch: 24 | Batch_idx: 280 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (28237/35968)
Epoch: 24 | Batch_idx: 290 |  Loss: (0.6213) |  Loss2: (0.0000) | Acc: (78.00%) (29235/37248)
Epoch: 24 | Batch_idx: 300 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (30254/38528)
Epoch: 24 | Batch_idx: 310 |  Loss: (0.6195) |  Loss2: (0.0000) | Acc: (78.00%) (31262/39808)
Epoch: 24 | Batch_idx: 320 |  Loss: (0.6201) |  Loss2: (0.0000) | Acc: (78.00%) (32264/41088)
Epoch: 24 | Batch_idx: 330 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (78.00%) (33291/42368)
Epoch: 24 | Batch_idx: 340 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (34265/43648)
Epoch: 24 | Batch_idx: 350 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (78.00%) (35268/44928)
Epoch: 24 | Batch_idx: 360 |  Loss: (0.6204) |  Loss2: (0.0000) | Acc: (78.00%) (36291/46208)
Epoch: 24 | Batch_idx: 370 |  Loss: (0.6210) |  Loss2: (0.0000) | Acc: (78.00%) (37288/47488)
Epoch: 24 | Batch_idx: 380 |  Loss: (0.6205) |  Loss2: (0.0000) | Acc: (78.00%) (38301/48768)
Epoch: 24 | Batch_idx: 390 |  Loss: (0.6209) |  Loss2: (0.0000) | Acc: (78.00%) (39260/50000)
# TEST : Loss: (0.7882) | Acc: (73.00%) (7355/10000)
percent tensor([0.5193, 0.4807], device='cuda:0')
percent tensor([0.5106, 0.4894], device='cuda:0')
percent tensor([0.4820, 0.5180], device='cuda:0')
percent tensor([0.5418, 0.4582], device='cuda:0')
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5140, 0.4860], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.9552, 0.0448], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 25 | Batch_idx: 0 |  Loss: (0.5369) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 25 | Batch_idx: 10 |  Loss: (0.6326) |  Loss2: (0.0000) | Acc: (77.00%) (1098/1408)
Epoch: 25 | Batch_idx: 20 |  Loss: (0.6712) |  Loss2: (0.0000) | Acc: (76.00%) (2057/2688)
Epoch: 25 | Batch_idx: 30 |  Loss: (0.6960) |  Loss2: (0.0000) | Acc: (75.00%) (2999/3968)
Epoch: 25 | Batch_idx: 40 |  Loss: (0.7193) |  Loss2: (0.0000) | Acc: (74.00%) (3927/5248)
Epoch: 25 | Batch_idx: 50 |  Loss: (0.7286) |  Loss2: (0.0000) | Acc: (74.00%) (4875/6528)
Epoch: 25 | Batch_idx: 60 |  Loss: (0.7269) |  Loss2: (0.0000) | Acc: (74.00%) (5841/7808)
Epoch: 25 | Batch_idx: 70 |  Loss: (0.7367) |  Loss2: (0.0000) | Acc: (74.00%) (6778/9088)
Epoch: 25 | Batch_idx: 80 |  Loss: (0.7374) |  Loss2: (0.0000) | Acc: (74.00%) (7745/10368)
Epoch: 25 | Batch_idx: 90 |  Loss: (0.7317) |  Loss2: (0.0000) | Acc: (74.00%) (8726/11648)
Epoch: 25 | Batch_idx: 100 |  Loss: (0.7318) |  Loss2: (0.0000) | Acc: (74.00%) (9672/12928)
Epoch: 25 | Batch_idx: 110 |  Loss: (0.7305) |  Loss2: (0.0000) | Acc: (74.00%) (10627/14208)
Epoch: 25 | Batch_idx: 120 |  Loss: (0.7275) |  Loss2: (0.0000) | Acc: (74.00%) (11602/15488)
Epoch: 25 | Batch_idx: 130 |  Loss: (0.7225) |  Loss2: (0.0000) | Acc: (75.00%) (12588/16768)
Epoch: 25 | Batch_idx: 140 |  Loss: (0.7200) |  Loss2: (0.0000) | Acc: (75.00%) (13560/18048)
Epoch: 25 | Batch_idx: 150 |  Loss: (0.7183) |  Loss2: (0.0000) | Acc: (75.00%) (14511/19328)
Epoch: 25 | Batch_idx: 160 |  Loss: (0.7169) |  Loss2: (0.0000) | Acc: (75.00%) (15473/20608)
Epoch: 25 | Batch_idx: 170 |  Loss: (0.7138) |  Loss2: (0.0000) | Acc: (75.00%) (16458/21888)
Epoch: 25 | Batch_idx: 180 |  Loss: (0.7105) |  Loss2: (0.0000) | Acc: (75.00%) (17447/23168)
Epoch: 25 | Batch_idx: 190 |  Loss: (0.7085) |  Loss2: (0.0000) | Acc: (75.00%) (18429/24448)
Epoch: 25 | Batch_idx: 200 |  Loss: (0.7038) |  Loss2: (0.0000) | Acc: (75.00%) (19448/25728)
Epoch: 25 | Batch_idx: 210 |  Loss: (0.7020) |  Loss2: (0.0000) | Acc: (75.00%) (20431/27008)
Epoch: 25 | Batch_idx: 220 |  Loss: (0.7006) |  Loss2: (0.0000) | Acc: (75.00%) (21402/28288)
Epoch: 25 | Batch_idx: 230 |  Loss: (0.6997) |  Loss2: (0.0000) | Acc: (75.00%) (22354/29568)
Epoch: 25 | Batch_idx: 240 |  Loss: (0.6996) |  Loss2: (0.0000) | Acc: (75.00%) (23311/30848)
Epoch: 25 | Batch_idx: 250 |  Loss: (0.6986) |  Loss2: (0.0000) | Acc: (75.00%) (24289/32128)
Epoch: 25 | Batch_idx: 260 |  Loss: (0.6972) |  Loss2: (0.0000) | Acc: (75.00%) (25266/33408)
Epoch: 25 | Batch_idx: 270 |  Loss: (0.6968) |  Loss2: (0.0000) | Acc: (75.00%) (26248/34688)
Epoch: 25 | Batch_idx: 280 |  Loss: (0.6969) |  Loss2: (0.0000) | Acc: (75.00%) (27221/35968)
Epoch: 25 | Batch_idx: 290 |  Loss: (0.6956) |  Loss2: (0.0000) | Acc: (75.00%) (28216/37248)
Epoch: 25 | Batch_idx: 300 |  Loss: (0.6924) |  Loss2: (0.0000) | Acc: (75.00%) (29228/38528)
Epoch: 25 | Batch_idx: 310 |  Loss: (0.6923) |  Loss2: (0.0000) | Acc: (75.00%) (30179/39808)
Epoch: 25 | Batch_idx: 320 |  Loss: (0.6914) |  Loss2: (0.0000) | Acc: (75.00%) (31177/41088)
Epoch: 25 | Batch_idx: 330 |  Loss: (0.6895) |  Loss2: (0.0000) | Acc: (75.00%) (32171/42368)
Epoch: 25 | Batch_idx: 340 |  Loss: (0.6898) |  Loss2: (0.0000) | Acc: (75.00%) (33135/43648)
Epoch: 25 | Batch_idx: 350 |  Loss: (0.6892) |  Loss2: (0.0000) | Acc: (75.00%) (34098/44928)
Epoch: 25 | Batch_idx: 360 |  Loss: (0.6889) |  Loss2: (0.0000) | Acc: (75.00%) (35065/46208)
Epoch: 25 | Batch_idx: 370 |  Loss: (0.6887) |  Loss2: (0.0000) | Acc: (75.00%) (36049/47488)
Epoch: 25 | Batch_idx: 380 |  Loss: (0.6870) |  Loss2: (0.0000) | Acc: (75.00%) (37047/48768)
Epoch: 25 | Batch_idx: 390 |  Loss: (0.6873) |  Loss2: (0.0000) | Acc: (75.00%) (37977/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_025.pth.tar'
# TEST : Loss: (0.6894) | Acc: (76.00%) (7623/10000)
percent tensor([0.5191, 0.4809], device='cuda:0')
percent tensor([0.5137, 0.4863], device='cuda:0')
percent tensor([0.4877, 0.5123], device='cuda:0')
percent tensor([0.5885, 0.4115], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.5279, 0.4721], device='cuda:0')
percent tensor([0.5389, 0.4611], device='cuda:0')
percent tensor([0.9710, 0.0290], device='cuda:0')
Epoch: 26 | Batch_idx: 0 |  Loss: (0.5585) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 26 | Batch_idx: 10 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (1130/1408)
Epoch: 26 | Batch_idx: 20 |  Loss: (0.5861) |  Loss2: (0.0000) | Acc: (80.00%) (2153/2688)
Epoch: 26 | Batch_idx: 30 |  Loss: (0.5963) |  Loss2: (0.0000) | Acc: (79.00%) (3147/3968)
Epoch: 26 | Batch_idx: 40 |  Loss: (0.6093) |  Loss2: (0.0000) | Acc: (78.00%) (4137/5248)
Epoch: 26 | Batch_idx: 50 |  Loss: (0.6221) |  Loss2: (0.0000) | Acc: (78.00%) (5109/6528)
Epoch: 26 | Batch_idx: 60 |  Loss: (0.6228) |  Loss2: (0.0000) | Acc: (78.00%) (6110/7808)
Epoch: 26 | Batch_idx: 70 |  Loss: (0.6249) |  Loss2: (0.0000) | Acc: (78.00%) (7106/9088)
Epoch: 26 | Batch_idx: 80 |  Loss: (0.6254) |  Loss2: (0.0000) | Acc: (78.00%) (8089/10368)
Epoch: 26 | Batch_idx: 90 |  Loss: (0.6302) |  Loss2: (0.0000) | Acc: (77.00%) (9078/11648)
Epoch: 26 | Batch_idx: 100 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (10072/12928)
Epoch: 26 | Batch_idx: 110 |  Loss: (0.6309) |  Loss2: (0.0000) | Acc: (77.00%) (11066/14208)
Epoch: 26 | Batch_idx: 120 |  Loss: (0.6337) |  Loss2: (0.0000) | Acc: (77.00%) (12060/15488)
Epoch: 26 | Batch_idx: 130 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (13071/16768)
Epoch: 26 | Batch_idx: 140 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (14060/18048)
Epoch: 26 | Batch_idx: 150 |  Loss: (0.6334) |  Loss2: (0.0000) | Acc: (77.00%) (15030/19328)
Epoch: 26 | Batch_idx: 160 |  Loss: (0.6335) |  Loss2: (0.0000) | Acc: (77.00%) (16027/20608)
Epoch: 26 | Batch_idx: 170 |  Loss: (0.6338) |  Loss2: (0.0000) | Acc: (77.00%) (17012/21888)
Epoch: 26 | Batch_idx: 180 |  Loss: (0.6321) |  Loss2: (0.0000) | Acc: (77.00%) (18031/23168)
Epoch: 26 | Batch_idx: 190 |  Loss: (0.6310) |  Loss2: (0.0000) | Acc: (77.00%) (19032/24448)
Epoch: 26 | Batch_idx: 200 |  Loss: (0.6307) |  Loss2: (0.0000) | Acc: (77.00%) (20022/25728)
Epoch: 26 | Batch_idx: 210 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (21020/27008)
Epoch: 26 | Batch_idx: 220 |  Loss: (0.6294) |  Loss2: (0.0000) | Acc: (77.00%) (22030/28288)
Epoch: 26 | Batch_idx: 230 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (23019/29568)
Epoch: 26 | Batch_idx: 240 |  Loss: (0.6291) |  Loss2: (0.0000) | Acc: (77.00%) (24037/30848)
Epoch: 26 | Batch_idx: 250 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (77.00%) (25032/32128)
Epoch: 26 | Batch_idx: 260 |  Loss: (0.6300) |  Loss2: (0.0000) | Acc: (77.00%) (26015/33408)
Epoch: 26 | Batch_idx: 270 |  Loss: (0.6304) |  Loss2: (0.0000) | Acc: (77.00%) (27016/34688)
Epoch: 26 | Batch_idx: 280 |  Loss: (0.6311) |  Loss2: (0.0000) | Acc: (77.00%) (28008/35968)
Epoch: 26 | Batch_idx: 290 |  Loss: (0.6316) |  Loss2: (0.0000) | Acc: (77.00%) (28993/37248)
Epoch: 26 | Batch_idx: 300 |  Loss: (0.6315) |  Loss2: (0.0000) | Acc: (77.00%) (29992/38528)
Epoch: 26 | Batch_idx: 310 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (31010/39808)
Epoch: 26 | Batch_idx: 320 |  Loss: (0.6308) |  Loss2: (0.0000) | Acc: (77.00%) (32009/41088)
Epoch: 26 | Batch_idx: 330 |  Loss: (0.6304) |  Loss2: (0.0000) | Acc: (77.00%) (33028/42368)
Epoch: 26 | Batch_idx: 340 |  Loss: (0.6299) |  Loss2: (0.0000) | Acc: (77.00%) (34030/43648)
Epoch: 26 | Batch_idx: 350 |  Loss: (0.6300) |  Loss2: (0.0000) | Acc: (77.00%) (35032/44928)
Epoch: 26 | Batch_idx: 360 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (36045/46208)
Epoch: 26 | Batch_idx: 370 |  Loss: (0.6289) |  Loss2: (0.0000) | Acc: (78.00%) (37048/47488)
Epoch: 26 | Batch_idx: 380 |  Loss: (0.6296) |  Loss2: (0.0000) | Acc: (77.00%) (38025/48768)
Epoch: 26 | Batch_idx: 390 |  Loss: (0.6288) |  Loss2: (0.0000) | Acc: (77.00%) (38993/50000)
# TEST : Loss: (0.6618) | Acc: (77.00%) (7714/10000)
percent tensor([0.5207, 0.4793], device='cuda:0')
percent tensor([0.5182, 0.4818], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.5889, 0.4111], device='cuda:0')
percent tensor([0.5401, 0.4599], device='cuda:0')
percent tensor([0.5390, 0.4610], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.9834, 0.0166], device='cuda:0')
Epoch: 27 | Batch_idx: 0 |  Loss: (0.5435) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 27 | Batch_idx: 10 |  Loss: (0.6320) |  Loss2: (0.0000) | Acc: (77.00%) (1092/1408)
Epoch: 27 | Batch_idx: 20 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (2078/2688)
Epoch: 27 | Batch_idx: 30 |  Loss: (0.6239) |  Loss2: (0.0000) | Acc: (78.00%) (3097/3968)
Epoch: 27 | Batch_idx: 40 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (77.00%) (4080/5248)
Epoch: 27 | Batch_idx: 50 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (5092/6528)
Epoch: 27 | Batch_idx: 60 |  Loss: (0.6220) |  Loss2: (0.0000) | Acc: (78.00%) (6097/7808)
Epoch: 27 | Batch_idx: 70 |  Loss: (0.6193) |  Loss2: (0.0000) | Acc: (78.00%) (7108/9088)
Epoch: 27 | Batch_idx: 80 |  Loss: (0.6174) |  Loss2: (0.0000) | Acc: (78.00%) (8125/10368)
Epoch: 27 | Batch_idx: 90 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (9122/11648)
Epoch: 27 | Batch_idx: 100 |  Loss: (0.6187) |  Loss2: (0.0000) | Acc: (78.00%) (10106/12928)
Epoch: 27 | Batch_idx: 110 |  Loss: (0.6181) |  Loss2: (0.0000) | Acc: (78.00%) (11119/14208)
Epoch: 27 | Batch_idx: 120 |  Loss: (0.6157) |  Loss2: (0.0000) | Acc: (78.00%) (12142/15488)
Epoch: 27 | Batch_idx: 130 |  Loss: (0.6152) |  Loss2: (0.0000) | Acc: (78.00%) (13155/16768)
Epoch: 27 | Batch_idx: 140 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (14168/18048)
Epoch: 27 | Batch_idx: 150 |  Loss: (0.6134) |  Loss2: (0.0000) | Acc: (78.00%) (15173/19328)
Epoch: 27 | Batch_idx: 160 |  Loss: (0.6150) |  Loss2: (0.0000) | Acc: (78.00%) (16171/20608)
Epoch: 27 | Batch_idx: 170 |  Loss: (0.6147) |  Loss2: (0.0000) | Acc: (78.00%) (17183/21888)
Epoch: 27 | Batch_idx: 180 |  Loss: (0.6172) |  Loss2: (0.0000) | Acc: (78.00%) (18156/23168)
Epoch: 27 | Batch_idx: 190 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (78.00%) (19165/24448)
Epoch: 27 | Batch_idx: 200 |  Loss: (0.6166) |  Loss2: (0.0000) | Acc: (78.00%) (20165/25728)
Epoch: 27 | Batch_idx: 210 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (21173/27008)
Epoch: 27 | Batch_idx: 220 |  Loss: (0.6162) |  Loss2: (0.0000) | Acc: (78.00%) (22165/28288)
Epoch: 27 | Batch_idx: 230 |  Loss: (0.6160) |  Loss2: (0.0000) | Acc: (78.00%) (23162/29568)
Epoch: 27 | Batch_idx: 240 |  Loss: (0.6159) |  Loss2: (0.0000) | Acc: (78.00%) (24174/30848)
Epoch: 27 | Batch_idx: 250 |  Loss: (0.6151) |  Loss2: (0.0000) | Acc: (78.00%) (25178/32128)
Epoch: 27 | Batch_idx: 260 |  Loss: (0.6154) |  Loss2: (0.0000) | Acc: (78.00%) (26190/33408)
Epoch: 27 | Batch_idx: 270 |  Loss: (0.6156) |  Loss2: (0.0000) | Acc: (78.00%) (27191/34688)
Epoch: 27 | Batch_idx: 280 |  Loss: (0.6147) |  Loss2: (0.0000) | Acc: (78.00%) (28204/35968)
Epoch: 27 | Batch_idx: 290 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (29205/37248)
Epoch: 27 | Batch_idx: 300 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (30201/38528)
Epoch: 27 | Batch_idx: 310 |  Loss: (0.6141) |  Loss2: (0.0000) | Acc: (78.00%) (31222/39808)
Epoch: 27 | Batch_idx: 320 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (32235/41088)
Epoch: 27 | Batch_idx: 330 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (33241/42368)
Epoch: 27 | Batch_idx: 340 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (34242/43648)
Epoch: 27 | Batch_idx: 350 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (35246/44928)
Epoch: 27 | Batch_idx: 360 |  Loss: (0.6139) |  Loss2: (0.0000) | Acc: (78.00%) (36259/46208)
Epoch: 27 | Batch_idx: 370 |  Loss: (0.6137) |  Loss2: (0.0000) | Acc: (78.00%) (37273/47488)
Epoch: 27 | Batch_idx: 380 |  Loss: (0.6133) |  Loss2: (0.0000) | Acc: (78.00%) (38280/48768)
Epoch: 27 | Batch_idx: 390 |  Loss: (0.6140) |  Loss2: (0.0000) | Acc: (78.00%) (39248/50000)
# TEST : Loss: (0.6438) | Acc: (77.00%) (7779/10000)
percent tensor([0.5232, 0.4768], device='cuda:0')
percent tensor([0.5223, 0.4777], device='cuda:0')
percent tensor([0.4923, 0.5077], device='cuda:0')
percent tensor([0.5895, 0.4105], device='cuda:0')
percent tensor([0.5416, 0.4584], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.5553, 0.4447], device='cuda:0')
percent tensor([0.9888, 0.0112], device='cuda:0')
Epoch: 28 | Batch_idx: 0 |  Loss: (0.5725) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 28 | Batch_idx: 10 |  Loss: (0.6189) |  Loss2: (0.0000) | Acc: (79.00%) (1114/1408)
Epoch: 28 | Batch_idx: 20 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (2097/2688)
Epoch: 28 | Batch_idx: 30 |  Loss: (0.6084) |  Loss2: (0.0000) | Acc: (78.00%) (3097/3968)
Epoch: 28 | Batch_idx: 40 |  Loss: (0.6158) |  Loss2: (0.0000) | Acc: (77.00%) (4087/5248)
Epoch: 28 | Batch_idx: 50 |  Loss: (0.6227) |  Loss2: (0.0000) | Acc: (77.00%) (5045/6528)
Epoch: 28 | Batch_idx: 60 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (77.00%) (6062/7808)
Epoch: 28 | Batch_idx: 70 |  Loss: (0.6161) |  Loss2: (0.0000) | Acc: (77.00%) (7061/9088)
Epoch: 28 | Batch_idx: 80 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (77.00%) (8069/10368)
Epoch: 28 | Batch_idx: 90 |  Loss: (0.6171) |  Loss2: (0.0000) | Acc: (77.00%) (9073/11648)
Epoch: 28 | Batch_idx: 100 |  Loss: (0.6163) |  Loss2: (0.0000) | Acc: (77.00%) (10070/12928)
Epoch: 28 | Batch_idx: 110 |  Loss: (0.6168) |  Loss2: (0.0000) | Acc: (77.00%) (11071/14208)
Epoch: 28 | Batch_idx: 120 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (77.00%) (12057/15488)
Epoch: 28 | Batch_idx: 130 |  Loss: (0.6216) |  Loss2: (0.0000) | Acc: (77.00%) (13048/16768)
Epoch: 28 | Batch_idx: 140 |  Loss: (0.6211) |  Loss2: (0.0000) | Acc: (77.00%) (14055/18048)
Epoch: 28 | Batch_idx: 150 |  Loss: (0.6212) |  Loss2: (0.0000) | Acc: (77.00%) (15057/19328)
Epoch: 28 | Batch_idx: 160 |  Loss: (0.6206) |  Loss2: (0.0000) | Acc: (78.00%) (16081/20608)
Epoch: 28 | Batch_idx: 170 |  Loss: (0.6204) |  Loss2: (0.0000) | Acc: (78.00%) (17108/21888)
Epoch: 28 | Batch_idx: 180 |  Loss: (0.6208) |  Loss2: (0.0000) | Acc: (78.00%) (18101/23168)
Epoch: 28 | Batch_idx: 190 |  Loss: (0.6177) |  Loss2: (0.0000) | Acc: (78.00%) (19139/24448)
Epoch: 28 | Batch_idx: 200 |  Loss: (0.6170) |  Loss2: (0.0000) | Acc: (78.00%) (20151/25728)
Epoch: 28 | Batch_idx: 210 |  Loss: (0.6176) |  Loss2: (0.0000) | Acc: (78.00%) (21141/27008)
Epoch: 28 | Batch_idx: 220 |  Loss: (0.6153) |  Loss2: (0.0000) | Acc: (78.00%) (22186/28288)
Epoch: 28 | Batch_idx: 230 |  Loss: (0.6145) |  Loss2: (0.0000) | Acc: (78.00%) (23192/29568)
Epoch: 28 | Batch_idx: 240 |  Loss: (0.6155) |  Loss2: (0.0000) | Acc: (78.00%) (24177/30848)
Epoch: 28 | Batch_idx: 250 |  Loss: (0.6141) |  Loss2: (0.0000) | Acc: (78.00%) (25215/32128)
Epoch: 28 | Batch_idx: 260 |  Loss: (0.6118) |  Loss2: (0.0000) | Acc: (78.00%) (26251/33408)
Epoch: 28 | Batch_idx: 270 |  Loss: (0.6111) |  Loss2: (0.0000) | Acc: (78.00%) (27252/34688)
Epoch: 28 | Batch_idx: 280 |  Loss: (0.6115) |  Loss2: (0.0000) | Acc: (78.00%) (28248/35968)
Epoch: 28 | Batch_idx: 290 |  Loss: (0.6099) |  Loss2: (0.0000) | Acc: (78.00%) (29275/37248)
Epoch: 28 | Batch_idx: 300 |  Loss: (0.6083) |  Loss2: (0.0000) | Acc: (78.00%) (30325/38528)
Epoch: 28 | Batch_idx: 310 |  Loss: (0.6073) |  Loss2: (0.0000) | Acc: (78.00%) (31352/39808)
Epoch: 28 | Batch_idx: 320 |  Loss: (0.6060) |  Loss2: (0.0000) | Acc: (78.00%) (32374/41088)
Epoch: 28 | Batch_idx: 330 |  Loss: (0.6061) |  Loss2: (0.0000) | Acc: (78.00%) (33392/42368)
Epoch: 28 | Batch_idx: 340 |  Loss: (0.6054) |  Loss2: (0.0000) | Acc: (78.00%) (34407/43648)
Epoch: 28 | Batch_idx: 350 |  Loss: (0.6050) |  Loss2: (0.0000) | Acc: (78.00%) (35433/44928)
Epoch: 28 | Batch_idx: 360 |  Loss: (0.6044) |  Loss2: (0.0000) | Acc: (78.00%) (36455/46208)
Epoch: 28 | Batch_idx: 370 |  Loss: (0.6047) |  Loss2: (0.0000) | Acc: (78.00%) (37454/47488)
Epoch: 28 | Batch_idx: 380 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (78.00%) (38442/48768)
Epoch: 28 | Batch_idx: 390 |  Loss: (0.6059) |  Loss2: (0.0000) | Acc: (78.00%) (39413/50000)
# TEST : Loss: (0.6374) | Acc: (78.00%) (7806/10000)
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.5228, 0.4772], device='cuda:0')
percent tensor([0.4934, 0.5066], device='cuda:0')
percent tensor([0.5857, 0.4143], device='cuda:0')
percent tensor([0.5460, 0.4540], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.5586, 0.4414], device='cuda:0')
percent tensor([0.9918, 0.0082], device='cuda:0')
Epoch: 29 | Batch_idx: 0 |  Loss: (0.6946) |  Loss2: (0.0000) | Acc: (72.00%) (93/128)
Epoch: 29 | Batch_idx: 10 |  Loss: (0.6085) |  Loss2: (0.0000) | Acc: (78.00%) (1105/1408)
Epoch: 29 | Batch_idx: 20 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (2135/2688)
Epoch: 29 | Batch_idx: 30 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (3159/3968)
Epoch: 29 | Batch_idx: 40 |  Loss: (0.5898) |  Loss2: (0.0000) | Acc: (79.00%) (4177/5248)
Epoch: 29 | Batch_idx: 50 |  Loss: (0.5971) |  Loss2: (0.0000) | Acc: (79.00%) (5180/6528)
Epoch: 29 | Batch_idx: 60 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (6181/7808)
Epoch: 29 | Batch_idx: 70 |  Loss: (0.5960) |  Loss2: (0.0000) | Acc: (79.00%) (7208/9088)
Epoch: 29 | Batch_idx: 80 |  Loss: (0.6008) |  Loss2: (0.0000) | Acc: (79.00%) (8210/10368)
Epoch: 29 | Batch_idx: 90 |  Loss: (0.5992) |  Loss2: (0.0000) | Acc: (79.00%) (9221/11648)
Epoch: 29 | Batch_idx: 100 |  Loss: (0.5998) |  Loss2: (0.0000) | Acc: (79.00%) (10248/12928)
Epoch: 29 | Batch_idx: 110 |  Loss: (0.6046) |  Loss2: (0.0000) | Acc: (79.00%) (11255/14208)
Epoch: 29 | Batch_idx: 120 |  Loss: (0.6051) |  Loss2: (0.0000) | Acc: (79.00%) (12260/15488)
Epoch: 29 | Batch_idx: 130 |  Loss: (0.6017) |  Loss2: (0.0000) | Acc: (79.00%) (13293/16768)
Epoch: 29 | Batch_idx: 140 |  Loss: (0.6058) |  Loss2: (0.0000) | Acc: (79.00%) (14278/18048)
Epoch: 29 | Batch_idx: 150 |  Loss: (0.6053) |  Loss2: (0.0000) | Acc: (79.00%) (15289/19328)
Epoch: 29 | Batch_idx: 160 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (16311/20608)
Epoch: 29 | Batch_idx: 170 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (79.00%) (17316/21888)
Epoch: 29 | Batch_idx: 180 |  Loss: (0.6057) |  Loss2: (0.0000) | Acc: (78.00%) (18298/23168)
Epoch: 29 | Batch_idx: 190 |  Loss: (0.6048) |  Loss2: (0.0000) | Acc: (79.00%) (19330/24448)
Epoch: 29 | Batch_idx: 200 |  Loss: (0.6043) |  Loss2: (0.0000) | Acc: (79.00%) (20352/25728)
Epoch: 29 | Batch_idx: 210 |  Loss: (0.6056) |  Loss2: (0.0000) | Acc: (79.00%) (21348/27008)
Epoch: 29 | Batch_idx: 220 |  Loss: (0.6049) |  Loss2: (0.0000) | Acc: (79.00%) (22363/28288)
Epoch: 29 | Batch_idx: 230 |  Loss: (0.6032) |  Loss2: (0.0000) | Acc: (79.00%) (23405/29568)
Epoch: 29 | Batch_idx: 240 |  Loss: (0.6024) |  Loss2: (0.0000) | Acc: (79.00%) (24432/30848)
Epoch: 29 | Batch_idx: 250 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (25456/32128)
Epoch: 29 | Batch_idx: 260 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (79.00%) (26486/33408)
Epoch: 29 | Batch_idx: 270 |  Loss: (0.5996) |  Loss2: (0.0000) | Acc: (79.00%) (27503/34688)
Epoch: 29 | Batch_idx: 280 |  Loss: (0.5987) |  Loss2: (0.0000) | Acc: (79.00%) (28520/35968)
Epoch: 29 | Batch_idx: 290 |  Loss: (0.5988) |  Loss2: (0.0000) | Acc: (79.00%) (29525/37248)
Epoch: 29 | Batch_idx: 300 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (79.00%) (30522/38528)
Epoch: 29 | Batch_idx: 310 |  Loss: (0.5982) |  Loss2: (0.0000) | Acc: (79.00%) (31544/39808)
Epoch: 29 | Batch_idx: 320 |  Loss: (0.5970) |  Loss2: (0.0000) | Acc: (79.00%) (32572/41088)
Epoch: 29 | Batch_idx: 330 |  Loss: (0.5958) |  Loss2: (0.0000) | Acc: (79.00%) (33608/42368)
Epoch: 29 | Batch_idx: 340 |  Loss: (0.5970) |  Loss2: (0.0000) | Acc: (79.00%) (34598/43648)
Epoch: 29 | Batch_idx: 350 |  Loss: (0.5956) |  Loss2: (0.0000) | Acc: (79.00%) (35625/44928)
Epoch: 29 | Batch_idx: 360 |  Loss: (0.5963) |  Loss2: (0.0000) | Acc: (79.00%) (36621/46208)
Epoch: 29 | Batch_idx: 370 |  Loss: (0.5969) |  Loss2: (0.0000) | Acc: (79.00%) (37618/47488)
Epoch: 29 | Batch_idx: 380 |  Loss: (0.5985) |  Loss2: (0.0000) | Acc: (79.00%) (38596/48768)
Epoch: 29 | Batch_idx: 390 |  Loss: (0.5984) |  Loss2: (0.0000) | Acc: (79.00%) (39574/50000)
# TEST : Loss: (0.6301) | Acc: (78.00%) (7830/10000)
percent tensor([0.5253, 0.4747], device='cuda:0')
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4936, 0.5064], device='cuda:0')
percent tensor([0.5836, 0.4164], device='cuda:0')
percent tensor([0.5472, 0.4528], device='cuda:0')
percent tensor([0.5591, 0.4409], device='cuda:0')
percent tensor([0.5638, 0.4362], device='cuda:0')
percent tensor([0.9937, 0.0063], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 30 | Batch_idx: 0 |  Loss: (0.4722) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 30 | Batch_idx: 10 |  Loss: (0.5620) |  Loss2: (0.0000) | Acc: (80.00%) (1128/1408)
Epoch: 30 | Batch_idx: 20 |  Loss: (0.5793) |  Loss2: (0.0000) | Acc: (79.00%) (2148/2688)
Epoch: 30 | Batch_idx: 30 |  Loss: (0.5952) |  Loss2: (0.0000) | Acc: (79.00%) (3136/3968)
Epoch: 30 | Batch_idx: 40 |  Loss: (0.5887) |  Loss2: (0.0000) | Acc: (79.00%) (4154/5248)
Epoch: 30 | Batch_idx: 50 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (5170/6528)
Epoch: 30 | Batch_idx: 60 |  Loss: (0.5981) |  Loss2: (0.0000) | Acc: (79.00%) (6185/7808)
Epoch: 30 | Batch_idx: 70 |  Loss: (0.6005) |  Loss2: (0.0000) | Acc: (79.00%) (7195/9088)
Epoch: 30 | Batch_idx: 80 |  Loss: (0.5976) |  Loss2: (0.0000) | Acc: (79.00%) (8210/10368)
Epoch: 30 | Batch_idx: 90 |  Loss: (0.6001) |  Loss2: (0.0000) | Acc: (79.00%) (9224/11648)
Epoch: 30 | Batch_idx: 100 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (10242/12928)
Epoch: 30 | Batch_idx: 110 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (11244/14208)
Epoch: 30 | Batch_idx: 120 |  Loss: (0.6007) |  Loss2: (0.0000) | Acc: (79.00%) (12253/15488)
Epoch: 30 | Batch_idx: 130 |  Loss: (0.6012) |  Loss2: (0.0000) | Acc: (79.00%) (13264/16768)
Epoch: 30 | Batch_idx: 140 |  Loss: (0.6010) |  Loss2: (0.0000) | Acc: (79.00%) (14274/18048)
Epoch: 30 | Batch_idx: 150 |  Loss: (0.6019) |  Loss2: (0.0000) | Acc: (79.00%) (15285/19328)
Epoch: 30 | Batch_idx: 160 |  Loss: (0.6009) |  Loss2: (0.0000) | Acc: (79.00%) (16309/20608)
Epoch: 30 | Batch_idx: 170 |  Loss: (0.6029) |  Loss2: (0.0000) | Acc: (79.00%) (17296/21888)
Epoch: 30 | Batch_idx: 180 |  Loss: (0.6027) |  Loss2: (0.0000) | Acc: (78.00%) (18302/23168)
Epoch: 30 | Batch_idx: 190 |  Loss: (0.6028) |  Loss2: (0.0000) | Acc: (78.00%) (19309/24448)
Epoch: 30 | Batch_idx: 200 |  Loss: (0.6018) |  Loss2: (0.0000) | Acc: (79.00%) (20328/25728)
Epoch: 30 | Batch_idx: 210 |  Loss: (0.6015) |  Loss2: (0.0000) | Acc: (79.00%) (21341/27008)
Epoch: 30 | Batch_idx: 220 |  Loss: (0.6006) |  Loss2: (0.0000) | Acc: (79.00%) (22378/28288)
Epoch: 30 | Batch_idx: 230 |  Loss: (0.5999) |  Loss2: (0.0000) | Acc: (79.00%) (23391/29568)
Epoch: 30 | Batch_idx: 240 |  Loss: (0.5989) |  Loss2: (0.0000) | Acc: (79.00%) (24410/30848)
Epoch: 30 | Batch_idx: 250 |  Loss: (0.5994) |  Loss2: (0.0000) | Acc: (79.00%) (25429/32128)
Epoch: 30 | Batch_idx: 260 |  Loss: (0.5983) |  Loss2: (0.0000) | Acc: (79.00%) (26463/33408)
Epoch: 30 | Batch_idx: 270 |  Loss: (0.5968) |  Loss2: (0.0000) | Acc: (79.00%) (27483/34688)
Epoch: 30 | Batch_idx: 280 |  Loss: (0.5991) |  Loss2: (0.0000) | Acc: (79.00%) (28460/35968)
Epoch: 30 | Batch_idx: 290 |  Loss: (0.5989) |  Loss2: (0.0000) | Acc: (79.00%) (29469/37248)
Epoch: 30 | Batch_idx: 300 |  Loss: (0.5986) |  Loss2: (0.0000) | Acc: (79.00%) (30492/38528)
Epoch: 30 | Batch_idx: 310 |  Loss: (0.5974) |  Loss2: (0.0000) | Acc: (79.00%) (31537/39808)
Epoch: 30 | Batch_idx: 320 |  Loss: (0.5968) |  Loss2: (0.0000) | Acc: (79.00%) (32567/41088)
Epoch: 30 | Batch_idx: 330 |  Loss: (0.5964) |  Loss2: (0.0000) | Acc: (79.00%) (33591/42368)
Epoch: 30 | Batch_idx: 340 |  Loss: (0.5961) |  Loss2: (0.0000) | Acc: (79.00%) (34606/43648)
Epoch: 30 | Batch_idx: 350 |  Loss: (0.5953) |  Loss2: (0.0000) | Acc: (79.00%) (35647/44928)
Epoch: 30 | Batch_idx: 360 |  Loss: (0.5936) |  Loss2: (0.0000) | Acc: (79.00%) (36682/46208)
Epoch: 30 | Batch_idx: 370 |  Loss: (0.5941) |  Loss2: (0.0000) | Acc: (79.00%) (37702/47488)
Epoch: 30 | Batch_idx: 380 |  Loss: (0.5959) |  Loss2: (0.0000) | Acc: (79.00%) (38688/48768)
Epoch: 30 | Batch_idx: 390 |  Loss: (0.5951) |  Loss2: (0.0000) | Acc: (79.00%) (39680/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_030.pth.tar'
# TEST : Loss: (0.7693) | Acc: (73.00%) (7390/10000)
percent tensor([0.5254, 0.4746], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5606, 0.4394], device='cuda:0')
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.9953, 0.0047], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(171.8327, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(787.8500, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(783.4050, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1529.3767, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(504.9400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2181.9280, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4310.4673, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1428.8828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6086.5161, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12136.1875, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4039.6501, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17124.3613, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 31 | Batch_idx: 0 |  Loss: (0.5433) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 31 | Batch_idx: 10 |  Loss: (0.5909) |  Loss2: (0.0000) | Acc: (79.00%) (1121/1408)
Epoch: 31 | Batch_idx: 20 |  Loss: (0.5755) |  Loss2: (0.0000) | Acc: (80.00%) (2154/2688)
Epoch: 31 | Batch_idx: 30 |  Loss: (0.5695) |  Loss2: (0.0000) | Acc: (80.00%) (3180/3968)
Epoch: 31 | Batch_idx: 40 |  Loss: (0.5747) |  Loss2: (0.0000) | Acc: (79.00%) (4188/5248)
Epoch: 31 | Batch_idx: 50 |  Loss: (0.5654) |  Loss2: (0.0000) | Acc: (80.00%) (5248/6528)
Epoch: 31 | Batch_idx: 60 |  Loss: (0.5629) |  Loss2: (0.0000) | Acc: (80.00%) (6272/7808)
Epoch: 31 | Batch_idx: 70 |  Loss: (0.5622) |  Loss2: (0.0000) | Acc: (80.00%) (7312/9088)
Epoch: 31 | Batch_idx: 80 |  Loss: (0.5598) |  Loss2: (0.0000) | Acc: (80.00%) (8341/10368)
Epoch: 31 | Batch_idx: 90 |  Loss: (0.5579) |  Loss2: (0.0000) | Acc: (80.00%) (9369/11648)
Epoch: 31 | Batch_idx: 100 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (10408/12928)
Epoch: 31 | Batch_idx: 110 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (11429/14208)
Epoch: 31 | Batch_idx: 120 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (12459/15488)
Epoch: 31 | Batch_idx: 130 |  Loss: (0.5581) |  Loss2: (0.0000) | Acc: (80.00%) (13498/16768)
Epoch: 31 | Batch_idx: 140 |  Loss: (0.5587) |  Loss2: (0.0000) | Acc: (80.00%) (14522/18048)
Epoch: 31 | Batch_idx: 150 |  Loss: (0.5602) |  Loss2: (0.0000) | Acc: (80.00%) (15559/19328)
Epoch: 31 | Batch_idx: 160 |  Loss: (0.5618) |  Loss2: (0.0000) | Acc: (80.00%) (16573/20608)
Epoch: 31 | Batch_idx: 170 |  Loss: (0.5614) |  Loss2: (0.0000) | Acc: (80.00%) (17592/21888)
Epoch: 31 | Batch_idx: 180 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (18618/23168)
Epoch: 31 | Batch_idx: 190 |  Loss: (0.5628) |  Loss2: (0.0000) | Acc: (80.00%) (19653/24448)
Epoch: 31 | Batch_idx: 200 |  Loss: (0.5657) |  Loss2: (0.0000) | Acc: (80.00%) (20659/25728)
Epoch: 31 | Batch_idx: 210 |  Loss: (0.5647) |  Loss2: (0.0000) | Acc: (80.00%) (21701/27008)
Epoch: 31 | Batch_idx: 220 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (80.00%) (22731/28288)
Epoch: 31 | Batch_idx: 230 |  Loss: (0.5662) |  Loss2: (0.0000) | Acc: (80.00%) (23757/29568)
Epoch: 31 | Batch_idx: 240 |  Loss: (0.5655) |  Loss2: (0.0000) | Acc: (80.00%) (24791/30848)
Epoch: 31 | Batch_idx: 250 |  Loss: (0.5653) |  Loss2: (0.0000) | Acc: (80.00%) (25820/32128)
Epoch: 31 | Batch_idx: 260 |  Loss: (0.5646) |  Loss2: (0.0000) | Acc: (80.00%) (26872/33408)
Epoch: 31 | Batch_idx: 270 |  Loss: (0.5641) |  Loss2: (0.0000) | Acc: (80.00%) (27898/34688)
Epoch: 31 | Batch_idx: 280 |  Loss: (0.5648) |  Loss2: (0.0000) | Acc: (80.00%) (28922/35968)
Epoch: 31 | Batch_idx: 290 |  Loss: (0.5670) |  Loss2: (0.0000) | Acc: (80.00%) (29929/37248)
Epoch: 31 | Batch_idx: 300 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (80.00%) (30969/38528)
Epoch: 31 | Batch_idx: 310 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (31983/39808)
Epoch: 31 | Batch_idx: 320 |  Loss: (0.5686) |  Loss2: (0.0000) | Acc: (80.00%) (33017/41088)
Epoch: 31 | Batch_idx: 330 |  Loss: (0.5694) |  Loss2: (0.0000) | Acc: (80.00%) (34019/42368)
Epoch: 31 | Batch_idx: 340 |  Loss: (0.5700) |  Loss2: (0.0000) | Acc: (80.00%) (35027/43648)
Epoch: 31 | Batch_idx: 350 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (36058/44928)
Epoch: 31 | Batch_idx: 360 |  Loss: (0.5688) |  Loss2: (0.0000) | Acc: (80.00%) (37078/46208)
Epoch: 31 | Batch_idx: 370 |  Loss: (0.5696) |  Loss2: (0.0000) | Acc: (80.00%) (38101/47488)
Epoch: 31 | Batch_idx: 380 |  Loss: (0.5687) |  Loss2: (0.0000) | Acc: (80.00%) (39126/48768)
Epoch: 31 | Batch_idx: 390 |  Loss: (0.5692) |  Loss2: (0.0000) | Acc: (80.00%) (40112/50000)
# TEST : Loss: (0.7399) | Acc: (75.00%) (7519/10000)
percent tensor([0.5250, 0.4750], device='cuda:0')
percent tensor([0.5243, 0.4757], device='cuda:0')
percent tensor([0.4937, 0.5063], device='cuda:0')
percent tensor([0.5823, 0.4177], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.5662, 0.4338], device='cuda:0')
percent tensor([0.9954, 0.0046], device='cuda:0')
Epoch: 32 | Batch_idx: 0 |  Loss: (0.7115) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 32 | Batch_idx: 10 |  Loss: (0.5667) |  Loss2: (0.0000) | Acc: (79.00%) (1118/1408)
Epoch: 32 | Batch_idx: 20 |  Loss: (0.5532) |  Loss2: (0.0000) | Acc: (80.00%) (2174/2688)
Epoch: 32 | Batch_idx: 30 |  Loss: (0.5396) |  Loss2: (0.0000) | Acc: (81.00%) (3233/3968)
Epoch: 32 | Batch_idx: 40 |  Loss: (0.5333) |  Loss2: (0.0000) | Acc: (81.00%) (4292/5248)
Epoch: 32 | Batch_idx: 50 |  Loss: (0.5315) |  Loss2: (0.0000) | Acc: (81.00%) (5345/6528)
Epoch: 32 | Batch_idx: 60 |  Loss: (0.5321) |  Loss2: (0.0000) | Acc: (81.00%) (6389/7808)
Epoch: 32 | Batch_idx: 70 |  Loss: (0.5366) |  Loss2: (0.0000) | Acc: (81.00%) (7430/9088)
Epoch: 32 | Batch_idx: 80 |  Loss: (0.5368) |  Loss2: (0.0000) | Acc: (81.00%) (8477/10368)
Epoch: 32 | Batch_idx: 90 |  Loss: (0.5416) |  Loss2: (0.0000) | Acc: (81.00%) (9491/11648)
Epoch: 32 | Batch_idx: 100 |  Loss: (0.5423) |  Loss2: (0.0000) | Acc: (81.00%) (10527/12928)
Epoch: 32 | Batch_idx: 110 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (11566/14208)
Epoch: 32 | Batch_idx: 120 |  Loss: (0.5443) |  Loss2: (0.0000) | Acc: (81.00%) (12605/15488)
Epoch: 32 | Batch_idx: 130 |  Loss: (0.5438) |  Loss2: (0.0000) | Acc: (81.00%) (13650/16768)
Epoch: 32 | Batch_idx: 140 |  Loss: (0.5439) |  Loss2: (0.0000) | Acc: (81.00%) (14689/18048)
Epoch: 32 | Batch_idx: 150 |  Loss: (0.5418) |  Loss2: (0.0000) | Acc: (81.00%) (15731/19328)
Epoch: 32 | Batch_idx: 160 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (81.00%) (16769/20608)
Epoch: 32 | Batch_idx: 170 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (17790/21888)
Epoch: 32 | Batch_idx: 180 |  Loss: (0.5442) |  Loss2: (0.0000) | Acc: (81.00%) (18844/23168)
Epoch: 32 | Batch_idx: 190 |  Loss: (0.5434) |  Loss2: (0.0000) | Acc: (81.00%) (19891/24448)
Epoch: 32 | Batch_idx: 200 |  Loss: (0.5428) |  Loss2: (0.0000) | Acc: (81.00%) (20939/25728)
Epoch: 32 | Batch_idx: 210 |  Loss: (0.5424) |  Loss2: (0.0000) | Acc: (81.00%) (21991/27008)
Epoch: 32 | Batch_idx: 220 |  Loss: (0.5440) |  Loss2: (0.0000) | Acc: (81.00%) (23002/28288)
Epoch: 32 | Batch_idx: 230 |  Loss: (0.5467) |  Loss2: (0.0000) | Acc: (81.00%) (24010/29568)
Epoch: 32 | Batch_idx: 240 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (25075/30848)
Epoch: 32 | Batch_idx: 250 |  Loss: (0.5473) |  Loss2: (0.0000) | Acc: (81.00%) (26091/32128)
Epoch: 32 | Batch_idx: 260 |  Loss: (0.5462) |  Loss2: (0.0000) | Acc: (81.00%) (27146/33408)
Epoch: 32 | Batch_idx: 270 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (28187/34688)
Epoch: 32 | Batch_idx: 280 |  Loss: (0.5460) |  Loss2: (0.0000) | Acc: (81.00%) (29228/35968)
Epoch: 32 | Batch_idx: 290 |  Loss: (0.5468) |  Loss2: (0.0000) | Acc: (81.00%) (30254/37248)
Epoch: 32 | Batch_idx: 300 |  Loss: (0.5469) |  Loss2: (0.0000) | Acc: (81.00%) (31286/38528)
Epoch: 32 | Batch_idx: 310 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (32334/39808)
Epoch: 32 | Batch_idx: 320 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (33373/41088)
Epoch: 32 | Batch_idx: 330 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (81.00%) (34396/42368)
Epoch: 32 | Batch_idx: 340 |  Loss: (0.5452) |  Loss2: (0.0000) | Acc: (81.00%) (35428/43648)
Epoch: 32 | Batch_idx: 350 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (36462/44928)
Epoch: 32 | Batch_idx: 360 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (37518/46208)
Epoch: 32 | Batch_idx: 370 |  Loss: (0.5456) |  Loss2: (0.0000) | Acc: (81.00%) (38543/47488)
Epoch: 32 | Batch_idx: 380 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (39597/48768)
Epoch: 32 | Batch_idx: 390 |  Loss: (0.5461) |  Loss2: (0.0000) | Acc: (81.00%) (40574/50000)
# TEST : Loss: (0.6330) | Acc: (78.00%) (7827/10000)
percent tensor([0.5250, 0.4750], device='cuda:0')
percent tensor([0.5238, 0.4762], device='cuda:0')
percent tensor([0.4937, 0.5063], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.5589, 0.4411], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.9947, 0.0053], device='cuda:0')
Epoch: 33 | Batch_idx: 0 |  Loss: (0.6192) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 33 | Batch_idx: 10 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (82.00%) (1155/1408)
Epoch: 33 | Batch_idx: 20 |  Loss: (0.5447) |  Loss2: (0.0000) | Acc: (81.00%) (2181/2688)
Epoch: 33 | Batch_idx: 30 |  Loss: (0.5218) |  Loss2: (0.0000) | Acc: (82.00%) (3257/3968)
Epoch: 33 | Batch_idx: 40 |  Loss: (0.5264) |  Loss2: (0.0000) | Acc: (81.00%) (4286/5248)
Epoch: 33 | Batch_idx: 50 |  Loss: (0.5344) |  Loss2: (0.0000) | Acc: (81.00%) (5314/6528)
Epoch: 33 | Batch_idx: 60 |  Loss: (0.5322) |  Loss2: (0.0000) | Acc: (81.00%) (6364/7808)
Epoch: 33 | Batch_idx: 70 |  Loss: (0.5301) |  Loss2: (0.0000) | Acc: (81.00%) (7405/9088)
Epoch: 33 | Batch_idx: 80 |  Loss: (0.5287) |  Loss2: (0.0000) | Acc: (81.00%) (8453/10368)
Epoch: 33 | Batch_idx: 90 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (9511/11648)
Epoch: 33 | Batch_idx: 100 |  Loss: (0.5243) |  Loss2: (0.0000) | Acc: (81.00%) (10571/12928)
Epoch: 33 | Batch_idx: 110 |  Loss: (0.5275) |  Loss2: (0.0000) | Acc: (81.00%) (11608/14208)
Epoch: 33 | Batch_idx: 120 |  Loss: (0.5269) |  Loss2: (0.0000) | Acc: (81.00%) (12652/15488)
Epoch: 33 | Batch_idx: 130 |  Loss: (0.5292) |  Loss2: (0.0000) | Acc: (81.00%) (13685/16768)
Epoch: 33 | Batch_idx: 140 |  Loss: (0.5314) |  Loss2: (0.0000) | Acc: (81.00%) (14714/18048)
Epoch: 33 | Batch_idx: 150 |  Loss: (0.5312) |  Loss2: (0.0000) | Acc: (81.00%) (15760/19328)
Epoch: 33 | Batch_idx: 160 |  Loss: (0.5290) |  Loss2: (0.0000) | Acc: (81.00%) (16816/20608)
Epoch: 33 | Batch_idx: 170 |  Loss: (0.5279) |  Loss2: (0.0000) | Acc: (81.00%) (17882/21888)
Epoch: 33 | Batch_idx: 180 |  Loss: (0.5294) |  Loss2: (0.0000) | Acc: (81.00%) (18918/23168)
Epoch: 33 | Batch_idx: 190 |  Loss: (0.5291) |  Loss2: (0.0000) | Acc: (81.00%) (19964/24448)
Epoch: 33 | Batch_idx: 200 |  Loss: (0.5277) |  Loss2: (0.0000) | Acc: (81.00%) (21017/25728)
Epoch: 33 | Batch_idx: 210 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (22082/27008)
Epoch: 33 | Batch_idx: 220 |  Loss: (0.5254) |  Loss2: (0.0000) | Acc: (81.00%) (23142/28288)
Epoch: 33 | Batch_idx: 230 |  Loss: (0.5259) |  Loss2: (0.0000) | Acc: (81.00%) (24185/29568)
Epoch: 33 | Batch_idx: 240 |  Loss: (0.5258) |  Loss2: (0.0000) | Acc: (81.00%) (25236/30848)
Epoch: 33 | Batch_idx: 250 |  Loss: (0.5252) |  Loss2: (0.0000) | Acc: (81.00%) (26286/32128)
Epoch: 33 | Batch_idx: 260 |  Loss: (0.5240) |  Loss2: (0.0000) | Acc: (81.00%) (27336/33408)
Epoch: 33 | Batch_idx: 270 |  Loss: (0.5233) |  Loss2: (0.0000) | Acc: (81.00%) (28381/34688)
Epoch: 33 | Batch_idx: 280 |  Loss: (0.5224) |  Loss2: (0.0000) | Acc: (81.00%) (29448/35968)
Epoch: 33 | Batch_idx: 290 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (30512/37248)
Epoch: 33 | Batch_idx: 300 |  Loss: (0.5230) |  Loss2: (0.0000) | Acc: (81.00%) (31531/38528)
Epoch: 33 | Batch_idx: 310 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (32581/39808)
Epoch: 33 | Batch_idx: 320 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (81.00%) (33632/41088)
Epoch: 33 | Batch_idx: 330 |  Loss: (0.5227) |  Loss2: (0.0000) | Acc: (81.00%) (34687/42368)
Epoch: 33 | Batch_idx: 340 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (35745/43648)
Epoch: 33 | Batch_idx: 350 |  Loss: (0.5226) |  Loss2: (0.0000) | Acc: (81.00%) (36778/44928)
Epoch: 33 | Batch_idx: 360 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (81.00%) (37812/46208)
Epoch: 33 | Batch_idx: 370 |  Loss: (0.5223) |  Loss2: (0.0000) | Acc: (81.00%) (38877/47488)
Epoch: 33 | Batch_idx: 380 |  Loss: (0.5232) |  Loss2: (0.0000) | Acc: (81.00%) (39912/48768)
Epoch: 33 | Batch_idx: 390 |  Loss: (0.5234) |  Loss2: (0.0000) | Acc: (81.00%) (40913/50000)
# TEST : Loss: (0.6824) | Acc: (77.00%) (7719/10000)
percent tensor([0.5246, 0.4754], device='cuda:0')
percent tensor([0.5239, 0.4761], device='cuda:0')
percent tensor([0.4936, 0.5064], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.5454, 0.4546], device='cuda:0')
percent tensor([0.5586, 0.4414], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.9944, 0.0056], device='cuda:0')
Epoch: 34 | Batch_idx: 0 |  Loss: (0.5642) |  Loss2: (0.0000) | Acc: (80.00%) (103/128)
Epoch: 34 | Batch_idx: 10 |  Loss: (0.5229) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 34 | Batch_idx: 20 |  Loss: (0.5102) |  Loss2: (0.0000) | Acc: (83.00%) (2238/2688)
Epoch: 34 | Batch_idx: 30 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (3320/3968)
Epoch: 34 | Batch_idx: 40 |  Loss: (0.5038) |  Loss2: (0.0000) | Acc: (82.00%) (4350/5248)
Epoch: 34 | Batch_idx: 50 |  Loss: (0.5001) |  Loss2: (0.0000) | Acc: (82.00%) (5409/6528)
Epoch: 34 | Batch_idx: 60 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (6452/7808)
Epoch: 34 | Batch_idx: 70 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (7514/9088)
Epoch: 34 | Batch_idx: 80 |  Loss: (0.4997) |  Loss2: (0.0000) | Acc: (82.00%) (8586/10368)
Epoch: 34 | Batch_idx: 90 |  Loss: (0.5014) |  Loss2: (0.0000) | Acc: (82.00%) (9644/11648)
Epoch: 34 | Batch_idx: 100 |  Loss: (0.5013) |  Loss2: (0.0000) | Acc: (82.00%) (10714/12928)
Epoch: 34 | Batch_idx: 110 |  Loss: (0.5000) |  Loss2: (0.0000) | Acc: (82.00%) (11775/14208)
Epoch: 34 | Batch_idx: 120 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (12814/15488)
Epoch: 34 | Batch_idx: 130 |  Loss: (0.5004) |  Loss2: (0.0000) | Acc: (82.00%) (13883/16768)
Epoch: 34 | Batch_idx: 140 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (14937/18048)
Epoch: 34 | Batch_idx: 150 |  Loss: (0.5051) |  Loss2: (0.0000) | Acc: (82.00%) (15977/19328)
Epoch: 34 | Batch_idx: 160 |  Loss: (0.5068) |  Loss2: (0.0000) | Acc: (82.00%) (17004/20608)
Epoch: 34 | Batch_idx: 170 |  Loss: (0.5045) |  Loss2: (0.0000) | Acc: (82.00%) (18083/21888)
Epoch: 34 | Batch_idx: 180 |  Loss: (0.5043) |  Loss2: (0.0000) | Acc: (82.00%) (19153/23168)
Epoch: 34 | Batch_idx: 190 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (20201/24448)
Epoch: 34 | Batch_idx: 200 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (21251/25728)
Epoch: 34 | Batch_idx: 210 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (82.00%) (22313/27008)
Epoch: 34 | Batch_idx: 220 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (23366/28288)
Epoch: 34 | Batch_idx: 230 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (24426/29568)
Epoch: 34 | Batch_idx: 240 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (25479/30848)
Epoch: 34 | Batch_idx: 250 |  Loss: (0.5020) |  Loss2: (0.0000) | Acc: (82.00%) (26532/32128)
Epoch: 34 | Batch_idx: 260 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (27612/33408)
Epoch: 34 | Batch_idx: 270 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (82.00%) (28664/34688)
Epoch: 34 | Batch_idx: 280 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (29698/35968)
Epoch: 34 | Batch_idx: 290 |  Loss: (0.5027) |  Loss2: (0.0000) | Acc: (82.00%) (30737/37248)
Epoch: 34 | Batch_idx: 300 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (31797/38528)
Epoch: 34 | Batch_idx: 310 |  Loss: (0.5046) |  Loss2: (0.0000) | Acc: (82.00%) (32836/39808)
Epoch: 34 | Batch_idx: 320 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (33893/41088)
Epoch: 34 | Batch_idx: 330 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (34970/42368)
Epoch: 34 | Batch_idx: 340 |  Loss: (0.5047) |  Loss2: (0.0000) | Acc: (82.00%) (36017/43648)
Epoch: 34 | Batch_idx: 350 |  Loss: (0.5038) |  Loss2: (0.0000) | Acc: (82.00%) (37083/44928)
Epoch: 34 | Batch_idx: 360 |  Loss: (0.5017) |  Loss2: (0.0000) | Acc: (82.00%) (38173/46208)
Epoch: 34 | Batch_idx: 370 |  Loss: (0.5007) |  Loss2: (0.0000) | Acc: (82.00%) (39264/47488)
Epoch: 34 | Batch_idx: 380 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (40322/48768)
Epoch: 34 | Batch_idx: 390 |  Loss: (0.5008) |  Loss2: (0.0000) | Acc: (82.00%) (41327/50000)
# TEST : Loss: (0.6236) | Acc: (79.00%) (7939/10000)
percent tensor([0.5248, 0.4752], device='cuda:0')
percent tensor([0.5244, 0.4756], device='cuda:0')
percent tensor([0.4935, 0.5065], device='cuda:0')
percent tensor([0.5830, 0.4170], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.5591, 0.4409], device='cuda:0')
percent tensor([0.5649, 0.4351], device='cuda:0')
percent tensor([0.9946, 0.0054], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 35 | Batch_idx: 0 |  Loss: (0.3281) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 35 | Batch_idx: 10 |  Loss: (0.4981) |  Loss2: (0.0000) | Acc: (82.00%) (1167/1408)
Epoch: 35 | Batch_idx: 20 |  Loss: (0.5668) |  Loss2: (0.0000) | Acc: (80.00%) (2168/2688)
Epoch: 35 | Batch_idx: 30 |  Loss: (0.5962) |  Loss2: (0.0000) | Acc: (79.00%) (3156/3968)
Epoch: 35 | Batch_idx: 40 |  Loss: (0.6079) |  Loss2: (0.0000) | Acc: (78.00%) (4142/5248)
Epoch: 35 | Batch_idx: 50 |  Loss: (0.6194) |  Loss2: (0.0000) | Acc: (78.00%) (5130/6528)
Epoch: 35 | Batch_idx: 60 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (6114/7808)
Epoch: 35 | Batch_idx: 70 |  Loss: (0.6389) |  Loss2: (0.0000) | Acc: (77.00%) (7080/9088)
Epoch: 35 | Batch_idx: 80 |  Loss: (0.6450) |  Loss2: (0.0000) | Acc: (77.00%) (8066/10368)
Epoch: 35 | Batch_idx: 90 |  Loss: (0.6518) |  Loss2: (0.0000) | Acc: (77.00%) (9023/11648)
Epoch: 35 | Batch_idx: 100 |  Loss: (0.6544) |  Loss2: (0.0000) | Acc: (77.00%) (10001/12928)
Epoch: 35 | Batch_idx: 110 |  Loss: (0.6593) |  Loss2: (0.0000) | Acc: (77.00%) (10964/14208)
Epoch: 35 | Batch_idx: 120 |  Loss: (0.6609) |  Loss2: (0.0000) | Acc: (77.00%) (11949/15488)
Epoch: 35 | Batch_idx: 130 |  Loss: (0.6592) |  Loss2: (0.0000) | Acc: (77.00%) (12956/16768)
Epoch: 35 | Batch_idx: 140 |  Loss: (0.6551) |  Loss2: (0.0000) | Acc: (77.00%) (13958/18048)
Epoch: 35 | Batch_idx: 150 |  Loss: (0.6546) |  Loss2: (0.0000) | Acc: (77.00%) (14938/19328)
Epoch: 35 | Batch_idx: 160 |  Loss: (0.6519) |  Loss2: (0.0000) | Acc: (77.00%) (15945/20608)
Epoch: 35 | Batch_idx: 170 |  Loss: (0.6496) |  Loss2: (0.0000) | Acc: (77.00%) (16952/21888)
Epoch: 35 | Batch_idx: 180 |  Loss: (0.6452) |  Loss2: (0.0000) | Acc: (77.00%) (17967/23168)
Epoch: 35 | Batch_idx: 190 |  Loss: (0.6438) |  Loss2: (0.0000) | Acc: (77.00%) (18976/24448)
Epoch: 35 | Batch_idx: 200 |  Loss: (0.6393) |  Loss2: (0.0000) | Acc: (77.00%) (20015/25728)
Epoch: 35 | Batch_idx: 210 |  Loss: (0.6379) |  Loss2: (0.0000) | Acc: (77.00%) (21025/27008)
Epoch: 35 | Batch_idx: 220 |  Loss: (0.6358) |  Loss2: (0.0000) | Acc: (77.00%) (22048/28288)
Epoch: 35 | Batch_idx: 230 |  Loss: (0.6347) |  Loss2: (0.0000) | Acc: (77.00%) (23060/29568)
Epoch: 35 | Batch_idx: 240 |  Loss: (0.6336) |  Loss2: (0.0000) | Acc: (78.00%) (24065/30848)
Epoch: 35 | Batch_idx: 250 |  Loss: (0.6330) |  Loss2: (0.0000) | Acc: (78.00%) (25072/32128)
Epoch: 35 | Batch_idx: 260 |  Loss: (0.6312) |  Loss2: (0.0000) | Acc: (78.00%) (26086/33408)
Epoch: 35 | Batch_idx: 270 |  Loss: (0.6292) |  Loss2: (0.0000) | Acc: (78.00%) (27116/34688)
Epoch: 35 | Batch_idx: 280 |  Loss: (0.6272) |  Loss2: (0.0000) | Acc: (78.00%) (28136/35968)
Epoch: 35 | Batch_idx: 290 |  Loss: (0.6265) |  Loss2: (0.0000) | Acc: (78.00%) (29143/37248)
Epoch: 35 | Batch_idx: 300 |  Loss: (0.6237) |  Loss2: (0.0000) | Acc: (78.00%) (30180/38528)
Epoch: 35 | Batch_idx: 310 |  Loss: (0.6224) |  Loss2: (0.0000) | Acc: (78.00%) (31200/39808)
Epoch: 35 | Batch_idx: 320 |  Loss: (0.6223) |  Loss2: (0.0000) | Acc: (78.00%) (32216/41088)
Epoch: 35 | Batch_idx: 330 |  Loss: (0.6203) |  Loss2: (0.0000) | Acc: (78.00%) (33243/42368)
Epoch: 35 | Batch_idx: 340 |  Loss: (0.6196) |  Loss2: (0.0000) | Acc: (78.00%) (34260/43648)
Epoch: 35 | Batch_idx: 350 |  Loss: (0.6180) |  Loss2: (0.0000) | Acc: (78.00%) (35289/44928)
Epoch: 35 | Batch_idx: 360 |  Loss: (0.6158) |  Loss2: (0.0000) | Acc: (78.00%) (36302/46208)
Epoch: 35 | Batch_idx: 370 |  Loss: (0.6149) |  Loss2: (0.0000) | Acc: (78.00%) (37325/47488)
Epoch: 35 | Batch_idx: 380 |  Loss: (0.6138) |  Loss2: (0.0000) | Acc: (78.00%) (38356/48768)
Epoch: 35 | Batch_idx: 390 |  Loss: (0.6124) |  Loss2: (0.0000) | Acc: (78.00%) (39335/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_035.pth.tar'
# TEST : Loss: (0.6350) | Acc: (78.00%) (7832/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5262, 0.4738], device='cuda:0')
percent tensor([0.4891, 0.5109], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.5351, 0.4649], device='cuda:0')
percent tensor([0.5611, 0.4389], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.9943, 0.0057], device='cuda:0')
Epoch: 36 | Batch_idx: 0 |  Loss: (0.6135) |  Loss2: (0.0000) | Acc: (78.00%) (100/128)
Epoch: 36 | Batch_idx: 10 |  Loss: (0.5732) |  Loss2: (0.0000) | Acc: (79.00%) (1126/1408)
Epoch: 36 | Batch_idx: 20 |  Loss: (0.5720) |  Loss2: (0.0000) | Acc: (79.00%) (2144/2688)
Epoch: 36 | Batch_idx: 30 |  Loss: (0.5772) |  Loss2: (0.0000) | Acc: (79.00%) (3155/3968)
Epoch: 36 | Batch_idx: 40 |  Loss: (0.5679) |  Loss2: (0.0000) | Acc: (80.00%) (4199/5248)
Epoch: 36 | Batch_idx: 50 |  Loss: (0.5624) |  Loss2: (0.0000) | Acc: (80.00%) (5237/6528)
Epoch: 36 | Batch_idx: 60 |  Loss: (0.5611) |  Loss2: (0.0000) | Acc: (80.00%) (6267/7808)
Epoch: 36 | Batch_idx: 70 |  Loss: (0.5583) |  Loss2: (0.0000) | Acc: (80.00%) (7312/9088)
Epoch: 36 | Batch_idx: 80 |  Loss: (0.5630) |  Loss2: (0.0000) | Acc: (80.00%) (8321/10368)
Epoch: 36 | Batch_idx: 90 |  Loss: (0.5627) |  Loss2: (0.0000) | Acc: (80.00%) (9355/11648)
Epoch: 36 | Batch_idx: 100 |  Loss: (0.5673) |  Loss2: (0.0000) | Acc: (80.00%) (10375/12928)
Epoch: 36 | Batch_idx: 110 |  Loss: (0.5684) |  Loss2: (0.0000) | Acc: (80.00%) (11402/14208)
Epoch: 36 | Batch_idx: 120 |  Loss: (0.5635) |  Loss2: (0.0000) | Acc: (80.00%) (12456/15488)
Epoch: 36 | Batch_idx: 130 |  Loss: (0.5648) |  Loss2: (0.0000) | Acc: (80.00%) (13465/16768)
Epoch: 36 | Batch_idx: 140 |  Loss: (0.5625) |  Loss2: (0.0000) | Acc: (80.00%) (14501/18048)
Epoch: 36 | Batch_idx: 150 |  Loss: (0.5594) |  Loss2: (0.0000) | Acc: (80.00%) (15551/19328)
Epoch: 36 | Batch_idx: 160 |  Loss: (0.5584) |  Loss2: (0.0000) | Acc: (80.00%) (16599/20608)
Epoch: 36 | Batch_idx: 170 |  Loss: (0.5571) |  Loss2: (0.0000) | Acc: (80.00%) (17649/21888)
Epoch: 36 | Batch_idx: 180 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (80.00%) (18677/23168)
Epoch: 36 | Batch_idx: 190 |  Loss: (0.5582) |  Loss2: (0.0000) | Acc: (80.00%) (19711/24448)
Epoch: 36 | Batch_idx: 200 |  Loss: (0.5591) |  Loss2: (0.0000) | Acc: (80.00%) (20747/25728)
Epoch: 36 | Batch_idx: 210 |  Loss: (0.5569) |  Loss2: (0.0000) | Acc: (80.00%) (21796/27008)
Epoch: 36 | Batch_idx: 220 |  Loss: (0.5541) |  Loss2: (0.0000) | Acc: (80.00%) (22854/28288)
Epoch: 36 | Batch_idx: 230 |  Loss: (0.5516) |  Loss2: (0.0000) | Acc: (80.00%) (23919/29568)
Epoch: 36 | Batch_idx: 240 |  Loss: (0.5510) |  Loss2: (0.0000) | Acc: (80.00%) (24962/30848)
Epoch: 36 | Batch_idx: 250 |  Loss: (0.5494) |  Loss2: (0.0000) | Acc: (80.00%) (26017/32128)
Epoch: 36 | Batch_idx: 260 |  Loss: (0.5501) |  Loss2: (0.0000) | Acc: (80.00%) (27043/33408)
Epoch: 36 | Batch_idx: 270 |  Loss: (0.5499) |  Loss2: (0.0000) | Acc: (80.00%) (28067/34688)
Epoch: 36 | Batch_idx: 280 |  Loss: (0.5479) |  Loss2: (0.0000) | Acc: (80.00%) (29125/35968)
Epoch: 36 | Batch_idx: 290 |  Loss: (0.5471) |  Loss2: (0.0000) | Acc: (80.00%) (30168/37248)
Epoch: 36 | Batch_idx: 300 |  Loss: (0.5466) |  Loss2: (0.0000) | Acc: (81.00%) (31215/38528)
Epoch: 36 | Batch_idx: 310 |  Loss: (0.5471) |  Loss2: (0.0000) | Acc: (81.00%) (32255/39808)
Epoch: 36 | Batch_idx: 320 |  Loss: (0.5457) |  Loss2: (0.0000) | Acc: (81.00%) (33311/41088)
Epoch: 36 | Batch_idx: 330 |  Loss: (0.5448) |  Loss2: (0.0000) | Acc: (81.00%) (34360/42368)
Epoch: 36 | Batch_idx: 340 |  Loss: (0.5453) |  Loss2: (0.0000) | Acc: (81.00%) (35392/43648)
Epoch: 36 | Batch_idx: 350 |  Loss: (0.5455) |  Loss2: (0.0000) | Acc: (81.00%) (36425/44928)
Epoch: 36 | Batch_idx: 360 |  Loss: (0.5459) |  Loss2: (0.0000) | Acc: (81.00%) (37450/46208)
Epoch: 36 | Batch_idx: 370 |  Loss: (0.5451) |  Loss2: (0.0000) | Acc: (81.00%) (38509/47488)
Epoch: 36 | Batch_idx: 380 |  Loss: (0.5446) |  Loss2: (0.0000) | Acc: (81.00%) (39550/48768)
Epoch: 36 | Batch_idx: 390 |  Loss: (0.5432) |  Loss2: (0.0000) | Acc: (81.00%) (40582/50000)
# TEST : Loss: (0.5855) | Acc: (79.00%) (7989/10000)
percent tensor([0.5512, 0.4488], device='cuda:0')
percent tensor([0.5272, 0.4728], device='cuda:0')
percent tensor([0.4906, 0.5094], device='cuda:0')
percent tensor([0.5707, 0.4293], device='cuda:0')
percent tensor([0.5322, 0.4678], device='cuda:0')
percent tensor([0.5686, 0.4314], device='cuda:0')
percent tensor([0.5386, 0.4614], device='cuda:0')
percent tensor([0.9954, 0.0046], device='cuda:0')
Epoch: 37 | Batch_idx: 0 |  Loss: (0.4772) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 37 | Batch_idx: 10 |  Loss: (0.5307) |  Loss2: (0.0000) | Acc: (81.00%) (1144/1408)
Epoch: 37 | Batch_idx: 20 |  Loss: (0.5274) |  Loss2: (0.0000) | Acc: (81.00%) (2179/2688)
Epoch: 37 | Batch_idx: 30 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (81.00%) (3249/3968)
Epoch: 37 | Batch_idx: 40 |  Loss: (0.5199) |  Loss2: (0.0000) | Acc: (81.00%) (4299/5248)
Epoch: 37 | Batch_idx: 50 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (82.00%) (5354/6528)
Epoch: 37 | Batch_idx: 60 |  Loss: (0.5220) |  Loss2: (0.0000) | Acc: (82.00%) (6407/7808)
Epoch: 37 | Batch_idx: 70 |  Loss: (0.5231) |  Loss2: (0.0000) | Acc: (81.00%) (7439/9088)
Epoch: 37 | Batch_idx: 80 |  Loss: (0.5260) |  Loss2: (0.0000) | Acc: (81.00%) (8481/10368)
Epoch: 37 | Batch_idx: 90 |  Loss: (0.5237) |  Loss2: (0.0000) | Acc: (81.00%) (9538/11648)
Epoch: 37 | Batch_idx: 100 |  Loss: (0.5191) |  Loss2: (0.0000) | Acc: (82.00%) (10602/12928)
Epoch: 37 | Batch_idx: 110 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (11643/14208)
Epoch: 37 | Batch_idx: 120 |  Loss: (0.5189) |  Loss2: (0.0000) | Acc: (82.00%) (12707/15488)
Epoch: 37 | Batch_idx: 130 |  Loss: (0.5198) |  Loss2: (0.0000) | Acc: (82.00%) (13759/16768)
Epoch: 37 | Batch_idx: 140 |  Loss: (0.5209) |  Loss2: (0.0000) | Acc: (82.00%) (14814/18048)
Epoch: 37 | Batch_idx: 150 |  Loss: (0.5219) |  Loss2: (0.0000) | Acc: (81.00%) (15838/19328)
Epoch: 37 | Batch_idx: 160 |  Loss: (0.5216) |  Loss2: (0.0000) | Acc: (81.00%) (16876/20608)
Epoch: 37 | Batch_idx: 170 |  Loss: (0.5204) |  Loss2: (0.0000) | Acc: (81.00%) (17937/21888)
Epoch: 37 | Batch_idx: 180 |  Loss: (0.5215) |  Loss2: (0.0000) | Acc: (81.00%) (18982/23168)
Epoch: 37 | Batch_idx: 190 |  Loss: (0.5200) |  Loss2: (0.0000) | Acc: (81.00%) (20040/24448)
Epoch: 37 | Batch_idx: 200 |  Loss: (0.5178) |  Loss2: (0.0000) | Acc: (82.00%) (21106/25728)
Epoch: 37 | Batch_idx: 210 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (22152/27008)
Epoch: 37 | Batch_idx: 220 |  Loss: (0.5186) |  Loss2: (0.0000) | Acc: (82.00%) (23202/28288)
Epoch: 37 | Batch_idx: 230 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (82.00%) (24248/29568)
Epoch: 37 | Batch_idx: 240 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (25288/30848)
Epoch: 37 | Batch_idx: 250 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (81.00%) (26333/32128)
Epoch: 37 | Batch_idx: 260 |  Loss: (0.5196) |  Loss2: (0.0000) | Acc: (81.00%) (27369/33408)
Epoch: 37 | Batch_idx: 270 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (81.00%) (28427/34688)
Epoch: 37 | Batch_idx: 280 |  Loss: (0.5187) |  Loss2: (0.0000) | Acc: (81.00%) (29489/35968)
Epoch: 37 | Batch_idx: 290 |  Loss: (0.5184) |  Loss2: (0.0000) | Acc: (82.00%) (30547/37248)
Epoch: 37 | Batch_idx: 300 |  Loss: (0.5190) |  Loss2: (0.0000) | Acc: (82.00%) (31598/38528)
Epoch: 37 | Batch_idx: 310 |  Loss: (0.5183) |  Loss2: (0.0000) | Acc: (82.00%) (32656/39808)
Epoch: 37 | Batch_idx: 320 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (33714/41088)
Epoch: 37 | Batch_idx: 330 |  Loss: (0.5175) |  Loss2: (0.0000) | Acc: (82.00%) (34783/42368)
Epoch: 37 | Batch_idx: 340 |  Loss: (0.5174) |  Loss2: (0.0000) | Acc: (82.00%) (35822/43648)
Epoch: 37 | Batch_idx: 350 |  Loss: (0.5171) |  Loss2: (0.0000) | Acc: (82.00%) (36885/44928)
Epoch: 37 | Batch_idx: 360 |  Loss: (0.5176) |  Loss2: (0.0000) | Acc: (82.00%) (37928/46208)
Epoch: 37 | Batch_idx: 370 |  Loss: (0.5173) |  Loss2: (0.0000) | Acc: (82.00%) (38982/47488)
Epoch: 37 | Batch_idx: 380 |  Loss: (0.5177) |  Loss2: (0.0000) | Acc: (82.00%) (40022/48768)
Epoch: 37 | Batch_idx: 390 |  Loss: (0.5181) |  Loss2: (0.0000) | Acc: (82.00%) (41019/50000)
# TEST : Loss: (0.5674) | Acc: (80.00%) (8063/10000)
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5265, 0.4735], device='cuda:0')
percent tensor([0.4924, 0.5076], device='cuda:0')
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5532, 0.4468], device='cuda:0')
percent tensor([0.9964, 0.0036], device='cuda:0')
Epoch: 38 | Batch_idx: 0 |  Loss: (0.3551) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 38 | Batch_idx: 10 |  Loss: (0.4639) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 38 | Batch_idx: 20 |  Loss: (0.4941) |  Loss2: (0.0000) | Acc: (83.00%) (2232/2688)
Epoch: 38 | Batch_idx: 30 |  Loss: (0.5083) |  Loss2: (0.0000) | Acc: (82.00%) (3277/3968)
Epoch: 38 | Batch_idx: 40 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (83.00%) (4357/5248)
Epoch: 38 | Batch_idx: 50 |  Loss: (0.4990) |  Loss2: (0.0000) | Acc: (82.00%) (5407/6528)
Epoch: 38 | Batch_idx: 60 |  Loss: (0.5002) |  Loss2: (0.0000) | Acc: (82.00%) (6450/7808)
Epoch: 38 | Batch_idx: 70 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (7499/9088)
Epoch: 38 | Batch_idx: 80 |  Loss: (0.5018) |  Loss2: (0.0000) | Acc: (82.00%) (8567/10368)
Epoch: 38 | Batch_idx: 90 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (9619/11648)
Epoch: 38 | Batch_idx: 100 |  Loss: (0.5033) |  Loss2: (0.0000) | Acc: (82.00%) (10669/12928)
Epoch: 38 | Batch_idx: 110 |  Loss: (0.5049) |  Loss2: (0.0000) | Acc: (82.00%) (11711/14208)
Epoch: 38 | Batch_idx: 120 |  Loss: (0.5095) |  Loss2: (0.0000) | Acc: (82.00%) (12736/15488)
Epoch: 38 | Batch_idx: 130 |  Loss: (0.5096) |  Loss2: (0.0000) | Acc: (82.00%) (13791/16768)
Epoch: 38 | Batch_idx: 140 |  Loss: (0.5098) |  Loss2: (0.0000) | Acc: (82.00%) (14846/18048)
Epoch: 38 | Batch_idx: 150 |  Loss: (0.5071) |  Loss2: (0.0000) | Acc: (82.00%) (15931/19328)
Epoch: 38 | Batch_idx: 160 |  Loss: (0.5066) |  Loss2: (0.0000) | Acc: (82.00%) (16981/20608)
Epoch: 38 | Batch_idx: 170 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (18040/21888)
Epoch: 38 | Batch_idx: 180 |  Loss: (0.5041) |  Loss2: (0.0000) | Acc: (82.00%) (19108/23168)
Epoch: 38 | Batch_idx: 190 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (20186/24448)
Epoch: 38 | Batch_idx: 200 |  Loss: (0.5042) |  Loss2: (0.0000) | Acc: (82.00%) (21241/25728)
Epoch: 38 | Batch_idx: 210 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (22333/27008)
Epoch: 38 | Batch_idx: 220 |  Loss: (0.5024) |  Loss2: (0.0000) | Acc: (82.00%) (23380/28288)
Epoch: 38 | Batch_idx: 230 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (24437/29568)
Epoch: 38 | Batch_idx: 240 |  Loss: (0.5012) |  Loss2: (0.0000) | Acc: (82.00%) (25505/30848)
Epoch: 38 | Batch_idx: 250 |  Loss: (0.5010) |  Loss2: (0.0000) | Acc: (82.00%) (26569/32128)
Epoch: 38 | Batch_idx: 260 |  Loss: (0.5022) |  Loss2: (0.0000) | Acc: (82.00%) (27608/33408)
Epoch: 38 | Batch_idx: 270 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (28664/34688)
Epoch: 38 | Batch_idx: 280 |  Loss: (0.5025) |  Loss2: (0.0000) | Acc: (82.00%) (29725/35968)
Epoch: 38 | Batch_idx: 290 |  Loss: (0.5032) |  Loss2: (0.0000) | Acc: (82.00%) (30777/37248)
Epoch: 38 | Batch_idx: 300 |  Loss: (0.5029) |  Loss2: (0.0000) | Acc: (82.00%) (31829/38528)
Epoch: 38 | Batch_idx: 310 |  Loss: (0.5037) |  Loss2: (0.0000) | Acc: (82.00%) (32874/39808)
Epoch: 38 | Batch_idx: 320 |  Loss: (0.5058) |  Loss2: (0.0000) | Acc: (82.00%) (33902/41088)
Epoch: 38 | Batch_idx: 330 |  Loss: (0.5053) |  Loss2: (0.0000) | Acc: (82.00%) (34970/42368)
Epoch: 38 | Batch_idx: 340 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (36057/43648)
Epoch: 38 | Batch_idx: 350 |  Loss: (0.5028) |  Loss2: (0.0000) | Acc: (82.00%) (37138/44928)
Epoch: 38 | Batch_idx: 360 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (38212/46208)
Epoch: 38 | Batch_idx: 370 |  Loss: (0.5023) |  Loss2: (0.0000) | Acc: (82.00%) (39255/47488)
Epoch: 38 | Batch_idx: 380 |  Loss: (0.5021) |  Loss2: (0.0000) | Acc: (82.00%) (40320/48768)
Epoch: 38 | Batch_idx: 390 |  Loss: (0.5031) |  Loss2: (0.0000) | Acc: (82.00%) (41317/50000)
# TEST : Loss: (0.5557) | Acc: (81.00%) (8103/10000)
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.4949, 0.5051], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5357, 0.4643], device='cuda:0')
percent tensor([0.5780, 0.4220], device='cuda:0')
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.9969, 0.0031], device='cuda:0')
Epoch: 39 | Batch_idx: 0 |  Loss: (0.5606) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 39 | Batch_idx: 10 |  Loss: (0.5062) |  Loss2: (0.0000) | Acc: (83.00%) (1172/1408)
Epoch: 39 | Batch_idx: 20 |  Loss: (0.5016) |  Loss2: (0.0000) | Acc: (83.00%) (2235/2688)
Epoch: 39 | Batch_idx: 30 |  Loss: (0.4939) |  Loss2: (0.0000) | Acc: (83.00%) (3296/3968)
Epoch: 39 | Batch_idx: 40 |  Loss: (0.5011) |  Loss2: (0.0000) | Acc: (82.00%) (4331/5248)
Epoch: 39 | Batch_idx: 50 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (83.00%) (5424/6528)
Epoch: 39 | Batch_idx: 60 |  Loss: (0.4892) |  Loss2: (0.0000) | Acc: (83.00%) (6510/7808)
Epoch: 39 | Batch_idx: 70 |  Loss: (0.4947) |  Loss2: (0.0000) | Acc: (83.00%) (7556/9088)
Epoch: 39 | Batch_idx: 80 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (8626/10368)
Epoch: 39 | Batch_idx: 90 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (9695/11648)
Epoch: 39 | Batch_idx: 100 |  Loss: (0.4932) |  Loss2: (0.0000) | Acc: (83.00%) (10754/12928)
Epoch: 39 | Batch_idx: 110 |  Loss: (0.4966) |  Loss2: (0.0000) | Acc: (83.00%) (11801/14208)
Epoch: 39 | Batch_idx: 120 |  Loss: (0.4965) |  Loss2: (0.0000) | Acc: (83.00%) (12870/15488)
Epoch: 39 | Batch_idx: 130 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (13908/16768)
Epoch: 39 | Batch_idx: 140 |  Loss: (0.4995) |  Loss2: (0.0000) | Acc: (82.00%) (14968/18048)
Epoch: 39 | Batch_idx: 150 |  Loss: (0.4986) |  Loss2: (0.0000) | Acc: (82.00%) (16040/19328)
Epoch: 39 | Batch_idx: 160 |  Loss: (0.4989) |  Loss2: (0.0000) | Acc: (82.00%) (17091/20608)
Epoch: 39 | Batch_idx: 170 |  Loss: (0.4993) |  Loss2: (0.0000) | Acc: (82.00%) (18156/21888)
Epoch: 39 | Batch_idx: 180 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (19221/23168)
Epoch: 39 | Batch_idx: 190 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (20264/24448)
Epoch: 39 | Batch_idx: 200 |  Loss: (0.4984) |  Loss2: (0.0000) | Acc: (82.00%) (21324/25728)
Epoch: 39 | Batch_idx: 210 |  Loss: (0.4975) |  Loss2: (0.0000) | Acc: (82.00%) (22387/27008)
Epoch: 39 | Batch_idx: 220 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (23451/28288)
Epoch: 39 | Batch_idx: 230 |  Loss: (0.4980) |  Loss2: (0.0000) | Acc: (82.00%) (24515/29568)
Epoch: 39 | Batch_idx: 240 |  Loss: (0.4976) |  Loss2: (0.0000) | Acc: (82.00%) (25576/30848)
Epoch: 39 | Batch_idx: 250 |  Loss: (0.4954) |  Loss2: (0.0000) | Acc: (82.00%) (26657/32128)
Epoch: 39 | Batch_idx: 260 |  Loss: (0.4945) |  Loss2: (0.0000) | Acc: (83.00%) (27729/33408)
Epoch: 39 | Batch_idx: 270 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (28758/34688)
Epoch: 39 | Batch_idx: 280 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (29811/35968)
Epoch: 39 | Batch_idx: 290 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (30880/37248)
Epoch: 39 | Batch_idx: 300 |  Loss: (0.4963) |  Loss2: (0.0000) | Acc: (82.00%) (31934/38528)
Epoch: 39 | Batch_idx: 310 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (32980/39808)
Epoch: 39 | Batch_idx: 320 |  Loss: (0.4978) |  Loss2: (0.0000) | Acc: (82.00%) (34036/41088)
Epoch: 39 | Batch_idx: 330 |  Loss: (0.4963) |  Loss2: (0.0000) | Acc: (82.00%) (35115/42368)
Epoch: 39 | Batch_idx: 340 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (36187/43648)
Epoch: 39 | Batch_idx: 350 |  Loss: (0.4956) |  Loss2: (0.0000) | Acc: (82.00%) (37257/44928)
Epoch: 39 | Batch_idx: 360 |  Loss: (0.4966) |  Loss2: (0.0000) | Acc: (82.00%) (38294/46208)
Epoch: 39 | Batch_idx: 370 |  Loss: (0.4961) |  Loss2: (0.0000) | Acc: (82.00%) (39365/47488)
Epoch: 39 | Batch_idx: 380 |  Loss: (0.4968) |  Loss2: (0.0000) | Acc: (82.00%) (40416/48768)
Epoch: 39 | Batch_idx: 390 |  Loss: (0.4972) |  Loss2: (0.0000) | Acc: (82.00%) (41427/50000)
# TEST : Loss: (0.5510) | Acc: (81.00%) (8119/10000)
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5288, 0.4712], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5674, 0.4326], device='cuda:0')
percent tensor([0.5362, 0.4638], device='cuda:0')
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.9975, 0.0025], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 40 | Batch_idx: 0 |  Loss: (0.4998) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 40 | Batch_idx: 10 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (81.00%) (1154/1408)
Epoch: 40 | Batch_idx: 20 |  Loss: (0.5040) |  Loss2: (0.0000) | Acc: (82.00%) (2217/2688)
Epoch: 40 | Batch_idx: 30 |  Loss: (0.5030) |  Loss2: (0.0000) | Acc: (82.00%) (3278/3968)
Epoch: 40 | Batch_idx: 40 |  Loss: (0.5050) |  Loss2: (0.0000) | Acc: (82.00%) (4335/5248)
Epoch: 40 | Batch_idx: 50 |  Loss: (0.5034) |  Loss2: (0.0000) | Acc: (82.00%) (5395/6528)
Epoch: 40 | Batch_idx: 60 |  Loss: (0.5015) |  Loss2: (0.0000) | Acc: (82.00%) (6456/7808)
Epoch: 40 | Batch_idx: 70 |  Loss: (0.4977) |  Loss2: (0.0000) | Acc: (82.00%) (7535/9088)
Epoch: 40 | Batch_idx: 80 |  Loss: (0.4979) |  Loss2: (0.0000) | Acc: (82.00%) (8594/10368)
Epoch: 40 | Batch_idx: 90 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (9677/11648)
Epoch: 40 | Batch_idx: 100 |  Loss: (0.4900) |  Loss2: (0.0000) | Acc: (83.00%) (10761/12928)
Epoch: 40 | Batch_idx: 110 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (11818/14208)
Epoch: 40 | Batch_idx: 120 |  Loss: (0.4923) |  Loss2: (0.0000) | Acc: (83.00%) (12879/15488)
Epoch: 40 | Batch_idx: 130 |  Loss: (0.4926) |  Loss2: (0.0000) | Acc: (83.00%) (13944/16768)
Epoch: 40 | Batch_idx: 140 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (14996/18048)
Epoch: 40 | Batch_idx: 150 |  Loss: (0.4897) |  Loss2: (0.0000) | Acc: (83.00%) (16089/19328)
Epoch: 40 | Batch_idx: 160 |  Loss: (0.4928) |  Loss2: (0.0000) | Acc: (83.00%) (17139/20608)
Epoch: 40 | Batch_idx: 170 |  Loss: (0.4899) |  Loss2: (0.0000) | Acc: (83.00%) (18231/21888)
Epoch: 40 | Batch_idx: 180 |  Loss: (0.4911) |  Loss2: (0.0000) | Acc: (83.00%) (19281/23168)
Epoch: 40 | Batch_idx: 190 |  Loss: (0.4907) |  Loss2: (0.0000) | Acc: (83.00%) (20354/24448)
Epoch: 40 | Batch_idx: 200 |  Loss: (0.4916) |  Loss2: (0.0000) | Acc: (83.00%) (21406/25728)
Epoch: 40 | Batch_idx: 210 |  Loss: (0.4910) |  Loss2: (0.0000) | Acc: (83.00%) (22475/27008)
Epoch: 40 | Batch_idx: 220 |  Loss: (0.4927) |  Loss2: (0.0000) | Acc: (83.00%) (23505/28288)
Epoch: 40 | Batch_idx: 230 |  Loss: (0.4934) |  Loss2: (0.0000) | Acc: (83.00%) (24556/29568)
Epoch: 40 | Batch_idx: 240 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (25606/30848)
Epoch: 40 | Batch_idx: 250 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (82.00%) (26661/32128)
Epoch: 40 | Batch_idx: 260 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (27734/33408)
Epoch: 40 | Batch_idx: 270 |  Loss: (0.4935) |  Loss2: (0.0000) | Acc: (83.00%) (28809/34688)
Epoch: 40 | Batch_idx: 280 |  Loss: (0.4938) |  Loss2: (0.0000) | Acc: (83.00%) (29865/35968)
Epoch: 40 | Batch_idx: 290 |  Loss: (0.4940) |  Loss2: (0.0000) | Acc: (83.00%) (30928/37248)
Epoch: 40 | Batch_idx: 300 |  Loss: (0.4942) |  Loss2: (0.0000) | Acc: (83.00%) (31989/38528)
Epoch: 40 | Batch_idx: 310 |  Loss: (0.4953) |  Loss2: (0.0000) | Acc: (83.00%) (33044/39808)
Epoch: 40 | Batch_idx: 320 |  Loss: (0.4952) |  Loss2: (0.0000) | Acc: (83.00%) (34106/41088)
Epoch: 40 | Batch_idx: 330 |  Loss: (0.4951) |  Loss2: (0.0000) | Acc: (83.00%) (35171/42368)
Epoch: 40 | Batch_idx: 340 |  Loss: (0.4944) |  Loss2: (0.0000) | Acc: (83.00%) (36242/43648)
Epoch: 40 | Batch_idx: 350 |  Loss: (0.4941) |  Loss2: (0.0000) | Acc: (83.00%) (37306/44928)
Epoch: 40 | Batch_idx: 360 |  Loss: (0.4943) |  Loss2: (0.0000) | Acc: (83.00%) (38363/46208)
Epoch: 40 | Batch_idx: 370 |  Loss: (0.4929) |  Loss2: (0.0000) | Acc: (83.00%) (39442/47488)
Epoch: 40 | Batch_idx: 380 |  Loss: (0.4924) |  Loss2: (0.0000) | Acc: (83.00%) (40521/48768)
Epoch: 40 | Batch_idx: 390 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (83.00%) (41555/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_040.pth.tar'
# TEST : Loss: (0.6020) | Acc: (80.00%) (8005/10000)
percent tensor([0.5508, 0.4492], device='cuda:0')
percent tensor([0.5293, 0.4707], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5673, 0.4327], device='cuda:0')
percent tensor([0.5363, 0.4637], device='cuda:0')
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.5795, 0.4205], device='cuda:0')
percent tensor([0.9973, 0.0027], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(173.4489, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.3569, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(788.5699, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1528.4121, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(503.2297, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2192.6248, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4304.7378, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1423.8434, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6090.1562, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12094.0361, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4024.0537, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(17052.0371, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 41 | Batch_idx: 0 |  Loss: (0.4702) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 41 | Batch_idx: 10 |  Loss: (0.4559) |  Loss2: (0.0000) | Acc: (84.00%) (1189/1408)
Epoch: 41 | Batch_idx: 20 |  Loss: (0.4662) |  Loss2: (0.0000) | Acc: (83.00%) (2253/2688)
Epoch: 41 | Batch_idx: 30 |  Loss: (0.4700) |  Loss2: (0.0000) | Acc: (83.00%) (3322/3968)
Epoch: 41 | Batch_idx: 40 |  Loss: (0.4749) |  Loss2: (0.0000) | Acc: (83.00%) (4375/5248)
Epoch: 41 | Batch_idx: 50 |  Loss: (0.4741) |  Loss2: (0.0000) | Acc: (83.00%) (5440/6528)
Epoch: 41 | Batch_idx: 60 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (6507/7808)
Epoch: 41 | Batch_idx: 70 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (7590/9088)
Epoch: 41 | Batch_idx: 80 |  Loss: (0.4750) |  Loss2: (0.0000) | Acc: (83.00%) (8653/10368)
Epoch: 41 | Batch_idx: 90 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (9717/11648)
Epoch: 41 | Batch_idx: 100 |  Loss: (0.4746) |  Loss2: (0.0000) | Acc: (83.00%) (10802/12928)
Epoch: 41 | Batch_idx: 110 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (11867/14208)
Epoch: 41 | Batch_idx: 120 |  Loss: (0.4779) |  Loss2: (0.0000) | Acc: (83.00%) (12929/15488)
Epoch: 41 | Batch_idx: 130 |  Loss: (0.4765) |  Loss2: (0.0000) | Acc: (83.00%) (14004/16768)
Epoch: 41 | Batch_idx: 140 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (15087/18048)
Epoch: 41 | Batch_idx: 150 |  Loss: (0.4751) |  Loss2: (0.0000) | Acc: (83.00%) (16174/19328)
Epoch: 41 | Batch_idx: 160 |  Loss: (0.4735) |  Loss2: (0.0000) | Acc: (83.00%) (17258/20608)
Epoch: 41 | Batch_idx: 170 |  Loss: (0.4742) |  Loss2: (0.0000) | Acc: (83.00%) (18325/21888)
Epoch: 41 | Batch_idx: 180 |  Loss: (0.4751) |  Loss2: (0.0000) | Acc: (83.00%) (19390/23168)
Epoch: 41 | Batch_idx: 190 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (20446/24448)
Epoch: 41 | Batch_idx: 200 |  Loss: (0.4767) |  Loss2: (0.0000) | Acc: (83.00%) (21519/25728)
Epoch: 41 | Batch_idx: 210 |  Loss: (0.4773) |  Loss2: (0.0000) | Acc: (83.00%) (22589/27008)
Epoch: 41 | Batch_idx: 220 |  Loss: (0.4770) |  Loss2: (0.0000) | Acc: (83.00%) (23648/28288)
Epoch: 41 | Batch_idx: 230 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (24731/29568)
Epoch: 41 | Batch_idx: 240 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (25795/30848)
Epoch: 41 | Batch_idx: 250 |  Loss: (0.4757) |  Loss2: (0.0000) | Acc: (83.00%) (26862/32128)
Epoch: 41 | Batch_idx: 260 |  Loss: (0.4745) |  Loss2: (0.0000) | Acc: (83.00%) (27949/33408)
Epoch: 41 | Batch_idx: 270 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (29039/34688)
Epoch: 41 | Batch_idx: 280 |  Loss: (0.4740) |  Loss2: (0.0000) | Acc: (83.00%) (30113/35968)
Epoch: 41 | Batch_idx: 290 |  Loss: (0.4741) |  Loss2: (0.0000) | Acc: (83.00%) (31175/37248)
Epoch: 41 | Batch_idx: 300 |  Loss: (0.4734) |  Loss2: (0.0000) | Acc: (83.00%) (32252/38528)
Epoch: 41 | Batch_idx: 310 |  Loss: (0.4760) |  Loss2: (0.0000) | Acc: (83.00%) (33281/39808)
Epoch: 41 | Batch_idx: 320 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (34340/41088)
Epoch: 41 | Batch_idx: 330 |  Loss: (0.4761) |  Loss2: (0.0000) | Acc: (83.00%) (35404/42368)
Epoch: 41 | Batch_idx: 340 |  Loss: (0.4758) |  Loss2: (0.0000) | Acc: (83.00%) (36481/43648)
Epoch: 41 | Batch_idx: 350 |  Loss: (0.4754) |  Loss2: (0.0000) | Acc: (83.00%) (37563/44928)
Epoch: 41 | Batch_idx: 360 |  Loss: (0.4762) |  Loss2: (0.0000) | Acc: (83.00%) (38611/46208)
Epoch: 41 | Batch_idx: 370 |  Loss: (0.4756) |  Loss2: (0.0000) | Acc: (83.00%) (39687/47488)
Epoch: 41 | Batch_idx: 380 |  Loss: (0.4748) |  Loss2: (0.0000) | Acc: (83.00%) (40767/48768)
Epoch: 41 | Batch_idx: 390 |  Loss: (0.4744) |  Loss2: (0.0000) | Acc: (83.00%) (41811/50000)
# TEST : Loss: (0.5882) | Acc: (80.00%) (8053/10000)
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5291, 0.4709], device='cuda:0')
percent tensor([0.4965, 0.5035], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.5365, 0.4635], device='cuda:0')
percent tensor([0.5785, 0.4215], device='cuda:0')
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 42 | Batch_idx: 0 |  Loss: (0.5118) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 42 | Batch_idx: 10 |  Loss: (0.4789) |  Loss2: (0.0000) | Acc: (83.00%) (1180/1408)
Epoch: 42 | Batch_idx: 20 |  Loss: (0.4641) |  Loss2: (0.0000) | Acc: (84.00%) (2262/2688)
Epoch: 42 | Batch_idx: 30 |  Loss: (0.4632) |  Loss2: (0.0000) | Acc: (84.00%) (3342/3968)
Epoch: 42 | Batch_idx: 40 |  Loss: (0.4596) |  Loss2: (0.0000) | Acc: (84.00%) (4428/5248)
Epoch: 42 | Batch_idx: 50 |  Loss: (0.4552) |  Loss2: (0.0000) | Acc: (84.00%) (5509/6528)
Epoch: 42 | Batch_idx: 60 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (6572/7808)
Epoch: 42 | Batch_idx: 70 |  Loss: (0.4594) |  Loss2: (0.0000) | Acc: (84.00%) (7649/9088)
Epoch: 42 | Batch_idx: 80 |  Loss: (0.4610) |  Loss2: (0.0000) | Acc: (84.00%) (8723/10368)
Epoch: 42 | Batch_idx: 90 |  Loss: (0.4607) |  Loss2: (0.0000) | Acc: (84.00%) (9798/11648)
Epoch: 42 | Batch_idx: 100 |  Loss: (0.4604) |  Loss2: (0.0000) | Acc: (84.00%) (10887/12928)
Epoch: 42 | Batch_idx: 110 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (11980/14208)
Epoch: 42 | Batch_idx: 120 |  Loss: (0.4577) |  Loss2: (0.0000) | Acc: (84.00%) (13055/15488)
Epoch: 42 | Batch_idx: 130 |  Loss: (0.4547) |  Loss2: (0.0000) | Acc: (84.00%) (14159/16768)
Epoch: 42 | Batch_idx: 140 |  Loss: (0.4549) |  Loss2: (0.0000) | Acc: (84.00%) (15242/18048)
Epoch: 42 | Batch_idx: 150 |  Loss: (0.4553) |  Loss2: (0.0000) | Acc: (84.00%) (16319/19328)
Epoch: 42 | Batch_idx: 160 |  Loss: (0.4579) |  Loss2: (0.0000) | Acc: (84.00%) (17381/20608)
Epoch: 42 | Batch_idx: 170 |  Loss: (0.4570) |  Loss2: (0.0000) | Acc: (84.00%) (18458/21888)
Epoch: 42 | Batch_idx: 180 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (19552/23168)
Epoch: 42 | Batch_idx: 190 |  Loss: (0.4556) |  Loss2: (0.0000) | Acc: (84.00%) (20630/24448)
Epoch: 42 | Batch_idx: 200 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (21702/25728)
Epoch: 42 | Batch_idx: 210 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (22775/27008)
Epoch: 42 | Batch_idx: 220 |  Loss: (0.4581) |  Loss2: (0.0000) | Acc: (84.00%) (23865/28288)
Epoch: 42 | Batch_idx: 230 |  Loss: (0.4583) |  Loss2: (0.0000) | Acc: (84.00%) (24939/29568)
Epoch: 42 | Batch_idx: 240 |  Loss: (0.4566) |  Loss2: (0.0000) | Acc: (84.00%) (26032/30848)
Epoch: 42 | Batch_idx: 250 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (27102/32128)
Epoch: 42 | Batch_idx: 260 |  Loss: (0.4583) |  Loss2: (0.0000) | Acc: (84.00%) (28160/33408)
Epoch: 42 | Batch_idx: 270 |  Loss: (0.4582) |  Loss2: (0.0000) | Acc: (84.00%) (29220/34688)
Epoch: 42 | Batch_idx: 280 |  Loss: (0.4588) |  Loss2: (0.0000) | Acc: (84.00%) (30293/35968)
Epoch: 42 | Batch_idx: 290 |  Loss: (0.4585) |  Loss2: (0.0000) | Acc: (84.00%) (31374/37248)
Epoch: 42 | Batch_idx: 300 |  Loss: (0.4576) |  Loss2: (0.0000) | Acc: (84.00%) (32470/38528)
Epoch: 42 | Batch_idx: 310 |  Loss: (0.4558) |  Loss2: (0.0000) | Acc: (84.00%) (33562/39808)
Epoch: 42 | Batch_idx: 320 |  Loss: (0.4565) |  Loss2: (0.0000) | Acc: (84.00%) (34628/41088)
Epoch: 42 | Batch_idx: 330 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (35696/42368)
Epoch: 42 | Batch_idx: 340 |  Loss: (0.4579) |  Loss2: (0.0000) | Acc: (84.00%) (36756/43648)
Epoch: 42 | Batch_idx: 350 |  Loss: (0.4584) |  Loss2: (0.0000) | Acc: (84.00%) (37824/44928)
Epoch: 42 | Batch_idx: 360 |  Loss: (0.4589) |  Loss2: (0.0000) | Acc: (84.00%) (38898/46208)
Epoch: 42 | Batch_idx: 370 |  Loss: (0.4571) |  Loss2: (0.0000) | Acc: (84.00%) (40004/47488)
Epoch: 42 | Batch_idx: 380 |  Loss: (0.4573) |  Loss2: (0.0000) | Acc: (84.00%) (41077/48768)
Epoch: 42 | Batch_idx: 390 |  Loss: (0.4574) |  Loss2: (0.0000) | Acc: (84.00%) (42121/50000)
# TEST : Loss: (0.5733) | Acc: (80.00%) (8080/10000)
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5292, 0.4708], device='cuda:0')
percent tensor([0.4967, 0.5033], device='cuda:0')
percent tensor([0.5666, 0.4334], device='cuda:0')
percent tensor([0.5368, 0.4632], device='cuda:0')
percent tensor([0.5775, 0.4225], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.9974, 0.0026], device='cuda:0')
Epoch: 43 | Batch_idx: 0 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 43 | Batch_idx: 10 |  Loss: (0.4375) |  Loss2: (0.0000) | Acc: (84.00%) (1185/1408)
Epoch: 43 | Batch_idx: 20 |  Loss: (0.4330) |  Loss2: (0.0000) | Acc: (84.00%) (2282/2688)
Epoch: 43 | Batch_idx: 30 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (3356/3968)
Epoch: 43 | Batch_idx: 40 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (4450/5248)
Epoch: 43 | Batch_idx: 50 |  Loss: (0.4386) |  Loss2: (0.0000) | Acc: (84.00%) (5529/6528)
Epoch: 43 | Batch_idx: 60 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (6624/7808)
Epoch: 43 | Batch_idx: 70 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (7725/9088)
Epoch: 43 | Batch_idx: 80 |  Loss: (0.4303) |  Loss2: (0.0000) | Acc: (85.00%) (8829/10368)
Epoch: 43 | Batch_idx: 90 |  Loss: (0.4366) |  Loss2: (0.0000) | Acc: (84.00%) (9894/11648)
Epoch: 43 | Batch_idx: 100 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (10955/12928)
Epoch: 43 | Batch_idx: 110 |  Loss: (0.4428) |  Loss2: (0.0000) | Acc: (84.00%) (12026/14208)
Epoch: 43 | Batch_idx: 120 |  Loss: (0.4440) |  Loss2: (0.0000) | Acc: (84.00%) (13106/15488)
Epoch: 43 | Batch_idx: 130 |  Loss: (0.4403) |  Loss2: (0.0000) | Acc: (84.00%) (14206/16768)
Epoch: 43 | Batch_idx: 140 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (15272/18048)
Epoch: 43 | Batch_idx: 150 |  Loss: (0.4407) |  Loss2: (0.0000) | Acc: (84.00%) (16365/19328)
Epoch: 43 | Batch_idx: 160 |  Loss: (0.4406) |  Loss2: (0.0000) | Acc: (84.00%) (17453/20608)
Epoch: 43 | Batch_idx: 170 |  Loss: (0.4389) |  Loss2: (0.0000) | Acc: (84.00%) (18549/21888)
Epoch: 43 | Batch_idx: 180 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (84.00%) (19643/23168)
Epoch: 43 | Batch_idx: 190 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (20749/24448)
Epoch: 43 | Batch_idx: 200 |  Loss: (0.4337) |  Loss2: (0.0000) | Acc: (84.00%) (21859/25728)
Epoch: 43 | Batch_idx: 210 |  Loss: (0.4351) |  Loss2: (0.0000) | Acc: (84.00%) (22932/27008)
Epoch: 43 | Batch_idx: 220 |  Loss: (0.4377) |  Loss2: (0.0000) | Acc: (84.00%) (24003/28288)
Epoch: 43 | Batch_idx: 230 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (25083/29568)
Epoch: 43 | Batch_idx: 240 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (26146/30848)
Epoch: 43 | Batch_idx: 250 |  Loss: (0.4402) |  Loss2: (0.0000) | Acc: (84.00%) (27241/32128)
Epoch: 43 | Batch_idx: 260 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (28323/33408)
Epoch: 43 | Batch_idx: 270 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (29422/34688)
Epoch: 43 | Batch_idx: 280 |  Loss: (0.4422) |  Loss2: (0.0000) | Acc: (84.00%) (30497/35968)
Epoch: 43 | Batch_idx: 290 |  Loss: (0.4429) |  Loss2: (0.0000) | Acc: (84.00%) (31573/37248)
Epoch: 43 | Batch_idx: 300 |  Loss: (0.4426) |  Loss2: (0.0000) | Acc: (84.00%) (32651/38528)
Epoch: 43 | Batch_idx: 310 |  Loss: (0.4420) |  Loss2: (0.0000) | Acc: (84.00%) (33728/39808)
Epoch: 43 | Batch_idx: 320 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (34823/41088)
Epoch: 43 | Batch_idx: 330 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (35904/42368)
Epoch: 43 | Batch_idx: 340 |  Loss: (0.4413) |  Loss2: (0.0000) | Acc: (84.00%) (36996/43648)
Epoch: 43 | Batch_idx: 350 |  Loss: (0.4409) |  Loss2: (0.0000) | Acc: (84.00%) (38097/44928)
Epoch: 43 | Batch_idx: 360 |  Loss: (0.4410) |  Loss2: (0.0000) | Acc: (84.00%) (39184/46208)
Epoch: 43 | Batch_idx: 370 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (40265/47488)
Epoch: 43 | Batch_idx: 380 |  Loss: (0.4408) |  Loss2: (0.0000) | Acc: (84.00%) (41355/48768)
Epoch: 43 | Batch_idx: 390 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (84.00%) (42384/50000)
# TEST : Loss: (0.6232) | Acc: (78.00%) (7898/10000)
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.5297, 0.4703], device='cuda:0')
percent tensor([0.4968, 0.5032], device='cuda:0')
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.5380, 0.4620], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 44 | Batch_idx: 0 |  Loss: (0.5431) |  Loss2: (0.0000) | Acc: (78.00%) (101/128)
Epoch: 44 | Batch_idx: 10 |  Loss: (0.4590) |  Loss2: (0.0000) | Acc: (84.00%) (1187/1408)
Epoch: 44 | Batch_idx: 20 |  Loss: (0.4506) |  Loss2: (0.0000) | Acc: (84.00%) (2269/2688)
Epoch: 44 | Batch_idx: 30 |  Loss: (0.4401) |  Loss2: (0.0000) | Acc: (84.00%) (3354/3968)
Epoch: 44 | Batch_idx: 40 |  Loss: (0.4449) |  Loss2: (0.0000) | Acc: (84.00%) (4432/5248)
Epoch: 44 | Batch_idx: 50 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (85.00%) (5551/6528)
Epoch: 44 | Batch_idx: 60 |  Loss: (0.4398) |  Loss2: (0.0000) | Acc: (85.00%) (6640/7808)
Epoch: 44 | Batch_idx: 70 |  Loss: (0.4472) |  Loss2: (0.0000) | Acc: (84.00%) (7703/9088)
Epoch: 44 | Batch_idx: 80 |  Loss: (0.4416) |  Loss2: (0.0000) | Acc: (85.00%) (8814/10368)
Epoch: 44 | Batch_idx: 90 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (9914/11648)
Epoch: 44 | Batch_idx: 100 |  Loss: (0.4388) |  Loss2: (0.0000) | Acc: (85.00%) (10990/12928)
Epoch: 44 | Batch_idx: 110 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (85.00%) (12091/14208)
Epoch: 44 | Batch_idx: 120 |  Loss: (0.4370) |  Loss2: (0.0000) | Acc: (85.00%) (13182/15488)
Epoch: 44 | Batch_idx: 130 |  Loss: (0.4363) |  Loss2: (0.0000) | Acc: (85.00%) (14271/16768)
Epoch: 44 | Batch_idx: 140 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (85.00%) (15373/18048)
Epoch: 44 | Batch_idx: 150 |  Loss: (0.4344) |  Loss2: (0.0000) | Acc: (85.00%) (16459/19328)
Epoch: 44 | Batch_idx: 160 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (85.00%) (17541/20608)
Epoch: 44 | Batch_idx: 170 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (85.00%) (18631/21888)
Epoch: 44 | Batch_idx: 180 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (19727/23168)
Epoch: 44 | Batch_idx: 190 |  Loss: (0.4331) |  Loss2: (0.0000) | Acc: (85.00%) (20821/24448)
Epoch: 44 | Batch_idx: 200 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (85.00%) (21930/25728)
Epoch: 44 | Batch_idx: 210 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (85.00%) (23001/27008)
Epoch: 44 | Batch_idx: 220 |  Loss: (0.4324) |  Loss2: (0.0000) | Acc: (85.00%) (24088/28288)
Epoch: 44 | Batch_idx: 230 |  Loss: (0.4315) |  Loss2: (0.0000) | Acc: (85.00%) (25191/29568)
Epoch: 44 | Batch_idx: 240 |  Loss: (0.4308) |  Loss2: (0.0000) | Acc: (85.00%) (26298/30848)
Epoch: 44 | Batch_idx: 250 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (27404/32128)
Epoch: 44 | Batch_idx: 260 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (28500/33408)
Epoch: 44 | Batch_idx: 270 |  Loss: (0.4307) |  Loss2: (0.0000) | Acc: (85.00%) (29576/34688)
Epoch: 44 | Batch_idx: 280 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (30674/35968)
Epoch: 44 | Batch_idx: 290 |  Loss: (0.4300) |  Loss2: (0.0000) | Acc: (85.00%) (31765/37248)
Epoch: 44 | Batch_idx: 300 |  Loss: (0.4297) |  Loss2: (0.0000) | Acc: (85.00%) (32859/38528)
Epoch: 44 | Batch_idx: 310 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (33958/39808)
Epoch: 44 | Batch_idx: 320 |  Loss: (0.4294) |  Loss2: (0.0000) | Acc: (85.00%) (35044/41088)
Epoch: 44 | Batch_idx: 330 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (85.00%) (36153/42368)
Epoch: 44 | Batch_idx: 340 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (85.00%) (37238/43648)
Epoch: 44 | Batch_idx: 350 |  Loss: (0.4295) |  Loss2: (0.0000) | Acc: (85.00%) (38309/44928)
Epoch: 44 | Batch_idx: 360 |  Loss: (0.4289) |  Loss2: (0.0000) | Acc: (85.00%) (39410/46208)
Epoch: 44 | Batch_idx: 370 |  Loss: (0.4282) |  Loss2: (0.0000) | Acc: (85.00%) (40513/47488)
Epoch: 44 | Batch_idx: 380 |  Loss: (0.4280) |  Loss2: (0.0000) | Acc: (85.00%) (41615/48768)
Epoch: 44 | Batch_idx: 390 |  Loss: (0.4269) |  Loss2: (0.0000) | Acc: (85.00%) (42679/50000)
# TEST : Loss: (0.5610) | Acc: (81.00%) (8144/10000)
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.5298, 0.4702], device='cuda:0')
percent tensor([0.4966, 0.5034], device='cuda:0')
percent tensor([0.5660, 0.4340], device='cuda:0')
percent tensor([0.5370, 0.4630], device='cuda:0')
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.9977, 0.0023], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 45 | Batch_idx: 0 |  Loss: (0.4219) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 45 | Batch_idx: 10 |  Loss: (0.4201) |  Loss2: (0.0000) | Acc: (85.00%) (1210/1408)
Epoch: 45 | Batch_idx: 20 |  Loss: (0.4317) |  Loss2: (0.0000) | Acc: (85.00%) (2293/2688)
Epoch: 45 | Batch_idx: 30 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (3363/3968)
Epoch: 45 | Batch_idx: 40 |  Loss: (0.4466) |  Loss2: (0.0000) | Acc: (84.00%) (4420/5248)
Epoch: 45 | Batch_idx: 50 |  Loss: (0.4414) |  Loss2: (0.0000) | Acc: (84.00%) (5505/6528)
Epoch: 45 | Batch_idx: 60 |  Loss: (0.4419) |  Loss2: (0.0000) | Acc: (84.00%) (6576/7808)
Epoch: 45 | Batch_idx: 70 |  Loss: (0.4452) |  Loss2: (0.0000) | Acc: (84.00%) (7652/9088)
Epoch: 45 | Batch_idx: 80 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (8726/10368)
Epoch: 45 | Batch_idx: 90 |  Loss: (0.4470) |  Loss2: (0.0000) | Acc: (84.00%) (9810/11648)
Epoch: 45 | Batch_idx: 100 |  Loss: (0.4482) |  Loss2: (0.0000) | Acc: (84.00%) (10893/12928)
Epoch: 45 | Batch_idx: 110 |  Loss: (0.4485) |  Loss2: (0.0000) | Acc: (84.00%) (11969/14208)
Epoch: 45 | Batch_idx: 120 |  Loss: (0.4489) |  Loss2: (0.0000) | Acc: (84.00%) (13053/15488)
Epoch: 45 | Batch_idx: 130 |  Loss: (0.4509) |  Loss2: (0.0000) | Acc: (84.00%) (14129/16768)
Epoch: 45 | Batch_idx: 140 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (15203/18048)
Epoch: 45 | Batch_idx: 150 |  Loss: (0.4523) |  Loss2: (0.0000) | Acc: (84.00%) (16271/19328)
Epoch: 45 | Batch_idx: 160 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (17358/20608)
Epoch: 45 | Batch_idx: 170 |  Loss: (0.4517) |  Loss2: (0.0000) | Acc: (84.00%) (18433/21888)
Epoch: 45 | Batch_idx: 180 |  Loss: (0.4502) |  Loss2: (0.0000) | Acc: (84.00%) (19525/23168)
Epoch: 45 | Batch_idx: 190 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (20596/24448)
Epoch: 45 | Batch_idx: 200 |  Loss: (0.4493) |  Loss2: (0.0000) | Acc: (84.00%) (21704/25728)
Epoch: 45 | Batch_idx: 210 |  Loss: (0.4491) |  Loss2: (0.0000) | Acc: (84.00%) (22780/27008)
Epoch: 45 | Batch_idx: 220 |  Loss: (0.4501) |  Loss2: (0.0000) | Acc: (84.00%) (23866/28288)
Epoch: 45 | Batch_idx: 230 |  Loss: (0.4512) |  Loss2: (0.0000) | Acc: (84.00%) (24931/29568)
Epoch: 45 | Batch_idx: 240 |  Loss: (0.4521) |  Loss2: (0.0000) | Acc: (84.00%) (26005/30848)
Epoch: 45 | Batch_idx: 250 |  Loss: (0.4514) |  Loss2: (0.0000) | Acc: (84.00%) (27099/32128)
Epoch: 45 | Batch_idx: 260 |  Loss: (0.4516) |  Loss2: (0.0000) | Acc: (84.00%) (28176/33408)
Epoch: 45 | Batch_idx: 270 |  Loss: (0.4504) |  Loss2: (0.0000) | Acc: (84.00%) (29283/34688)
Epoch: 45 | Batch_idx: 280 |  Loss: (0.4495) |  Loss2: (0.0000) | Acc: (84.00%) (30362/35968)
Epoch: 45 | Batch_idx: 290 |  Loss: (0.4494) |  Loss2: (0.0000) | Acc: (84.00%) (31455/37248)
Epoch: 45 | Batch_idx: 300 |  Loss: (0.4474) |  Loss2: (0.0000) | Acc: (84.00%) (32564/38528)
Epoch: 45 | Batch_idx: 310 |  Loss: (0.4465) |  Loss2: (0.0000) | Acc: (84.00%) (33649/39808)
Epoch: 45 | Batch_idx: 320 |  Loss: (0.4454) |  Loss2: (0.0000) | Acc: (84.00%) (34742/41088)
Epoch: 45 | Batch_idx: 330 |  Loss: (0.4447) |  Loss2: (0.0000) | Acc: (84.00%) (35836/42368)
Epoch: 45 | Batch_idx: 340 |  Loss: (0.4442) |  Loss2: (0.0000) | Acc: (84.00%) (36916/43648)
Epoch: 45 | Batch_idx: 350 |  Loss: (0.4451) |  Loss2: (0.0000) | Acc: (84.00%) (38009/44928)
Epoch: 45 | Batch_idx: 360 |  Loss: (0.4455) |  Loss2: (0.0000) | Acc: (84.00%) (39087/46208)
Epoch: 45 | Batch_idx: 370 |  Loss: (0.4446) |  Loss2: (0.0000) | Acc: (84.00%) (40188/47488)
Epoch: 45 | Batch_idx: 380 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (41276/48768)
Epoch: 45 | Batch_idx: 390 |  Loss: (0.4443) |  Loss2: (0.0000) | Acc: (84.00%) (42319/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_045.pth.tar'
# TEST : Loss: (0.5086) | Acc: (83.00%) (8301/10000)
percent tensor([0.5559, 0.4441], device='cuda:0')
percent tensor([0.5387, 0.4613], device='cuda:0')
percent tensor([0.5005, 0.4995], device='cuda:0')
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5459, 0.4541], device='cuda:0')
percent tensor([0.6176, 0.3824], device='cuda:0')
percent tensor([0.6007, 0.3993], device='cuda:0')
percent tensor([0.9979, 0.0021], device='cuda:0')
Epoch: 46 | Batch_idx: 0 |  Loss: (0.4533) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 46 | Batch_idx: 10 |  Loss: (0.4784) |  Loss2: (0.0000) | Acc: (83.00%) (1177/1408)
Epoch: 46 | Batch_idx: 20 |  Loss: (0.4675) |  Loss2: (0.0000) | Acc: (84.00%) (2262/2688)
Epoch: 46 | Batch_idx: 30 |  Loss: (0.4626) |  Loss2: (0.0000) | Acc: (83.00%) (3333/3968)
Epoch: 46 | Batch_idx: 40 |  Loss: (0.4532) |  Loss2: (0.0000) | Acc: (84.00%) (4415/5248)
Epoch: 46 | Batch_idx: 50 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (5498/6528)
Epoch: 46 | Batch_idx: 60 |  Loss: (0.4427) |  Loss2: (0.0000) | Acc: (84.00%) (6595/7808)
Epoch: 46 | Batch_idx: 70 |  Loss: (0.4464) |  Loss2: (0.0000) | Acc: (84.00%) (7662/9088)
Epoch: 46 | Batch_idx: 80 |  Loss: (0.4458) |  Loss2: (0.0000) | Acc: (84.00%) (8741/10368)
Epoch: 46 | Batch_idx: 90 |  Loss: (0.4404) |  Loss2: (0.0000) | Acc: (84.00%) (9840/11648)
Epoch: 46 | Batch_idx: 100 |  Loss: (0.4354) |  Loss2: (0.0000) | Acc: (84.00%) (10939/12928)
Epoch: 46 | Batch_idx: 110 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (12014/14208)
Epoch: 46 | Batch_idx: 120 |  Loss: (0.4371) |  Loss2: (0.0000) | Acc: (84.00%) (13104/15488)
Epoch: 46 | Batch_idx: 130 |  Loss: (0.4378) |  Loss2: (0.0000) | Acc: (84.00%) (14190/16768)
Epoch: 46 | Batch_idx: 140 |  Loss: (0.4348) |  Loss2: (0.0000) | Acc: (84.00%) (15305/18048)
Epoch: 46 | Batch_idx: 150 |  Loss: (0.4345) |  Loss2: (0.0000) | Acc: (84.00%) (16395/19328)
Epoch: 46 | Batch_idx: 160 |  Loss: (0.4314) |  Loss2: (0.0000) | Acc: (84.00%) (17507/20608)
Epoch: 46 | Batch_idx: 170 |  Loss: (0.4329) |  Loss2: (0.0000) | Acc: (84.00%) (18580/21888)
Epoch: 46 | Batch_idx: 180 |  Loss: (0.4321) |  Loss2: (0.0000) | Acc: (84.00%) (19682/23168)
Epoch: 46 | Batch_idx: 190 |  Loss: (0.4305) |  Loss2: (0.0000) | Acc: (85.00%) (20791/24448)
Epoch: 46 | Batch_idx: 200 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (21878/25728)
Epoch: 46 | Batch_idx: 210 |  Loss: (0.4301) |  Loss2: (0.0000) | Acc: (85.00%) (22961/27008)
Epoch: 46 | Batch_idx: 220 |  Loss: (0.4293) |  Loss2: (0.0000) | Acc: (85.00%) (24067/28288)
Epoch: 46 | Batch_idx: 230 |  Loss: (0.4281) |  Loss2: (0.0000) | Acc: (85.00%) (25170/29568)
Epoch: 46 | Batch_idx: 240 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (26273/30848)
Epoch: 46 | Batch_idx: 250 |  Loss: (0.4273) |  Loss2: (0.0000) | Acc: (85.00%) (27361/32128)
Epoch: 46 | Batch_idx: 260 |  Loss: (0.4262) |  Loss2: (0.0000) | Acc: (85.00%) (28470/33408)
Epoch: 46 | Batch_idx: 270 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (85.00%) (29558/34688)
Epoch: 46 | Batch_idx: 280 |  Loss: (0.4263) |  Loss2: (0.0000) | Acc: (85.00%) (30651/35968)
Epoch: 46 | Batch_idx: 290 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (31748/37248)
Epoch: 46 | Batch_idx: 300 |  Loss: (0.4247) |  Loss2: (0.0000) | Acc: (85.00%) (32850/38528)
Epoch: 46 | Batch_idx: 310 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (33945/39808)
Epoch: 46 | Batch_idx: 320 |  Loss: (0.4232) |  Loss2: (0.0000) | Acc: (85.00%) (35035/41088)
Epoch: 46 | Batch_idx: 330 |  Loss: (0.4225) |  Loss2: (0.0000) | Acc: (85.00%) (36140/42368)
Epoch: 46 | Batch_idx: 340 |  Loss: (0.4206) |  Loss2: (0.0000) | Acc: (85.00%) (37254/43648)
Epoch: 46 | Batch_idx: 350 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (38350/44928)
Epoch: 46 | Batch_idx: 360 |  Loss: (0.4215) |  Loss2: (0.0000) | Acc: (85.00%) (39429/46208)
Epoch: 46 | Batch_idx: 370 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (40522/47488)
Epoch: 46 | Batch_idx: 380 |  Loss: (0.4208) |  Loss2: (0.0000) | Acc: (85.00%) (41615/48768)
Epoch: 46 | Batch_idx: 390 |  Loss: (0.4212) |  Loss2: (0.0000) | Acc: (85.00%) (42659/50000)
# TEST : Loss: (0.4865) | Acc: (83.00%) (8365/10000)
percent tensor([0.5554, 0.4446], device='cuda:0')
percent tensor([0.5442, 0.4558], device='cuda:0')
percent tensor([0.5032, 0.4968], device='cuda:0')
percent tensor([0.5546, 0.4454], device='cuda:0')
percent tensor([0.5538, 0.4462], device='cuda:0')
percent tensor([0.6394, 0.3606], device='cuda:0')
percent tensor([0.6172, 0.3828], device='cuda:0')
percent tensor([0.9980, 0.0020], device='cuda:0')
Epoch: 47 | Batch_idx: 0 |  Loss: (0.4421) |  Loss2: (0.0000) | Acc: (82.00%) (106/128)
Epoch: 47 | Batch_idx: 10 |  Loss: (0.4127) |  Loss2: (0.0000) | Acc: (84.00%) (1195/1408)
Epoch: 47 | Batch_idx: 20 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (85.00%) (2301/2688)
Epoch: 47 | Batch_idx: 30 |  Loss: (0.4162) |  Loss2: (0.0000) | Acc: (85.00%) (3385/3968)
Epoch: 47 | Batch_idx: 40 |  Loss: (0.4136) |  Loss2: (0.0000) | Acc: (85.00%) (4486/5248)
Epoch: 47 | Batch_idx: 50 |  Loss: (0.4084) |  Loss2: (0.0000) | Acc: (85.00%) (5583/6528)
Epoch: 47 | Batch_idx: 60 |  Loss: (0.4057) |  Loss2: (0.0000) | Acc: (85.00%) (6690/7808)
Epoch: 47 | Batch_idx: 70 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (7792/9088)
Epoch: 47 | Batch_idx: 80 |  Loss: (0.4079) |  Loss2: (0.0000) | Acc: (85.00%) (8889/10368)
Epoch: 47 | Batch_idx: 90 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (9986/11648)
Epoch: 47 | Batch_idx: 100 |  Loss: (0.4056) |  Loss2: (0.0000) | Acc: (85.00%) (11104/12928)
Epoch: 47 | Batch_idx: 110 |  Loss: (0.4070) |  Loss2: (0.0000) | Acc: (85.00%) (12207/14208)
Epoch: 47 | Batch_idx: 120 |  Loss: (0.4041) |  Loss2: (0.0000) | Acc: (85.00%) (13318/15488)
Epoch: 47 | Batch_idx: 130 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (14429/16768)
Epoch: 47 | Batch_idx: 140 |  Loss: (0.4051) |  Loss2: (0.0000) | Acc: (86.00%) (15524/18048)
Epoch: 47 | Batch_idx: 150 |  Loss: (0.4027) |  Loss2: (0.0000) | Acc: (86.00%) (16646/19328)
Epoch: 47 | Batch_idx: 160 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (86.00%) (17723/20608)
Epoch: 47 | Batch_idx: 170 |  Loss: (0.4072) |  Loss2: (0.0000) | Acc: (85.00%) (18817/21888)
Epoch: 47 | Batch_idx: 180 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (85.00%) (19910/23168)
Epoch: 47 | Batch_idx: 190 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (20996/24448)
Epoch: 47 | Batch_idx: 200 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (85.00%) (22102/25728)
Epoch: 47 | Batch_idx: 210 |  Loss: (0.4085) |  Loss2: (0.0000) | Acc: (85.00%) (23197/27008)
Epoch: 47 | Batch_idx: 220 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (85.00%) (24314/28288)
Epoch: 47 | Batch_idx: 230 |  Loss: (0.4076) |  Loss2: (0.0000) | Acc: (85.00%) (25415/29568)
Epoch: 47 | Batch_idx: 240 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (85.00%) (26514/30848)
Epoch: 47 | Batch_idx: 250 |  Loss: (0.4078) |  Loss2: (0.0000) | Acc: (85.00%) (27605/32128)
Epoch: 47 | Batch_idx: 260 |  Loss: (0.4087) |  Loss2: (0.0000) | Acc: (85.00%) (28689/33408)
Epoch: 47 | Batch_idx: 270 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (29776/34688)
Epoch: 47 | Batch_idx: 280 |  Loss: (0.4090) |  Loss2: (0.0000) | Acc: (85.00%) (30879/35968)
Epoch: 47 | Batch_idx: 290 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (31962/37248)
Epoch: 47 | Batch_idx: 300 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (33061/38528)
Epoch: 47 | Batch_idx: 310 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (34176/39808)
Epoch: 47 | Batch_idx: 320 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (35246/41088)
Epoch: 47 | Batch_idx: 330 |  Loss: (0.4096) |  Loss2: (0.0000) | Acc: (85.00%) (36359/42368)
Epoch: 47 | Batch_idx: 340 |  Loss: (0.4102) |  Loss2: (0.0000) | Acc: (85.00%) (37456/43648)
Epoch: 47 | Batch_idx: 350 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (38565/44928)
Epoch: 47 | Batch_idx: 360 |  Loss: (0.4097) |  Loss2: (0.0000) | Acc: (85.00%) (39662/46208)
Epoch: 47 | Batch_idx: 370 |  Loss: (0.4111) |  Loss2: (0.0000) | Acc: (85.00%) (40747/47488)
Epoch: 47 | Batch_idx: 380 |  Loss: (0.4108) |  Loss2: (0.0000) | Acc: (85.00%) (41853/48768)
Epoch: 47 | Batch_idx: 390 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (42931/50000)
# TEST : Loss: (0.4791) | Acc: (83.00%) (8383/10000)
percent tensor([0.5537, 0.4463], device='cuda:0')
percent tensor([0.5464, 0.4536], device='cuda:0')
percent tensor([0.5045, 0.4955], device='cuda:0')
percent tensor([0.5545, 0.4455], device='cuda:0')
percent tensor([0.5533, 0.4467], device='cuda:0')
percent tensor([0.6503, 0.3497], device='cuda:0')
percent tensor([0.6292, 0.3708], device='cuda:0')
percent tensor([0.9982, 0.0018], device='cuda:0')
Epoch: 48 | Batch_idx: 0 |  Loss: (0.4919) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 48 | Batch_idx: 10 |  Loss: (0.4115) |  Loss2: (0.0000) | Acc: (86.00%) (1211/1408)
Epoch: 48 | Batch_idx: 20 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (2317/2688)
Epoch: 48 | Batch_idx: 30 |  Loss: (0.4133) |  Loss2: (0.0000) | Acc: (85.00%) (3406/3968)
Epoch: 48 | Batch_idx: 40 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (4498/5248)
Epoch: 48 | Batch_idx: 50 |  Loss: (0.4135) |  Loss2: (0.0000) | Acc: (85.00%) (5597/6528)
Epoch: 48 | Batch_idx: 60 |  Loss: (0.4106) |  Loss2: (0.0000) | Acc: (85.00%) (6708/7808)
Epoch: 48 | Batch_idx: 70 |  Loss: (0.4118) |  Loss2: (0.0000) | Acc: (85.00%) (7801/9088)
Epoch: 48 | Batch_idx: 80 |  Loss: (0.4192) |  Loss2: (0.0000) | Acc: (85.00%) (8884/10368)
Epoch: 48 | Batch_idx: 90 |  Loss: (0.4140) |  Loss2: (0.0000) | Acc: (85.00%) (10005/11648)
Epoch: 48 | Batch_idx: 100 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (86.00%) (11123/12928)
Epoch: 48 | Batch_idx: 110 |  Loss: (0.4080) |  Loss2: (0.0000) | Acc: (86.00%) (12236/14208)
Epoch: 48 | Batch_idx: 120 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (13312/15488)
Epoch: 48 | Batch_idx: 130 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (86.00%) (14441/16768)
Epoch: 48 | Batch_idx: 140 |  Loss: (0.4054) |  Loss2: (0.0000) | Acc: (86.00%) (15551/18048)
Epoch: 48 | Batch_idx: 150 |  Loss: (0.4053) |  Loss2: (0.0000) | Acc: (86.00%) (16652/19328)
Epoch: 48 | Batch_idx: 160 |  Loss: (0.4010) |  Loss2: (0.0000) | Acc: (86.00%) (17787/20608)
Epoch: 48 | Batch_idx: 170 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (86.00%) (18907/21888)
Epoch: 48 | Batch_idx: 180 |  Loss: (0.3989) |  Loss2: (0.0000) | Acc: (86.00%) (20017/23168)
Epoch: 48 | Batch_idx: 190 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (21102/24448)
Epoch: 48 | Batch_idx: 200 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (22211/25728)
Epoch: 48 | Batch_idx: 210 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (23295/27008)
Epoch: 48 | Batch_idx: 220 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (24405/28288)
Epoch: 48 | Batch_idx: 230 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (25500/29568)
Epoch: 48 | Batch_idx: 240 |  Loss: (0.4013) |  Loss2: (0.0000) | Acc: (86.00%) (26604/30848)
Epoch: 48 | Batch_idx: 250 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (27699/32128)
Epoch: 48 | Batch_idx: 260 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (28815/33408)
Epoch: 48 | Batch_idx: 270 |  Loss: (0.4026) |  Loss2: (0.0000) | Acc: (86.00%) (29909/34688)
Epoch: 48 | Batch_idx: 280 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (31020/35968)
Epoch: 48 | Batch_idx: 290 |  Loss: (0.4016) |  Loss2: (0.0000) | Acc: (86.00%) (32141/37248)
Epoch: 48 | Batch_idx: 300 |  Loss: (0.4017) |  Loss2: (0.0000) | Acc: (86.00%) (33246/38528)
Epoch: 48 | Batch_idx: 310 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (34338/39808)
Epoch: 48 | Batch_idx: 320 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (35431/41088)
Epoch: 48 | Batch_idx: 330 |  Loss: (0.4033) |  Loss2: (0.0000) | Acc: (86.00%) (36531/42368)
Epoch: 48 | Batch_idx: 340 |  Loss: (0.4032) |  Loss2: (0.0000) | Acc: (86.00%) (37637/43648)
Epoch: 48 | Batch_idx: 350 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (38733/44928)
Epoch: 48 | Batch_idx: 360 |  Loss: (0.4030) |  Loss2: (0.0000) | Acc: (86.00%) (39844/46208)
Epoch: 48 | Batch_idx: 370 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (40958/47488)
Epoch: 48 | Batch_idx: 380 |  Loss: (0.4022) |  Loss2: (0.0000) | Acc: (86.00%) (42069/48768)
Epoch: 48 | Batch_idx: 390 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (86.00%) (43135/50000)
# TEST : Loss: (0.4732) | Acc: (83.00%) (8397/10000)
percent tensor([0.5575, 0.4425], device='cuda:0')
percent tensor([0.5448, 0.4552], device='cuda:0')
percent tensor([0.5061, 0.4939], device='cuda:0')
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5521, 0.4479], device='cuda:0')
percent tensor([0.6528, 0.3472], device='cuda:0')
percent tensor([0.6395, 0.3605], device='cuda:0')
percent tensor([0.9984, 0.0016], device='cuda:0')
Epoch: 49 | Batch_idx: 0 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 49 | Batch_idx: 10 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (1225/1408)
Epoch: 49 | Batch_idx: 20 |  Loss: (0.3870) |  Loss2: (0.0000) | Acc: (86.00%) (2328/2688)
Epoch: 49 | Batch_idx: 30 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (3436/3968)
Epoch: 49 | Batch_idx: 40 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (4522/5248)
Epoch: 49 | Batch_idx: 50 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (5645/6528)
Epoch: 49 | Batch_idx: 60 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (6751/7808)
Epoch: 49 | Batch_idx: 70 |  Loss: (0.3952) |  Loss2: (0.0000) | Acc: (86.00%) (7860/9088)
Epoch: 49 | Batch_idx: 80 |  Loss: (0.3960) |  Loss2: (0.0000) | Acc: (86.00%) (8973/10368)
Epoch: 49 | Batch_idx: 90 |  Loss: (0.3943) |  Loss2: (0.0000) | Acc: (86.00%) (10094/11648)
Epoch: 49 | Batch_idx: 100 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (11188/12928)
Epoch: 49 | Batch_idx: 110 |  Loss: (0.3968) |  Loss2: (0.0000) | Acc: (86.00%) (12280/14208)
Epoch: 49 | Batch_idx: 120 |  Loss: (0.3956) |  Loss2: (0.0000) | Acc: (86.00%) (13386/15488)
Epoch: 49 | Batch_idx: 130 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (14492/16768)
Epoch: 49 | Batch_idx: 140 |  Loss: (0.3960) |  Loss2: (0.0000) | Acc: (86.00%) (15594/18048)
Epoch: 49 | Batch_idx: 150 |  Loss: (0.3954) |  Loss2: (0.0000) | Acc: (86.00%) (16704/19328)
Epoch: 49 | Batch_idx: 160 |  Loss: (0.3936) |  Loss2: (0.0000) | Acc: (86.00%) (17818/20608)
Epoch: 49 | Batch_idx: 170 |  Loss: (0.3933) |  Loss2: (0.0000) | Acc: (86.00%) (18922/21888)
Epoch: 49 | Batch_idx: 180 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (20009/23168)
Epoch: 49 | Batch_idx: 190 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (21100/24448)
Epoch: 49 | Batch_idx: 200 |  Loss: (0.3955) |  Loss2: (0.0000) | Acc: (86.00%) (22192/25728)
Epoch: 49 | Batch_idx: 210 |  Loss: (0.3961) |  Loss2: (0.0000) | Acc: (86.00%) (23302/27008)
Epoch: 49 | Batch_idx: 220 |  Loss: (0.3946) |  Loss2: (0.0000) | Acc: (86.00%) (24423/28288)
Epoch: 49 | Batch_idx: 230 |  Loss: (0.3947) |  Loss2: (0.0000) | Acc: (86.00%) (25533/29568)
Epoch: 49 | Batch_idx: 240 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (26626/30848)
Epoch: 49 | Batch_idx: 250 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (27748/32128)
Epoch: 49 | Batch_idx: 260 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (28846/33408)
Epoch: 49 | Batch_idx: 270 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (29972/34688)
Epoch: 49 | Batch_idx: 280 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (31084/35968)
Epoch: 49 | Batch_idx: 290 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (32188/37248)
Epoch: 49 | Batch_idx: 300 |  Loss: (0.3937) |  Loss2: (0.0000) | Acc: (86.00%) (33312/38528)
Epoch: 49 | Batch_idx: 310 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (34404/39808)
Epoch: 49 | Batch_idx: 320 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (35521/41088)
Epoch: 49 | Batch_idx: 330 |  Loss: (0.3945) |  Loss2: (0.0000) | Acc: (86.00%) (36625/42368)
Epoch: 49 | Batch_idx: 340 |  Loss: (0.3958) |  Loss2: (0.0000) | Acc: (86.00%) (37723/43648)
Epoch: 49 | Batch_idx: 350 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (38833/44928)
Epoch: 49 | Batch_idx: 360 |  Loss: (0.3950) |  Loss2: (0.0000) | Acc: (86.00%) (39939/46208)
Epoch: 49 | Batch_idx: 370 |  Loss: (0.3953) |  Loss2: (0.0000) | Acc: (86.00%) (41043/47488)
Epoch: 49 | Batch_idx: 380 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (42140/48768)
Epoch: 49 | Batch_idx: 390 |  Loss: (0.3966) |  Loss2: (0.0000) | Acc: (86.00%) (43202/50000)
# TEST : Loss: (0.4676) | Acc: (84.00%) (8418/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5426, 0.4574], device='cuda:0')
percent tensor([0.5072, 0.4928], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.6465, 0.3535], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 50 | Batch_idx: 0 |  Loss: (0.4518) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 50 | Batch_idx: 10 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (85.00%) (1202/1408)
Epoch: 50 | Batch_idx: 20 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 50 | Batch_idx: 30 |  Loss: (0.4009) |  Loss2: (0.0000) | Acc: (86.00%) (3418/3968)
Epoch: 50 | Batch_idx: 40 |  Loss: (0.4077) |  Loss2: (0.0000) | Acc: (85.00%) (4501/5248)
Epoch: 50 | Batch_idx: 50 |  Loss: (0.4119) |  Loss2: (0.0000) | Acc: (85.00%) (5590/6528)
Epoch: 50 | Batch_idx: 60 |  Loss: (0.4074) |  Loss2: (0.0000) | Acc: (85.00%) (6697/7808)
Epoch: 50 | Batch_idx: 70 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (7821/9088)
Epoch: 50 | Batch_idx: 80 |  Loss: (0.4064) |  Loss2: (0.0000) | Acc: (85.00%) (8913/10368)
Epoch: 50 | Batch_idx: 90 |  Loss: (0.4086) |  Loss2: (0.0000) | Acc: (85.00%) (10001/11648)
Epoch: 50 | Batch_idx: 100 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (85.00%) (11098/12928)
Epoch: 50 | Batch_idx: 110 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (12174/14208)
Epoch: 50 | Batch_idx: 120 |  Loss: (0.4131) |  Loss2: (0.0000) | Acc: (85.00%) (13269/15488)
Epoch: 50 | Batch_idx: 130 |  Loss: (0.4095) |  Loss2: (0.0000) | Acc: (85.00%) (14381/16768)
Epoch: 50 | Batch_idx: 140 |  Loss: (0.4082) |  Loss2: (0.0000) | Acc: (85.00%) (15496/18048)
Epoch: 50 | Batch_idx: 150 |  Loss: (0.4068) |  Loss2: (0.0000) | Acc: (85.00%) (16600/19328)
Epoch: 50 | Batch_idx: 160 |  Loss: (0.4091) |  Loss2: (0.0000) | Acc: (85.00%) (17691/20608)
Epoch: 50 | Batch_idx: 170 |  Loss: (0.4103) |  Loss2: (0.0000) | Acc: (85.00%) (18773/21888)
Epoch: 50 | Batch_idx: 180 |  Loss: (0.4105) |  Loss2: (0.0000) | Acc: (85.00%) (19872/23168)
Epoch: 50 | Batch_idx: 190 |  Loss: (0.4109) |  Loss2: (0.0000) | Acc: (85.00%) (20965/24448)
Epoch: 50 | Batch_idx: 200 |  Loss: (0.4110) |  Loss2: (0.0000) | Acc: (85.00%) (22057/25728)
Epoch: 50 | Batch_idx: 210 |  Loss: (0.4100) |  Loss2: (0.0000) | Acc: (85.00%) (23169/27008)
Epoch: 50 | Batch_idx: 220 |  Loss: (0.4104) |  Loss2: (0.0000) | Acc: (85.00%) (24275/28288)
Epoch: 50 | Batch_idx: 230 |  Loss: (0.4114) |  Loss2: (0.0000) | Acc: (85.00%) (25351/29568)
Epoch: 50 | Batch_idx: 240 |  Loss: (0.4126) |  Loss2: (0.0000) | Acc: (85.00%) (26433/30848)
Epoch: 50 | Batch_idx: 250 |  Loss: (0.4130) |  Loss2: (0.0000) | Acc: (85.00%) (27525/32128)
Epoch: 50 | Batch_idx: 260 |  Loss: (0.4146) |  Loss2: (0.0000) | Acc: (85.00%) (28601/33408)
Epoch: 50 | Batch_idx: 270 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (29712/34688)
Epoch: 50 | Batch_idx: 280 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (30796/35968)
Epoch: 50 | Batch_idx: 290 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (31876/37248)
Epoch: 50 | Batch_idx: 300 |  Loss: (0.4164) |  Loss2: (0.0000) | Acc: (85.00%) (32948/38528)
Epoch: 50 | Batch_idx: 310 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (34040/39808)
Epoch: 50 | Batch_idx: 320 |  Loss: (0.4155) |  Loss2: (0.0000) | Acc: (85.00%) (35151/41088)
Epoch: 50 | Batch_idx: 330 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (36248/42368)
Epoch: 50 | Batch_idx: 340 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (37337/43648)
Epoch: 50 | Batch_idx: 350 |  Loss: (0.4159) |  Loss2: (0.0000) | Acc: (85.00%) (38431/44928)
Epoch: 50 | Batch_idx: 360 |  Loss: (0.4148) |  Loss2: (0.0000) | Acc: (85.00%) (39543/46208)
Epoch: 50 | Batch_idx: 370 |  Loss: (0.4145) |  Loss2: (0.0000) | Acc: (85.00%) (40634/47488)
Epoch: 50 | Batch_idx: 380 |  Loss: (0.4153) |  Loss2: (0.0000) | Acc: (85.00%) (41722/48768)
Epoch: 50 | Batch_idx: 390 |  Loss: (0.4158) |  Loss2: (0.0000) | Acc: (85.00%) (42762/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_050.pth.tar'
# TEST : Loss: (0.5325) | Acc: (82.00%) (8248/10000)
percent tensor([0.5568, 0.4432], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5077, 0.4923], device='cuda:0')
percent tensor([0.5555, 0.4445], device='cuda:0')
percent tensor([0.5570, 0.4430], device='cuda:0')
percent tensor([0.6584, 0.3416], device='cuda:0')
percent tensor([0.6468, 0.3532], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(174.7440, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(798.0910, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(793.1094, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.7993, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(501.6890, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2202.1140, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4300.1274, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1418.9355, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6097.2949, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12053.9443, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(4008.5034, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16981.6504, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 51 | Batch_idx: 0 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 51 | Batch_idx: 10 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (1212/1408)
Epoch: 51 | Batch_idx: 20 |  Loss: (0.3981) |  Loss2: (0.0000) | Acc: (86.00%) (2318/2688)
Epoch: 51 | Batch_idx: 30 |  Loss: (0.3974) |  Loss2: (0.0000) | Acc: (86.00%) (3424/3968)
Epoch: 51 | Batch_idx: 40 |  Loss: (0.3973) |  Loss2: (0.0000) | Acc: (86.00%) (4533/5248)
Epoch: 51 | Batch_idx: 50 |  Loss: (0.3991) |  Loss2: (0.0000) | Acc: (86.00%) (5635/6528)
Epoch: 51 | Batch_idx: 60 |  Loss: (0.3990) |  Loss2: (0.0000) | Acc: (86.00%) (6739/7808)
Epoch: 51 | Batch_idx: 70 |  Loss: (0.4008) |  Loss2: (0.0000) | Acc: (86.00%) (7848/9088)
Epoch: 51 | Batch_idx: 80 |  Loss: (0.3993) |  Loss2: (0.0000) | Acc: (86.00%) (8950/10368)
Epoch: 51 | Batch_idx: 90 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (10030/11648)
Epoch: 51 | Batch_idx: 100 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (11129/12928)
Epoch: 51 | Batch_idx: 110 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (86.00%) (12219/14208)
Epoch: 51 | Batch_idx: 120 |  Loss: (0.4069) |  Loss2: (0.0000) | Acc: (86.00%) (13320/15488)
Epoch: 51 | Batch_idx: 130 |  Loss: (0.4059) |  Loss2: (0.0000) | Acc: (86.00%) (14427/16768)
Epoch: 51 | Batch_idx: 140 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (85.00%) (15519/18048)
Epoch: 51 | Batch_idx: 150 |  Loss: (0.4065) |  Loss2: (0.0000) | Acc: (85.00%) (16622/19328)
Epoch: 51 | Batch_idx: 160 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (86.00%) (17724/20608)
Epoch: 51 | Batch_idx: 170 |  Loss: (0.4066) |  Loss2: (0.0000) | Acc: (85.00%) (18821/21888)
Epoch: 51 | Batch_idx: 180 |  Loss: (0.4073) |  Loss2: (0.0000) | Acc: (85.00%) (19922/23168)
Epoch: 51 | Batch_idx: 190 |  Loss: (0.4067) |  Loss2: (0.0000) | Acc: (86.00%) (21027/24448)
Epoch: 51 | Batch_idx: 200 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (22133/25728)
Epoch: 51 | Batch_idx: 210 |  Loss: (0.4061) |  Loss2: (0.0000) | Acc: (86.00%) (23227/27008)
Epoch: 51 | Batch_idx: 220 |  Loss: (0.4055) |  Loss2: (0.0000) | Acc: (86.00%) (24342/28288)
Epoch: 51 | Batch_idx: 230 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (25445/29568)
Epoch: 51 | Batch_idx: 240 |  Loss: (0.4046) |  Loss2: (0.0000) | Acc: (86.00%) (26534/30848)
Epoch: 51 | Batch_idx: 250 |  Loss: (0.4036) |  Loss2: (0.0000) | Acc: (86.00%) (27658/32128)
Epoch: 51 | Batch_idx: 260 |  Loss: (0.4028) |  Loss2: (0.0000) | Acc: (86.00%) (28768/33408)
Epoch: 51 | Batch_idx: 270 |  Loss: (0.4023) |  Loss2: (0.0000) | Acc: (86.00%) (29892/34688)
Epoch: 51 | Batch_idx: 280 |  Loss: (0.4015) |  Loss2: (0.0000) | Acc: (86.00%) (31006/35968)
Epoch: 51 | Batch_idx: 290 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (32097/37248)
Epoch: 51 | Batch_idx: 300 |  Loss: (0.4020) |  Loss2: (0.0000) | Acc: (86.00%) (33191/38528)
Epoch: 51 | Batch_idx: 310 |  Loss: (0.4025) |  Loss2: (0.0000) | Acc: (86.00%) (34280/39808)
Epoch: 51 | Batch_idx: 320 |  Loss: (0.4021) |  Loss2: (0.0000) | Acc: (86.00%) (35389/41088)
Epoch: 51 | Batch_idx: 330 |  Loss: (0.4035) |  Loss2: (0.0000) | Acc: (86.00%) (36484/42368)
Epoch: 51 | Batch_idx: 340 |  Loss: (0.4044) |  Loss2: (0.0000) | Acc: (86.00%) (37579/43648)
Epoch: 51 | Batch_idx: 350 |  Loss: (0.4042) |  Loss2: (0.0000) | Acc: (86.00%) (38676/44928)
Epoch: 51 | Batch_idx: 360 |  Loss: (0.4039) |  Loss2: (0.0000) | Acc: (86.00%) (39785/46208)
Epoch: 51 | Batch_idx: 370 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (40871/47488)
Epoch: 51 | Batch_idx: 380 |  Loss: (0.4043) |  Loss2: (0.0000) | Acc: (86.00%) (41983/48768)
Epoch: 51 | Batch_idx: 390 |  Loss: (0.4048) |  Loss2: (0.0000) | Acc: (86.00%) (43041/50000)
# TEST : Loss: (0.5306) | Acc: (82.00%) (8273/10000)
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5422, 0.4578], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.5544, 0.4456], device='cuda:0')
percent tensor([0.6554, 0.3446], device='cuda:0')
percent tensor([0.6466, 0.3534], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 52 | Batch_idx: 0 |  Loss: (0.3159) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 52 | Batch_idx: 10 |  Loss: (0.3944) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 52 | Batch_idx: 20 |  Loss: (0.3917) |  Loss2: (0.0000) | Acc: (86.00%) (2316/2688)
Epoch: 52 | Batch_idx: 30 |  Loss: (0.4024) |  Loss2: (0.0000) | Acc: (85.00%) (3403/3968)
Epoch: 52 | Batch_idx: 40 |  Loss: (0.3971) |  Loss2: (0.0000) | Acc: (86.00%) (4516/5248)
Epoch: 52 | Batch_idx: 50 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (5634/6528)
Epoch: 52 | Batch_idx: 60 |  Loss: (0.3909) |  Loss2: (0.0000) | Acc: (86.00%) (6746/7808)
Epoch: 52 | Batch_idx: 70 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (86.00%) (7838/9088)
Epoch: 52 | Batch_idx: 80 |  Loss: (0.3922) |  Loss2: (0.0000) | Acc: (86.00%) (8948/10368)
Epoch: 52 | Batch_idx: 90 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (10064/11648)
Epoch: 52 | Batch_idx: 100 |  Loss: (0.3926) |  Loss2: (0.0000) | Acc: (86.00%) (11177/12928)
Epoch: 52 | Batch_idx: 110 |  Loss: (0.3908) |  Loss2: (0.0000) | Acc: (86.00%) (12301/14208)
Epoch: 52 | Batch_idx: 120 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (13409/15488)
Epoch: 52 | Batch_idx: 130 |  Loss: (0.3914) |  Loss2: (0.0000) | Acc: (86.00%) (14511/16768)
Epoch: 52 | Batch_idx: 140 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (15626/18048)
Epoch: 52 | Batch_idx: 150 |  Loss: (0.3878) |  Loss2: (0.0000) | Acc: (86.00%) (16761/19328)
Epoch: 52 | Batch_idx: 160 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (17870/20608)
Epoch: 52 | Batch_idx: 170 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (18970/21888)
Epoch: 52 | Batch_idx: 180 |  Loss: (0.3877) |  Loss2: (0.0000) | Acc: (86.00%) (20066/23168)
Epoch: 52 | Batch_idx: 190 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (21193/24448)
Epoch: 52 | Batch_idx: 200 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (22291/25728)
Epoch: 52 | Batch_idx: 210 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (23409/27008)
Epoch: 52 | Batch_idx: 220 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (24520/28288)
Epoch: 52 | Batch_idx: 230 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (25636/29568)
Epoch: 52 | Batch_idx: 240 |  Loss: (0.3872) |  Loss2: (0.0000) | Acc: (86.00%) (26729/30848)
Epoch: 52 | Batch_idx: 250 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (27841/32128)
Epoch: 52 | Batch_idx: 260 |  Loss: (0.3853) |  Loss2: (0.0000) | Acc: (86.00%) (28970/33408)
Epoch: 52 | Batch_idx: 270 |  Loss: (0.3856) |  Loss2: (0.0000) | Acc: (86.00%) (30081/34688)
Epoch: 52 | Batch_idx: 280 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (31205/35968)
Epoch: 52 | Batch_idx: 290 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (32303/37248)
Epoch: 52 | Batch_idx: 300 |  Loss: (0.3861) |  Loss2: (0.0000) | Acc: (86.00%) (33428/38528)
Epoch: 52 | Batch_idx: 310 |  Loss: (0.3858) |  Loss2: (0.0000) | Acc: (86.00%) (34531/39808)
Epoch: 52 | Batch_idx: 320 |  Loss: (0.3857) |  Loss2: (0.0000) | Acc: (86.00%) (35639/41088)
Epoch: 52 | Batch_idx: 330 |  Loss: (0.3848) |  Loss2: (0.0000) | Acc: (86.00%) (36768/42368)
Epoch: 52 | Batch_idx: 340 |  Loss: (0.3849) |  Loss2: (0.0000) | Acc: (86.00%) (37883/43648)
Epoch: 52 | Batch_idx: 350 |  Loss: (0.3854) |  Loss2: (0.0000) | Acc: (86.00%) (38993/44928)
Epoch: 52 | Batch_idx: 360 |  Loss: (0.3859) |  Loss2: (0.0000) | Acc: (86.00%) (40106/46208)
Epoch: 52 | Batch_idx: 370 |  Loss: (0.3864) |  Loss2: (0.0000) | Acc: (86.00%) (41205/47488)
Epoch: 52 | Batch_idx: 380 |  Loss: (0.3862) |  Loss2: (0.0000) | Acc: (86.00%) (42316/48768)
Epoch: 52 | Batch_idx: 390 |  Loss: (0.3863) |  Loss2: (0.0000) | Acc: (86.00%) (43376/50000)
# TEST : Loss: (0.4952) | Acc: (83.00%) (8349/10000)
percent tensor([0.5567, 0.4433], device='cuda:0')
percent tensor([0.5423, 0.4577], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5556, 0.4444], device='cuda:0')
percent tensor([0.5561, 0.4439], device='cuda:0')
percent tensor([0.6559, 0.3441], device='cuda:0')
percent tensor([0.6439, 0.3561], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 53 | Batch_idx: 0 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 53 | Batch_idx: 10 |  Loss: (0.4176) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 53 | Batch_idx: 20 |  Loss: (0.3822) |  Loss2: (0.0000) | Acc: (87.00%) (2341/2688)
Epoch: 53 | Batch_idx: 30 |  Loss: (0.3808) |  Loss2: (0.0000) | Acc: (87.00%) (3462/3968)
Epoch: 53 | Batch_idx: 40 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (87.00%) (4566/5248)
Epoch: 53 | Batch_idx: 50 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (5665/6528)
Epoch: 53 | Batch_idx: 60 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (6766/7808)
Epoch: 53 | Batch_idx: 70 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (7871/9088)
Epoch: 53 | Batch_idx: 80 |  Loss: (0.3860) |  Loss2: (0.0000) | Acc: (86.00%) (8978/10368)
Epoch: 53 | Batch_idx: 90 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (10107/11648)
Epoch: 53 | Batch_idx: 100 |  Loss: (0.3812) |  Loss2: (0.0000) | Acc: (86.00%) (11232/12928)
Epoch: 53 | Batch_idx: 110 |  Loss: (0.3816) |  Loss2: (0.0000) | Acc: (86.00%) (12344/14208)
Epoch: 53 | Batch_idx: 120 |  Loss: (0.3788) |  Loss2: (0.0000) | Acc: (87.00%) (13486/15488)
Epoch: 53 | Batch_idx: 130 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (14612/16768)
Epoch: 53 | Batch_idx: 140 |  Loss: (0.3776) |  Loss2: (0.0000) | Acc: (87.00%) (15730/18048)
Epoch: 53 | Batch_idx: 150 |  Loss: (0.3786) |  Loss2: (0.0000) | Acc: (87.00%) (16833/19328)
Epoch: 53 | Batch_idx: 160 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (86.00%) (17924/20608)
Epoch: 53 | Batch_idx: 170 |  Loss: (0.3803) |  Loss2: (0.0000) | Acc: (86.00%) (19039/21888)
Epoch: 53 | Batch_idx: 180 |  Loss: (0.3810) |  Loss2: (0.0000) | Acc: (86.00%) (20133/23168)
Epoch: 53 | Batch_idx: 190 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (86.00%) (21262/24448)
Epoch: 53 | Batch_idx: 200 |  Loss: (0.3780) |  Loss2: (0.0000) | Acc: (87.00%) (22388/25728)
Epoch: 53 | Batch_idx: 210 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (23512/27008)
Epoch: 53 | Batch_idx: 220 |  Loss: (0.3778) |  Loss2: (0.0000) | Acc: (87.00%) (24624/28288)
Epoch: 53 | Batch_idx: 230 |  Loss: (0.3770) |  Loss2: (0.0000) | Acc: (87.00%) (25740/29568)
Epoch: 53 | Batch_idx: 240 |  Loss: (0.3782) |  Loss2: (0.0000) | Acc: (87.00%) (26843/30848)
Epoch: 53 | Batch_idx: 250 |  Loss: (0.3785) |  Loss2: (0.0000) | Acc: (86.00%) (27946/32128)
Epoch: 53 | Batch_idx: 260 |  Loss: (0.3796) |  Loss2: (0.0000) | Acc: (86.00%) (29044/33408)
Epoch: 53 | Batch_idx: 270 |  Loss: (0.3789) |  Loss2: (0.0000) | Acc: (86.00%) (30160/34688)
Epoch: 53 | Batch_idx: 280 |  Loss: (0.3795) |  Loss2: (0.0000) | Acc: (86.00%) (31264/35968)
Epoch: 53 | Batch_idx: 290 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (32374/37248)
Epoch: 53 | Batch_idx: 300 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (33470/38528)
Epoch: 53 | Batch_idx: 310 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (86.00%) (34585/39808)
Epoch: 53 | Batch_idx: 320 |  Loss: (0.3804) |  Loss2: (0.0000) | Acc: (86.00%) (35719/41088)
Epoch: 53 | Batch_idx: 330 |  Loss: (0.3807) |  Loss2: (0.0000) | Acc: (86.00%) (36836/42368)
Epoch: 53 | Batch_idx: 340 |  Loss: (0.3801) |  Loss2: (0.0000) | Acc: (86.00%) (37972/43648)
Epoch: 53 | Batch_idx: 350 |  Loss: (0.3799) |  Loss2: (0.0000) | Acc: (87.00%) (39089/44928)
Epoch: 53 | Batch_idx: 360 |  Loss: (0.3815) |  Loss2: (0.0000) | Acc: (86.00%) (40177/46208)
Epoch: 53 | Batch_idx: 370 |  Loss: (0.3823) |  Loss2: (0.0000) | Acc: (86.00%) (41264/47488)
Epoch: 53 | Batch_idx: 380 |  Loss: (0.3827) |  Loss2: (0.0000) | Acc: (86.00%) (42376/48768)
Epoch: 53 | Batch_idx: 390 |  Loss: (0.3824) |  Loss2: (0.0000) | Acc: (86.00%) (43454/50000)
# TEST : Loss: (0.5668) | Acc: (81.00%) (8164/10000)
percent tensor([0.5566, 0.4434], device='cuda:0')
percent tensor([0.5431, 0.4569], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5557, 0.4443], device='cuda:0')
percent tensor([0.5549, 0.4451], device='cuda:0')
percent tensor([0.6541, 0.3459], device='cuda:0')
percent tensor([0.6395, 0.3605], device='cuda:0')
percent tensor([0.9985, 0.0015], device='cuda:0')
Epoch: 54 | Batch_idx: 0 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 54 | Batch_idx: 10 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 54 | Batch_idx: 20 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (88.00%) (2388/2688)
Epoch: 54 | Batch_idx: 30 |  Loss: (0.3583) |  Loss2: (0.0000) | Acc: (88.00%) (3520/3968)
Epoch: 54 | Batch_idx: 40 |  Loss: (0.3550) |  Loss2: (0.0000) | Acc: (88.00%) (4651/5248)
Epoch: 54 | Batch_idx: 50 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (88.00%) (5793/6528)
Epoch: 54 | Batch_idx: 60 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (88.00%) (6928/7808)
Epoch: 54 | Batch_idx: 70 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (8062/9088)
Epoch: 54 | Batch_idx: 80 |  Loss: (0.3490) |  Loss2: (0.0000) | Acc: (88.00%) (9178/10368)
Epoch: 54 | Batch_idx: 90 |  Loss: (0.3508) |  Loss2: (0.0000) | Acc: (88.00%) (10291/11648)
Epoch: 54 | Batch_idx: 100 |  Loss: (0.3511) |  Loss2: (0.0000) | Acc: (88.00%) (11410/12928)
Epoch: 54 | Batch_idx: 110 |  Loss: (0.3538) |  Loss2: (0.0000) | Acc: (88.00%) (12516/14208)
Epoch: 54 | Batch_idx: 120 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (13611/15488)
Epoch: 54 | Batch_idx: 130 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (14741/16768)
Epoch: 54 | Batch_idx: 140 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (15849/18048)
Epoch: 54 | Batch_idx: 150 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (16956/19328)
Epoch: 54 | Batch_idx: 160 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (18069/20608)
Epoch: 54 | Batch_idx: 170 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (19201/21888)
Epoch: 54 | Batch_idx: 180 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (20317/23168)
Epoch: 54 | Batch_idx: 190 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (21424/24448)
Epoch: 54 | Batch_idx: 200 |  Loss: (0.3613) |  Loss2: (0.0000) | Acc: (87.00%) (22553/25728)
Epoch: 54 | Batch_idx: 210 |  Loss: (0.3618) |  Loss2: (0.0000) | Acc: (87.00%) (23661/27008)
Epoch: 54 | Batch_idx: 220 |  Loss: (0.3630) |  Loss2: (0.0000) | Acc: (87.00%) (24774/28288)
Epoch: 54 | Batch_idx: 230 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (87.00%) (25884/29568)
Epoch: 54 | Batch_idx: 240 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (26994/30848)
Epoch: 54 | Batch_idx: 250 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (28130/32128)
Epoch: 54 | Batch_idx: 260 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (29265/33408)
Epoch: 54 | Batch_idx: 270 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (30376/34688)
Epoch: 54 | Batch_idx: 280 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (31509/35968)
Epoch: 54 | Batch_idx: 290 |  Loss: (0.3637) |  Loss2: (0.0000) | Acc: (87.00%) (32624/37248)
Epoch: 54 | Batch_idx: 300 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (33741/38528)
Epoch: 54 | Batch_idx: 310 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (34841/39808)
Epoch: 54 | Batch_idx: 320 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (35938/41088)
Epoch: 54 | Batch_idx: 330 |  Loss: (0.3652) |  Loss2: (0.0000) | Acc: (87.00%) (37057/42368)
Epoch: 54 | Batch_idx: 340 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (38191/43648)
Epoch: 54 | Batch_idx: 350 |  Loss: (0.3646) |  Loss2: (0.0000) | Acc: (87.00%) (39302/44928)
Epoch: 54 | Batch_idx: 360 |  Loss: (0.3647) |  Loss2: (0.0000) | Acc: (87.00%) (40422/46208)
Epoch: 54 | Batch_idx: 370 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (41524/47488)
Epoch: 54 | Batch_idx: 380 |  Loss: (0.3678) |  Loss2: (0.0000) | Acc: (87.00%) (42614/48768)
Epoch: 54 | Batch_idx: 390 |  Loss: (0.3677) |  Loss2: (0.0000) | Acc: (87.00%) (43695/50000)
# TEST : Loss: (0.4775) | Acc: (84.00%) (8412/10000)
percent tensor([0.5563, 0.4437], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.5074, 0.4926], device='cuda:0')
percent tensor([0.5556, 0.4444], device='cuda:0')
percent tensor([0.5552, 0.4448], device='cuda:0')
percent tensor([0.6573, 0.3427], device='cuda:0')
percent tensor([0.6444, 0.3556], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 55 | Batch_idx: 0 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (83.00%) (107/128)
Epoch: 55 | Batch_idx: 10 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (1213/1408)
Epoch: 55 | Batch_idx: 20 |  Loss: (0.3995) |  Loss2: (0.0000) | Acc: (85.00%) (2306/2688)
Epoch: 55 | Batch_idx: 30 |  Loss: (0.4170) |  Loss2: (0.0000) | Acc: (85.00%) (3387/3968)
Epoch: 55 | Batch_idx: 40 |  Loss: (0.4239) |  Loss2: (0.0000) | Acc: (85.00%) (4475/5248)
Epoch: 55 | Batch_idx: 50 |  Loss: (0.4343) |  Loss2: (0.0000) | Acc: (85.00%) (5552/6528)
Epoch: 55 | Batch_idx: 60 |  Loss: (0.4486) |  Loss2: (0.0000) | Acc: (84.00%) (6599/7808)
Epoch: 55 | Batch_idx: 70 |  Loss: (0.4498) |  Loss2: (0.0000) | Acc: (84.00%) (7666/9088)
Epoch: 55 | Batch_idx: 80 |  Loss: (0.4511) |  Loss2: (0.0000) | Acc: (84.00%) (8735/10368)
Epoch: 55 | Batch_idx: 90 |  Loss: (0.4436) |  Loss2: (0.0000) | Acc: (84.00%) (9851/11648)
Epoch: 55 | Batch_idx: 100 |  Loss: (0.4441) |  Loss2: (0.0000) | Acc: (84.00%) (10916/12928)
Epoch: 55 | Batch_idx: 110 |  Loss: (0.4431) |  Loss2: (0.0000) | Acc: (84.00%) (12007/14208)
Epoch: 55 | Batch_idx: 120 |  Loss: (0.4448) |  Loss2: (0.0000) | Acc: (84.00%) (13077/15488)
Epoch: 55 | Batch_idx: 130 |  Loss: (0.4433) |  Loss2: (0.0000) | Acc: (84.00%) (14176/16768)
Epoch: 55 | Batch_idx: 140 |  Loss: (0.4423) |  Loss2: (0.0000) | Acc: (84.00%) (15258/18048)
Epoch: 55 | Batch_idx: 150 |  Loss: (0.4411) |  Loss2: (0.0000) | Acc: (84.00%) (16351/19328)
Epoch: 55 | Batch_idx: 160 |  Loss: (0.4394) |  Loss2: (0.0000) | Acc: (84.00%) (17451/20608)
Epoch: 55 | Batch_idx: 170 |  Loss: (0.4373) |  Loss2: (0.0000) | Acc: (84.00%) (18547/21888)
Epoch: 55 | Batch_idx: 180 |  Loss: (0.4379) |  Loss2: (0.0000) | Acc: (84.00%) (19627/23168)
Epoch: 55 | Batch_idx: 190 |  Loss: (0.4385) |  Loss2: (0.0000) | Acc: (84.00%) (20711/24448)
Epoch: 55 | Batch_idx: 200 |  Loss: (0.4367) |  Loss2: (0.0000) | Acc: (84.00%) (21808/25728)
Epoch: 55 | Batch_idx: 210 |  Loss: (0.4359) |  Loss2: (0.0000) | Acc: (84.00%) (22899/27008)
Epoch: 55 | Batch_idx: 220 |  Loss: (0.4362) |  Loss2: (0.0000) | Acc: (84.00%) (23979/28288)
Epoch: 55 | Batch_idx: 230 |  Loss: (0.4342) |  Loss2: (0.0000) | Acc: (84.00%) (25084/29568)
Epoch: 55 | Batch_idx: 240 |  Loss: (0.4316) |  Loss2: (0.0000) | Acc: (84.00%) (26199/30848)
Epoch: 55 | Batch_idx: 250 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (85.00%) (27310/32128)
Epoch: 55 | Batch_idx: 260 |  Loss: (0.4288) |  Loss2: (0.0000) | Acc: (85.00%) (28411/33408)
Epoch: 55 | Batch_idx: 270 |  Loss: (0.4275) |  Loss2: (0.0000) | Acc: (85.00%) (29512/34688)
Epoch: 55 | Batch_idx: 280 |  Loss: (0.4279) |  Loss2: (0.0000) | Acc: (85.00%) (30591/35968)
Epoch: 55 | Batch_idx: 290 |  Loss: (0.4268) |  Loss2: (0.0000) | Acc: (85.00%) (31696/37248)
Epoch: 55 | Batch_idx: 300 |  Loss: (0.4252) |  Loss2: (0.0000) | Acc: (85.00%) (32808/38528)
Epoch: 55 | Batch_idx: 310 |  Loss: (0.4255) |  Loss2: (0.0000) | Acc: (85.00%) (33901/39808)
Epoch: 55 | Batch_idx: 320 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (85.00%) (35013/41088)
Epoch: 55 | Batch_idx: 330 |  Loss: (0.4221) |  Loss2: (0.0000) | Acc: (85.00%) (36138/42368)
Epoch: 55 | Batch_idx: 340 |  Loss: (0.4213) |  Loss2: (0.0000) | Acc: (85.00%) (37249/43648)
Epoch: 55 | Batch_idx: 350 |  Loss: (0.4194) |  Loss2: (0.0000) | Acc: (85.00%) (38369/44928)
Epoch: 55 | Batch_idx: 360 |  Loss: (0.4188) |  Loss2: (0.0000) | Acc: (85.00%) (39465/46208)
Epoch: 55 | Batch_idx: 370 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (40560/47488)
Epoch: 55 | Batch_idx: 380 |  Loss: (0.4167) |  Loss2: (0.0000) | Acc: (85.00%) (41678/48768)
Epoch: 55 | Batch_idx: 390 |  Loss: (0.4166) |  Loss2: (0.0000) | Acc: (85.00%) (42737/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_055.pth.tar'
# TEST : Loss: (0.4897) | Acc: (83.00%) (8347/10000)
percent tensor([0.5872, 0.4128], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5091, 0.4909], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.7193, 0.2807], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 56 | Batch_idx: 0 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (84.00%) (108/128)
Epoch: 56 | Batch_idx: 10 |  Loss: (0.3745) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 56 | Batch_idx: 20 |  Loss: (0.3986) |  Loss2: (0.0000) | Acc: (85.00%) (2311/2688)
Epoch: 56 | Batch_idx: 30 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (3424/3968)
Epoch: 56 | Batch_idx: 40 |  Loss: (0.3951) |  Loss2: (0.0000) | Acc: (86.00%) (4532/5248)
Epoch: 56 | Batch_idx: 50 |  Loss: (0.3959) |  Loss2: (0.0000) | Acc: (86.00%) (5638/6528)
Epoch: 56 | Batch_idx: 60 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (6759/7808)
Epoch: 56 | Batch_idx: 70 |  Loss: (0.3919) |  Loss2: (0.0000) | Acc: (86.00%) (7863/9088)
Epoch: 56 | Batch_idx: 80 |  Loss: (0.3898) |  Loss2: (0.0000) | Acc: (86.00%) (8976/10368)
Epoch: 56 | Batch_idx: 90 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (10092/11648)
Epoch: 56 | Batch_idx: 100 |  Loss: (0.3925) |  Loss2: (0.0000) | Acc: (86.00%) (11192/12928)
Epoch: 56 | Batch_idx: 110 |  Loss: (0.3920) |  Loss2: (0.0000) | Acc: (86.00%) (12307/14208)
Epoch: 56 | Batch_idx: 120 |  Loss: (0.3913) |  Loss2: (0.0000) | Acc: (86.00%) (13419/15488)
Epoch: 56 | Batch_idx: 130 |  Loss: (0.3907) |  Loss2: (0.0000) | Acc: (86.00%) (14521/16768)
Epoch: 56 | Batch_idx: 140 |  Loss: (0.3924) |  Loss2: (0.0000) | Acc: (86.00%) (15624/18048)
Epoch: 56 | Batch_idx: 150 |  Loss: (0.3929) |  Loss2: (0.0000) | Acc: (86.00%) (16721/19328)
Epoch: 56 | Batch_idx: 160 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (17862/20608)
Epoch: 56 | Batch_idx: 170 |  Loss: (0.3911) |  Loss2: (0.0000) | Acc: (86.00%) (18959/21888)
Epoch: 56 | Batch_idx: 180 |  Loss: (0.3899) |  Loss2: (0.0000) | Acc: (86.00%) (20069/23168)
Epoch: 56 | Batch_idx: 190 |  Loss: (0.3886) |  Loss2: (0.0000) | Acc: (86.00%) (21199/24448)
Epoch: 56 | Batch_idx: 200 |  Loss: (0.3897) |  Loss2: (0.0000) | Acc: (86.00%) (22294/25728)
Epoch: 56 | Batch_idx: 210 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (23400/27008)
Epoch: 56 | Batch_idx: 220 |  Loss: (0.3893) |  Loss2: (0.0000) | Acc: (86.00%) (24490/28288)
Epoch: 56 | Batch_idx: 230 |  Loss: (0.3894) |  Loss2: (0.0000) | Acc: (86.00%) (25603/29568)
Epoch: 56 | Batch_idx: 240 |  Loss: (0.3889) |  Loss2: (0.0000) | Acc: (86.00%) (26718/30848)
Epoch: 56 | Batch_idx: 250 |  Loss: (0.3883) |  Loss2: (0.0000) | Acc: (86.00%) (27836/32128)
Epoch: 56 | Batch_idx: 260 |  Loss: (0.3885) |  Loss2: (0.0000) | Acc: (86.00%) (28945/33408)
Epoch: 56 | Batch_idx: 270 |  Loss: (0.3880) |  Loss2: (0.0000) | Acc: (86.00%) (30056/34688)
Epoch: 56 | Batch_idx: 280 |  Loss: (0.3865) |  Loss2: (0.0000) | Acc: (86.00%) (31190/35968)
Epoch: 56 | Batch_idx: 290 |  Loss: (0.3855) |  Loss2: (0.0000) | Acc: (86.00%) (32319/37248)
Epoch: 56 | Batch_idx: 300 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (33445/38528)
Epoch: 56 | Batch_idx: 310 |  Loss: (0.3837) |  Loss2: (0.0000) | Acc: (86.00%) (34569/39808)
Epoch: 56 | Batch_idx: 320 |  Loss: (0.3847) |  Loss2: (0.0000) | Acc: (86.00%) (35675/41088)
Epoch: 56 | Batch_idx: 330 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (36811/42368)
Epoch: 56 | Batch_idx: 340 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (37917/43648)
Epoch: 56 | Batch_idx: 350 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (39038/44928)
Epoch: 56 | Batch_idx: 360 |  Loss: (0.3841) |  Loss2: (0.0000) | Acc: (86.00%) (40151/46208)
Epoch: 56 | Batch_idx: 370 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (41252/47488)
Epoch: 56 | Batch_idx: 380 |  Loss: (0.3846) |  Loss2: (0.0000) | Acc: (86.00%) (42356/48768)
Epoch: 56 | Batch_idx: 390 |  Loss: (0.3840) |  Loss2: (0.0000) | Acc: (86.00%) (43438/50000)
# TEST : Loss: (0.4676) | Acc: (84.00%) (8440/10000)
percent tensor([0.5855, 0.4145], device='cuda:0')
percent tensor([0.5348, 0.4652], device='cuda:0')
percent tensor([0.5097, 0.4903], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5886, 0.4114], device='cuda:0')
percent tensor([0.7309, 0.2691], device='cuda:0')
percent tensor([0.6802, 0.3198], device='cuda:0')
percent tensor([0.9987, 0.0013], device='cuda:0')
Epoch: 57 | Batch_idx: 0 |  Loss: (0.5568) |  Loss2: (0.0000) | Acc: (81.00%) (104/128)
Epoch: 57 | Batch_idx: 10 |  Loss: (0.4245) |  Loss2: (0.0000) | Acc: (85.00%) (1207/1408)
Epoch: 57 | Batch_idx: 20 |  Loss: (0.3988) |  Loss2: (0.0000) | Acc: (86.00%) (2324/2688)
Epoch: 57 | Batch_idx: 30 |  Loss: (0.3915) |  Loss2: (0.0000) | Acc: (86.00%) (3439/3968)
Epoch: 57 | Batch_idx: 40 |  Loss: (0.3835) |  Loss2: (0.0000) | Acc: (86.00%) (4565/5248)
Epoch: 57 | Batch_idx: 50 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (5679/6528)
Epoch: 57 | Batch_idx: 60 |  Loss: (0.3852) |  Loss2: (0.0000) | Acc: (86.00%) (6776/7808)
Epoch: 57 | Batch_idx: 70 |  Loss: (0.3866) |  Loss2: (0.0000) | Acc: (86.00%) (7886/9088)
Epoch: 57 | Batch_idx: 80 |  Loss: (0.3842) |  Loss2: (0.0000) | Acc: (86.00%) (9017/10368)
Epoch: 57 | Batch_idx: 90 |  Loss: (0.3814) |  Loss2: (0.0000) | Acc: (87.00%) (10151/11648)
Epoch: 57 | Batch_idx: 100 |  Loss: (0.3805) |  Loss2: (0.0000) | Acc: (87.00%) (11265/12928)
Epoch: 57 | Batch_idx: 110 |  Loss: (0.3811) |  Loss2: (0.0000) | Acc: (87.00%) (12383/14208)
Epoch: 57 | Batch_idx: 120 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (87.00%) (13511/15488)
Epoch: 57 | Batch_idx: 130 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (14649/16768)
Epoch: 57 | Batch_idx: 140 |  Loss: (0.3794) |  Loss2: (0.0000) | Acc: (87.00%) (15740/18048)
Epoch: 57 | Batch_idx: 150 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (87.00%) (16866/19328)
Epoch: 57 | Batch_idx: 160 |  Loss: (0.3771) |  Loss2: (0.0000) | Acc: (87.00%) (17980/20608)
Epoch: 57 | Batch_idx: 170 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (87.00%) (19123/21888)
Epoch: 57 | Batch_idx: 180 |  Loss: (0.3757) |  Loss2: (0.0000) | Acc: (87.00%) (20229/23168)
Epoch: 57 | Batch_idx: 190 |  Loss: (0.3746) |  Loss2: (0.0000) | Acc: (87.00%) (21346/24448)
Epoch: 57 | Batch_idx: 200 |  Loss: (0.3742) |  Loss2: (0.0000) | Acc: (87.00%) (22461/25728)
Epoch: 57 | Batch_idx: 210 |  Loss: (0.3726) |  Loss2: (0.0000) | Acc: (87.00%) (23604/27008)
Epoch: 57 | Batch_idx: 220 |  Loss: (0.3728) |  Loss2: (0.0000) | Acc: (87.00%) (24725/28288)
Epoch: 57 | Batch_idx: 230 |  Loss: (0.3725) |  Loss2: (0.0000) | Acc: (87.00%) (25840/29568)
Epoch: 57 | Batch_idx: 240 |  Loss: (0.3712) |  Loss2: (0.0000) | Acc: (87.00%) (26965/30848)
Epoch: 57 | Batch_idx: 250 |  Loss: (0.3708) |  Loss2: (0.0000) | Acc: (87.00%) (28077/32128)
Epoch: 57 | Batch_idx: 260 |  Loss: (0.3705) |  Loss2: (0.0000) | Acc: (87.00%) (29190/33408)
Epoch: 57 | Batch_idx: 270 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (87.00%) (30300/34688)
Epoch: 57 | Batch_idx: 280 |  Loss: (0.3688) |  Loss2: (0.0000) | Acc: (87.00%) (31422/35968)
Epoch: 57 | Batch_idx: 290 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (32523/37248)
Epoch: 57 | Batch_idx: 300 |  Loss: (0.3703) |  Loss2: (0.0000) | Acc: (87.00%) (33633/38528)
Epoch: 57 | Batch_idx: 310 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (34771/39808)
Epoch: 57 | Batch_idx: 320 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (35899/41088)
Epoch: 57 | Batch_idx: 330 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (37010/42368)
Epoch: 57 | Batch_idx: 340 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (38125/43648)
Epoch: 57 | Batch_idx: 350 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (39256/44928)
Epoch: 57 | Batch_idx: 360 |  Loss: (0.3684) |  Loss2: (0.0000) | Acc: (87.00%) (40377/46208)
Epoch: 57 | Batch_idx: 370 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (41504/47488)
Epoch: 57 | Batch_idx: 380 |  Loss: (0.3697) |  Loss2: (0.0000) | Acc: (87.00%) (42601/48768)
Epoch: 57 | Batch_idx: 390 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (87.00%) (43673/50000)
# TEST : Loss: (0.4597) | Acc: (84.00%) (8447/10000)
percent tensor([0.5747, 0.4253], device='cuda:0')
percent tensor([0.5347, 0.4653], device='cuda:0')
percent tensor([0.5096, 0.4904], device='cuda:0')
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.7289, 0.2711], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 58 | Batch_idx: 0 |  Loss: (0.3157) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 58 | Batch_idx: 10 |  Loss: (0.3643) |  Loss2: (0.0000) | Acc: (86.00%) (1221/1408)
Epoch: 58 | Batch_idx: 20 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (86.00%) (2330/2688)
Epoch: 58 | Batch_idx: 30 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (86.00%) (3440/3968)
Epoch: 58 | Batch_idx: 40 |  Loss: (0.3699) |  Loss2: (0.0000) | Acc: (86.00%) (4561/5248)
Epoch: 58 | Batch_idx: 50 |  Loss: (0.3663) |  Loss2: (0.0000) | Acc: (87.00%) (5693/6528)
Epoch: 58 | Batch_idx: 60 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (6805/7808)
Epoch: 58 | Batch_idx: 70 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (7927/9088)
Epoch: 58 | Batch_idx: 80 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (9036/10368)
Epoch: 58 | Batch_idx: 90 |  Loss: (0.3680) |  Loss2: (0.0000) | Acc: (87.00%) (10148/11648)
Epoch: 58 | Batch_idx: 100 |  Loss: (0.3683) |  Loss2: (0.0000) | Acc: (87.00%) (11259/12928)
Epoch: 58 | Batch_idx: 110 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (12409/14208)
Epoch: 58 | Batch_idx: 120 |  Loss: (0.3674) |  Loss2: (0.0000) | Acc: (87.00%) (13499/15488)
Epoch: 58 | Batch_idx: 130 |  Loss: (0.3689) |  Loss2: (0.0000) | Acc: (87.00%) (14611/16768)
Epoch: 58 | Batch_idx: 140 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (15769/18048)
Epoch: 58 | Batch_idx: 150 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (16888/19328)
Epoch: 58 | Batch_idx: 160 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (18017/20608)
Epoch: 58 | Batch_idx: 170 |  Loss: (0.3640) |  Loss2: (0.0000) | Acc: (87.00%) (19123/21888)
Epoch: 58 | Batch_idx: 180 |  Loss: (0.3639) |  Loss2: (0.0000) | Acc: (87.00%) (20247/23168)
Epoch: 58 | Batch_idx: 190 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (21384/24448)
Epoch: 58 | Batch_idx: 200 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (22519/25728)
Epoch: 58 | Batch_idx: 210 |  Loss: (0.3634) |  Loss2: (0.0000) | Acc: (87.00%) (23640/27008)
Epoch: 58 | Batch_idx: 220 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (24739/28288)
Epoch: 58 | Batch_idx: 230 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (25863/29568)
Epoch: 58 | Batch_idx: 240 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (26984/30848)
Epoch: 58 | Batch_idx: 250 |  Loss: (0.3621) |  Loss2: (0.0000) | Acc: (87.00%) (28109/32128)
Epoch: 58 | Batch_idx: 260 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (29227/33408)
Epoch: 58 | Batch_idx: 270 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (30352/34688)
Epoch: 58 | Batch_idx: 280 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (31473/35968)
Epoch: 58 | Batch_idx: 290 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (32603/37248)
Epoch: 58 | Batch_idx: 300 |  Loss: (0.3616) |  Loss2: (0.0000) | Acc: (87.00%) (33718/38528)
Epoch: 58 | Batch_idx: 310 |  Loss: (0.3612) |  Loss2: (0.0000) | Acc: (87.00%) (34838/39808)
Epoch: 58 | Batch_idx: 320 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (35946/41088)
Epoch: 58 | Batch_idx: 330 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (37044/42368)
Epoch: 58 | Batch_idx: 340 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (38185/43648)
Epoch: 58 | Batch_idx: 350 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (39299/44928)
Epoch: 58 | Batch_idx: 360 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (40420/46208)
Epoch: 58 | Batch_idx: 370 |  Loss: (0.3628) |  Loss2: (0.0000) | Acc: (87.00%) (41552/47488)
Epoch: 58 | Batch_idx: 380 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (42683/48768)
Epoch: 58 | Batch_idx: 390 |  Loss: (0.3627) |  Loss2: (0.0000) | Acc: (87.00%) (43754/50000)
# TEST : Loss: (0.4525) | Acc: (84.00%) (8477/10000)
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.5094, 0.4906], device='cuda:0')
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5905, 0.4095], device='cuda:0')
percent tensor([0.7272, 0.2728], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 59 | Batch_idx: 0 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 59 | Batch_idx: 10 |  Loss: (0.3963) |  Loss2: (0.0000) | Acc: (86.00%) (1220/1408)
Epoch: 59 | Batch_idx: 20 |  Loss: (0.3845) |  Loss2: (0.0000) | Acc: (86.00%) (2337/2688)
Epoch: 59 | Batch_idx: 30 |  Loss: (0.3749) |  Loss2: (0.0000) | Acc: (86.00%) (3451/3968)
Epoch: 59 | Batch_idx: 40 |  Loss: (0.3764) |  Loss2: (0.0000) | Acc: (87.00%) (4568/5248)
Epoch: 59 | Batch_idx: 50 |  Loss: (0.3744) |  Loss2: (0.0000) | Acc: (87.00%) (5684/6528)
Epoch: 59 | Batch_idx: 60 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (6827/7808)
Epoch: 59 | Batch_idx: 70 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (7961/9088)
Epoch: 59 | Batch_idx: 80 |  Loss: (0.3655) |  Loss2: (0.0000) | Acc: (87.00%) (9066/10368)
Epoch: 59 | Batch_idx: 90 |  Loss: (0.3662) |  Loss2: (0.0000) | Acc: (87.00%) (10172/11648)
Epoch: 59 | Batch_idx: 100 |  Loss: (0.3665) |  Loss2: (0.0000) | Acc: (87.00%) (11293/12928)
Epoch: 59 | Batch_idx: 110 |  Loss: (0.3648) |  Loss2: (0.0000) | Acc: (87.00%) (12411/14208)
Epoch: 59 | Batch_idx: 120 |  Loss: (0.3650) |  Loss2: (0.0000) | Acc: (87.00%) (13527/15488)
Epoch: 59 | Batch_idx: 130 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (14666/16768)
Epoch: 59 | Batch_idx: 140 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (15787/18048)
Epoch: 59 | Batch_idx: 150 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (16901/19328)
Epoch: 59 | Batch_idx: 160 |  Loss: (0.3624) |  Loss2: (0.0000) | Acc: (87.00%) (18046/20608)
Epoch: 59 | Batch_idx: 170 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (19197/21888)
Epoch: 59 | Batch_idx: 180 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (20316/23168)
Epoch: 59 | Batch_idx: 190 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (21426/24448)
Epoch: 59 | Batch_idx: 200 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (22554/25728)
Epoch: 59 | Batch_idx: 210 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (23665/27008)
Epoch: 59 | Batch_idx: 220 |  Loss: (0.3595) |  Loss2: (0.0000) | Acc: (87.00%) (24796/28288)
Epoch: 59 | Batch_idx: 230 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (25924/29568)
Epoch: 59 | Batch_idx: 240 |  Loss: (0.3594) |  Loss2: (0.0000) | Acc: (87.00%) (27035/30848)
Epoch: 59 | Batch_idx: 250 |  Loss: (0.3580) |  Loss2: (0.0000) | Acc: (87.00%) (28173/32128)
Epoch: 59 | Batch_idx: 260 |  Loss: (0.3589) |  Loss2: (0.0000) | Acc: (87.00%) (29297/33408)
Epoch: 59 | Batch_idx: 270 |  Loss: (0.3597) |  Loss2: (0.0000) | Acc: (87.00%) (30400/34688)
Epoch: 59 | Batch_idx: 280 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (31524/35968)
Epoch: 59 | Batch_idx: 290 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (32634/37248)
Epoch: 59 | Batch_idx: 300 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (33748/38528)
Epoch: 59 | Batch_idx: 310 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (34860/39808)
Epoch: 59 | Batch_idx: 320 |  Loss: (0.3625) |  Loss2: (0.0000) | Acc: (87.00%) (35978/41088)
Epoch: 59 | Batch_idx: 330 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (37110/42368)
Epoch: 59 | Batch_idx: 340 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (38237/43648)
Epoch: 59 | Batch_idx: 350 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (39380/44928)
Epoch: 59 | Batch_idx: 360 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (40506/46208)
Epoch: 59 | Batch_idx: 370 |  Loss: (0.3606) |  Loss2: (0.0000) | Acc: (87.00%) (41635/47488)
Epoch: 59 | Batch_idx: 380 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (42752/48768)
Epoch: 59 | Batch_idx: 390 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (43826/50000)
# TEST : Loss: (0.4531) | Acc: (84.00%) (8476/10000)
percent tensor([0.5732, 0.4268], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5103, 0.4897], device='cuda:0')
percent tensor([0.5737, 0.4263], device='cuda:0')
percent tensor([0.5919, 0.4081], device='cuda:0')
percent tensor([0.7256, 0.2744], device='cuda:0')
percent tensor([0.6952, 0.3048], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 60 | Batch_idx: 0 |  Loss: (0.3520) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 60 | Batch_idx: 10 |  Loss: (0.3694) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 60 | Batch_idx: 20 |  Loss: (0.3614) |  Loss2: (0.0000) | Acc: (87.00%) (2353/2688)
Epoch: 60 | Batch_idx: 30 |  Loss: (0.3575) |  Loss2: (0.0000) | Acc: (87.00%) (3473/3968)
Epoch: 60 | Batch_idx: 40 |  Loss: (0.3564) |  Loss2: (0.0000) | Acc: (87.00%) (4591/5248)
Epoch: 60 | Batch_idx: 50 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (5721/6528)
Epoch: 60 | Batch_idx: 60 |  Loss: (0.3556) |  Loss2: (0.0000) | Acc: (87.00%) (6844/7808)
Epoch: 60 | Batch_idx: 70 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (7937/9088)
Epoch: 60 | Batch_idx: 80 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (9058/10368)
Epoch: 60 | Batch_idx: 90 |  Loss: (0.3642) |  Loss2: (0.0000) | Acc: (87.00%) (10182/11648)
Epoch: 60 | Batch_idx: 100 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (11312/12928)
Epoch: 60 | Batch_idx: 110 |  Loss: (0.3617) |  Loss2: (0.0000) | Acc: (87.00%) (12424/14208)
Epoch: 60 | Batch_idx: 120 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (13539/15488)
Epoch: 60 | Batch_idx: 130 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (14660/16768)
Epoch: 60 | Batch_idx: 140 |  Loss: (0.3631) |  Loss2: (0.0000) | Acc: (87.00%) (15778/18048)
Epoch: 60 | Batch_idx: 150 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (16906/19328)
Epoch: 60 | Batch_idx: 160 |  Loss: (0.3610) |  Loss2: (0.0000) | Acc: (87.00%) (18042/20608)
Epoch: 60 | Batch_idx: 170 |  Loss: (0.3600) |  Loss2: (0.0000) | Acc: (87.00%) (19173/21888)
Epoch: 60 | Batch_idx: 180 |  Loss: (0.3590) |  Loss2: (0.0000) | Acc: (87.00%) (20292/23168)
Epoch: 60 | Batch_idx: 190 |  Loss: (0.3592) |  Loss2: (0.0000) | Acc: (87.00%) (21420/24448)
Epoch: 60 | Batch_idx: 200 |  Loss: (0.3587) |  Loss2: (0.0000) | Acc: (87.00%) (22534/25728)
Epoch: 60 | Batch_idx: 210 |  Loss: (0.3585) |  Loss2: (0.0000) | Acc: (87.00%) (23657/27008)
Epoch: 60 | Batch_idx: 220 |  Loss: (0.3579) |  Loss2: (0.0000) | Acc: (87.00%) (24780/28288)
Epoch: 60 | Batch_idx: 230 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (25904/29568)
Epoch: 60 | Batch_idx: 240 |  Loss: (0.3602) |  Loss2: (0.0000) | Acc: (87.00%) (27008/30848)
Epoch: 60 | Batch_idx: 250 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (28119/32128)
Epoch: 60 | Batch_idx: 260 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (29238/33408)
Epoch: 60 | Batch_idx: 270 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (30359/34688)
Epoch: 60 | Batch_idx: 280 |  Loss: (0.3604) |  Loss2: (0.0000) | Acc: (87.00%) (31488/35968)
Epoch: 60 | Batch_idx: 290 |  Loss: (0.3603) |  Loss2: (0.0000) | Acc: (87.00%) (32613/37248)
Epoch: 60 | Batch_idx: 300 |  Loss: (0.3615) |  Loss2: (0.0000) | Acc: (87.00%) (33726/38528)
Epoch: 60 | Batch_idx: 310 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (34845/39808)
Epoch: 60 | Batch_idx: 320 |  Loss: (0.3611) |  Loss2: (0.0000) | Acc: (87.00%) (35973/41088)
Epoch: 60 | Batch_idx: 330 |  Loss: (0.3608) |  Loss2: (0.0000) | Acc: (87.00%) (37111/42368)
Epoch: 60 | Batch_idx: 340 |  Loss: (0.3607) |  Loss2: (0.0000) | Acc: (87.00%) (38234/43648)
Epoch: 60 | Batch_idx: 350 |  Loss: (0.3620) |  Loss2: (0.0000) | Acc: (87.00%) (39338/44928)
Epoch: 60 | Batch_idx: 360 |  Loss: (0.3623) |  Loss2: (0.0000) | Acc: (87.00%) (40465/46208)
Epoch: 60 | Batch_idx: 370 |  Loss: (0.3629) |  Loss2: (0.0000) | Acc: (87.00%) (41570/47488)
Epoch: 60 | Batch_idx: 380 |  Loss: (0.3635) |  Loss2: (0.0000) | Acc: (87.00%) (42671/48768)
Epoch: 60 | Batch_idx: 390 |  Loss: (0.3636) |  Loss2: (0.0000) | Acc: (87.00%) (43755/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_060.pth.tar'
# TEST : Loss: (0.4928) | Acc: (83.00%) (8392/10000)
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.5354, 0.4646], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.7235, 0.2765], device='cuda:0')
percent tensor([0.6940, 0.3060], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(176.0275, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(802.3510, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(797.3834, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1527.4336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(500.0065, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2211.0747, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4295.6577, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1413.7385, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6106.8257, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(12015.6143, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3992.9863, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16912.4277, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 61 | Batch_idx: 0 |  Loss: (0.2408) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 61 | Batch_idx: 10 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 61 | Batch_idx: 20 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (88.00%) (2380/2688)
Epoch: 61 | Batch_idx: 30 |  Loss: (0.3491) |  Loss2: (0.0000) | Acc: (88.00%) (3512/3968)
Epoch: 61 | Batch_idx: 40 |  Loss: (0.3561) |  Loss2: (0.0000) | Acc: (88.00%) (4630/5248)
Epoch: 61 | Batch_idx: 50 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (88.00%) (5758/6528)
Epoch: 61 | Batch_idx: 60 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (6857/7808)
Epoch: 61 | Batch_idx: 70 |  Loss: (0.3619) |  Loss2: (0.0000) | Acc: (87.00%) (7970/9088)
Epoch: 61 | Batch_idx: 80 |  Loss: (0.3609) |  Loss2: (0.0000) | Acc: (87.00%) (9088/10368)
Epoch: 61 | Batch_idx: 90 |  Loss: (0.3570) |  Loss2: (0.0000) | Acc: (87.00%) (10221/11648)
Epoch: 61 | Batch_idx: 100 |  Loss: (0.3560) |  Loss2: (0.0000) | Acc: (87.00%) (11356/12928)
Epoch: 61 | Batch_idx: 110 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (12481/14208)
Epoch: 61 | Batch_idx: 120 |  Loss: (0.3552) |  Loss2: (0.0000) | Acc: (87.00%) (13599/15488)
Epoch: 61 | Batch_idx: 130 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (14739/16768)
Epoch: 61 | Batch_idx: 140 |  Loss: (0.3519) |  Loss2: (0.0000) | Acc: (87.00%) (15878/18048)
Epoch: 61 | Batch_idx: 150 |  Loss: (0.3531) |  Loss2: (0.0000) | Acc: (87.00%) (16993/19328)
Epoch: 61 | Batch_idx: 160 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (18144/20608)
Epoch: 61 | Batch_idx: 170 |  Loss: (0.3517) |  Loss2: (0.0000) | Acc: (88.00%) (19268/21888)
Epoch: 61 | Batch_idx: 180 |  Loss: (0.3526) |  Loss2: (0.0000) | Acc: (88.00%) (20396/23168)
Epoch: 61 | Batch_idx: 190 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (88.00%) (21515/24448)
Epoch: 61 | Batch_idx: 200 |  Loss: (0.3544) |  Loss2: (0.0000) | Acc: (87.00%) (22613/25728)
Epoch: 61 | Batch_idx: 210 |  Loss: (0.3546) |  Loss2: (0.0000) | Acc: (87.00%) (23734/27008)
Epoch: 61 | Batch_idx: 220 |  Loss: (0.3547) |  Loss2: (0.0000) | Acc: (87.00%) (24856/28288)
Epoch: 61 | Batch_idx: 230 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (26001/29568)
Epoch: 61 | Batch_idx: 240 |  Loss: (0.3541) |  Loss2: (0.0000) | Acc: (87.00%) (27115/30848)
Epoch: 61 | Batch_idx: 250 |  Loss: (0.3537) |  Loss2: (0.0000) | Acc: (87.00%) (28236/32128)
Epoch: 61 | Batch_idx: 260 |  Loss: (0.3534) |  Loss2: (0.0000) | Acc: (87.00%) (29366/33408)
Epoch: 61 | Batch_idx: 270 |  Loss: (0.3533) |  Loss2: (0.0000) | Acc: (87.00%) (30494/34688)
Epoch: 61 | Batch_idx: 280 |  Loss: (0.3532) |  Loss2: (0.0000) | Acc: (87.00%) (31610/35968)
Epoch: 61 | Batch_idx: 290 |  Loss: (0.3530) |  Loss2: (0.0000) | Acc: (87.00%) (32740/37248)
Epoch: 61 | Batch_idx: 300 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (33880/38528)
Epoch: 61 | Batch_idx: 310 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (87.00%) (35008/39808)
Epoch: 61 | Batch_idx: 320 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (88.00%) (36168/41088)
Epoch: 61 | Batch_idx: 330 |  Loss: (0.3505) |  Loss2: (0.0000) | Acc: (88.00%) (37303/42368)
Epoch: 61 | Batch_idx: 340 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (38418/43648)
Epoch: 61 | Batch_idx: 350 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (88.00%) (39543/44928)
Epoch: 61 | Batch_idx: 360 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (88.00%) (40667/46208)
Epoch: 61 | Batch_idx: 370 |  Loss: (0.3500) |  Loss2: (0.0000) | Acc: (88.00%) (41805/47488)
Epoch: 61 | Batch_idx: 380 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (88.00%) (42927/48768)
Epoch: 61 | Batch_idx: 390 |  Loss: (0.3498) |  Loss2: (0.0000) | Acc: (88.00%) (44006/50000)
# TEST : Loss: (0.5118) | Acc: (83.00%) (8335/10000)
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5745, 0.4255], device='cuda:0')
percent tensor([0.5924, 0.4076], device='cuda:0')
percent tensor([0.7274, 0.2726], device='cuda:0')
percent tensor([0.6959, 0.3041], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 62 | Batch_idx: 0 |  Loss: (0.3701) |  Loss2: (0.0000) | Acc: (85.00%) (109/128)
Epoch: 62 | Batch_idx: 10 |  Loss: (0.3542) |  Loss2: (0.0000) | Acc: (87.00%) (1226/1408)
Epoch: 62 | Batch_idx: 20 |  Loss: (0.3469) |  Loss2: (0.0000) | Acc: (87.00%) (2365/2688)
Epoch: 62 | Batch_idx: 30 |  Loss: (0.3387) |  Loss2: (0.0000) | Acc: (88.00%) (3505/3968)
Epoch: 62 | Batch_idx: 40 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 62 | Batch_idx: 50 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (5799/6528)
Epoch: 62 | Batch_idx: 60 |  Loss: (0.3276) |  Loss2: (0.0000) | Acc: (88.00%) (6943/7808)
Epoch: 62 | Batch_idx: 70 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (8050/9088)
Epoch: 62 | Batch_idx: 80 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (9171/10368)
Epoch: 62 | Batch_idx: 90 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (10318/11648)
Epoch: 62 | Batch_idx: 100 |  Loss: (0.3353) |  Loss2: (0.0000) | Acc: (88.00%) (11460/12928)
Epoch: 62 | Batch_idx: 110 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (12589/14208)
Epoch: 62 | Batch_idx: 120 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (13726/15488)
Epoch: 62 | Batch_idx: 130 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (14865/16768)
Epoch: 62 | Batch_idx: 140 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (15993/18048)
Epoch: 62 | Batch_idx: 150 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (88.00%) (17110/19328)
Epoch: 62 | Batch_idx: 160 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (18248/20608)
Epoch: 62 | Batch_idx: 170 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (19377/21888)
Epoch: 62 | Batch_idx: 180 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (20528/23168)
Epoch: 62 | Batch_idx: 190 |  Loss: (0.3364) |  Loss2: (0.0000) | Acc: (88.00%) (21654/24448)
Epoch: 62 | Batch_idx: 200 |  Loss: (0.3373) |  Loss2: (0.0000) | Acc: (88.00%) (22778/25728)
Epoch: 62 | Batch_idx: 210 |  Loss: (0.3367) |  Loss2: (0.0000) | Acc: (88.00%) (23908/27008)
Epoch: 62 | Batch_idx: 220 |  Loss: (0.3365) |  Loss2: (0.0000) | Acc: (88.00%) (25049/28288)
Epoch: 62 | Batch_idx: 230 |  Loss: (0.3380) |  Loss2: (0.0000) | Acc: (88.00%) (26177/29568)
Epoch: 62 | Batch_idx: 240 |  Loss: (0.3385) |  Loss2: (0.0000) | Acc: (88.00%) (27292/30848)
Epoch: 62 | Batch_idx: 250 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (28423/32128)
Epoch: 62 | Batch_idx: 260 |  Loss: (0.3384) |  Loss2: (0.0000) | Acc: (88.00%) (29563/33408)
Epoch: 62 | Batch_idx: 270 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (30680/34688)
Epoch: 62 | Batch_idx: 280 |  Loss: (0.3393) |  Loss2: (0.0000) | Acc: (88.00%) (31803/35968)
Epoch: 62 | Batch_idx: 290 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (32915/37248)
Epoch: 62 | Batch_idx: 300 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (34045/38528)
Epoch: 62 | Batch_idx: 310 |  Loss: (0.3414) |  Loss2: (0.0000) | Acc: (88.00%) (35168/39808)
Epoch: 62 | Batch_idx: 320 |  Loss: (0.3411) |  Loss2: (0.0000) | Acc: (88.00%) (36289/41088)
Epoch: 62 | Batch_idx: 330 |  Loss: (0.3408) |  Loss2: (0.0000) | Acc: (88.00%) (37422/42368)
Epoch: 62 | Batch_idx: 340 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (38550/43648)
Epoch: 62 | Batch_idx: 350 |  Loss: (0.3416) |  Loss2: (0.0000) | Acc: (88.00%) (39672/44928)
Epoch: 62 | Batch_idx: 360 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (40784/46208)
Epoch: 62 | Batch_idx: 370 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (41909/47488)
Epoch: 62 | Batch_idx: 380 |  Loss: (0.3423) |  Loss2: (0.0000) | Acc: (88.00%) (43045/48768)
Epoch: 62 | Batch_idx: 390 |  Loss: (0.3432) |  Loss2: (0.0000) | Acc: (88.00%) (44103/50000)
# TEST : Loss: (0.5365) | Acc: (83.00%) (8356/10000)
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5100, 0.4900], device='cuda:0')
percent tensor([0.5741, 0.4259], device='cuda:0')
percent tensor([0.5895, 0.4105], device='cuda:0')
percent tensor([0.7240, 0.2760], device='cuda:0')
percent tensor([0.6968, 0.3032], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 63 | Batch_idx: 0 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 63 | Batch_idx: 10 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (1242/1408)
Epoch: 63 | Batch_idx: 20 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (2369/2688)
Epoch: 63 | Batch_idx: 30 |  Loss: (0.3363) |  Loss2: (0.0000) | Acc: (88.00%) (3497/3968)
Epoch: 63 | Batch_idx: 40 |  Loss: (0.3352) |  Loss2: (0.0000) | Acc: (88.00%) (4627/5248)
Epoch: 63 | Batch_idx: 50 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (5762/6528)
Epoch: 63 | Batch_idx: 60 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (6896/7808)
Epoch: 63 | Batch_idx: 70 |  Loss: (0.3295) |  Loss2: (0.0000) | Acc: (88.00%) (8038/9088)
Epoch: 63 | Batch_idx: 80 |  Loss: (0.3275) |  Loss2: (0.0000) | Acc: (88.00%) (9192/10368)
Epoch: 63 | Batch_idx: 90 |  Loss: (0.3290) |  Loss2: (0.0000) | Acc: (88.00%) (10316/11648)
Epoch: 63 | Batch_idx: 100 |  Loss: (0.3273) |  Loss2: (0.0000) | Acc: (88.00%) (11450/12928)
Epoch: 63 | Batch_idx: 110 |  Loss: (0.3272) |  Loss2: (0.0000) | Acc: (88.00%) (12590/14208)
Epoch: 63 | Batch_idx: 120 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (13724/15488)
Epoch: 63 | Batch_idx: 130 |  Loss: (0.3251) |  Loss2: (0.0000) | Acc: (88.00%) (14864/16768)
Epoch: 63 | Batch_idx: 140 |  Loss: (0.3250) |  Loss2: (0.0000) | Acc: (88.00%) (16000/18048)
Epoch: 63 | Batch_idx: 150 |  Loss: (0.3234) |  Loss2: (0.0000) | Acc: (88.00%) (17150/19328)
Epoch: 63 | Batch_idx: 160 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (18301/20608)
Epoch: 63 | Batch_idx: 170 |  Loss: (0.3229) |  Loss2: (0.0000) | Acc: (88.00%) (19435/21888)
Epoch: 63 | Batch_idx: 180 |  Loss: (0.3227) |  Loss2: (0.0000) | Acc: (88.00%) (20571/23168)
Epoch: 63 | Batch_idx: 190 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (21703/24448)
Epoch: 63 | Batch_idx: 200 |  Loss: (0.3220) |  Loss2: (0.0000) | Acc: (88.00%) (22846/25728)
Epoch: 63 | Batch_idx: 210 |  Loss: (0.3244) |  Loss2: (0.0000) | Acc: (88.00%) (23965/27008)
Epoch: 63 | Batch_idx: 220 |  Loss: (0.3261) |  Loss2: (0.0000) | Acc: (88.00%) (25077/28288)
Epoch: 63 | Batch_idx: 230 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (26205/29568)
Epoch: 63 | Batch_idx: 240 |  Loss: (0.3264) |  Loss2: (0.0000) | Acc: (88.00%) (27343/30848)
Epoch: 63 | Batch_idx: 250 |  Loss: (0.3268) |  Loss2: (0.0000) | Acc: (88.00%) (28464/32128)
Epoch: 63 | Batch_idx: 260 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (29603/33408)
Epoch: 63 | Batch_idx: 270 |  Loss: (0.3270) |  Loss2: (0.0000) | Acc: (88.00%) (30734/34688)
Epoch: 63 | Batch_idx: 280 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (88.00%) (31868/35968)
Epoch: 63 | Batch_idx: 290 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (32990/37248)
Epoch: 63 | Batch_idx: 300 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (34117/38528)
Epoch: 63 | Batch_idx: 310 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (35249/39808)
Epoch: 63 | Batch_idx: 320 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (36364/41088)
Epoch: 63 | Batch_idx: 330 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (37507/42368)
Epoch: 63 | Batch_idx: 340 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (38656/43648)
Epoch: 63 | Batch_idx: 350 |  Loss: (0.3287) |  Loss2: (0.0000) | Acc: (88.00%) (39789/44928)
Epoch: 63 | Batch_idx: 360 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (40911/46208)
Epoch: 63 | Batch_idx: 370 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (42054/47488)
Epoch: 63 | Batch_idx: 380 |  Loss: (0.3298) |  Loss2: (0.0000) | Acc: (88.00%) (43199/48768)
Epoch: 63 | Batch_idx: 390 |  Loss: (0.3308) |  Loss2: (0.0000) | Acc: (88.00%) (44288/50000)
# TEST : Loss: (0.5133) | Acc: (83.00%) (8325/10000)
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.5104, 0.4896], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.5900, 0.4100], device='cuda:0')
percent tensor([0.7216, 0.2784], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 64 | Batch_idx: 0 |  Loss: (0.3146) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 64 | Batch_idx: 10 |  Loss: (0.3512) |  Loss2: (0.0000) | Acc: (88.00%) (1245/1408)
Epoch: 64 | Batch_idx: 20 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 64 | Batch_idx: 30 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (89.00%) (3541/3968)
Epoch: 64 | Batch_idx: 40 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (89.00%) (4679/5248)
Epoch: 64 | Batch_idx: 50 |  Loss: (0.3249) |  Loss2: (0.0000) | Acc: (89.00%) (5836/6528)
Epoch: 64 | Batch_idx: 60 |  Loss: (0.3258) |  Loss2: (0.0000) | Acc: (89.00%) (6966/7808)
Epoch: 64 | Batch_idx: 70 |  Loss: (0.3271) |  Loss2: (0.0000) | Acc: (89.00%) (8102/9088)
Epoch: 64 | Batch_idx: 80 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (89.00%) (9253/10368)
Epoch: 64 | Batch_idx: 90 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (10405/11648)
Epoch: 64 | Batch_idx: 100 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (11545/12928)
Epoch: 64 | Batch_idx: 110 |  Loss: (0.3210) |  Loss2: (0.0000) | Acc: (89.00%) (12651/14208)
Epoch: 64 | Batch_idx: 120 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (13790/15488)
Epoch: 64 | Batch_idx: 130 |  Loss: (0.3207) |  Loss2: (0.0000) | Acc: (88.00%) (14910/16768)
Epoch: 64 | Batch_idx: 140 |  Loss: (0.3208) |  Loss2: (0.0000) | Acc: (88.00%) (16048/18048)
Epoch: 64 | Batch_idx: 150 |  Loss: (0.3202) |  Loss2: (0.0000) | Acc: (88.00%) (17189/19328)
Epoch: 64 | Batch_idx: 160 |  Loss: (0.3218) |  Loss2: (0.0000) | Acc: (88.00%) (18312/20608)
Epoch: 64 | Batch_idx: 170 |  Loss: (0.3226) |  Loss2: (0.0000) | Acc: (88.00%) (19437/21888)
Epoch: 64 | Batch_idx: 180 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (88.00%) (20583/23168)
Epoch: 64 | Batch_idx: 190 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (21735/24448)
Epoch: 64 | Batch_idx: 200 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (22884/25728)
Epoch: 64 | Batch_idx: 210 |  Loss: (0.3203) |  Loss2: (0.0000) | Acc: (88.00%) (24024/27008)
Epoch: 64 | Batch_idx: 220 |  Loss: (0.3206) |  Loss2: (0.0000) | Acc: (88.00%) (25169/28288)
Epoch: 64 | Batch_idx: 230 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (26318/29568)
Epoch: 64 | Batch_idx: 240 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (27468/30848)
Epoch: 64 | Batch_idx: 250 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (28604/32128)
Epoch: 64 | Batch_idx: 260 |  Loss: (0.3181) |  Loss2: (0.0000) | Acc: (89.00%) (29751/33408)
Epoch: 64 | Batch_idx: 270 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (30888/34688)
Epoch: 64 | Batch_idx: 280 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (32020/35968)
Epoch: 64 | Batch_idx: 290 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (33164/37248)
Epoch: 64 | Batch_idx: 300 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (34302/38528)
Epoch: 64 | Batch_idx: 310 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (35446/39808)
Epoch: 64 | Batch_idx: 320 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (36597/41088)
Epoch: 64 | Batch_idx: 330 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (37725/42368)
Epoch: 64 | Batch_idx: 340 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (38847/43648)
Epoch: 64 | Batch_idx: 350 |  Loss: (0.3187) |  Loss2: (0.0000) | Acc: (89.00%) (40001/44928)
Epoch: 64 | Batch_idx: 360 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (41147/46208)
Epoch: 64 | Batch_idx: 370 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (42269/47488)
Epoch: 64 | Batch_idx: 380 |  Loss: (0.3212) |  Loss2: (0.0000) | Acc: (88.00%) (43400/48768)
Epoch: 64 | Batch_idx: 390 |  Loss: (0.3215) |  Loss2: (0.0000) | Acc: (88.00%) (44489/50000)
# TEST : Loss: (0.4979) | Acc: (84.00%) (8444/10000)
percent tensor([0.5712, 0.4288], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.5099, 0.4901], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5918, 0.4082], device='cuda:0')
percent tensor([0.7213, 0.2787], device='cuda:0')
percent tensor([0.6950, 0.3050], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 65 | Batch_idx: 0 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 65 | Batch_idx: 10 |  Loss: (0.3379) |  Loss2: (0.0000) | Acc: (87.00%) (1233/1408)
Epoch: 65 | Batch_idx: 20 |  Loss: (0.3724) |  Loss2: (0.0000) | Acc: (87.00%) (2339/2688)
Epoch: 65 | Batch_idx: 30 |  Loss: (0.4093) |  Loss2: (0.0000) | Acc: (85.00%) (3403/3968)
Epoch: 65 | Batch_idx: 40 |  Loss: (0.4254) |  Loss2: (0.0000) | Acc: (85.00%) (4479/5248)
Epoch: 65 | Batch_idx: 50 |  Loss: (0.4322) |  Loss2: (0.0000) | Acc: (84.00%) (5535/6528)
Epoch: 65 | Batch_idx: 60 |  Loss: (0.4350) |  Loss2: (0.0000) | Acc: (84.00%) (6618/7808)
Epoch: 65 | Batch_idx: 70 |  Loss: (0.4356) |  Loss2: (0.0000) | Acc: (84.00%) (7707/9088)
Epoch: 65 | Batch_idx: 80 |  Loss: (0.4361) |  Loss2: (0.0000) | Acc: (84.00%) (8770/10368)
Epoch: 65 | Batch_idx: 90 |  Loss: (0.4340) |  Loss2: (0.0000) | Acc: (84.00%) (9859/11648)
Epoch: 65 | Batch_idx: 100 |  Loss: (0.4353) |  Loss2: (0.0000) | Acc: (84.00%) (10937/12928)
Epoch: 65 | Batch_idx: 110 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (12018/14208)
Epoch: 65 | Batch_idx: 120 |  Loss: (0.4334) |  Loss2: (0.0000) | Acc: (84.00%) (13100/15488)
Epoch: 65 | Batch_idx: 130 |  Loss: (0.4296) |  Loss2: (0.0000) | Acc: (84.00%) (14211/16768)
Epoch: 65 | Batch_idx: 140 |  Loss: (0.4311) |  Loss2: (0.0000) | Acc: (84.00%) (15273/18048)
Epoch: 65 | Batch_idx: 150 |  Loss: (0.4318) |  Loss2: (0.0000) | Acc: (84.00%) (16354/19328)
Epoch: 65 | Batch_idx: 160 |  Loss: (0.4306) |  Loss2: (0.0000) | Acc: (84.00%) (17446/20608)
Epoch: 65 | Batch_idx: 170 |  Loss: (0.4286) |  Loss2: (0.0000) | Acc: (84.00%) (18544/21888)
Epoch: 65 | Batch_idx: 180 |  Loss: (0.4264) |  Loss2: (0.0000) | Acc: (84.00%) (19643/23168)
Epoch: 65 | Batch_idx: 190 |  Loss: (0.4265) |  Loss2: (0.0000) | Acc: (84.00%) (20728/24448)
Epoch: 65 | Batch_idx: 200 |  Loss: (0.4285) |  Loss2: (0.0000) | Acc: (84.00%) (21803/25728)
Epoch: 65 | Batch_idx: 210 |  Loss: (0.4312) |  Loss2: (0.0000) | Acc: (84.00%) (22868/27008)
Epoch: 65 | Batch_idx: 220 |  Loss: (0.4291) |  Loss2: (0.0000) | Acc: (84.00%) (23968/28288)
Epoch: 65 | Batch_idx: 230 |  Loss: (0.4292) |  Loss2: (0.0000) | Acc: (84.00%) (25056/29568)
Epoch: 65 | Batch_idx: 240 |  Loss: (0.4276) |  Loss2: (0.0000) | Acc: (84.00%) (26156/30848)
Epoch: 65 | Batch_idx: 250 |  Loss: (0.4261) |  Loss2: (0.0000) | Acc: (84.00%) (27273/32128)
Epoch: 65 | Batch_idx: 260 |  Loss: (0.4241) |  Loss2: (0.0000) | Acc: (84.00%) (28383/33408)
Epoch: 65 | Batch_idx: 270 |  Loss: (0.4217) |  Loss2: (0.0000) | Acc: (85.00%) (29490/34688)
Epoch: 65 | Batch_idx: 280 |  Loss: (0.4205) |  Loss2: (0.0000) | Acc: (85.00%) (30598/35968)
Epoch: 65 | Batch_idx: 290 |  Loss: (0.4200) |  Loss2: (0.0000) | Acc: (85.00%) (31700/37248)
Epoch: 65 | Batch_idx: 300 |  Loss: (0.4182) |  Loss2: (0.0000) | Acc: (85.00%) (32817/38528)
Epoch: 65 | Batch_idx: 310 |  Loss: (0.4175) |  Loss2: (0.0000) | Acc: (85.00%) (33919/39808)
Epoch: 65 | Batch_idx: 320 |  Loss: (0.4163) |  Loss2: (0.0000) | Acc: (85.00%) (35034/41088)
Epoch: 65 | Batch_idx: 330 |  Loss: (0.4168) |  Loss2: (0.0000) | Acc: (85.00%) (36111/42368)
Epoch: 65 | Batch_idx: 340 |  Loss: (0.4149) |  Loss2: (0.0000) | Acc: (85.00%) (37227/43648)
Epoch: 65 | Batch_idx: 350 |  Loss: (0.4142) |  Loss2: (0.0000) | Acc: (85.00%) (38341/44928)
Epoch: 65 | Batch_idx: 360 |  Loss: (0.4137) |  Loss2: (0.0000) | Acc: (85.00%) (39447/46208)
Epoch: 65 | Batch_idx: 370 |  Loss: (0.4138) |  Loss2: (0.0000) | Acc: (85.00%) (40540/47488)
Epoch: 65 | Batch_idx: 380 |  Loss: (0.4134) |  Loss2: (0.0000) | Acc: (85.00%) (41650/48768)
Epoch: 65 | Batch_idx: 390 |  Loss: (0.4116) |  Loss2: (0.0000) | Acc: (85.00%) (42728/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_065.pth.tar'
# TEST : Loss: (0.5212) | Acc: (83.00%) (8326/10000)
percent tensor([0.5596, 0.4404], device='cuda:0')
percent tensor([0.5344, 0.4656], device='cuda:0')
percent tensor([0.5156, 0.4844], device='cuda:0')
percent tensor([0.6194, 0.3806], device='cuda:0')
percent tensor([0.5954, 0.4046], device='cuda:0')
percent tensor([0.6958, 0.3042], device='cuda:0')
percent tensor([0.6514, 0.3486], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 66 | Batch_idx: 0 |  Loss: (0.4931) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 66 | Batch_idx: 10 |  Loss: (0.3710) |  Loss2: (0.0000) | Acc: (86.00%) (1222/1408)
Epoch: 66 | Batch_idx: 20 |  Loss: (0.3873) |  Loss2: (0.0000) | Acc: (86.00%) (2324/2688)
Epoch: 66 | Batch_idx: 30 |  Loss: (0.3787) |  Loss2: (0.0000) | Acc: (86.00%) (3443/3968)
Epoch: 66 | Batch_idx: 40 |  Loss: (0.3821) |  Loss2: (0.0000) | Acc: (86.00%) (4550/5248)
Epoch: 66 | Batch_idx: 50 |  Loss: (0.3797) |  Loss2: (0.0000) | Acc: (86.00%) (5663/6528)
Epoch: 66 | Batch_idx: 60 |  Loss: (0.3791) |  Loss2: (0.0000) | Acc: (86.00%) (6778/7808)
Epoch: 66 | Batch_idx: 70 |  Loss: (0.3784) |  Loss2: (0.0000) | Acc: (86.00%) (7904/9088)
Epoch: 66 | Batch_idx: 80 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (9021/10368)
Epoch: 66 | Batch_idx: 90 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (10125/11648)
Epoch: 66 | Batch_idx: 100 |  Loss: (0.3802) |  Loss2: (0.0000) | Acc: (86.00%) (11240/12928)
Epoch: 66 | Batch_idx: 110 |  Loss: (0.3819) |  Loss2: (0.0000) | Acc: (86.00%) (12336/14208)
Epoch: 66 | Batch_idx: 120 |  Loss: (0.3793) |  Loss2: (0.0000) | Acc: (86.00%) (13454/15488)
Epoch: 66 | Batch_idx: 130 |  Loss: (0.3763) |  Loss2: (0.0000) | Acc: (87.00%) (14595/16768)
Epoch: 66 | Batch_idx: 140 |  Loss: (0.3774) |  Loss2: (0.0000) | Acc: (86.00%) (15690/18048)
Epoch: 66 | Batch_idx: 150 |  Loss: (0.3761) |  Loss2: (0.0000) | Acc: (87.00%) (16817/19328)
Epoch: 66 | Batch_idx: 160 |  Loss: (0.3727) |  Loss2: (0.0000) | Acc: (87.00%) (17962/20608)
Epoch: 66 | Batch_idx: 170 |  Loss: (0.3740) |  Loss2: (0.0000) | Acc: (87.00%) (19066/21888)
Epoch: 66 | Batch_idx: 180 |  Loss: (0.3756) |  Loss2: (0.0000) | Acc: (87.00%) (20163/23168)
Epoch: 66 | Batch_idx: 190 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (87.00%) (21272/24448)
Epoch: 66 | Batch_idx: 200 |  Loss: (0.3772) |  Loss2: (0.0000) | Acc: (86.00%) (22380/25728)
Epoch: 66 | Batch_idx: 210 |  Loss: (0.3759) |  Loss2: (0.0000) | Acc: (87.00%) (23499/27008)
Epoch: 66 | Batch_idx: 220 |  Loss: (0.3737) |  Loss2: (0.0000) | Acc: (87.00%) (24637/28288)
Epoch: 66 | Batch_idx: 230 |  Loss: (0.3716) |  Loss2: (0.0000) | Acc: (87.00%) (25778/29568)
Epoch: 66 | Batch_idx: 240 |  Loss: (0.3717) |  Loss2: (0.0000) | Acc: (87.00%) (26891/30848)
Epoch: 66 | Batch_idx: 250 |  Loss: (0.3719) |  Loss2: (0.0000) | Acc: (87.00%) (28008/32128)
Epoch: 66 | Batch_idx: 260 |  Loss: (0.3713) |  Loss2: (0.0000) | Acc: (87.00%) (29134/33408)
Epoch: 66 | Batch_idx: 270 |  Loss: (0.3706) |  Loss2: (0.0000) | Acc: (87.00%) (30241/34688)
Epoch: 66 | Batch_idx: 280 |  Loss: (0.3714) |  Loss2: (0.0000) | Acc: (87.00%) (31348/35968)
Epoch: 66 | Batch_idx: 290 |  Loss: (0.3702) |  Loss2: (0.0000) | Acc: (87.00%) (32465/37248)
Epoch: 66 | Batch_idx: 300 |  Loss: (0.3696) |  Loss2: (0.0000) | Acc: (87.00%) (33604/38528)
Epoch: 66 | Batch_idx: 310 |  Loss: (0.3690) |  Loss2: (0.0000) | Acc: (87.00%) (34735/39808)
Epoch: 66 | Batch_idx: 320 |  Loss: (0.3682) |  Loss2: (0.0000) | Acc: (87.00%) (35859/41088)
Epoch: 66 | Batch_idx: 330 |  Loss: (0.3672) |  Loss2: (0.0000) | Acc: (87.00%) (36994/42368)
Epoch: 66 | Batch_idx: 340 |  Loss: (0.3675) |  Loss2: (0.0000) | Acc: (87.00%) (38089/43648)
Epoch: 66 | Batch_idx: 350 |  Loss: (0.3658) |  Loss2: (0.0000) | Acc: (87.00%) (39235/44928)
Epoch: 66 | Batch_idx: 360 |  Loss: (0.3660) |  Loss2: (0.0000) | Acc: (87.00%) (40337/46208)
Epoch: 66 | Batch_idx: 370 |  Loss: (0.3654) |  Loss2: (0.0000) | Acc: (87.00%) (41463/47488)
Epoch: 66 | Batch_idx: 380 |  Loss: (0.3644) |  Loss2: (0.0000) | Acc: (87.00%) (42598/48768)
Epoch: 66 | Batch_idx: 390 |  Loss: (0.3651) |  Loss2: (0.0000) | Acc: (87.00%) (43659/50000)
# TEST : Loss: (0.4829) | Acc: (84.00%) (8415/10000)
percent tensor([0.5668, 0.4332], device='cuda:0')
percent tensor([0.5331, 0.4669], device='cuda:0')
percent tensor([0.5184, 0.4816], device='cuda:0')
percent tensor([0.6313, 0.3687], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.6518, 0.3482], device='cuda:0')
percent tensor([0.9988, 0.0012], device='cuda:0')
Epoch: 67 | Batch_idx: 0 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 67 | Batch_idx: 10 |  Loss: (0.3223) |  Loss2: (0.0000) | Acc: (89.00%) (1255/1408)
Epoch: 67 | Batch_idx: 20 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (2386/2688)
Epoch: 67 | Batch_idx: 30 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (3489/3968)
Epoch: 67 | Batch_idx: 40 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (4621/5248)
Epoch: 67 | Batch_idx: 50 |  Loss: (0.3447) |  Loss2: (0.0000) | Acc: (88.00%) (5745/6528)
Epoch: 67 | Batch_idx: 60 |  Loss: (0.3470) |  Loss2: (0.0000) | Acc: (87.00%) (6864/7808)
Epoch: 67 | Batch_idx: 70 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (87.00%) (7991/9088)
Epoch: 67 | Batch_idx: 80 |  Loss: (0.3495) |  Loss2: (0.0000) | Acc: (87.00%) (9106/10368)
Epoch: 67 | Batch_idx: 90 |  Loss: (0.3463) |  Loss2: (0.0000) | Acc: (87.00%) (10247/11648)
Epoch: 67 | Batch_idx: 100 |  Loss: (0.3464) |  Loss2: (0.0000) | Acc: (87.00%) (11368/12928)
Epoch: 67 | Batch_idx: 110 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (12477/14208)
Epoch: 67 | Batch_idx: 120 |  Loss: (0.3510) |  Loss2: (0.0000) | Acc: (87.00%) (13600/15488)
Epoch: 67 | Batch_idx: 130 |  Loss: (0.3503) |  Loss2: (0.0000) | Acc: (87.00%) (14731/16768)
Epoch: 67 | Batch_idx: 140 |  Loss: (0.3507) |  Loss2: (0.0000) | Acc: (87.00%) (15852/18048)
Epoch: 67 | Batch_idx: 150 |  Loss: (0.3506) |  Loss2: (0.0000) | Acc: (87.00%) (16980/19328)
Epoch: 67 | Batch_idx: 160 |  Loss: (0.3497) |  Loss2: (0.0000) | Acc: (87.00%) (18102/20608)
Epoch: 67 | Batch_idx: 170 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (87.00%) (19237/21888)
Epoch: 67 | Batch_idx: 180 |  Loss: (0.3484) |  Loss2: (0.0000) | Acc: (87.00%) (20359/23168)
Epoch: 67 | Batch_idx: 190 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (21488/24448)
Epoch: 67 | Batch_idx: 200 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (22610/25728)
Epoch: 67 | Batch_idx: 210 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (23737/27008)
Epoch: 67 | Batch_idx: 220 |  Loss: (0.3471) |  Loss2: (0.0000) | Acc: (87.00%) (24863/28288)
Epoch: 67 | Batch_idx: 230 |  Loss: (0.3477) |  Loss2: (0.0000) | Acc: (87.00%) (25983/29568)
Epoch: 67 | Batch_idx: 240 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (27120/30848)
Epoch: 67 | Batch_idx: 250 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (87.00%) (28263/32128)
Epoch: 67 | Batch_idx: 260 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (29401/33408)
Epoch: 67 | Batch_idx: 270 |  Loss: (0.3446) |  Loss2: (0.0000) | Acc: (88.00%) (30537/34688)
Epoch: 67 | Batch_idx: 280 |  Loss: (0.3457) |  Loss2: (0.0000) | Acc: (88.00%) (31662/35968)
Epoch: 67 | Batch_idx: 290 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (32779/37248)
Epoch: 67 | Batch_idx: 300 |  Loss: (0.3466) |  Loss2: (0.0000) | Acc: (88.00%) (33909/38528)
Epoch: 67 | Batch_idx: 310 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (87.00%) (35030/39808)
Epoch: 67 | Batch_idx: 320 |  Loss: (0.3465) |  Loss2: (0.0000) | Acc: (88.00%) (36163/41088)
Epoch: 67 | Batch_idx: 330 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (37303/42368)
Epoch: 67 | Batch_idx: 340 |  Loss: (0.3459) |  Loss2: (0.0000) | Acc: (88.00%) (38441/43648)
Epoch: 67 | Batch_idx: 350 |  Loss: (0.3461) |  Loss2: (0.0000) | Acc: (88.00%) (39553/44928)
Epoch: 67 | Batch_idx: 360 |  Loss: (0.3454) |  Loss2: (0.0000) | Acc: (88.00%) (40695/46208)
Epoch: 67 | Batch_idx: 370 |  Loss: (0.3449) |  Loss2: (0.0000) | Acc: (88.00%) (41826/47488)
Epoch: 67 | Batch_idx: 380 |  Loss: (0.3451) |  Loss2: (0.0000) | Acc: (88.00%) (42949/48768)
Epoch: 67 | Batch_idx: 390 |  Loss: (0.3453) |  Loss2: (0.0000) | Acc: (88.00%) (44015/50000)
# TEST : Loss: (0.4669) | Acc: (84.00%) (8497/10000)
percent tensor([0.5721, 0.4279], device='cuda:0')
percent tensor([0.5332, 0.4668], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.6983, 0.3017], device='cuda:0')
percent tensor([0.6625, 0.3375], device='cuda:0')
percent tensor([0.9989, 0.0011], device='cuda:0')
Epoch: 68 | Batch_idx: 0 |  Loss: (0.2998) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 68 | Batch_idx: 10 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 68 | Batch_idx: 20 |  Loss: (0.3529) |  Loss2: (0.0000) | Acc: (88.00%) (2372/2688)
Epoch: 68 | Batch_idx: 30 |  Loss: (0.3524) |  Loss2: (0.0000) | Acc: (88.00%) (3507/3968)
Epoch: 68 | Batch_idx: 40 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (4646/5248)
Epoch: 68 | Batch_idx: 50 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (88.00%) (5768/6528)
Epoch: 68 | Batch_idx: 60 |  Loss: (0.3468) |  Loss2: (0.0000) | Acc: (88.00%) (6888/7808)
Epoch: 68 | Batch_idx: 70 |  Loss: (0.3445) |  Loss2: (0.0000) | Acc: (88.00%) (8022/9088)
Epoch: 68 | Batch_idx: 80 |  Loss: (0.3413) |  Loss2: (0.0000) | Acc: (88.00%) (9164/10368)
Epoch: 68 | Batch_idx: 90 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (88.00%) (10273/11648)
Epoch: 68 | Batch_idx: 100 |  Loss: (0.3427) |  Loss2: (0.0000) | Acc: (88.00%) (11409/12928)
Epoch: 68 | Batch_idx: 110 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (12547/14208)
Epoch: 68 | Batch_idx: 120 |  Loss: (0.3407) |  Loss2: (0.0000) | Acc: (88.00%) (13675/15488)
Epoch: 68 | Batch_idx: 130 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (88.00%) (14805/16768)
Epoch: 68 | Batch_idx: 140 |  Loss: (0.3381) |  Loss2: (0.0000) | Acc: (88.00%) (15946/18048)
Epoch: 68 | Batch_idx: 150 |  Loss: (0.3370) |  Loss2: (0.0000) | Acc: (88.00%) (17081/19328)
Epoch: 68 | Batch_idx: 160 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (18227/20608)
Epoch: 68 | Batch_idx: 170 |  Loss: (0.3358) |  Loss2: (0.0000) | Acc: (88.00%) (19365/21888)
Epoch: 68 | Batch_idx: 180 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (88.00%) (20488/23168)
Epoch: 68 | Batch_idx: 190 |  Loss: (0.3359) |  Loss2: (0.0000) | Acc: (88.00%) (21632/24448)
Epoch: 68 | Batch_idx: 200 |  Loss: (0.3361) |  Loss2: (0.0000) | Acc: (88.00%) (22751/25728)
Epoch: 68 | Batch_idx: 210 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (23889/27008)
Epoch: 68 | Batch_idx: 220 |  Loss: (0.3356) |  Loss2: (0.0000) | Acc: (88.00%) (25007/28288)
Epoch: 68 | Batch_idx: 230 |  Loss: (0.3355) |  Loss2: (0.0000) | Acc: (88.00%) (26136/29568)
Epoch: 68 | Batch_idx: 240 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (27271/30848)
Epoch: 68 | Batch_idx: 250 |  Loss: (0.3349) |  Loss2: (0.0000) | Acc: (88.00%) (28414/32128)
Epoch: 68 | Batch_idx: 260 |  Loss: (0.3357) |  Loss2: (0.0000) | Acc: (88.00%) (29537/33408)
Epoch: 68 | Batch_idx: 270 |  Loss: (0.3360) |  Loss2: (0.0000) | Acc: (88.00%) (30671/34688)
Epoch: 68 | Batch_idx: 280 |  Loss: (0.3354) |  Loss2: (0.0000) | Acc: (88.00%) (31807/35968)
Epoch: 68 | Batch_idx: 290 |  Loss: (0.3351) |  Loss2: (0.0000) | Acc: (88.00%) (32942/37248)
Epoch: 68 | Batch_idx: 300 |  Loss: (0.3348) |  Loss2: (0.0000) | Acc: (88.00%) (34082/38528)
Epoch: 68 | Batch_idx: 310 |  Loss: (0.3345) |  Loss2: (0.0000) | Acc: (88.00%) (35225/39808)
Epoch: 68 | Batch_idx: 320 |  Loss: (0.3341) |  Loss2: (0.0000) | Acc: (88.00%) (36355/41088)
Epoch: 68 | Batch_idx: 330 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (37489/42368)
Epoch: 68 | Batch_idx: 340 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (88.00%) (38629/43648)
Epoch: 68 | Batch_idx: 350 |  Loss: (0.3329) |  Loss2: (0.0000) | Acc: (88.00%) (39784/44928)
Epoch: 68 | Batch_idx: 360 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (40928/46208)
Epoch: 68 | Batch_idx: 370 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (42048/47488)
Epoch: 68 | Batch_idx: 380 |  Loss: (0.3332) |  Loss2: (0.0000) | Acc: (88.00%) (43185/48768)
Epoch: 68 | Batch_idx: 390 |  Loss: (0.3327) |  Loss2: (0.0000) | Acc: (88.00%) (44286/50000)
# TEST : Loss: (0.4553) | Acc: (85.00%) (8513/10000)
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5183, 0.4817], device='cuda:0')
percent tensor([0.6324, 0.3676], device='cuda:0')
percent tensor([0.5818, 0.4182], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.6651, 0.3349], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 69 | Batch_idx: 0 |  Loss: (0.2387) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 69 | Batch_idx: 10 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 69 | Batch_idx: 20 |  Loss: (0.3262) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 69 | Batch_idx: 30 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (3524/3968)
Epoch: 69 | Batch_idx: 40 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (4648/5248)
Epoch: 69 | Batch_idx: 50 |  Loss: (0.3243) |  Loss2: (0.0000) | Acc: (88.00%) (5794/6528)
Epoch: 69 | Batch_idx: 60 |  Loss: (0.3245) |  Loss2: (0.0000) | Acc: (88.00%) (6935/7808)
Epoch: 69 | Batch_idx: 70 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (8071/9088)
Epoch: 69 | Batch_idx: 80 |  Loss: (0.3269) |  Loss2: (0.0000) | Acc: (88.00%) (9201/10368)
Epoch: 69 | Batch_idx: 90 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (10320/11648)
Epoch: 69 | Batch_idx: 100 |  Loss: (0.3326) |  Loss2: (0.0000) | Acc: (88.00%) (11450/12928)
Epoch: 69 | Batch_idx: 110 |  Loss: (0.3296) |  Loss2: (0.0000) | Acc: (88.00%) (12592/14208)
Epoch: 69 | Batch_idx: 120 |  Loss: (0.3294) |  Loss2: (0.0000) | Acc: (88.00%) (13721/15488)
Epoch: 69 | Batch_idx: 130 |  Loss: (0.3299) |  Loss2: (0.0000) | Acc: (88.00%) (14868/16768)
Epoch: 69 | Batch_idx: 140 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (15995/18048)
Epoch: 69 | Batch_idx: 150 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (17130/19328)
Epoch: 69 | Batch_idx: 160 |  Loss: (0.3318) |  Loss2: (0.0000) | Acc: (88.00%) (18247/20608)
Epoch: 69 | Batch_idx: 170 |  Loss: (0.3305) |  Loss2: (0.0000) | Acc: (88.00%) (19384/21888)
Epoch: 69 | Batch_idx: 180 |  Loss: (0.3302) |  Loss2: (0.0000) | Acc: (88.00%) (20526/23168)
Epoch: 69 | Batch_idx: 190 |  Loss: (0.3317) |  Loss2: (0.0000) | Acc: (88.00%) (21641/24448)
Epoch: 69 | Batch_idx: 200 |  Loss: (0.3307) |  Loss2: (0.0000) | Acc: (88.00%) (22794/25728)
Epoch: 69 | Batch_idx: 210 |  Loss: (0.3314) |  Loss2: (0.0000) | Acc: (88.00%) (23922/27008)
Epoch: 69 | Batch_idx: 220 |  Loss: (0.3310) |  Loss2: (0.0000) | Acc: (88.00%) (25065/28288)
Epoch: 69 | Batch_idx: 230 |  Loss: (0.3303) |  Loss2: (0.0000) | Acc: (88.00%) (26201/29568)
Epoch: 69 | Batch_idx: 240 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (27338/30848)
Epoch: 69 | Batch_idx: 250 |  Loss: (0.3280) |  Loss2: (0.0000) | Acc: (88.00%) (28498/32128)
Epoch: 69 | Batch_idx: 260 |  Loss: (0.3288) |  Loss2: (0.0000) | Acc: (88.00%) (29631/33408)
Epoch: 69 | Batch_idx: 270 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (30772/34688)
Epoch: 69 | Batch_idx: 280 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (31903/35968)
Epoch: 69 | Batch_idx: 290 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (33032/37248)
Epoch: 69 | Batch_idx: 300 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (34157/38528)
Epoch: 69 | Batch_idx: 310 |  Loss: (0.3282) |  Loss2: (0.0000) | Acc: (88.00%) (35301/39808)
Epoch: 69 | Batch_idx: 320 |  Loss: (0.3283) |  Loss2: (0.0000) | Acc: (88.00%) (36446/41088)
Epoch: 69 | Batch_idx: 330 |  Loss: (0.3289) |  Loss2: (0.0000) | Acc: (88.00%) (37575/42368)
Epoch: 69 | Batch_idx: 340 |  Loss: (0.3293) |  Loss2: (0.0000) | Acc: (88.00%) (38701/43648)
Epoch: 69 | Batch_idx: 350 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (39828/44928)
Epoch: 69 | Batch_idx: 360 |  Loss: (0.3301) |  Loss2: (0.0000) | Acc: (88.00%) (40968/46208)
Epoch: 69 | Batch_idx: 370 |  Loss: (0.3297) |  Loss2: (0.0000) | Acc: (88.00%) (42108/47488)
Epoch: 69 | Batch_idx: 380 |  Loss: (0.3292) |  Loss2: (0.0000) | Acc: (88.00%) (43241/48768)
Epoch: 69 | Batch_idx: 390 |  Loss: (0.3284) |  Loss2: (0.0000) | Acc: (88.00%) (44354/50000)
# TEST : Loss: (0.4462) | Acc: (85.00%) (8536/10000)
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.5336, 0.4664], device='cuda:0')
percent tensor([0.5163, 0.4837], device='cuda:0')
percent tensor([0.6293, 0.3707], device='cuda:0')
percent tensor([0.5779, 0.4221], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.6721, 0.3279], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 70 | Batch_idx: 0 |  Loss: (0.3047) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 70 | Batch_idx: 10 |  Loss: (0.3641) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 70 | Batch_idx: 20 |  Loss: (0.3377) |  Loss2: (0.0000) | Acc: (89.00%) (2394/2688)
Epoch: 70 | Batch_idx: 30 |  Loss: (0.3323) |  Loss2: (0.0000) | Acc: (88.00%) (3528/3968)
Epoch: 70 | Batch_idx: 40 |  Loss: (0.3336) |  Loss2: (0.0000) | Acc: (88.00%) (4661/5248)
Epoch: 70 | Batch_idx: 50 |  Loss: (0.3324) |  Loss2: (0.0000) | Acc: (88.00%) (5791/6528)
Epoch: 70 | Batch_idx: 60 |  Loss: (0.3300) |  Loss2: (0.0000) | Acc: (88.00%) (6930/7808)
Epoch: 70 | Batch_idx: 70 |  Loss: (0.3285) |  Loss2: (0.0000) | Acc: (88.00%) (8068/9088)
Epoch: 70 | Batch_idx: 80 |  Loss: (0.3265) |  Loss2: (0.0000) | Acc: (88.00%) (9220/10368)
Epoch: 70 | Batch_idx: 90 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (89.00%) (10367/11648)
Epoch: 70 | Batch_idx: 100 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (89.00%) (11510/12928)
Epoch: 70 | Batch_idx: 110 |  Loss: (0.3221) |  Loss2: (0.0000) | Acc: (89.00%) (12656/14208)
Epoch: 70 | Batch_idx: 120 |  Loss: (0.3235) |  Loss2: (0.0000) | Acc: (89.00%) (13786/15488)
Epoch: 70 | Batch_idx: 130 |  Loss: (0.3224) |  Loss2: (0.0000) | Acc: (89.00%) (14935/16768)
Epoch: 70 | Batch_idx: 140 |  Loss: (0.3199) |  Loss2: (0.0000) | Acc: (89.00%) (16088/18048)
Epoch: 70 | Batch_idx: 150 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (17238/19328)
Epoch: 70 | Batch_idx: 160 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (18380/20608)
Epoch: 70 | Batch_idx: 170 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (19526/21888)
Epoch: 70 | Batch_idx: 180 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (20651/23168)
Epoch: 70 | Batch_idx: 190 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (21783/24448)
Epoch: 70 | Batch_idx: 200 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (22924/25728)
Epoch: 70 | Batch_idx: 210 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (24048/27008)
Epoch: 70 | Batch_idx: 220 |  Loss: (0.3191) |  Loss2: (0.0000) | Acc: (89.00%) (25183/28288)
Epoch: 70 | Batch_idx: 230 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (26340/29568)
Epoch: 70 | Batch_idx: 240 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (27469/30848)
Epoch: 70 | Batch_idx: 250 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (28605/32128)
Epoch: 70 | Batch_idx: 260 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (29760/33408)
Epoch: 70 | Batch_idx: 270 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (30879/34688)
Epoch: 70 | Batch_idx: 280 |  Loss: (0.3179) |  Loss2: (0.0000) | Acc: (89.00%) (32019/35968)
Epoch: 70 | Batch_idx: 290 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (33177/37248)
Epoch: 70 | Batch_idx: 300 |  Loss: (0.3178) |  Loss2: (0.0000) | Acc: (89.00%) (34318/38528)
Epoch: 70 | Batch_idx: 310 |  Loss: (0.3188) |  Loss2: (0.0000) | Acc: (89.00%) (35440/39808)
Epoch: 70 | Batch_idx: 320 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (36579/41088)
Epoch: 70 | Batch_idx: 330 |  Loss: (0.3185) |  Loss2: (0.0000) | Acc: (89.00%) (37734/42368)
Epoch: 70 | Batch_idx: 340 |  Loss: (0.3176) |  Loss2: (0.0000) | Acc: (89.00%) (38887/43648)
Epoch: 70 | Batch_idx: 350 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (40004/44928)
Epoch: 70 | Batch_idx: 360 |  Loss: (0.3196) |  Loss2: (0.0000) | Acc: (89.00%) (41133/46208)
Epoch: 70 | Batch_idx: 370 |  Loss: (0.3194) |  Loss2: (0.0000) | Acc: (89.00%) (42286/47488)
Epoch: 70 | Batch_idx: 380 |  Loss: (0.3190) |  Loss2: (0.0000) | Acc: (89.00%) (43427/48768)
Epoch: 70 | Batch_idx: 390 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (44515/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_070.pth.tar'
# TEST : Loss: (0.4793) | Acc: (84.00%) (8449/10000)
percent tensor([0.5691, 0.4309], device='cuda:0')
percent tensor([0.5335, 0.4665], device='cuda:0')
percent tensor([0.5159, 0.4841], device='cuda:0')
percent tensor([0.6299, 0.3701], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.6972, 0.3028], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.1248, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(806.0308, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(801.2673, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1526.8949, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(498.5006, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2219.5400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4292.2446, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1408.5706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6117.7153, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11978.5146, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3977.5662, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16843.4844, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 71 | Batch_idx: 0 |  Loss: (0.3559) |  Loss2: (0.0000) | Acc: (86.00%) (111/128)
Epoch: 71 | Batch_idx: 10 |  Loss: (0.3419) |  Loss2: (0.0000) | Acc: (88.00%) (1247/1408)
Epoch: 71 | Batch_idx: 20 |  Loss: (0.3254) |  Loss2: (0.0000) | Acc: (89.00%) (2398/2688)
Epoch: 71 | Batch_idx: 30 |  Loss: (0.3232) |  Loss2: (0.0000) | Acc: (89.00%) (3539/3968)
Epoch: 71 | Batch_idx: 40 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (4689/5248)
Epoch: 71 | Batch_idx: 50 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (5852/6528)
Epoch: 71 | Batch_idx: 60 |  Loss: (0.3108) |  Loss2: (0.0000) | Acc: (89.00%) (7002/7808)
Epoch: 71 | Batch_idx: 70 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (8123/9088)
Epoch: 71 | Batch_idx: 80 |  Loss: (0.3082) |  Loss2: (0.0000) | Acc: (89.00%) (9264/10368)
Epoch: 71 | Batch_idx: 90 |  Loss: (0.3084) |  Loss2: (0.0000) | Acc: (89.00%) (10409/11648)
Epoch: 71 | Batch_idx: 100 |  Loss: (0.3092) |  Loss2: (0.0000) | Acc: (89.00%) (11550/12928)
Epoch: 71 | Batch_idx: 110 |  Loss: (0.3098) |  Loss2: (0.0000) | Acc: (89.00%) (12698/14208)
Epoch: 71 | Batch_idx: 120 |  Loss: (0.3118) |  Loss2: (0.0000) | Acc: (89.00%) (13822/15488)
Epoch: 71 | Batch_idx: 130 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (14963/16768)
Epoch: 71 | Batch_idx: 140 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (16110/18048)
Epoch: 71 | Batch_idx: 150 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (17255/19328)
Epoch: 71 | Batch_idx: 160 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (18395/20608)
Epoch: 71 | Batch_idx: 170 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (19551/21888)
Epoch: 71 | Batch_idx: 180 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (20690/23168)
Epoch: 71 | Batch_idx: 190 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (21831/24448)
Epoch: 71 | Batch_idx: 200 |  Loss: (0.3111) |  Loss2: (0.0000) | Acc: (89.00%) (22964/25728)
Epoch: 71 | Batch_idx: 210 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (24106/27008)
Epoch: 71 | Batch_idx: 220 |  Loss: (0.3104) |  Loss2: (0.0000) | Acc: (89.00%) (25258/28288)
Epoch: 71 | Batch_idx: 230 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (26400/29568)
Epoch: 71 | Batch_idx: 240 |  Loss: (0.3106) |  Loss2: (0.0000) | Acc: (89.00%) (27529/30848)
Epoch: 71 | Batch_idx: 250 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (28655/32128)
Epoch: 71 | Batch_idx: 260 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (29798/33408)
Epoch: 71 | Batch_idx: 270 |  Loss: (0.3114) |  Loss2: (0.0000) | Acc: (89.00%) (30946/34688)
Epoch: 71 | Batch_idx: 280 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (32095/35968)
Epoch: 71 | Batch_idx: 290 |  Loss: (0.3113) |  Loss2: (0.0000) | Acc: (89.00%) (33237/37248)
Epoch: 71 | Batch_idx: 300 |  Loss: (0.3119) |  Loss2: (0.0000) | Acc: (89.00%) (34368/38528)
Epoch: 71 | Batch_idx: 310 |  Loss: (0.3120) |  Loss2: (0.0000) | Acc: (89.00%) (35510/39808)
Epoch: 71 | Batch_idx: 320 |  Loss: (0.3117) |  Loss2: (0.0000) | Acc: (89.00%) (36651/41088)
Epoch: 71 | Batch_idx: 330 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (37803/42368)
Epoch: 71 | Batch_idx: 340 |  Loss: (0.3124) |  Loss2: (0.0000) | Acc: (89.00%) (38958/43648)
Epoch: 71 | Batch_idx: 350 |  Loss: (0.3115) |  Loss2: (0.0000) | Acc: (89.00%) (40108/44928)
Epoch: 71 | Batch_idx: 360 |  Loss: (0.3127) |  Loss2: (0.0000) | Acc: (89.00%) (41228/46208)
Epoch: 71 | Batch_idx: 370 |  Loss: (0.3130) |  Loss2: (0.0000) | Acc: (89.00%) (42362/47488)
Epoch: 71 | Batch_idx: 380 |  Loss: (0.3132) |  Loss2: (0.0000) | Acc: (89.00%) (43499/48768)
Epoch: 71 | Batch_idx: 390 |  Loss: (0.3131) |  Loss2: (0.0000) | Acc: (89.00%) (44607/50000)
# TEST : Loss: (0.4846) | Acc: (84.00%) (8403/10000)
percent tensor([0.5689, 0.4311], device='cuda:0')
percent tensor([0.5333, 0.4667], device='cuda:0')
percent tensor([0.5162, 0.4838], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6961, 0.3039], device='cuda:0')
percent tensor([0.6775, 0.3225], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 72 | Batch_idx: 0 |  Loss: (0.2374) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 72 | Batch_idx: 10 |  Loss: (0.2789) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 72 | Batch_idx: 20 |  Loss: (0.2942) |  Loss2: (0.0000) | Acc: (90.00%) (2424/2688)
Epoch: 72 | Batch_idx: 30 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (3581/3968)
Epoch: 72 | Batch_idx: 40 |  Loss: (0.3019) |  Loss2: (0.0000) | Acc: (89.00%) (4718/5248)
Epoch: 72 | Batch_idx: 50 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (90.00%) (5881/6528)
Epoch: 72 | Batch_idx: 60 |  Loss: (0.2961) |  Loss2: (0.0000) | Acc: (90.00%) (7033/7808)
Epoch: 72 | Batch_idx: 70 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (8167/9088)
Epoch: 72 | Batch_idx: 80 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (9289/10368)
Epoch: 72 | Batch_idx: 90 |  Loss: (0.2972) |  Loss2: (0.0000) | Acc: (89.00%) (10456/11648)
Epoch: 72 | Batch_idx: 100 |  Loss: (0.2963) |  Loss2: (0.0000) | Acc: (89.00%) (11614/12928)
Epoch: 72 | Batch_idx: 110 |  Loss: (0.2945) |  Loss2: (0.0000) | Acc: (89.00%) (12766/14208)
Epoch: 72 | Batch_idx: 120 |  Loss: (0.2971) |  Loss2: (0.0000) | Acc: (89.00%) (13895/15488)
Epoch: 72 | Batch_idx: 130 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (15033/16768)
Epoch: 72 | Batch_idx: 140 |  Loss: (0.2982) |  Loss2: (0.0000) | Acc: (89.00%) (16180/18048)
Epoch: 72 | Batch_idx: 150 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (17323/19328)
Epoch: 72 | Batch_idx: 160 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (18481/20608)
Epoch: 72 | Batch_idx: 170 |  Loss: (0.2997) |  Loss2: (0.0000) | Acc: (89.00%) (19613/21888)
Epoch: 72 | Batch_idx: 180 |  Loss: (0.3013) |  Loss2: (0.0000) | Acc: (89.00%) (20745/23168)
Epoch: 72 | Batch_idx: 190 |  Loss: (0.3014) |  Loss2: (0.0000) | Acc: (89.00%) (21891/24448)
Epoch: 72 | Batch_idx: 200 |  Loss: (0.2999) |  Loss2: (0.0000) | Acc: (89.00%) (23056/25728)
Epoch: 72 | Batch_idx: 210 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (24201/27008)
Epoch: 72 | Batch_idx: 220 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (25349/28288)
Epoch: 72 | Batch_idx: 230 |  Loss: (0.3006) |  Loss2: (0.0000) | Acc: (89.00%) (26484/29568)
Epoch: 72 | Batch_idx: 240 |  Loss: (0.3010) |  Loss2: (0.0000) | Acc: (89.00%) (27632/30848)
Epoch: 72 | Batch_idx: 250 |  Loss: (0.3016) |  Loss2: (0.0000) | Acc: (89.00%) (28768/32128)
Epoch: 72 | Batch_idx: 260 |  Loss: (0.3022) |  Loss2: (0.0000) | Acc: (89.00%) (29896/33408)
Epoch: 72 | Batch_idx: 270 |  Loss: (0.3012) |  Loss2: (0.0000) | Acc: (89.00%) (31054/34688)
Epoch: 72 | Batch_idx: 280 |  Loss: (0.3020) |  Loss2: (0.0000) | Acc: (89.00%) (32181/35968)
Epoch: 72 | Batch_idx: 290 |  Loss: (0.3002) |  Loss2: (0.0000) | Acc: (89.00%) (33358/37248)
Epoch: 72 | Batch_idx: 300 |  Loss: (0.3001) |  Loss2: (0.0000) | Acc: (89.00%) (34502/38528)
Epoch: 72 | Batch_idx: 310 |  Loss: (0.2994) |  Loss2: (0.0000) | Acc: (89.00%) (35655/39808)
Epoch: 72 | Batch_idx: 320 |  Loss: (0.2993) |  Loss2: (0.0000) | Acc: (89.00%) (36799/41088)
Epoch: 72 | Batch_idx: 330 |  Loss: (0.2987) |  Loss2: (0.0000) | Acc: (89.00%) (37957/42368)
Epoch: 72 | Batch_idx: 340 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (39103/43648)
Epoch: 72 | Batch_idx: 350 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (40254/44928)
Epoch: 72 | Batch_idx: 360 |  Loss: (0.2984) |  Loss2: (0.0000) | Acc: (89.00%) (41418/46208)
Epoch: 72 | Batch_idx: 370 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (42563/47488)
Epoch: 72 | Batch_idx: 380 |  Loss: (0.2980) |  Loss2: (0.0000) | Acc: (89.00%) (43709/48768)
Epoch: 72 | Batch_idx: 390 |  Loss: (0.2988) |  Loss2: (0.0000) | Acc: (89.00%) (44801/50000)
# TEST : Loss: (0.4757) | Acc: (84.00%) (8481/10000)
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5338, 0.4662], device='cuda:0')
percent tensor([0.5167, 0.4833], device='cuda:0')
percent tensor([0.6295, 0.3705], device='cuda:0')
percent tensor([0.5783, 0.4217], device='cuda:0')
percent tensor([0.6979, 0.3021], device='cuda:0')
percent tensor([0.6715, 0.3285], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 73 | Batch_idx: 0 |  Loss: (0.3867) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 73 | Batch_idx: 10 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (90.00%) (1278/1408)
Epoch: 73 | Batch_idx: 20 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (90.00%) (2439/2688)
Epoch: 73 | Batch_idx: 30 |  Loss: (0.2702) |  Loss2: (0.0000) | Acc: (90.00%) (3595/3968)
Epoch: 73 | Batch_idx: 40 |  Loss: (0.2822) |  Loss2: (0.0000) | Acc: (90.00%) (4732/5248)
Epoch: 73 | Batch_idx: 50 |  Loss: (0.2805) |  Loss2: (0.0000) | Acc: (90.00%) (5894/6528)
Epoch: 73 | Batch_idx: 60 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (7044/7808)
Epoch: 73 | Batch_idx: 70 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (8189/9088)
Epoch: 73 | Batch_idx: 80 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (90.00%) (9336/10368)
Epoch: 73 | Batch_idx: 90 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (10489/11648)
Epoch: 73 | Batch_idx: 100 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (11641/12928)
Epoch: 73 | Batch_idx: 110 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (12791/14208)
Epoch: 73 | Batch_idx: 120 |  Loss: (0.2855) |  Loss2: (0.0000) | Acc: (90.00%) (13966/15488)
Epoch: 73 | Batch_idx: 130 |  Loss: (0.2873) |  Loss2: (0.0000) | Acc: (90.00%) (15105/16768)
Epoch: 73 | Batch_idx: 140 |  Loss: (0.2880) |  Loss2: (0.0000) | Acc: (90.00%) (16252/18048)
Epoch: 73 | Batch_idx: 150 |  Loss: (0.2881) |  Loss2: (0.0000) | Acc: (90.00%) (17403/19328)
Epoch: 73 | Batch_idx: 160 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (18568/20608)
Epoch: 73 | Batch_idx: 170 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (19721/21888)
Epoch: 73 | Batch_idx: 180 |  Loss: (0.2870) |  Loss2: (0.0000) | Acc: (90.00%) (20862/23168)
Epoch: 73 | Batch_idx: 190 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (90.00%) (22005/24448)
Epoch: 73 | Batch_idx: 200 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (90.00%) (23163/25728)
Epoch: 73 | Batch_idx: 210 |  Loss: (0.2889) |  Loss2: (0.0000) | Acc: (89.00%) (24304/27008)
Epoch: 73 | Batch_idx: 220 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (25455/28288)
Epoch: 73 | Batch_idx: 230 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (26584/29568)
Epoch: 73 | Batch_idx: 240 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (27716/30848)
Epoch: 73 | Batch_idx: 250 |  Loss: (0.2911) |  Loss2: (0.0000) | Acc: (89.00%) (28871/32128)
Epoch: 73 | Batch_idx: 260 |  Loss: (0.2922) |  Loss2: (0.0000) | Acc: (89.00%) (30001/33408)
Epoch: 73 | Batch_idx: 270 |  Loss: (0.2925) |  Loss2: (0.0000) | Acc: (89.00%) (31149/34688)
Epoch: 73 | Batch_idx: 280 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (32291/35968)
Epoch: 73 | Batch_idx: 290 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (33452/37248)
Epoch: 73 | Batch_idx: 300 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (34593/38528)
Epoch: 73 | Batch_idx: 310 |  Loss: (0.2920) |  Loss2: (0.0000) | Acc: (89.00%) (35746/39808)
Epoch: 73 | Batch_idx: 320 |  Loss: (0.2918) |  Loss2: (0.0000) | Acc: (89.00%) (36886/41088)
Epoch: 73 | Batch_idx: 330 |  Loss: (0.2917) |  Loss2: (0.0000) | Acc: (89.00%) (38038/42368)
Epoch: 73 | Batch_idx: 340 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (89.00%) (39177/43648)
Epoch: 73 | Batch_idx: 350 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (40322/44928)
Epoch: 73 | Batch_idx: 360 |  Loss: (0.2933) |  Loss2: (0.0000) | Acc: (89.00%) (41458/46208)
Epoch: 73 | Batch_idx: 370 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (42597/47488)
Epoch: 73 | Batch_idx: 380 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (43728/48768)
Epoch: 73 | Batch_idx: 390 |  Loss: (0.2938) |  Loss2: (0.0000) | Acc: (89.00%) (44840/50000)
# TEST : Loss: (0.4636) | Acc: (85.00%) (8537/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5343, 0.4657], device='cuda:0')
percent tensor([0.5169, 0.4831], device='cuda:0')
percent tensor([0.6291, 0.3709], device='cuda:0')
percent tensor([0.5795, 0.4205], device='cuda:0')
percent tensor([0.6978, 0.3022], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 74 | Batch_idx: 0 |  Loss: (0.3097) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 74 | Batch_idx: 10 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (91.00%) (1284/1408)
Epoch: 74 | Batch_idx: 20 |  Loss: (0.2803) |  Loss2: (0.0000) | Acc: (90.00%) (2428/2688)
Epoch: 74 | Batch_idx: 30 |  Loss: (0.2788) |  Loss2: (0.0000) | Acc: (90.00%) (3580/3968)
Epoch: 74 | Batch_idx: 40 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (4725/5248)
Epoch: 74 | Batch_idx: 50 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (89.00%) (5873/6528)
Epoch: 74 | Batch_idx: 60 |  Loss: (0.2759) |  Loss2: (0.0000) | Acc: (90.00%) (7050/7808)
Epoch: 74 | Batch_idx: 70 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (8207/9088)
Epoch: 74 | Batch_idx: 80 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (9366/10368)
Epoch: 74 | Batch_idx: 90 |  Loss: (0.2749) |  Loss2: (0.0000) | Acc: (90.00%) (10526/11648)
Epoch: 74 | Batch_idx: 100 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (11669/12928)
Epoch: 74 | Batch_idx: 110 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (12812/14208)
Epoch: 74 | Batch_idx: 120 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (13958/15488)
Epoch: 74 | Batch_idx: 130 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (15130/16768)
Epoch: 74 | Batch_idx: 140 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (16279/18048)
Epoch: 74 | Batch_idx: 150 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (17431/19328)
Epoch: 74 | Batch_idx: 160 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (18593/20608)
Epoch: 74 | Batch_idx: 170 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (19737/21888)
Epoch: 74 | Batch_idx: 180 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (20864/23168)
Epoch: 74 | Batch_idx: 190 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (22027/24448)
Epoch: 74 | Batch_idx: 200 |  Loss: (0.2835) |  Loss2: (0.0000) | Acc: (90.00%) (23171/25728)
Epoch: 74 | Batch_idx: 210 |  Loss: (0.2840) |  Loss2: (0.0000) | Acc: (90.00%) (24322/27008)
Epoch: 74 | Batch_idx: 220 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (25485/28288)
Epoch: 74 | Batch_idx: 230 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (26637/29568)
Epoch: 74 | Batch_idx: 240 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (27772/30848)
Epoch: 74 | Batch_idx: 250 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (28948/32128)
Epoch: 74 | Batch_idx: 260 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (30098/33408)
Epoch: 74 | Batch_idx: 270 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (31225/34688)
Epoch: 74 | Batch_idx: 280 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (32386/35968)
Epoch: 74 | Batch_idx: 290 |  Loss: (0.2854) |  Loss2: (0.0000) | Acc: (90.00%) (33555/37248)
Epoch: 74 | Batch_idx: 300 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (34717/38528)
Epoch: 74 | Batch_idx: 310 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (90.00%) (35866/39808)
Epoch: 74 | Batch_idx: 320 |  Loss: (0.2844) |  Loss2: (0.0000) | Acc: (90.00%) (37021/41088)
Epoch: 74 | Batch_idx: 330 |  Loss: (0.2852) |  Loss2: (0.0000) | Acc: (90.00%) (38167/42368)
Epoch: 74 | Batch_idx: 340 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (39331/43648)
Epoch: 74 | Batch_idx: 350 |  Loss: (0.2847) |  Loss2: (0.0000) | Acc: (90.00%) (40492/44928)
Epoch: 74 | Batch_idx: 360 |  Loss: (0.2856) |  Loss2: (0.0000) | Acc: (90.00%) (41633/46208)
Epoch: 74 | Batch_idx: 370 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (42771/47488)
Epoch: 74 | Batch_idx: 380 |  Loss: (0.2866) |  Loss2: (0.0000) | Acc: (90.00%) (43924/48768)
Epoch: 74 | Batch_idx: 390 |  Loss: (0.2874) |  Loss2: (0.0000) | Acc: (90.00%) (45021/50000)
# TEST : Loss: (0.5420) | Acc: (83.00%) (8302/10000)
percent tensor([0.5695, 0.4305], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.5164, 0.4836], device='cuda:0')
percent tensor([0.6294, 0.3706], device='cuda:0')
percent tensor([0.5811, 0.4189], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.6700, 0.3300], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 75 | Batch_idx: 0 |  Loss: (0.3029) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 75 | Batch_idx: 10 |  Loss: (0.2723) |  Loss2: (0.0000) | Acc: (89.00%) (1259/1408)
Epoch: 75 | Batch_idx: 20 |  Loss: (0.3044) |  Loss2: (0.0000) | Acc: (88.00%) (2382/2688)
Epoch: 75 | Batch_idx: 30 |  Loss: (0.3225) |  Loss2: (0.0000) | Acc: (88.00%) (3494/3968)
Epoch: 75 | Batch_idx: 40 |  Loss: (0.3340) |  Loss2: (0.0000) | Acc: (87.00%) (4603/5248)
Epoch: 75 | Batch_idx: 50 |  Loss: (0.3315) |  Loss2: (0.0000) | Acc: (87.00%) (5738/6528)
Epoch: 75 | Batch_idx: 60 |  Loss: (0.3366) |  Loss2: (0.0000) | Acc: (87.00%) (6862/7808)
Epoch: 75 | Batch_idx: 70 |  Loss: (0.3394) |  Loss2: (0.0000) | Acc: (87.00%) (7988/9088)
Epoch: 75 | Batch_idx: 80 |  Loss: (0.3431) |  Loss2: (0.0000) | Acc: (87.00%) (9100/10368)
Epoch: 75 | Batch_idx: 90 |  Loss: (0.3437) |  Loss2: (0.0000) | Acc: (87.00%) (10220/11648)
Epoch: 75 | Batch_idx: 100 |  Loss: (0.3404) |  Loss2: (0.0000) | Acc: (87.00%) (11362/12928)
Epoch: 75 | Batch_idx: 110 |  Loss: (0.3409) |  Loss2: (0.0000) | Acc: (87.00%) (12485/14208)
Epoch: 75 | Batch_idx: 120 |  Loss: (0.3430) |  Loss2: (0.0000) | Acc: (87.00%) (13605/15488)
Epoch: 75 | Batch_idx: 130 |  Loss: (0.3433) |  Loss2: (0.0000) | Acc: (87.00%) (14722/16768)
Epoch: 75 | Batch_idx: 140 |  Loss: (0.3442) |  Loss2: (0.0000) | Acc: (87.00%) (15850/18048)
Epoch: 75 | Batch_idx: 150 |  Loss: (0.3450) |  Loss2: (0.0000) | Acc: (87.00%) (16977/19328)
Epoch: 75 | Batch_idx: 160 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (18089/20608)
Epoch: 75 | Batch_idx: 170 |  Loss: (0.3479) |  Loss2: (0.0000) | Acc: (87.00%) (19192/21888)
Epoch: 75 | Batch_idx: 180 |  Loss: (0.3478) |  Loss2: (0.0000) | Acc: (87.00%) (20312/23168)
Epoch: 75 | Batch_idx: 190 |  Loss: (0.3467) |  Loss2: (0.0000) | Acc: (87.00%) (21449/24448)
Epoch: 75 | Batch_idx: 200 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (22585/25728)
Epoch: 75 | Batch_idx: 210 |  Loss: (0.3473) |  Loss2: (0.0000) | Acc: (87.00%) (23706/27008)
Epoch: 75 | Batch_idx: 220 |  Loss: (0.3474) |  Loss2: (0.0000) | Acc: (87.00%) (24825/28288)
Epoch: 75 | Batch_idx: 230 |  Loss: (0.3472) |  Loss2: (0.0000) | Acc: (87.00%) (25955/29568)
Epoch: 75 | Batch_idx: 240 |  Loss: (0.3444) |  Loss2: (0.0000) | Acc: (87.00%) (27111/30848)
Epoch: 75 | Batch_idx: 250 |  Loss: (0.3435) |  Loss2: (0.0000) | Acc: (87.00%) (28261/32128)
Epoch: 75 | Batch_idx: 260 |  Loss: (0.3425) |  Loss2: (0.0000) | Acc: (88.00%) (29406/33408)
Epoch: 75 | Batch_idx: 270 |  Loss: (0.3415) |  Loss2: (0.0000) | Acc: (88.00%) (30535/34688)
Epoch: 75 | Batch_idx: 280 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (31677/35968)
Epoch: 75 | Batch_idx: 290 |  Loss: (0.3402) |  Loss2: (0.0000) | Acc: (88.00%) (32820/37248)
Epoch: 75 | Batch_idx: 300 |  Loss: (0.3406) |  Loss2: (0.0000) | Acc: (88.00%) (33945/38528)
Epoch: 75 | Batch_idx: 310 |  Loss: (0.3403) |  Loss2: (0.0000) | Acc: (88.00%) (35081/39808)
Epoch: 75 | Batch_idx: 320 |  Loss: (0.3400) |  Loss2: (0.0000) | Acc: (88.00%) (36209/41088)
Epoch: 75 | Batch_idx: 330 |  Loss: (0.3391) |  Loss2: (0.0000) | Acc: (88.00%) (37345/42368)
Epoch: 75 | Batch_idx: 340 |  Loss: (0.3375) |  Loss2: (0.0000) | Acc: (88.00%) (38501/43648)
Epoch: 75 | Batch_idx: 350 |  Loss: (0.3362) |  Loss2: (0.0000) | Acc: (88.00%) (39649/44928)
Epoch: 75 | Batch_idx: 360 |  Loss: (0.3350) |  Loss2: (0.0000) | Acc: (88.00%) (40813/46208)
Epoch: 75 | Batch_idx: 370 |  Loss: (0.3344) |  Loss2: (0.0000) | Acc: (88.00%) (41958/47488)
Epoch: 75 | Batch_idx: 380 |  Loss: (0.3331) |  Loss2: (0.0000) | Acc: (88.00%) (43113/48768)
Epoch: 75 | Batch_idx: 390 |  Loss: (0.3328) |  Loss2: (0.0000) | Acc: (88.00%) (44193/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_075.pth.tar'
# TEST : Loss: (0.4658) | Acc: (84.00%) (8475/10000)
percent tensor([0.5621, 0.4379], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.5261, 0.4739], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.5682, 0.4318], device='cuda:0')
percent tensor([0.6669, 0.3331], device='cuda:0')
percent tensor([0.6800, 0.3200], device='cuda:0')
percent tensor([0.9990, 0.0010], device='cuda:0')
Epoch: 76 | Batch_idx: 0 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 76 | Batch_idx: 10 |  Loss: (0.3238) |  Loss2: (0.0000) | Acc: (88.00%) (1252/1408)
Epoch: 76 | Batch_idx: 20 |  Loss: (0.3174) |  Loss2: (0.0000) | Acc: (89.00%) (2396/2688)
Epoch: 76 | Batch_idx: 30 |  Loss: (0.3248) |  Loss2: (0.0000) | Acc: (88.00%) (3523/3968)
Epoch: 76 | Batch_idx: 40 |  Loss: (0.3222) |  Loss2: (0.0000) | Acc: (88.00%) (4664/5248)
Epoch: 76 | Batch_idx: 50 |  Loss: (0.3180) |  Loss2: (0.0000) | Acc: (89.00%) (5821/6528)
Epoch: 76 | Batch_idx: 60 |  Loss: (0.3189) |  Loss2: (0.0000) | Acc: (89.00%) (6967/7808)
Epoch: 76 | Batch_idx: 70 |  Loss: (0.3193) |  Loss2: (0.0000) | Acc: (89.00%) (8114/9088)
Epoch: 76 | Batch_idx: 80 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (9257/10368)
Epoch: 76 | Batch_idx: 90 |  Loss: (0.3177) |  Loss2: (0.0000) | Acc: (89.00%) (10393/11648)
Epoch: 76 | Batch_idx: 100 |  Loss: (0.3164) |  Loss2: (0.0000) | Acc: (89.00%) (11527/12928)
Epoch: 76 | Batch_idx: 110 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (12669/14208)
Epoch: 76 | Batch_idx: 120 |  Loss: (0.3184) |  Loss2: (0.0000) | Acc: (89.00%) (13800/15488)
Epoch: 76 | Batch_idx: 130 |  Loss: (0.3192) |  Loss2: (0.0000) | Acc: (89.00%) (14936/16768)
Epoch: 76 | Batch_idx: 140 |  Loss: (0.3182) |  Loss2: (0.0000) | Acc: (89.00%) (16090/18048)
Epoch: 76 | Batch_idx: 150 |  Loss: (0.3167) |  Loss2: (0.0000) | Acc: (89.00%) (17243/19328)
Epoch: 76 | Batch_idx: 160 |  Loss: (0.3173) |  Loss2: (0.0000) | Acc: (89.00%) (18373/20608)
Epoch: 76 | Batch_idx: 170 |  Loss: (0.3169) |  Loss2: (0.0000) | Acc: (89.00%) (19516/21888)
Epoch: 76 | Batch_idx: 180 |  Loss: (0.3152) |  Loss2: (0.0000) | Acc: (89.00%) (20660/23168)
Epoch: 76 | Batch_idx: 190 |  Loss: (0.3148) |  Loss2: (0.0000) | Acc: (89.00%) (21794/24448)
Epoch: 76 | Batch_idx: 200 |  Loss: (0.3156) |  Loss2: (0.0000) | Acc: (89.00%) (22938/25728)
Epoch: 76 | Batch_idx: 210 |  Loss: (0.3151) |  Loss2: (0.0000) | Acc: (89.00%) (24094/27008)
Epoch: 76 | Batch_idx: 220 |  Loss: (0.3141) |  Loss2: (0.0000) | Acc: (89.00%) (25241/28288)
Epoch: 76 | Batch_idx: 230 |  Loss: (0.3137) |  Loss2: (0.0000) | Acc: (89.00%) (26380/29568)
Epoch: 76 | Batch_idx: 240 |  Loss: (0.3133) |  Loss2: (0.0000) | Acc: (89.00%) (27522/30848)
Epoch: 76 | Batch_idx: 250 |  Loss: (0.3112) |  Loss2: (0.0000) | Acc: (89.00%) (28686/32128)
Epoch: 76 | Batch_idx: 260 |  Loss: (0.3109) |  Loss2: (0.0000) | Acc: (89.00%) (29831/33408)
Epoch: 76 | Batch_idx: 270 |  Loss: (0.3102) |  Loss2: (0.0000) | Acc: (89.00%) (30983/34688)
Epoch: 76 | Batch_idx: 280 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (32143/35968)
Epoch: 76 | Batch_idx: 290 |  Loss: (0.3100) |  Loss2: (0.0000) | Acc: (89.00%) (33282/37248)
Epoch: 76 | Batch_idx: 300 |  Loss: (0.3096) |  Loss2: (0.0000) | Acc: (89.00%) (34434/38528)
Epoch: 76 | Batch_idx: 310 |  Loss: (0.3095) |  Loss2: (0.0000) | Acc: (89.00%) (35568/39808)
Epoch: 76 | Batch_idx: 320 |  Loss: (0.3091) |  Loss2: (0.0000) | Acc: (89.00%) (36720/41088)
Epoch: 76 | Batch_idx: 330 |  Loss: (0.3089) |  Loss2: (0.0000) | Acc: (89.00%) (37874/42368)
Epoch: 76 | Batch_idx: 340 |  Loss: (0.3080) |  Loss2: (0.0000) | Acc: (89.00%) (39023/43648)
Epoch: 76 | Batch_idx: 350 |  Loss: (0.3068) |  Loss2: (0.0000) | Acc: (89.00%) (40174/44928)
Epoch: 76 | Batch_idx: 360 |  Loss: (0.3062) |  Loss2: (0.0000) | Acc: (89.00%) (41325/46208)
Epoch: 76 | Batch_idx: 370 |  Loss: (0.3056) |  Loss2: (0.0000) | Acc: (89.00%) (42475/47488)
Epoch: 76 | Batch_idx: 380 |  Loss: (0.3051) |  Loss2: (0.0000) | Acc: (89.00%) (43629/48768)
Epoch: 76 | Batch_idx: 390 |  Loss: (0.3045) |  Loss2: (0.0000) | Acc: (89.00%) (44746/50000)
# TEST : Loss: (0.4439) | Acc: (85.00%) (8539/10000)
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5511, 0.4489], device='cuda:0')
percent tensor([0.5294, 0.4706], device='cuda:0')
percent tensor([0.6036, 0.3964], device='cuda:0')
percent tensor([0.5726, 0.4274], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.6864, 0.3136], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 77 | Batch_idx: 0 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 77 | Batch_idx: 10 |  Loss: (0.3153) |  Loss2: (0.0000) | Acc: (89.00%) (1256/1408)
Epoch: 77 | Batch_idx: 20 |  Loss: (0.3063) |  Loss2: (0.0000) | Acc: (89.00%) (2399/2688)
Epoch: 77 | Batch_idx: 30 |  Loss: (0.3033) |  Loss2: (0.0000) | Acc: (89.00%) (3542/3968)
Epoch: 77 | Batch_idx: 40 |  Loss: (0.3007) |  Loss2: (0.0000) | Acc: (89.00%) (4683/5248)
Epoch: 77 | Batch_idx: 50 |  Loss: (0.2991) |  Loss2: (0.0000) | Acc: (89.00%) (5833/6528)
Epoch: 77 | Batch_idx: 60 |  Loss: (0.2986) |  Loss2: (0.0000) | Acc: (89.00%) (6996/7808)
Epoch: 77 | Batch_idx: 70 |  Loss: (0.2916) |  Loss2: (0.0000) | Acc: (89.00%) (8162/9088)
Epoch: 77 | Batch_idx: 80 |  Loss: (0.2939) |  Loss2: (0.0000) | Acc: (89.00%) (9306/10368)
Epoch: 77 | Batch_idx: 90 |  Loss: (0.2932) |  Loss2: (0.0000) | Acc: (89.00%) (10461/11648)
Epoch: 77 | Batch_idx: 100 |  Loss: (0.2944) |  Loss2: (0.0000) | Acc: (89.00%) (11618/12928)
Epoch: 77 | Batch_idx: 110 |  Loss: (0.2940) |  Loss2: (0.0000) | Acc: (89.00%) (12765/14208)
Epoch: 77 | Batch_idx: 120 |  Loss: (0.2930) |  Loss2: (0.0000) | Acc: (89.00%) (13933/15488)
Epoch: 77 | Batch_idx: 130 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (89.00%) (15085/16768)
Epoch: 77 | Batch_idx: 140 |  Loss: (0.2924) |  Loss2: (0.0000) | Acc: (89.00%) (16241/18048)
Epoch: 77 | Batch_idx: 150 |  Loss: (0.2913) |  Loss2: (0.0000) | Acc: (89.00%) (17395/19328)
Epoch: 77 | Batch_idx: 160 |  Loss: (0.2914) |  Loss2: (0.0000) | Acc: (90.00%) (18551/20608)
Epoch: 77 | Batch_idx: 170 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (19706/21888)
Epoch: 77 | Batch_idx: 180 |  Loss: (0.2934) |  Loss2: (0.0000) | Acc: (89.00%) (20848/23168)
Epoch: 77 | Batch_idx: 190 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (22009/24448)
Epoch: 77 | Batch_idx: 200 |  Loss: (0.2909) |  Loss2: (0.0000) | Acc: (90.00%) (23183/25728)
Epoch: 77 | Batch_idx: 210 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (90.00%) (24343/27008)
Epoch: 77 | Batch_idx: 220 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (25483/28288)
Epoch: 77 | Batch_idx: 230 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (26624/29568)
Epoch: 77 | Batch_idx: 240 |  Loss: (0.2923) |  Loss2: (0.0000) | Acc: (90.00%) (27781/30848)
Epoch: 77 | Batch_idx: 250 |  Loss: (0.2927) |  Loss2: (0.0000) | Acc: (90.00%) (28924/32128)
Epoch: 77 | Batch_idx: 260 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (90.00%) (30071/33408)
Epoch: 77 | Batch_idx: 270 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (31226/34688)
Epoch: 77 | Batch_idx: 280 |  Loss: (0.2912) |  Loss2: (0.0000) | Acc: (90.00%) (32385/35968)
Epoch: 77 | Batch_idx: 290 |  Loss: (0.2900) |  Loss2: (0.0000) | Acc: (90.00%) (33556/37248)
Epoch: 77 | Batch_idx: 300 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (90.00%) (34710/38528)
Epoch: 77 | Batch_idx: 310 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (35863/39808)
Epoch: 77 | Batch_idx: 320 |  Loss: (0.2892) |  Loss2: (0.0000) | Acc: (90.00%) (37019/41088)
Epoch: 77 | Batch_idx: 330 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (38178/42368)
Epoch: 77 | Batch_idx: 340 |  Loss: (0.2895) |  Loss2: (0.0000) | Acc: (90.00%) (39314/43648)
Epoch: 77 | Batch_idx: 350 |  Loss: (0.2902) |  Loss2: (0.0000) | Acc: (90.00%) (40458/44928)
Epoch: 77 | Batch_idx: 360 |  Loss: (0.2896) |  Loss2: (0.0000) | Acc: (90.00%) (41623/46208)
Epoch: 77 | Batch_idx: 370 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (42776/47488)
Epoch: 77 | Batch_idx: 380 |  Loss: (0.2899) |  Loss2: (0.0000) | Acc: (90.00%) (43928/48768)
Epoch: 77 | Batch_idx: 390 |  Loss: (0.2891) |  Loss2: (0.0000) | Acc: (90.00%) (45047/50000)
# TEST : Loss: (0.4302) | Acc: (85.00%) (8561/10000)
percent tensor([0.5622, 0.4378], device='cuda:0')
percent tensor([0.5517, 0.4483], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6040, 0.3960], device='cuda:0')
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.6762, 0.3238], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 78 | Batch_idx: 0 |  Loss: (0.4019) |  Loss2: (0.0000) | Acc: (82.00%) (105/128)
Epoch: 78 | Batch_idx: 10 |  Loss: (0.3082) |  Loss2: (0.0000) | Acc: (89.00%) (1266/1408)
Epoch: 78 | Batch_idx: 20 |  Loss: (0.2921) |  Loss2: (0.0000) | Acc: (90.00%) (2435/2688)
Epoch: 78 | Batch_idx: 30 |  Loss: (0.2830) |  Loss2: (0.0000) | Acc: (90.00%) (3604/3968)
Epoch: 78 | Batch_idx: 40 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (90.00%) (4755/5248)
Epoch: 78 | Batch_idx: 50 |  Loss: (0.2862) |  Loss2: (0.0000) | Acc: (90.00%) (5924/6528)
Epoch: 78 | Batch_idx: 60 |  Loss: (0.2816) |  Loss2: (0.0000) | Acc: (90.00%) (7083/7808)
Epoch: 78 | Batch_idx: 70 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (8246/9088)
Epoch: 78 | Batch_idx: 80 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (9419/10368)
Epoch: 78 | Batch_idx: 90 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (10579/11648)
Epoch: 78 | Batch_idx: 100 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (11720/12928)
Epoch: 78 | Batch_idx: 110 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (12877/14208)
Epoch: 78 | Batch_idx: 120 |  Loss: (0.2815) |  Loss2: (0.0000) | Acc: (90.00%) (14048/15488)
Epoch: 78 | Batch_idx: 130 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (15201/16768)
Epoch: 78 | Batch_idx: 140 |  Loss: (0.2794) |  Loss2: (0.0000) | Acc: (90.00%) (16375/18048)
Epoch: 78 | Batch_idx: 150 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (17539/19328)
Epoch: 78 | Batch_idx: 160 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (18702/20608)
Epoch: 78 | Batch_idx: 170 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (19876/21888)
Epoch: 78 | Batch_idx: 180 |  Loss: (0.2773) |  Loss2: (0.0000) | Acc: (90.00%) (21029/23168)
Epoch: 78 | Batch_idx: 190 |  Loss: (0.2758) |  Loss2: (0.0000) | Acc: (90.00%) (22204/24448)
Epoch: 78 | Batch_idx: 200 |  Loss: (0.2764) |  Loss2: (0.0000) | Acc: (90.00%) (23367/25728)
Epoch: 78 | Batch_idx: 210 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (24519/27008)
Epoch: 78 | Batch_idx: 220 |  Loss: (0.2772) |  Loss2: (0.0000) | Acc: (90.00%) (25675/28288)
Epoch: 78 | Batch_idx: 230 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (26828/29568)
Epoch: 78 | Batch_idx: 240 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (27987/30848)
Epoch: 78 | Batch_idx: 250 |  Loss: (0.2784) |  Loss2: (0.0000) | Acc: (90.00%) (29146/32128)
Epoch: 78 | Batch_idx: 260 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (30306/33408)
Epoch: 78 | Batch_idx: 270 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (31460/34688)
Epoch: 78 | Batch_idx: 280 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (32615/35968)
Epoch: 78 | Batch_idx: 290 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (33770/37248)
Epoch: 78 | Batch_idx: 300 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (34920/38528)
Epoch: 78 | Batch_idx: 310 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (36064/39808)
Epoch: 78 | Batch_idx: 320 |  Loss: (0.2800) |  Loss2: (0.0000) | Acc: (90.00%) (37210/41088)
Epoch: 78 | Batch_idx: 330 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (38376/42368)
Epoch: 78 | Batch_idx: 340 |  Loss: (0.2798) |  Loss2: (0.0000) | Acc: (90.00%) (39527/43648)
Epoch: 78 | Batch_idx: 350 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (40696/44928)
Epoch: 78 | Batch_idx: 360 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (41854/46208)
Epoch: 78 | Batch_idx: 370 |  Loss: (0.2809) |  Loss2: (0.0000) | Acc: (90.00%) (42991/47488)
Epoch: 78 | Batch_idx: 380 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (44146/48768)
Epoch: 78 | Batch_idx: 390 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (45263/50000)
# TEST : Loss: (0.4183) | Acc: (85.00%) (8597/10000)
percent tensor([0.5663, 0.4337], device='cuda:0')
percent tensor([0.5503, 0.4497], device='cuda:0')
percent tensor([0.5311, 0.4689], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5808, 0.4192], device='cuda:0')
percent tensor([0.6826, 0.3174], device='cuda:0')
percent tensor([0.6960, 0.3040], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 79 | Batch_idx: 0 |  Loss: (0.3578) |  Loss2: (0.0000) | Acc: (87.00%) (112/128)
Epoch: 79 | Batch_idx: 10 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 79 | Batch_idx: 20 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 79 | Batch_idx: 30 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (3603/3968)
Epoch: 79 | Batch_idx: 40 |  Loss: (0.2761) |  Loss2: (0.0000) | Acc: (90.00%) (4760/5248)
Epoch: 79 | Batch_idx: 50 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (5909/6528)
Epoch: 79 | Batch_idx: 60 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (7067/7808)
Epoch: 79 | Batch_idx: 70 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (8213/9088)
Epoch: 79 | Batch_idx: 80 |  Loss: (0.2769) |  Loss2: (0.0000) | Acc: (90.00%) (9378/10368)
Epoch: 79 | Batch_idx: 90 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (10534/11648)
Epoch: 79 | Batch_idx: 100 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (11688/12928)
Epoch: 79 | Batch_idx: 110 |  Loss: (0.2785) |  Loss2: (0.0000) | Acc: (90.00%) (12838/14208)
Epoch: 79 | Batch_idx: 120 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (13995/15488)
Epoch: 79 | Batch_idx: 130 |  Loss: (0.2791) |  Loss2: (0.0000) | Acc: (90.00%) (15163/16768)
Epoch: 79 | Batch_idx: 140 |  Loss: (0.2814) |  Loss2: (0.0000) | Acc: (90.00%) (16309/18048)
Epoch: 79 | Batch_idx: 150 |  Loss: (0.2820) |  Loss2: (0.0000) | Acc: (90.00%) (17461/19328)
Epoch: 79 | Batch_idx: 160 |  Loss: (0.2812) |  Loss2: (0.0000) | Acc: (90.00%) (18620/20608)
Epoch: 79 | Batch_idx: 170 |  Loss: (0.2817) |  Loss2: (0.0000) | Acc: (90.00%) (19762/21888)
Epoch: 79 | Batch_idx: 180 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (20929/23168)
Epoch: 79 | Batch_idx: 190 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (22095/24448)
Epoch: 79 | Batch_idx: 200 |  Loss: (0.2779) |  Loss2: (0.0000) | Acc: (90.00%) (23263/25728)
Epoch: 79 | Batch_idx: 210 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (24432/27008)
Epoch: 79 | Batch_idx: 220 |  Loss: (0.2781) |  Loss2: (0.0000) | Acc: (90.00%) (25587/28288)
Epoch: 79 | Batch_idx: 230 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (26745/29568)
Epoch: 79 | Batch_idx: 240 |  Loss: (0.2786) |  Loss2: (0.0000) | Acc: (90.00%) (27896/30848)
Epoch: 79 | Batch_idx: 250 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (29063/32128)
Epoch: 79 | Batch_idx: 260 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (90.00%) (30229/33408)
Epoch: 79 | Batch_idx: 270 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (31408/34688)
Epoch: 79 | Batch_idx: 280 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (32579/35968)
Epoch: 79 | Batch_idx: 290 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (33746/37248)
Epoch: 79 | Batch_idx: 300 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (34907/38528)
Epoch: 79 | Batch_idx: 310 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (36062/39808)
Epoch: 79 | Batch_idx: 320 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (37238/41088)
Epoch: 79 | Batch_idx: 330 |  Loss: (0.2736) |  Loss2: (0.0000) | Acc: (90.00%) (38403/42368)
Epoch: 79 | Batch_idx: 340 |  Loss: (0.2730) |  Loss2: (0.0000) | Acc: (90.00%) (39579/43648)
Epoch: 79 | Batch_idx: 350 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (40724/44928)
Epoch: 79 | Batch_idx: 360 |  Loss: (0.2733) |  Loss2: (0.0000) | Acc: (90.00%) (41890/46208)
Epoch: 79 | Batch_idx: 370 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (43065/47488)
Epoch: 79 | Batch_idx: 380 |  Loss: (0.2728) |  Loss2: (0.0000) | Acc: (90.00%) (44213/48768)
Epoch: 79 | Batch_idx: 390 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (45335/50000)
# TEST : Loss: (0.4191) | Acc: (85.00%) (8580/10000)
percent tensor([0.5642, 0.4358], device='cuda:0')
percent tensor([0.5501, 0.4499], device='cuda:0')
percent tensor([0.5309, 0.4691], device='cuda:0')
percent tensor([0.6068, 0.3932], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.6925, 0.3075], device='cuda:0')
percent tensor([0.7018, 0.2982], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 80 | Batch_idx: 0 |  Loss: (0.2400) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 80 | Batch_idx: 10 |  Loss: (0.3064) |  Loss2: (0.0000) | Acc: (89.00%) (1263/1408)
Epoch: 80 | Batch_idx: 20 |  Loss: (0.2871) |  Loss2: (0.0000) | Acc: (90.00%) (2432/2688)
Epoch: 80 | Batch_idx: 30 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (3582/3968)
Epoch: 80 | Batch_idx: 40 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (90.00%) (4739/5248)
Epoch: 80 | Batch_idx: 50 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (5906/6528)
Epoch: 80 | Batch_idx: 60 |  Loss: (0.2801) |  Loss2: (0.0000) | Acc: (90.00%) (7077/7808)
Epoch: 80 | Batch_idx: 70 |  Loss: (0.2799) |  Loss2: (0.0000) | Acc: (90.00%) (8234/9088)
Epoch: 80 | Batch_idx: 80 |  Loss: (0.2768) |  Loss2: (0.0000) | Acc: (90.00%) (9403/10368)
Epoch: 80 | Batch_idx: 90 |  Loss: (0.2762) |  Loss2: (0.0000) | Acc: (90.00%) (10574/11648)
Epoch: 80 | Batch_idx: 100 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (11742/12928)
Epoch: 80 | Batch_idx: 110 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (12902/14208)
Epoch: 80 | Batch_idx: 120 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (14061/15488)
Epoch: 80 | Batch_idx: 130 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (15205/16768)
Epoch: 80 | Batch_idx: 140 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (16373/18048)
Epoch: 80 | Batch_idx: 150 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (17528/19328)
Epoch: 80 | Batch_idx: 160 |  Loss: (0.2756) |  Loss2: (0.0000) | Acc: (90.00%) (18684/20608)
Epoch: 80 | Batch_idx: 170 |  Loss: (0.2778) |  Loss2: (0.0000) | Acc: (90.00%) (19821/21888)
Epoch: 80 | Batch_idx: 180 |  Loss: (0.2782) |  Loss2: (0.0000) | Acc: (90.00%) (20978/23168)
Epoch: 80 | Batch_idx: 190 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (22150/24448)
Epoch: 80 | Batch_idx: 200 |  Loss: (0.2776) |  Loss2: (0.0000) | Acc: (90.00%) (23306/25728)
Epoch: 80 | Batch_idx: 210 |  Loss: (0.2783) |  Loss2: (0.0000) | Acc: (90.00%) (24448/27008)
Epoch: 80 | Batch_idx: 220 |  Loss: (0.2787) |  Loss2: (0.0000) | Acc: (90.00%) (25607/28288)
Epoch: 80 | Batch_idx: 230 |  Loss: (0.2780) |  Loss2: (0.0000) | Acc: (90.00%) (26765/29568)
Epoch: 80 | Batch_idx: 240 |  Loss: (0.2767) |  Loss2: (0.0000) | Acc: (90.00%) (27946/30848)
Epoch: 80 | Batch_idx: 250 |  Loss: (0.2765) |  Loss2: (0.0000) | Acc: (90.00%) (29101/32128)
Epoch: 80 | Batch_idx: 260 |  Loss: (0.2777) |  Loss2: (0.0000) | Acc: (90.00%) (30242/33408)
Epoch: 80 | Batch_idx: 270 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (31383/34688)
Epoch: 80 | Batch_idx: 280 |  Loss: (0.2793) |  Loss2: (0.0000) | Acc: (90.00%) (32561/35968)
Epoch: 80 | Batch_idx: 290 |  Loss: (0.2792) |  Loss2: (0.0000) | Acc: (90.00%) (33718/37248)
Epoch: 80 | Batch_idx: 300 |  Loss: (0.2790) |  Loss2: (0.0000) | Acc: (90.00%) (34886/38528)
Epoch: 80 | Batch_idx: 310 |  Loss: (0.2795) |  Loss2: (0.0000) | Acc: (90.00%) (36028/39808)
Epoch: 80 | Batch_idx: 320 |  Loss: (0.2807) |  Loss2: (0.0000) | Acc: (90.00%) (37160/41088)
Epoch: 80 | Batch_idx: 330 |  Loss: (0.2821) |  Loss2: (0.0000) | Acc: (90.00%) (38300/42368)
Epoch: 80 | Batch_idx: 340 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (39455/43648)
Epoch: 80 | Batch_idx: 350 |  Loss: (0.2818) |  Loss2: (0.0000) | Acc: (90.00%) (40627/44928)
Epoch: 80 | Batch_idx: 360 |  Loss: (0.2825) |  Loss2: (0.0000) | Acc: (90.00%) (41766/46208)
Epoch: 80 | Batch_idx: 370 |  Loss: (0.2829) |  Loss2: (0.0000) | Acc: (90.00%) (42906/47488)
Epoch: 80 | Batch_idx: 380 |  Loss: (0.2827) |  Loss2: (0.0000) | Acc: (90.00%) (44067/48768)
Epoch: 80 | Batch_idx: 390 |  Loss: (0.2828) |  Loss2: (0.0000) | Acc: (90.00%) (45172/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_080.pth.tar'
# TEST : Loss: (0.4911) | Acc: (84.00%) (8489/10000)
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5306, 0.4694], device='cuda:0')
percent tensor([0.6052, 0.3948], device='cuda:0')
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.6911, 0.3089], device='cuda:0')
percent tensor([0.7061, 0.2939], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(177.8474, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(809.2537, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(804.6404, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1525.8732, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(496.9120, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2228.3899, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4288.6484, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1403.5630, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6129.6523, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11942.1406, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3962.1802, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16775.2070, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 81 | Batch_idx: 0 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 81 | Batch_idx: 10 |  Loss: (0.2688) |  Loss2: (0.0000) | Acc: (91.00%) (1283/1408)
Epoch: 81 | Batch_idx: 20 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 81 | Batch_idx: 30 |  Loss: (0.2633) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 81 | Batch_idx: 40 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (91.00%) (4781/5248)
Epoch: 81 | Batch_idx: 50 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (5938/6528)
Epoch: 81 | Batch_idx: 60 |  Loss: (0.2652) |  Loss2: (0.0000) | Acc: (90.00%) (7096/7808)
Epoch: 81 | Batch_idx: 70 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (90.00%) (8252/9088)
Epoch: 81 | Batch_idx: 80 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (9422/10368)
Epoch: 81 | Batch_idx: 90 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (10571/11648)
Epoch: 81 | Batch_idx: 100 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (11733/12928)
Epoch: 81 | Batch_idx: 110 |  Loss: (0.2740) |  Loss2: (0.0000) | Acc: (90.00%) (12897/14208)
Epoch: 81 | Batch_idx: 120 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (14059/15488)
Epoch: 81 | Batch_idx: 130 |  Loss: (0.2720) |  Loss2: (0.0000) | Acc: (90.00%) (15234/16768)
Epoch: 81 | Batch_idx: 140 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (16402/18048)
Epoch: 81 | Batch_idx: 150 |  Loss: (0.2729) |  Loss2: (0.0000) | Acc: (90.00%) (17573/19328)
Epoch: 81 | Batch_idx: 160 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (18729/20608)
Epoch: 81 | Batch_idx: 170 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (19881/21888)
Epoch: 81 | Batch_idx: 180 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (21054/23168)
Epoch: 81 | Batch_idx: 190 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (22210/24448)
Epoch: 81 | Batch_idx: 200 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (23368/25728)
Epoch: 81 | Batch_idx: 210 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (24533/27008)
Epoch: 81 | Batch_idx: 220 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (25688/28288)
Epoch: 81 | Batch_idx: 230 |  Loss: (0.2711) |  Loss2: (0.0000) | Acc: (90.00%) (26861/29568)
Epoch: 81 | Batch_idx: 240 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (28006/30848)
Epoch: 81 | Batch_idx: 250 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (29175/32128)
Epoch: 81 | Batch_idx: 260 |  Loss: (0.2719) |  Loss2: (0.0000) | Acc: (90.00%) (30342/33408)
Epoch: 81 | Batch_idx: 270 |  Loss: (0.2718) |  Loss2: (0.0000) | Acc: (90.00%) (31503/34688)
Epoch: 81 | Batch_idx: 280 |  Loss: (0.2715) |  Loss2: (0.0000) | Acc: (90.00%) (32656/35968)
Epoch: 81 | Batch_idx: 290 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (33799/37248)
Epoch: 81 | Batch_idx: 300 |  Loss: (0.2724) |  Loss2: (0.0000) | Acc: (90.00%) (34949/38528)
Epoch: 81 | Batch_idx: 310 |  Loss: (0.2717) |  Loss2: (0.0000) | Acc: (90.00%) (36115/39808)
Epoch: 81 | Batch_idx: 320 |  Loss: (0.2722) |  Loss2: (0.0000) | Acc: (90.00%) (37270/41088)
Epoch: 81 | Batch_idx: 330 |  Loss: (0.2731) |  Loss2: (0.0000) | Acc: (90.00%) (38413/42368)
Epoch: 81 | Batch_idx: 340 |  Loss: (0.2744) |  Loss2: (0.0000) | Acc: (90.00%) (39551/43648)
Epoch: 81 | Batch_idx: 350 |  Loss: (0.2742) |  Loss2: (0.0000) | Acc: (90.00%) (40719/44928)
Epoch: 81 | Batch_idx: 360 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (41894/46208)
Epoch: 81 | Batch_idx: 370 |  Loss: (0.2741) |  Loss2: (0.0000) | Acc: (90.00%) (43052/47488)
Epoch: 81 | Batch_idx: 380 |  Loss: (0.2746) |  Loss2: (0.0000) | Acc: (90.00%) (44202/48768)
Epoch: 81 | Batch_idx: 390 |  Loss: (0.2748) |  Loss2: (0.0000) | Acc: (90.00%) (45329/50000)
# TEST : Loss: (0.5028) | Acc: (84.00%) (8462/10000)
percent tensor([0.5644, 0.4356], device='cuda:0')
percent tensor([0.5506, 0.4494], device='cuda:0')
percent tensor([0.5312, 0.4688], device='cuda:0')
percent tensor([0.6067, 0.3933], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6923, 0.3077], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 82 | Batch_idx: 0 |  Loss: (0.2901) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 82 | Batch_idx: 10 |  Loss: (0.2674) |  Loss2: (0.0000) | Acc: (90.00%) (1276/1408)
Epoch: 82 | Batch_idx: 20 |  Loss: (0.2578) |  Loss2: (0.0000) | Acc: (91.00%) (2452/2688)
Epoch: 82 | Batch_idx: 30 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (90.00%) (3610/3968)
Epoch: 82 | Batch_idx: 40 |  Loss: (0.2591) |  Loss2: (0.0000) | Acc: (91.00%) (4779/5248)
Epoch: 82 | Batch_idx: 50 |  Loss: (0.2623) |  Loss2: (0.0000) | Acc: (91.00%) (5941/6528)
Epoch: 82 | Batch_idx: 60 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (90.00%) (7099/7808)
Epoch: 82 | Batch_idx: 70 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (8274/9088)
Epoch: 82 | Batch_idx: 80 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (9441/10368)
Epoch: 82 | Batch_idx: 90 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (10602/11648)
Epoch: 82 | Batch_idx: 100 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (91.00%) (11770/12928)
Epoch: 82 | Batch_idx: 110 |  Loss: (0.2597) |  Loss2: (0.0000) | Acc: (91.00%) (12950/14208)
Epoch: 82 | Batch_idx: 120 |  Loss: (0.2602) |  Loss2: (0.0000) | Acc: (91.00%) (14119/15488)
Epoch: 82 | Batch_idx: 130 |  Loss: (0.2593) |  Loss2: (0.0000) | Acc: (91.00%) (15297/16768)
Epoch: 82 | Batch_idx: 140 |  Loss: (0.2599) |  Loss2: (0.0000) | Acc: (91.00%) (16458/18048)
Epoch: 82 | Batch_idx: 150 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (17621/19328)
Epoch: 82 | Batch_idx: 160 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (91.00%) (18780/20608)
Epoch: 82 | Batch_idx: 170 |  Loss: (0.2631) |  Loss2: (0.0000) | Acc: (91.00%) (19936/21888)
Epoch: 82 | Batch_idx: 180 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (91.00%) (21109/23168)
Epoch: 82 | Batch_idx: 190 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (91.00%) (22267/24448)
Epoch: 82 | Batch_idx: 200 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (23425/25728)
Epoch: 82 | Batch_idx: 210 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (91.00%) (24584/27008)
Epoch: 82 | Batch_idx: 220 |  Loss: (0.2650) |  Loss2: (0.0000) | Acc: (91.00%) (25760/28288)
Epoch: 82 | Batch_idx: 230 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (91.00%) (26933/29568)
Epoch: 82 | Batch_idx: 240 |  Loss: (0.2636) |  Loss2: (0.0000) | Acc: (91.00%) (28103/30848)
Epoch: 82 | Batch_idx: 250 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (29250/32128)
Epoch: 82 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (91.00%) (30416/33408)
Epoch: 82 | Batch_idx: 270 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (91.00%) (31580/34688)
Epoch: 82 | Batch_idx: 280 |  Loss: (0.2642) |  Loss2: (0.0000) | Acc: (91.00%) (32754/35968)
Epoch: 82 | Batch_idx: 290 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (91.00%) (33908/37248)
Epoch: 82 | Batch_idx: 300 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (35068/38528)
Epoch: 82 | Batch_idx: 310 |  Loss: (0.2659) |  Loss2: (0.0000) | Acc: (91.00%) (36234/39808)
Epoch: 82 | Batch_idx: 320 |  Loss: (0.2658) |  Loss2: (0.0000) | Acc: (91.00%) (37408/41088)
Epoch: 82 | Batch_idx: 330 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (91.00%) (38578/42368)
Epoch: 82 | Batch_idx: 340 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (91.00%) (39737/43648)
Epoch: 82 | Batch_idx: 350 |  Loss: (0.2668) |  Loss2: (0.0000) | Acc: (91.00%) (40891/44928)
Epoch: 82 | Batch_idx: 360 |  Loss: (0.2673) |  Loss2: (0.0000) | Acc: (90.00%) (42049/46208)
Epoch: 82 | Batch_idx: 370 |  Loss: (0.2671) |  Loss2: (0.0000) | Acc: (91.00%) (43219/47488)
Epoch: 82 | Batch_idx: 380 |  Loss: (0.2676) |  Loss2: (0.0000) | Acc: (90.00%) (44374/48768)
Epoch: 82 | Batch_idx: 390 |  Loss: (0.2672) |  Loss2: (0.0000) | Acc: (91.00%) (45502/50000)
# TEST : Loss: (0.4351) | Acc: (86.00%) (8608/10000)
percent tensor([0.5651, 0.4349], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.6923, 0.3077], device='cuda:0')
percent tensor([0.7063, 0.2937], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 83 | Batch_idx: 0 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 83 | Batch_idx: 10 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 83 | Batch_idx: 20 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (2476/2688)
Epoch: 83 | Batch_idx: 30 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (92.00%) (3651/3968)
Epoch: 83 | Batch_idx: 40 |  Loss: (0.2389) |  Loss2: (0.0000) | Acc: (92.00%) (4829/5248)
Epoch: 83 | Batch_idx: 50 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (6015/6528)
Epoch: 83 | Batch_idx: 60 |  Loss: (0.2367) |  Loss2: (0.0000) | Acc: (92.00%) (7189/7808)
Epoch: 83 | Batch_idx: 70 |  Loss: (0.2393) |  Loss2: (0.0000) | Acc: (91.00%) (8350/9088)
Epoch: 83 | Batch_idx: 80 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (9498/10368)
Epoch: 83 | Batch_idx: 90 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (10645/11648)
Epoch: 83 | Batch_idx: 100 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (11793/12928)
Epoch: 83 | Batch_idx: 110 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (12959/14208)
Epoch: 83 | Batch_idx: 120 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (14136/15488)
Epoch: 83 | Batch_idx: 130 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (15308/16768)
Epoch: 83 | Batch_idx: 140 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (16484/18048)
Epoch: 83 | Batch_idx: 150 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (17656/19328)
Epoch: 83 | Batch_idx: 160 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (18828/20608)
Epoch: 83 | Batch_idx: 170 |  Loss: (0.2493) |  Loss2: (0.0000) | Acc: (91.00%) (19999/21888)
Epoch: 83 | Batch_idx: 180 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (21155/23168)
Epoch: 83 | Batch_idx: 190 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (22324/24448)
Epoch: 83 | Batch_idx: 200 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (23481/25728)
Epoch: 83 | Batch_idx: 210 |  Loss: (0.2525) |  Loss2: (0.0000) | Acc: (91.00%) (24646/27008)
Epoch: 83 | Batch_idx: 220 |  Loss: (0.2520) |  Loss2: (0.0000) | Acc: (91.00%) (25819/28288)
Epoch: 83 | Batch_idx: 230 |  Loss: (0.2537) |  Loss2: (0.0000) | Acc: (91.00%) (26970/29568)
Epoch: 83 | Batch_idx: 240 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (28122/30848)
Epoch: 83 | Batch_idx: 250 |  Loss: (0.2559) |  Loss2: (0.0000) | Acc: (91.00%) (29290/32128)
Epoch: 83 | Batch_idx: 260 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (30457/33408)
Epoch: 83 | Batch_idx: 270 |  Loss: (0.2567) |  Loss2: (0.0000) | Acc: (91.00%) (31620/34688)
Epoch: 83 | Batch_idx: 280 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (32784/35968)
Epoch: 83 | Batch_idx: 290 |  Loss: (0.2573) |  Loss2: (0.0000) | Acc: (91.00%) (33948/37248)
Epoch: 83 | Batch_idx: 300 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (35118/38528)
Epoch: 83 | Batch_idx: 310 |  Loss: (0.2584) |  Loss2: (0.0000) | Acc: (91.00%) (36266/39808)
Epoch: 83 | Batch_idx: 320 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (91.00%) (37421/41088)
Epoch: 83 | Batch_idx: 330 |  Loss: (0.2600) |  Loss2: (0.0000) | Acc: (91.00%) (38567/42368)
Epoch: 83 | Batch_idx: 340 |  Loss: (0.2607) |  Loss2: (0.0000) | Acc: (91.00%) (39732/43648)
Epoch: 83 | Batch_idx: 350 |  Loss: (0.2603) |  Loss2: (0.0000) | Acc: (91.00%) (40906/44928)
Epoch: 83 | Batch_idx: 360 |  Loss: (0.2604) |  Loss2: (0.0000) | Acc: (91.00%) (42060/46208)
Epoch: 83 | Batch_idx: 370 |  Loss: (0.2609) |  Loss2: (0.0000) | Acc: (91.00%) (43220/47488)
Epoch: 83 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (44394/48768)
Epoch: 83 | Batch_idx: 390 |  Loss: (0.2611) |  Loss2: (0.0000) | Acc: (91.00%) (45514/50000)
# TEST : Loss: (0.4384) | Acc: (86.00%) (8614/10000)
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.5313, 0.4687], device='cuda:0')
percent tensor([0.6051, 0.3949], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.7013, 0.2987], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 84 | Batch_idx: 0 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 84 | Batch_idx: 10 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 84 | Batch_idx: 20 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (92.00%) (2474/2688)
Epoch: 84 | Batch_idx: 30 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 84 | Batch_idx: 40 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (4822/5248)
Epoch: 84 | Batch_idx: 50 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (92.00%) (6011/6528)
Epoch: 84 | Batch_idx: 60 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (92.00%) (7199/7808)
Epoch: 84 | Batch_idx: 70 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (92.00%) (8369/9088)
Epoch: 84 | Batch_idx: 80 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (9538/10368)
Epoch: 84 | Batch_idx: 90 |  Loss: (0.2381) |  Loss2: (0.0000) | Acc: (91.00%) (10707/11648)
Epoch: 84 | Batch_idx: 100 |  Loss: (0.2405) |  Loss2: (0.0000) | Acc: (91.00%) (11862/12928)
Epoch: 84 | Batch_idx: 110 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (13031/14208)
Epoch: 84 | Batch_idx: 120 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (14173/15488)
Epoch: 84 | Batch_idx: 130 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (15354/16768)
Epoch: 84 | Batch_idx: 140 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (16500/18048)
Epoch: 84 | Batch_idx: 150 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (17667/19328)
Epoch: 84 | Batch_idx: 160 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (18810/20608)
Epoch: 84 | Batch_idx: 170 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (19981/21888)
Epoch: 84 | Batch_idx: 180 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (21159/23168)
Epoch: 84 | Batch_idx: 190 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (22343/24448)
Epoch: 84 | Batch_idx: 200 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (23495/25728)
Epoch: 84 | Batch_idx: 210 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (24675/27008)
Epoch: 84 | Batch_idx: 220 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (25852/28288)
Epoch: 84 | Batch_idx: 230 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (27020/29568)
Epoch: 84 | Batch_idx: 240 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (28168/30848)
Epoch: 84 | Batch_idx: 250 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (29337/32128)
Epoch: 84 | Batch_idx: 260 |  Loss: (0.2483) |  Loss2: (0.0000) | Acc: (91.00%) (30503/33408)
Epoch: 84 | Batch_idx: 270 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (31679/34688)
Epoch: 84 | Batch_idx: 280 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (32863/35968)
Epoch: 84 | Batch_idx: 290 |  Loss: (0.2484) |  Loss2: (0.0000) | Acc: (91.00%) (34029/37248)
Epoch: 84 | Batch_idx: 300 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (35206/38528)
Epoch: 84 | Batch_idx: 310 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (36374/39808)
Epoch: 84 | Batch_idx: 320 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (91.00%) (37533/41088)
Epoch: 84 | Batch_idx: 330 |  Loss: (0.2491) |  Loss2: (0.0000) | Acc: (91.00%) (38701/42368)
Epoch: 84 | Batch_idx: 340 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (39866/43648)
Epoch: 84 | Batch_idx: 350 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (41020/44928)
Epoch: 84 | Batch_idx: 360 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (42192/46208)
Epoch: 84 | Batch_idx: 370 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (43364/47488)
Epoch: 84 | Batch_idx: 380 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (44531/48768)
Epoch: 84 | Batch_idx: 390 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (45640/50000)
# TEST : Loss: (0.4252) | Acc: (86.00%) (8652/10000)
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.5507, 0.4493], device='cuda:0')
percent tensor([0.5308, 0.4692], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.5841, 0.4159], device='cuda:0')
percent tensor([0.6910, 0.3090], device='cuda:0')
percent tensor([0.7026, 0.2974], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 85 | Batch_idx: 0 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 85 | Batch_idx: 10 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 85 | Batch_idx: 20 |  Loss: (0.2626) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 85 | Batch_idx: 30 |  Loss: (0.2714) |  Loss2: (0.0000) | Acc: (90.00%) (3594/3968)
Epoch: 85 | Batch_idx: 40 |  Loss: (0.2739) |  Loss2: (0.0000) | Acc: (90.00%) (4744/5248)
Epoch: 85 | Batch_idx: 50 |  Loss: (0.2826) |  Loss2: (0.0000) | Acc: (90.00%) (5892/6528)
Epoch: 85 | Batch_idx: 60 |  Loss: (0.2890) |  Loss2: (0.0000) | Acc: (90.00%) (7032/7808)
Epoch: 85 | Batch_idx: 70 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (8174/9088)
Epoch: 85 | Batch_idx: 80 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (9317/10368)
Epoch: 85 | Batch_idx: 90 |  Loss: (0.2888) |  Loss2: (0.0000) | Acc: (89.00%) (10467/11648)
Epoch: 85 | Batch_idx: 100 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (11602/12928)
Epoch: 85 | Batch_idx: 110 |  Loss: (0.2919) |  Loss2: (0.0000) | Acc: (89.00%) (12742/14208)
Epoch: 85 | Batch_idx: 120 |  Loss: (0.2910) |  Loss2: (0.0000) | Acc: (89.00%) (13892/15488)
Epoch: 85 | Batch_idx: 130 |  Loss: (0.2929) |  Loss2: (0.0000) | Acc: (89.00%) (15033/16768)
Epoch: 85 | Batch_idx: 140 |  Loss: (0.2915) |  Loss2: (0.0000) | Acc: (89.00%) (16187/18048)
Epoch: 85 | Batch_idx: 150 |  Loss: (0.2907) |  Loss2: (0.0000) | Acc: (89.00%) (17346/19328)
Epoch: 85 | Batch_idx: 160 |  Loss: (0.2893) |  Loss2: (0.0000) | Acc: (89.00%) (18505/20608)
Epoch: 85 | Batch_idx: 170 |  Loss: (0.2897) |  Loss2: (0.0000) | Acc: (89.00%) (19657/21888)
Epoch: 85 | Batch_idx: 180 |  Loss: (0.2879) |  Loss2: (0.0000) | Acc: (89.00%) (20825/23168)
Epoch: 85 | Batch_idx: 190 |  Loss: (0.2885) |  Loss2: (0.0000) | Acc: (89.00%) (21971/24448)
Epoch: 85 | Batch_idx: 200 |  Loss: (0.2878) |  Loss2: (0.0000) | Acc: (89.00%) (23124/25728)
Epoch: 85 | Batch_idx: 210 |  Loss: (0.2861) |  Loss2: (0.0000) | Acc: (89.00%) (24289/27008)
Epoch: 85 | Batch_idx: 220 |  Loss: (0.2868) |  Loss2: (0.0000) | Acc: (89.00%) (25436/28288)
Epoch: 85 | Batch_idx: 230 |  Loss: (0.2859) |  Loss2: (0.0000) | Acc: (89.00%) (26589/29568)
Epoch: 85 | Batch_idx: 240 |  Loss: (0.2857) |  Loss2: (0.0000) | Acc: (89.00%) (27751/30848)
Epoch: 85 | Batch_idx: 250 |  Loss: (0.2863) |  Loss2: (0.0000) | Acc: (89.00%) (28906/32128)
Epoch: 85 | Batch_idx: 260 |  Loss: (0.2851) |  Loss2: (0.0000) | Acc: (90.00%) (30080/33408)
Epoch: 85 | Batch_idx: 270 |  Loss: (0.2846) |  Loss2: (0.0000) | Acc: (90.00%) (31237/34688)
Epoch: 85 | Batch_idx: 280 |  Loss: (0.2842) |  Loss2: (0.0000) | Acc: (90.00%) (32392/35968)
Epoch: 85 | Batch_idx: 290 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (33550/37248)
Epoch: 85 | Batch_idx: 300 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (34715/38528)
Epoch: 85 | Batch_idx: 310 |  Loss: (0.2834) |  Loss2: (0.0000) | Acc: (90.00%) (35872/39808)
Epoch: 85 | Batch_idx: 320 |  Loss: (0.2832) |  Loss2: (0.0000) | Acc: (90.00%) (37036/41088)
Epoch: 85 | Batch_idx: 330 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (38177/42368)
Epoch: 85 | Batch_idx: 340 |  Loss: (0.2839) |  Loss2: (0.0000) | Acc: (90.00%) (39331/43648)
Epoch: 85 | Batch_idx: 350 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (40479/44928)
Epoch: 85 | Batch_idx: 360 |  Loss: (0.2841) |  Loss2: (0.0000) | Acc: (90.00%) (41633/46208)
Epoch: 85 | Batch_idx: 370 |  Loss: (0.2836) |  Loss2: (0.0000) | Acc: (90.00%) (42792/47488)
Epoch: 85 | Batch_idx: 380 |  Loss: (0.2837) |  Loss2: (0.0000) | Acc: (90.00%) (43944/48768)
Epoch: 85 | Batch_idx: 390 |  Loss: (0.2833) |  Loss2: (0.0000) | Acc: (90.00%) (45066/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_085.pth.tar'
# TEST : Loss: (0.4409) | Acc: (85.00%) (8576/10000)
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.5487, 0.4513], device='cuda:0')
percent tensor([0.5330, 0.4670], device='cuda:0')
percent tensor([0.6044, 0.3956], device='cuda:0')
percent tensor([0.5705, 0.4295], device='cuda:0')
percent tensor([0.6940, 0.3060], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 86 | Batch_idx: 0 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 86 | Batch_idx: 10 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (91.00%) (1282/1408)
Epoch: 86 | Batch_idx: 20 |  Loss: (0.2635) |  Loss2: (0.0000) | Acc: (90.00%) (2445/2688)
Epoch: 86 | Batch_idx: 30 |  Loss: (0.2727) |  Loss2: (0.0000) | Acc: (90.00%) (3592/3968)
Epoch: 86 | Batch_idx: 40 |  Loss: (0.2713) |  Loss2: (0.0000) | Acc: (90.00%) (4768/5248)
Epoch: 86 | Batch_idx: 50 |  Loss: (0.2734) |  Loss2: (0.0000) | Acc: (90.00%) (5920/6528)
Epoch: 86 | Batch_idx: 60 |  Loss: (0.2696) |  Loss2: (0.0000) | Acc: (90.00%) (7096/7808)
Epoch: 86 | Batch_idx: 70 |  Loss: (0.2687) |  Loss2: (0.0000) | Acc: (90.00%) (8260/9088)
Epoch: 86 | Batch_idx: 80 |  Loss: (0.2706) |  Loss2: (0.0000) | Acc: (90.00%) (9412/10368)
Epoch: 86 | Batch_idx: 90 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (10584/11648)
Epoch: 86 | Batch_idx: 100 |  Loss: (0.2680) |  Loss2: (0.0000) | Acc: (90.00%) (11745/12928)
Epoch: 86 | Batch_idx: 110 |  Loss: (0.2663) |  Loss2: (0.0000) | Acc: (90.00%) (12908/14208)
Epoch: 86 | Batch_idx: 120 |  Loss: (0.2660) |  Loss2: (0.0000) | Acc: (90.00%) (14072/15488)
Epoch: 86 | Batch_idx: 130 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (15236/16768)
Epoch: 86 | Batch_idx: 140 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (16407/18048)
Epoch: 86 | Batch_idx: 150 |  Loss: (0.2628) |  Loss2: (0.0000) | Acc: (90.00%) (17577/19328)
Epoch: 86 | Batch_idx: 160 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (18720/20608)
Epoch: 86 | Batch_idx: 170 |  Loss: (0.2665) |  Loss2: (0.0000) | Acc: (90.00%) (19873/21888)
Epoch: 86 | Batch_idx: 180 |  Loss: (0.2666) |  Loss2: (0.0000) | Acc: (90.00%) (21032/23168)
Epoch: 86 | Batch_idx: 190 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (22209/24448)
Epoch: 86 | Batch_idx: 200 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (23364/25728)
Epoch: 86 | Batch_idx: 210 |  Loss: (0.2667) |  Loss2: (0.0000) | Acc: (90.00%) (24522/27008)
Epoch: 86 | Batch_idx: 220 |  Loss: (0.2664) |  Loss2: (0.0000) | Acc: (90.00%) (25687/28288)
Epoch: 86 | Batch_idx: 230 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (26864/29568)
Epoch: 86 | Batch_idx: 240 |  Loss: (0.2654) |  Loss2: (0.0000) | Acc: (90.00%) (28027/30848)
Epoch: 86 | Batch_idx: 250 |  Loss: (0.2653) |  Loss2: (0.0000) | Acc: (90.00%) (29192/32128)
Epoch: 86 | Batch_idx: 260 |  Loss: (0.2646) |  Loss2: (0.0000) | Acc: (90.00%) (30364/33408)
Epoch: 86 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31536/34688)
Epoch: 86 | Batch_idx: 280 |  Loss: (0.2651) |  Loss2: (0.0000) | Acc: (90.00%) (32702/35968)
Epoch: 86 | Batch_idx: 290 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (33879/37248)
Epoch: 86 | Batch_idx: 300 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (35043/38528)
Epoch: 86 | Batch_idx: 310 |  Loss: (0.2639) |  Loss2: (0.0000) | Acc: (90.00%) (36214/39808)
Epoch: 86 | Batch_idx: 320 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (37368/41088)
Epoch: 86 | Batch_idx: 330 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (91.00%) (38560/42368)
Epoch: 86 | Batch_idx: 340 |  Loss: (0.2625) |  Loss2: (0.0000) | Acc: (91.00%) (39738/43648)
Epoch: 86 | Batch_idx: 350 |  Loss: (0.2617) |  Loss2: (0.0000) | Acc: (91.00%) (40916/44928)
Epoch: 86 | Batch_idx: 360 |  Loss: (0.2612) |  Loss2: (0.0000) | Acc: (91.00%) (42084/46208)
Epoch: 86 | Batch_idx: 370 |  Loss: (0.2613) |  Loss2: (0.0000) | Acc: (91.00%) (43252/47488)
Epoch: 86 | Batch_idx: 380 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (44430/48768)
Epoch: 86 | Batch_idx: 390 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (45557/50000)
# TEST : Loss: (0.4189) | Acc: (86.00%) (8642/10000)
percent tensor([0.5698, 0.4302], device='cuda:0')
percent tensor([0.5499, 0.4501], device='cuda:0')
percent tensor([0.5353, 0.4647], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.6973, 0.3027], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.9991, 0.0009], device='cuda:0')
Epoch: 87 | Batch_idx: 0 |  Loss: (0.2589) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 87 | Batch_idx: 10 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 87 | Batch_idx: 20 |  Loss: (0.2312) |  Loss2: (0.0000) | Acc: (92.00%) (2482/2688)
Epoch: 87 | Batch_idx: 30 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (3635/3968)
Epoch: 87 | Batch_idx: 40 |  Loss: (0.2476) |  Loss2: (0.0000) | Acc: (91.00%) (4806/5248)
Epoch: 87 | Batch_idx: 50 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (5980/6528)
Epoch: 87 | Batch_idx: 60 |  Loss: (0.2499) |  Loss2: (0.0000) | Acc: (91.00%) (7142/7808)
Epoch: 87 | Batch_idx: 70 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (8311/9088)
Epoch: 87 | Batch_idx: 80 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (9491/10368)
Epoch: 87 | Batch_idx: 90 |  Loss: (0.2489) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 87 | Batch_idx: 100 |  Loss: (0.2494) |  Loss2: (0.0000) | Acc: (91.00%) (11824/12928)
Epoch: 87 | Batch_idx: 110 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (13003/14208)
Epoch: 87 | Batch_idx: 120 |  Loss: (0.2509) |  Loss2: (0.0000) | Acc: (91.00%) (14170/15488)
Epoch: 87 | Batch_idx: 130 |  Loss: (0.2505) |  Loss2: (0.0000) | Acc: (91.00%) (15340/16768)
Epoch: 87 | Batch_idx: 140 |  Loss: (0.2490) |  Loss2: (0.0000) | Acc: (91.00%) (16523/18048)
Epoch: 87 | Batch_idx: 150 |  Loss: (0.2508) |  Loss2: (0.0000) | Acc: (91.00%) (17679/19328)
Epoch: 87 | Batch_idx: 160 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (18831/20608)
Epoch: 87 | Batch_idx: 170 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (20001/21888)
Epoch: 87 | Batch_idx: 180 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (21172/23168)
Epoch: 87 | Batch_idx: 190 |  Loss: (0.2552) |  Loss2: (0.0000) | Acc: (91.00%) (22337/24448)
Epoch: 87 | Batch_idx: 200 |  Loss: (0.2551) |  Loss2: (0.0000) | Acc: (91.00%) (23498/25728)
Epoch: 87 | Batch_idx: 210 |  Loss: (0.2546) |  Loss2: (0.0000) | Acc: (91.00%) (24664/27008)
Epoch: 87 | Batch_idx: 220 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (25804/28288)
Epoch: 87 | Batch_idx: 230 |  Loss: (0.2555) |  Loss2: (0.0000) | Acc: (91.00%) (26978/29568)
Epoch: 87 | Batch_idx: 240 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (28134/30848)
Epoch: 87 | Batch_idx: 250 |  Loss: (0.2566) |  Loss2: (0.0000) | Acc: (91.00%) (29298/32128)
Epoch: 87 | Batch_idx: 260 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (30466/33408)
Epoch: 87 | Batch_idx: 270 |  Loss: (0.2561) |  Loss2: (0.0000) | Acc: (91.00%) (31643/34688)
Epoch: 87 | Batch_idx: 280 |  Loss: (0.2572) |  Loss2: (0.0000) | Acc: (91.00%) (32804/35968)
Epoch: 87 | Batch_idx: 290 |  Loss: (0.2568) |  Loss2: (0.0000) | Acc: (91.00%) (33982/37248)
Epoch: 87 | Batch_idx: 300 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (35154/38528)
Epoch: 87 | Batch_idx: 310 |  Loss: (0.2562) |  Loss2: (0.0000) | Acc: (91.00%) (36325/39808)
Epoch: 87 | Batch_idx: 320 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (37491/41088)
Epoch: 87 | Batch_idx: 330 |  Loss: (0.2550) |  Loss2: (0.0000) | Acc: (91.00%) (38676/42368)
Epoch: 87 | Batch_idx: 340 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (39865/43648)
Epoch: 87 | Batch_idx: 350 |  Loss: (0.2534) |  Loss2: (0.0000) | Acc: (91.00%) (41040/44928)
Epoch: 87 | Batch_idx: 360 |  Loss: (0.2528) |  Loss2: (0.0000) | Acc: (91.00%) (42223/46208)
Epoch: 87 | Batch_idx: 370 |  Loss: (0.2527) |  Loss2: (0.0000) | Acc: (91.00%) (43395/47488)
Epoch: 87 | Batch_idx: 380 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (44571/48768)
Epoch: 87 | Batch_idx: 390 |  Loss: (0.2522) |  Loss2: (0.0000) | Acc: (91.00%) (45707/50000)
# TEST : Loss: (0.4114) | Acc: (86.00%) (8677/10000)
percent tensor([0.5681, 0.4319], device='cuda:0')
percent tensor([0.5481, 0.4519], device='cuda:0')
percent tensor([0.5361, 0.4639], device='cuda:0')
percent tensor([0.6103, 0.3897], device='cuda:0')
percent tensor([0.5788, 0.4212], device='cuda:0')
percent tensor([0.6988, 0.3012], device='cuda:0')
percent tensor([0.6743, 0.3257], device='cuda:0')
percent tensor([0.9992, 0.0008], device='cuda:0')
Epoch: 88 | Batch_idx: 0 |  Loss: (0.2216) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 88 | Batch_idx: 10 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (1292/1408)
Epoch: 88 | Batch_idx: 20 |  Loss: (0.2547) |  Loss2: (0.0000) | Acc: (91.00%) (2447/2688)
Epoch: 88 | Batch_idx: 30 |  Loss: (0.2685) |  Loss2: (0.0000) | Acc: (90.00%) (3593/3968)
Epoch: 88 | Batch_idx: 40 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (4765/5248)
Epoch: 88 | Batch_idx: 50 |  Loss: (0.2560) |  Loss2: (0.0000) | Acc: (91.00%) (5945/6528)
Epoch: 88 | Batch_idx: 60 |  Loss: (0.2504) |  Loss2: (0.0000) | Acc: (91.00%) (7130/7808)
Epoch: 88 | Batch_idx: 70 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (8310/9088)
Epoch: 88 | Batch_idx: 80 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (9486/10368)
Epoch: 88 | Batch_idx: 90 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 88 | Batch_idx: 100 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (11812/12928)
Epoch: 88 | Batch_idx: 110 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (12983/14208)
Epoch: 88 | Batch_idx: 120 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (14154/15488)
Epoch: 88 | Batch_idx: 130 |  Loss: (0.2496) |  Loss2: (0.0000) | Acc: (91.00%) (15326/16768)
Epoch: 88 | Batch_idx: 140 |  Loss: (0.2479) |  Loss2: (0.0000) | Acc: (91.00%) (16500/18048)
Epoch: 88 | Batch_idx: 150 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (17687/19328)
Epoch: 88 | Batch_idx: 160 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (18854/20608)
Epoch: 88 | Batch_idx: 170 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (20023/21888)
Epoch: 88 | Batch_idx: 180 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (21195/23168)
Epoch: 88 | Batch_idx: 190 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (91.00%) (22345/24448)
Epoch: 88 | Batch_idx: 200 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (23521/25728)
Epoch: 88 | Batch_idx: 210 |  Loss: (0.2477) |  Loss2: (0.0000) | Acc: (91.00%) (24683/27008)
Epoch: 88 | Batch_idx: 220 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (25850/28288)
Epoch: 88 | Batch_idx: 230 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (27032/29568)
Epoch: 88 | Batch_idx: 240 |  Loss: (0.2471) |  Loss2: (0.0000) | Acc: (91.00%) (28198/30848)
Epoch: 88 | Batch_idx: 250 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (29372/32128)
Epoch: 88 | Batch_idx: 260 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (30566/33408)
Epoch: 88 | Batch_idx: 270 |  Loss: (0.2460) |  Loss2: (0.0000) | Acc: (91.00%) (31740/34688)
Epoch: 88 | Batch_idx: 280 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (32910/35968)
Epoch: 88 | Batch_idx: 290 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (34103/37248)
Epoch: 88 | Batch_idx: 300 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (35283/38528)
Epoch: 88 | Batch_idx: 310 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (36461/39808)
Epoch: 88 | Batch_idx: 320 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (37642/41088)
Epoch: 88 | Batch_idx: 330 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (38807/42368)
Epoch: 88 | Batch_idx: 340 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (39975/43648)
Epoch: 88 | Batch_idx: 350 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (41171/44928)
Epoch: 88 | Batch_idx: 360 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (42343/46208)
Epoch: 88 | Batch_idx: 370 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (43538/47488)
Epoch: 88 | Batch_idx: 380 |  Loss: (0.2421) |  Loss2: (0.0000) | Acc: (91.00%) (44721/48768)
Epoch: 88 | Batch_idx: 390 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (45857/50000)
# TEST : Loss: (0.4067) | Acc: (86.00%) (8683/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5486, 0.4514], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.7042, 0.2958], device='cuda:0')
percent tensor([0.6795, 0.3205], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 89 | Batch_idx: 0 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 89 | Batch_idx: 10 |  Loss: (0.2309) |  Loss2: (0.0000) | Acc: (91.00%) (1294/1408)
Epoch: 89 | Batch_idx: 20 |  Loss: (0.2346) |  Loss2: (0.0000) | Acc: (91.00%) (2461/2688)
Epoch: 89 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (3646/3968)
Epoch: 89 | Batch_idx: 40 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (4803/5248)
Epoch: 89 | Batch_idx: 50 |  Loss: (0.2432) |  Loss2: (0.0000) | Acc: (91.00%) (5963/6528)
Epoch: 89 | Batch_idx: 60 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (7118/7808)
Epoch: 89 | Batch_idx: 70 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (8286/9088)
Epoch: 89 | Batch_idx: 80 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (9463/10368)
Epoch: 89 | Batch_idx: 90 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (10633/11648)
Epoch: 89 | Batch_idx: 100 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (11823/12928)
Epoch: 89 | Batch_idx: 110 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (12976/14208)
Epoch: 89 | Batch_idx: 120 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (14159/15488)
Epoch: 89 | Batch_idx: 130 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (15332/16768)
Epoch: 89 | Batch_idx: 140 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (16515/18048)
Epoch: 89 | Batch_idx: 150 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (17674/19328)
Epoch: 89 | Batch_idx: 160 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (18860/20608)
Epoch: 89 | Batch_idx: 170 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (20032/21888)
Epoch: 89 | Batch_idx: 180 |  Loss: (0.2425) |  Loss2: (0.0000) | Acc: (91.00%) (21206/23168)
Epoch: 89 | Batch_idx: 190 |  Loss: (0.2431) |  Loss2: (0.0000) | Acc: (91.00%) (22375/24448)
Epoch: 89 | Batch_idx: 200 |  Loss: (0.2426) |  Loss2: (0.0000) | Acc: (91.00%) (23547/25728)
Epoch: 89 | Batch_idx: 210 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (24719/27008)
Epoch: 89 | Batch_idx: 220 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (25894/28288)
Epoch: 89 | Batch_idx: 230 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (27041/29568)
Epoch: 89 | Batch_idx: 240 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (28219/30848)
Epoch: 89 | Batch_idx: 250 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (29391/32128)
Epoch: 89 | Batch_idx: 260 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (30557/33408)
Epoch: 89 | Batch_idx: 270 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (31739/34688)
Epoch: 89 | Batch_idx: 280 |  Loss: (0.2446) |  Loss2: (0.0000) | Acc: (91.00%) (32913/35968)
Epoch: 89 | Batch_idx: 290 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (34100/37248)
Epoch: 89 | Batch_idx: 300 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (35270/38528)
Epoch: 89 | Batch_idx: 310 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (36441/39808)
Epoch: 89 | Batch_idx: 320 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (37632/41088)
Epoch: 89 | Batch_idx: 330 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (38832/42368)
Epoch: 89 | Batch_idx: 340 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (40017/43648)
Epoch: 89 | Batch_idx: 350 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (41177/44928)
Epoch: 89 | Batch_idx: 360 |  Loss: (0.2416) |  Loss2: (0.0000) | Acc: (91.00%) (42361/46208)
Epoch: 89 | Batch_idx: 370 |  Loss: (0.2414) |  Loss2: (0.0000) | Acc: (91.00%) (43538/47488)
Epoch: 89 | Batch_idx: 380 |  Loss: (0.2418) |  Loss2: (0.0000) | Acc: (91.00%) (44713/48768)
Epoch: 89 | Batch_idx: 390 |  Loss: (0.2415) |  Loss2: (0.0000) | Acc: (91.00%) (45847/50000)
# TEST : Loss: (0.4027) | Acc: (86.00%) (8687/10000)
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.5488, 0.4512], device='cuda:0')
percent tensor([0.5345, 0.4655], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.7072, 0.2928], device='cuda:0')
percent tensor([0.6846, 0.3154], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 90 | Batch_idx: 0 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 90 | Batch_idx: 10 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 90 | Batch_idx: 20 |  Loss: (0.2498) |  Loss2: (0.0000) | Acc: (91.00%) (2468/2688)
Epoch: 90 | Batch_idx: 30 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (3642/3968)
Epoch: 90 | Batch_idx: 40 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (4823/5248)
Epoch: 90 | Batch_idx: 50 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (5993/6528)
Epoch: 90 | Batch_idx: 60 |  Loss: (0.2463) |  Loss2: (0.0000) | Acc: (91.00%) (7157/7808)
Epoch: 90 | Batch_idx: 70 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 90 | Batch_idx: 80 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (9487/10368)
Epoch: 90 | Batch_idx: 90 |  Loss: (0.2465) |  Loss2: (0.0000) | Acc: (91.00%) (10658/11648)
Epoch: 90 | Batch_idx: 100 |  Loss: (0.2473) |  Loss2: (0.0000) | Acc: (91.00%) (11836/12928)
Epoch: 90 | Batch_idx: 110 |  Loss: (0.2472) |  Loss2: (0.0000) | Acc: (91.00%) (13009/14208)
Epoch: 90 | Batch_idx: 120 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (14182/15488)
Epoch: 90 | Batch_idx: 130 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (15366/16768)
Epoch: 90 | Batch_idx: 140 |  Loss: (0.2443) |  Loss2: (0.0000) | Acc: (91.00%) (16542/18048)
Epoch: 90 | Batch_idx: 150 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (17711/19328)
Epoch: 90 | Batch_idx: 160 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (18859/20608)
Epoch: 90 | Batch_idx: 170 |  Loss: (0.2464) |  Loss2: (0.0000) | Acc: (91.00%) (20039/21888)
Epoch: 90 | Batch_idx: 180 |  Loss: (0.2462) |  Loss2: (0.0000) | Acc: (91.00%) (21212/23168)
Epoch: 90 | Batch_idx: 190 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (22395/24448)
Epoch: 90 | Batch_idx: 200 |  Loss: (0.2467) |  Loss2: (0.0000) | Acc: (91.00%) (23552/25728)
Epoch: 90 | Batch_idx: 210 |  Loss: (0.2454) |  Loss2: (0.0000) | Acc: (91.00%) (24741/27008)
Epoch: 90 | Batch_idx: 220 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (25896/28288)
Epoch: 90 | Batch_idx: 230 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (27066/29568)
Epoch: 90 | Batch_idx: 240 |  Loss: (0.2448) |  Loss2: (0.0000) | Acc: (91.00%) (28257/30848)
Epoch: 90 | Batch_idx: 250 |  Loss: (0.2457) |  Loss2: (0.0000) | Acc: (91.00%) (29412/32128)
Epoch: 90 | Batch_idx: 260 |  Loss: (0.2450) |  Loss2: (0.0000) | Acc: (91.00%) (30587/33408)
Epoch: 90 | Batch_idx: 270 |  Loss: (0.2447) |  Loss2: (0.0000) | Acc: (91.00%) (31767/34688)
Epoch: 90 | Batch_idx: 280 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (32940/35968)
Epoch: 90 | Batch_idx: 290 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (34105/37248)
Epoch: 90 | Batch_idx: 300 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (35285/38528)
Epoch: 90 | Batch_idx: 310 |  Loss: (0.2455) |  Loss2: (0.0000) | Acc: (91.00%) (36446/39808)
Epoch: 90 | Batch_idx: 320 |  Loss: (0.2451) |  Loss2: (0.0000) | Acc: (91.00%) (37625/41088)
Epoch: 90 | Batch_idx: 330 |  Loss: (0.2452) |  Loss2: (0.0000) | Acc: (91.00%) (38798/42368)
Epoch: 90 | Batch_idx: 340 |  Loss: (0.2459) |  Loss2: (0.0000) | Acc: (91.00%) (39963/43648)
Epoch: 90 | Batch_idx: 350 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (41096/44928)
Epoch: 90 | Batch_idx: 360 |  Loss: (0.2474) |  Loss2: (0.0000) | Acc: (91.00%) (42268/46208)
Epoch: 90 | Batch_idx: 370 |  Loss: (0.2481) |  Loss2: (0.0000) | Acc: (91.00%) (43430/47488)
Epoch: 90 | Batch_idx: 380 |  Loss: (0.2487) |  Loss2: (0.0000) | Acc: (91.00%) (44583/48768)
Epoch: 90 | Batch_idx: 390 |  Loss: (0.2485) |  Loss2: (0.0000) | Acc: (91.00%) (45707/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_090.pth.tar'
# TEST : Loss: (0.4854) | Acc: (84.00%) (8496/10000)
percent tensor([0.5744, 0.4256], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.5347, 0.4653], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.5838, 0.4162], device='cuda:0')
percent tensor([0.7088, 0.2912], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(178.5706, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.8528, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(808.0466, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.9700, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(495.3773, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2236.8115, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4284.9639, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1398.5808, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6142.2002, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11906.8779, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3946.8782, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16707.3066, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 91 | Batch_idx: 0 |  Loss: (0.2908) |  Loss2: (0.0000) | Acc: (85.00%) (110/128)
Epoch: 91 | Batch_idx: 10 |  Loss: (0.2391) |  Loss2: (0.0000) | Acc: (91.00%) (1293/1408)
Epoch: 91 | Batch_idx: 20 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (2459/2688)
Epoch: 91 | Batch_idx: 30 |  Loss: (0.2397) |  Loss2: (0.0000) | Acc: (91.00%) (3631/3968)
Epoch: 91 | Batch_idx: 40 |  Loss: (0.2353) |  Loss2: (0.0000) | Acc: (91.00%) (4817/5248)
Epoch: 91 | Batch_idx: 50 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (6000/6528)
Epoch: 91 | Batch_idx: 60 |  Loss: (0.2318) |  Loss2: (0.0000) | Acc: (92.00%) (7188/7808)
Epoch: 91 | Batch_idx: 70 |  Loss: (0.2390) |  Loss2: (0.0000) | Acc: (91.00%) (8355/9088)
Epoch: 91 | Batch_idx: 80 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (9530/10368)
Epoch: 91 | Batch_idx: 90 |  Loss: (0.2410) |  Loss2: (0.0000) | Acc: (91.00%) (10691/11648)
Epoch: 91 | Batch_idx: 100 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (11854/12928)
Epoch: 91 | Batch_idx: 110 |  Loss: (0.2412) |  Loss2: (0.0000) | Acc: (91.00%) (13038/14208)
Epoch: 91 | Batch_idx: 120 |  Loss: (0.2396) |  Loss2: (0.0000) | Acc: (91.00%) (14218/15488)
Epoch: 91 | Batch_idx: 130 |  Loss: (0.2404) |  Loss2: (0.0000) | Acc: (91.00%) (15386/16768)
Epoch: 91 | Batch_idx: 140 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (16535/18048)
Epoch: 91 | Batch_idx: 150 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (17716/19328)
Epoch: 91 | Batch_idx: 160 |  Loss: (0.2411) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 91 | Batch_idx: 170 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (20066/21888)
Epoch: 91 | Batch_idx: 180 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (21227/23168)
Epoch: 91 | Batch_idx: 190 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (22396/24448)
Epoch: 91 | Batch_idx: 200 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (23574/25728)
Epoch: 91 | Batch_idx: 210 |  Loss: (0.2427) |  Loss2: (0.0000) | Acc: (91.00%) (24736/27008)
Epoch: 91 | Batch_idx: 220 |  Loss: (0.2417) |  Loss2: (0.0000) | Acc: (91.00%) (25916/28288)
Epoch: 91 | Batch_idx: 230 |  Loss: (0.2424) |  Loss2: (0.0000) | Acc: (91.00%) (27087/29568)
Epoch: 91 | Batch_idx: 240 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (28258/30848)
Epoch: 91 | Batch_idx: 250 |  Loss: (0.2428) |  Loss2: (0.0000) | Acc: (91.00%) (29429/32128)
Epoch: 91 | Batch_idx: 260 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (30603/33408)
Epoch: 91 | Batch_idx: 270 |  Loss: (0.2420) |  Loss2: (0.0000) | Acc: (91.00%) (31781/34688)
Epoch: 91 | Batch_idx: 280 |  Loss: (0.2419) |  Loss2: (0.0000) | Acc: (91.00%) (32964/35968)
Epoch: 91 | Batch_idx: 290 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (91.00%) (34140/37248)
Epoch: 91 | Batch_idx: 300 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (35299/38528)
Epoch: 91 | Batch_idx: 310 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (36471/39808)
Epoch: 91 | Batch_idx: 320 |  Loss: (0.2435) |  Loss2: (0.0000) | Acc: (91.00%) (37644/41088)
Epoch: 91 | Batch_idx: 330 |  Loss: (0.2434) |  Loss2: (0.0000) | Acc: (91.00%) (38810/42368)
Epoch: 91 | Batch_idx: 340 |  Loss: (0.2433) |  Loss2: (0.0000) | Acc: (91.00%) (39984/43648)
Epoch: 91 | Batch_idx: 350 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (41152/44928)
Epoch: 91 | Batch_idx: 360 |  Loss: (0.2440) |  Loss2: (0.0000) | Acc: (91.00%) (42320/46208)
Epoch: 91 | Batch_idx: 370 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (43476/47488)
Epoch: 91 | Batch_idx: 380 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (44645/48768)
Epoch: 91 | Batch_idx: 390 |  Loss: (0.2444) |  Loss2: (0.0000) | Acc: (91.00%) (45774/50000)
# TEST : Loss: (0.5171) | Acc: (84.00%) (8447/10000)
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.5350, 0.4650], device='cuda:0')
percent tensor([0.6107, 0.3893], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.7065, 0.2935], device='cuda:0')
percent tensor([0.6865, 0.3135], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 92 | Batch_idx: 0 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 92 | Batch_idx: 10 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (1304/1408)
Epoch: 92 | Batch_idx: 20 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (2488/2688)
Epoch: 92 | Batch_idx: 30 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (3660/3968)
Epoch: 92 | Batch_idx: 40 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (4835/5248)
Epoch: 92 | Batch_idx: 50 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (91.00%) (6000/6528)
Epoch: 92 | Batch_idx: 60 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (7176/7808)
Epoch: 92 | Batch_idx: 70 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (91.00%) (8351/9088)
Epoch: 92 | Batch_idx: 80 |  Loss: (0.2337) |  Loss2: (0.0000) | Acc: (91.00%) (9532/10368)
Epoch: 92 | Batch_idx: 90 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (10700/11648)
Epoch: 92 | Batch_idx: 100 |  Loss: (0.2362) |  Loss2: (0.0000) | Acc: (91.00%) (11871/12928)
Epoch: 92 | Batch_idx: 110 |  Loss: (0.2388) |  Loss2: (0.0000) | Acc: (91.00%) (13031/14208)
Epoch: 92 | Batch_idx: 120 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (14223/15488)
Epoch: 92 | Batch_idx: 130 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (15428/16768)
Epoch: 92 | Batch_idx: 140 |  Loss: (0.2350) |  Loss2: (0.0000) | Acc: (91.00%) (16601/18048)
Epoch: 92 | Batch_idx: 150 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (17768/19328)
Epoch: 92 | Batch_idx: 160 |  Loss: (0.2355) |  Loss2: (0.0000) | Acc: (91.00%) (18951/20608)
Epoch: 92 | Batch_idx: 170 |  Loss: (0.2359) |  Loss2: (0.0000) | Acc: (91.00%) (20131/21888)
Epoch: 92 | Batch_idx: 180 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (21293/23168)
Epoch: 92 | Batch_idx: 190 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (22479/24448)
Epoch: 92 | Batch_idx: 200 |  Loss: (0.2356) |  Loss2: (0.0000) | Acc: (92.00%) (23673/25728)
Epoch: 92 | Batch_idx: 210 |  Loss: (0.2357) |  Loss2: (0.0000) | Acc: (92.00%) (24858/27008)
Epoch: 92 | Batch_idx: 220 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (92.00%) (26028/28288)
Epoch: 92 | Batch_idx: 230 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (92.00%) (27207/29568)
Epoch: 92 | Batch_idx: 240 |  Loss: (0.2372) |  Loss2: (0.0000) | Acc: (91.00%) (28374/30848)
Epoch: 92 | Batch_idx: 250 |  Loss: (0.2368) |  Loss2: (0.0000) | Acc: (91.00%) (29552/32128)
Epoch: 92 | Batch_idx: 260 |  Loss: (0.2363) |  Loss2: (0.0000) | Acc: (91.00%) (30733/33408)
Epoch: 92 | Batch_idx: 270 |  Loss: (0.2371) |  Loss2: (0.0000) | Acc: (91.00%) (31897/34688)
Epoch: 92 | Batch_idx: 280 |  Loss: (0.2370) |  Loss2: (0.0000) | Acc: (91.00%) (33057/35968)
Epoch: 92 | Batch_idx: 290 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (34245/37248)
Epoch: 92 | Batch_idx: 300 |  Loss: (0.2366) |  Loss2: (0.0000) | Acc: (91.00%) (35418/38528)
Epoch: 92 | Batch_idx: 310 |  Loss: (0.2369) |  Loss2: (0.0000) | Acc: (91.00%) (36592/39808)
Epoch: 92 | Batch_idx: 320 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (37768/41088)
Epoch: 92 | Batch_idx: 330 |  Loss: (0.2364) |  Loss2: (0.0000) | Acc: (91.00%) (38946/42368)
Epoch: 92 | Batch_idx: 340 |  Loss: (0.2375) |  Loss2: (0.0000) | Acc: (91.00%) (40105/43648)
Epoch: 92 | Batch_idx: 350 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (41284/44928)
Epoch: 92 | Batch_idx: 360 |  Loss: (0.2373) |  Loss2: (0.0000) | Acc: (91.00%) (42466/46208)
Epoch: 92 | Batch_idx: 370 |  Loss: (0.2376) |  Loss2: (0.0000) | Acc: (91.00%) (43633/47488)
Epoch: 92 | Batch_idx: 380 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (44808/48768)
Epoch: 92 | Batch_idx: 390 |  Loss: (0.2379) |  Loss2: (0.0000) | Acc: (91.00%) (45938/50000)
# TEST : Loss: (0.4318) | Acc: (86.00%) (8639/10000)
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5497, 0.4503], device='cuda:0')
percent tensor([0.5355, 0.4645], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.5816, 0.4184], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.6846, 0.3154], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 93 | Batch_idx: 0 |  Loss: (0.2848) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 93 | Batch_idx: 10 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (93.00%) (1312/1408)
Epoch: 93 | Batch_idx: 20 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (2483/2688)
Epoch: 93 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (3652/3968)
Epoch: 93 | Batch_idx: 40 |  Loss: (0.2365) |  Loss2: (0.0000) | Acc: (91.00%) (4817/5248)
Epoch: 93 | Batch_idx: 50 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (6009/6528)
Epoch: 93 | Batch_idx: 60 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (7195/7808)
Epoch: 93 | Batch_idx: 70 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (8369/9088)
Epoch: 93 | Batch_idx: 80 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (9562/10368)
Epoch: 93 | Batch_idx: 90 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (10733/11648)
Epoch: 93 | Batch_idx: 100 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (11921/12928)
Epoch: 93 | Batch_idx: 110 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (13098/14208)
Epoch: 93 | Batch_idx: 120 |  Loss: (0.2294) |  Loss2: (0.0000) | Acc: (92.00%) (14283/15488)
Epoch: 93 | Batch_idx: 130 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (15468/16768)
Epoch: 93 | Batch_idx: 140 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (16646/18048)
Epoch: 93 | Batch_idx: 150 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (17819/19328)
Epoch: 93 | Batch_idx: 160 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (92.00%) (18986/20608)
Epoch: 93 | Batch_idx: 170 |  Loss: (0.2314) |  Loss2: (0.0000) | Acc: (92.00%) (20153/21888)
Epoch: 93 | Batch_idx: 180 |  Loss: (0.2321) |  Loss2: (0.0000) | Acc: (92.00%) (21333/23168)
Epoch: 93 | Batch_idx: 190 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (22528/24448)
Epoch: 93 | Batch_idx: 200 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (23711/25728)
Epoch: 93 | Batch_idx: 210 |  Loss: (0.2279) |  Loss2: (0.0000) | Acc: (92.00%) (24906/27008)
Epoch: 93 | Batch_idx: 220 |  Loss: (0.2267) |  Loss2: (0.0000) | Acc: (92.00%) (26107/28288)
Epoch: 93 | Batch_idx: 230 |  Loss: (0.2284) |  Loss2: (0.0000) | Acc: (92.00%) (27268/29568)
Epoch: 93 | Batch_idx: 240 |  Loss: (0.2276) |  Loss2: (0.0000) | Acc: (92.00%) (28449/30848)
Epoch: 93 | Batch_idx: 250 |  Loss: (0.2291) |  Loss2: (0.0000) | Acc: (92.00%) (29616/32128)
Epoch: 93 | Batch_idx: 260 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (30802/33408)
Epoch: 93 | Batch_idx: 270 |  Loss: (0.2287) |  Loss2: (0.0000) | Acc: (92.00%) (31980/34688)
Epoch: 93 | Batch_idx: 280 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (33152/35968)
Epoch: 93 | Batch_idx: 290 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (34321/37248)
Epoch: 93 | Batch_idx: 300 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (92.00%) (35482/38528)
Epoch: 93 | Batch_idx: 310 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (92.00%) (36648/39808)
Epoch: 93 | Batch_idx: 320 |  Loss: (0.2298) |  Loss2: (0.0000) | Acc: (92.00%) (37835/41088)
Epoch: 93 | Batch_idx: 330 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (39003/42368)
Epoch: 93 | Batch_idx: 340 |  Loss: (0.2292) |  Loss2: (0.0000) | Acc: (92.00%) (40190/43648)
Epoch: 93 | Batch_idx: 350 |  Loss: (0.2296) |  Loss2: (0.0000) | Acc: (92.00%) (41360/44928)
Epoch: 93 | Batch_idx: 360 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (92.00%) (42552/46208)
Epoch: 93 | Batch_idx: 370 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (92.00%) (43713/47488)
Epoch: 93 | Batch_idx: 380 |  Loss: (0.2297) |  Loss2: (0.0000) | Acc: (92.00%) (44893/48768)
Epoch: 93 | Batch_idx: 390 |  Loss: (0.2300) |  Loss2: (0.0000) | Acc: (92.00%) (46019/50000)
# TEST : Loss: (0.4294) | Acc: (86.00%) (8654/10000)
percent tensor([0.5741, 0.4259], device='cuda:0')
percent tensor([0.5493, 0.4507], device='cuda:0')
percent tensor([0.5356, 0.4644], device='cuda:0')
percent tensor([0.6114, 0.3886], device='cuda:0')
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.7024, 0.2976], device='cuda:0')
percent tensor([0.6808, 0.3192], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 94 | Batch_idx: 0 |  Loss: (0.3286) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 94 | Batch_idx: 10 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (1298/1408)
Epoch: 94 | Batch_idx: 20 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (91.00%) (2470/2688)
Epoch: 94 | Batch_idx: 30 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (3658/3968)
Epoch: 94 | Batch_idx: 40 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 94 | Batch_idx: 50 |  Loss: (0.2185) |  Loss2: (0.0000) | Acc: (92.00%) (6022/6528)
Epoch: 94 | Batch_idx: 60 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (7191/7808)
Epoch: 94 | Batch_idx: 70 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (8398/9088)
Epoch: 94 | Batch_idx: 80 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (9569/10368)
Epoch: 94 | Batch_idx: 90 |  Loss: (0.2192) |  Loss2: (0.0000) | Acc: (92.00%) (10751/11648)
Epoch: 94 | Batch_idx: 100 |  Loss: (0.2183) |  Loss2: (0.0000) | Acc: (92.00%) (11942/12928)
Epoch: 94 | Batch_idx: 110 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (13125/14208)
Epoch: 94 | Batch_idx: 120 |  Loss: (0.2174) |  Loss2: (0.0000) | Acc: (92.00%) (14312/15488)
Epoch: 94 | Batch_idx: 130 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (15491/16768)
Epoch: 94 | Batch_idx: 140 |  Loss: (0.2172) |  Loss2: (0.0000) | Acc: (92.00%) (16685/18048)
Epoch: 94 | Batch_idx: 150 |  Loss: (0.2177) |  Loss2: (0.0000) | Acc: (92.00%) (17863/19328)
Epoch: 94 | Batch_idx: 160 |  Loss: (0.2166) |  Loss2: (0.0000) | Acc: (92.00%) (19046/20608)
Epoch: 94 | Batch_idx: 170 |  Loss: (0.2190) |  Loss2: (0.0000) | Acc: (92.00%) (20219/21888)
Epoch: 94 | Batch_idx: 180 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (21402/23168)
Epoch: 94 | Batch_idx: 190 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (22560/24448)
Epoch: 94 | Batch_idx: 200 |  Loss: (0.2197) |  Loss2: (0.0000) | Acc: (92.00%) (23750/25728)
Epoch: 94 | Batch_idx: 210 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (24933/27008)
Epoch: 94 | Batch_idx: 220 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (26108/28288)
Epoch: 94 | Batch_idx: 230 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (92.00%) (27288/29568)
Epoch: 94 | Batch_idx: 240 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (28459/30848)
Epoch: 94 | Batch_idx: 250 |  Loss: (0.2222) |  Loss2: (0.0000) | Acc: (92.00%) (29647/32128)
Epoch: 94 | Batch_idx: 260 |  Loss: (0.2211) |  Loss2: (0.0000) | Acc: (92.00%) (30840/33408)
Epoch: 94 | Batch_idx: 270 |  Loss: (0.2207) |  Loss2: (0.0000) | Acc: (92.00%) (32037/34688)
Epoch: 94 | Batch_idx: 280 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (33198/35968)
Epoch: 94 | Batch_idx: 290 |  Loss: (0.2234) |  Loss2: (0.0000) | Acc: (92.00%) (34366/37248)
Epoch: 94 | Batch_idx: 300 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (35550/38528)
Epoch: 94 | Batch_idx: 310 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (36733/39808)
Epoch: 94 | Batch_idx: 320 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (37895/41088)
Epoch: 94 | Batch_idx: 330 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (39069/42368)
Epoch: 94 | Batch_idx: 340 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (40253/43648)
Epoch: 94 | Batch_idx: 350 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (41422/44928)
Epoch: 94 | Batch_idx: 360 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (42610/46208)
Epoch: 94 | Batch_idx: 370 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (43792/47488)
Epoch: 94 | Batch_idx: 380 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (44970/48768)
Epoch: 94 | Batch_idx: 390 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (46099/50000)
# TEST : Loss: (0.4436) | Acc: (85.00%) (8574/10000)
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.5491, 0.4509], device='cuda:0')
percent tensor([0.5349, 0.4651], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.7039, 0.2961], device='cuda:0')
percent tensor([0.6870, 0.3130], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 95 | Batch_idx: 0 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 95 | Batch_idx: 10 |  Loss: (0.2413) |  Loss2: (0.0000) | Acc: (91.00%) (1295/1408)
Epoch: 95 | Batch_idx: 20 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (2464/2688)
Epoch: 95 | Batch_idx: 30 |  Loss: (0.2526) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 95 | Batch_idx: 40 |  Loss: (0.2563) |  Loss2: (0.0000) | Acc: (91.00%) (4783/5248)
Epoch: 95 | Batch_idx: 50 |  Loss: (0.2648) |  Loss2: (0.0000) | Acc: (90.00%) (5934/6528)
Epoch: 95 | Batch_idx: 60 |  Loss: (0.2703) |  Loss2: (0.0000) | Acc: (90.00%) (7085/7808)
Epoch: 95 | Batch_idx: 70 |  Loss: (0.2735) |  Loss2: (0.0000) | Acc: (90.00%) (8229/9088)
Epoch: 95 | Batch_idx: 80 |  Loss: (0.2755) |  Loss2: (0.0000) | Acc: (90.00%) (9385/10368)
Epoch: 95 | Batch_idx: 90 |  Loss: (0.2763) |  Loss2: (0.0000) | Acc: (90.00%) (10539/11648)
Epoch: 95 | Batch_idx: 100 |  Loss: (0.2771) |  Loss2: (0.0000) | Acc: (90.00%) (11684/12928)
Epoch: 95 | Batch_idx: 110 |  Loss: (0.2774) |  Loss2: (0.0000) | Acc: (90.00%) (12841/14208)
Epoch: 95 | Batch_idx: 120 |  Loss: (0.2753) |  Loss2: (0.0000) | Acc: (90.00%) (14009/15488)
Epoch: 95 | Batch_idx: 130 |  Loss: (0.2745) |  Loss2: (0.0000) | Acc: (90.00%) (15166/16768)
Epoch: 95 | Batch_idx: 140 |  Loss: (0.2738) |  Loss2: (0.0000) | Acc: (90.00%) (16319/18048)
Epoch: 95 | Batch_idx: 150 |  Loss: (0.2725) |  Loss2: (0.0000) | Acc: (90.00%) (17480/19328)
Epoch: 95 | Batch_idx: 160 |  Loss: (0.2721) |  Loss2: (0.0000) | Acc: (90.00%) (18647/20608)
Epoch: 95 | Batch_idx: 170 |  Loss: (0.2716) |  Loss2: (0.0000) | Acc: (90.00%) (19807/21888)
Epoch: 95 | Batch_idx: 180 |  Loss: (0.2697) |  Loss2: (0.0000) | Acc: (90.00%) (20975/23168)
Epoch: 95 | Batch_idx: 190 |  Loss: (0.2692) |  Loss2: (0.0000) | Acc: (90.00%) (22138/24448)
Epoch: 95 | Batch_idx: 200 |  Loss: (0.2679) |  Loss2: (0.0000) | Acc: (90.00%) (23311/25728)
Epoch: 95 | Batch_idx: 210 |  Loss: (0.2656) |  Loss2: (0.0000) | Acc: (90.00%) (24502/27008)
Epoch: 95 | Batch_idx: 220 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (90.00%) (25694/28288)
Epoch: 95 | Batch_idx: 230 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (26854/29568)
Epoch: 95 | Batch_idx: 240 |  Loss: (0.2638) |  Loss2: (0.0000) | Acc: (90.00%) (28017/30848)
Epoch: 95 | Batch_idx: 250 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (29165/32128)
Epoch: 95 | Batch_idx: 260 |  Loss: (0.2644) |  Loss2: (0.0000) | Acc: (90.00%) (30327/33408)
Epoch: 95 | Batch_idx: 270 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (90.00%) (31482/34688)
Epoch: 95 | Batch_idx: 280 |  Loss: (0.2629) |  Loss2: (0.0000) | Acc: (90.00%) (32663/35968)
Epoch: 95 | Batch_idx: 290 |  Loss: (0.2632) |  Loss2: (0.0000) | Acc: (90.00%) (33816/37248)
Epoch: 95 | Batch_idx: 300 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (34992/38528)
Epoch: 95 | Batch_idx: 310 |  Loss: (0.2614) |  Loss2: (0.0000) | Acc: (90.00%) (36166/39808)
Epoch: 95 | Batch_idx: 320 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (37323/41088)
Epoch: 95 | Batch_idx: 330 |  Loss: (0.2616) |  Loss2: (0.0000) | Acc: (90.00%) (38484/42368)
Epoch: 95 | Batch_idx: 340 |  Loss: (0.2624) |  Loss2: (0.0000) | Acc: (90.00%) (39646/43648)
Epoch: 95 | Batch_idx: 350 |  Loss: (0.2618) |  Loss2: (0.0000) | Acc: (90.00%) (40822/44928)
Epoch: 95 | Batch_idx: 360 |  Loss: (0.2620) |  Loss2: (0.0000) | Acc: (90.00%) (41988/46208)
Epoch: 95 | Batch_idx: 370 |  Loss: (0.2622) |  Loss2: (0.0000) | Acc: (90.00%) (43144/47488)
Epoch: 95 | Batch_idx: 380 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (44306/48768)
Epoch: 95 | Batch_idx: 390 |  Loss: (0.2615) |  Loss2: (0.0000) | Acc: (90.00%) (45440/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_095.pth.tar'
# TEST : Loss: (0.4631) | Acc: (85.00%) (8543/10000)
percent tensor([0.5709, 0.4291], device='cuda:0')
percent tensor([0.5569, 0.4431], device='cuda:0')
percent tensor([0.5445, 0.4555], device='cuda:0')
percent tensor([0.6023, 0.3977], device='cuda:0')
percent tensor([0.5625, 0.4375], device='cuda:0')
percent tensor([0.6820, 0.3180], device='cuda:0')
percent tensor([0.6899, 0.3101], device='cuda:0')
percent tensor([0.9993, 0.0007], device='cuda:0')
Epoch: 96 | Batch_idx: 0 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 96 | Batch_idx: 10 |  Loss: (0.2497) |  Loss2: (0.0000) | Acc: (91.00%) (1291/1408)
Epoch: 96 | Batch_idx: 20 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (91.00%) (2449/2688)
Epoch: 96 | Batch_idx: 30 |  Loss: (0.2605) |  Loss2: (0.0000) | Acc: (91.00%) (3614/3968)
Epoch: 96 | Batch_idx: 40 |  Loss: (0.2634) |  Loss2: (0.0000) | Acc: (91.00%) (4776/5248)
Epoch: 96 | Batch_idx: 50 |  Loss: (0.2627) |  Loss2: (0.0000) | Acc: (91.00%) (5942/6528)
Epoch: 96 | Batch_idx: 60 |  Loss: (0.2655) |  Loss2: (0.0000) | Acc: (90.00%) (7086/7808)
Epoch: 96 | Batch_idx: 70 |  Loss: (0.2640) |  Loss2: (0.0000) | Acc: (90.00%) (8267/9088)
Epoch: 96 | Batch_idx: 80 |  Loss: (0.2621) |  Loss2: (0.0000) | Acc: (90.00%) (9434/10368)
Epoch: 96 | Batch_idx: 90 |  Loss: (0.2601) |  Loss2: (0.0000) | Acc: (91.00%) (10604/11648)
Epoch: 96 | Batch_idx: 100 |  Loss: (0.2574) |  Loss2: (0.0000) | Acc: (91.00%) (11784/12928)
Epoch: 96 | Batch_idx: 110 |  Loss: (0.2557) |  Loss2: (0.0000) | Acc: (91.00%) (12960/14208)
Epoch: 96 | Batch_idx: 120 |  Loss: (0.2558) |  Loss2: (0.0000) | Acc: (91.00%) (14128/15488)
Epoch: 96 | Batch_idx: 130 |  Loss: (0.2543) |  Loss2: (0.0000) | Acc: (91.00%) (15305/16768)
Epoch: 96 | Batch_idx: 140 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (16484/18048)
Epoch: 96 | Batch_idx: 150 |  Loss: (0.2532) |  Loss2: (0.0000) | Acc: (91.00%) (17644/19328)
Epoch: 96 | Batch_idx: 160 |  Loss: (0.2530) |  Loss2: (0.0000) | Acc: (91.00%) (18818/20608)
Epoch: 96 | Batch_idx: 170 |  Loss: (0.2541) |  Loss2: (0.0000) | Acc: (91.00%) (19983/21888)
Epoch: 96 | Batch_idx: 180 |  Loss: (0.2538) |  Loss2: (0.0000) | Acc: (91.00%) (21147/23168)
Epoch: 96 | Batch_idx: 190 |  Loss: (0.2536) |  Loss2: (0.0000) | Acc: (91.00%) (22305/24448)
Epoch: 96 | Batch_idx: 200 |  Loss: (0.2518) |  Loss2: (0.0000) | Acc: (91.00%) (23485/25728)
Epoch: 96 | Batch_idx: 210 |  Loss: (0.2523) |  Loss2: (0.0000) | Acc: (91.00%) (24641/27008)
Epoch: 96 | Batch_idx: 220 |  Loss: (0.2512) |  Loss2: (0.0000) | Acc: (91.00%) (25810/28288)
Epoch: 96 | Batch_idx: 230 |  Loss: (0.2506) |  Loss2: (0.0000) | Acc: (91.00%) (26985/29568)
Epoch: 96 | Batch_idx: 240 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (28154/30848)
Epoch: 96 | Batch_idx: 250 |  Loss: (0.2503) |  Loss2: (0.0000) | Acc: (91.00%) (29324/32128)
Epoch: 96 | Batch_idx: 260 |  Loss: (0.2501) |  Loss2: (0.0000) | Acc: (91.00%) (30494/33408)
Epoch: 96 | Batch_idx: 270 |  Loss: (0.2486) |  Loss2: (0.0000) | Acc: (91.00%) (31688/34688)
Epoch: 96 | Batch_idx: 280 |  Loss: (0.2482) |  Loss2: (0.0000) | Acc: (91.00%) (32862/35968)
Epoch: 96 | Batch_idx: 290 |  Loss: (0.2480) |  Loss2: (0.0000) | Acc: (91.00%) (34039/37248)
Epoch: 96 | Batch_idx: 300 |  Loss: (0.2470) |  Loss2: (0.0000) | Acc: (91.00%) (35223/38528)
Epoch: 96 | Batch_idx: 310 |  Loss: (0.2468) |  Loss2: (0.0000) | Acc: (91.00%) (36397/39808)
Epoch: 96 | Batch_idx: 320 |  Loss: (0.2461) |  Loss2: (0.0000) | Acc: (91.00%) (37569/41088)
Epoch: 96 | Batch_idx: 330 |  Loss: (0.2458) |  Loss2: (0.0000) | Acc: (91.00%) (38743/42368)
Epoch: 96 | Batch_idx: 340 |  Loss: (0.2453) |  Loss2: (0.0000) | Acc: (91.00%) (39922/43648)
Epoch: 96 | Batch_idx: 350 |  Loss: (0.2449) |  Loss2: (0.0000) | Acc: (91.00%) (41102/44928)
Epoch: 96 | Batch_idx: 360 |  Loss: (0.2442) |  Loss2: (0.0000) | Acc: (91.00%) (42283/46208)
Epoch: 96 | Batch_idx: 370 |  Loss: (0.2439) |  Loss2: (0.0000) | Acc: (91.00%) (43455/47488)
Epoch: 96 | Batch_idx: 380 |  Loss: (0.2438) |  Loss2: (0.0000) | Acc: (91.00%) (44637/48768)
Epoch: 96 | Batch_idx: 390 |  Loss: (0.2436) |  Loss2: (0.0000) | Acc: (91.00%) (45779/50000)
# TEST : Loss: (0.4393) | Acc: (86.00%) (8602/10000)
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6084, 0.3916], device='cuda:0')
percent tensor([0.5609, 0.4391], device='cuda:0')
percent tensor([0.6779, 0.3221], device='cuda:0')
percent tensor([0.6942, 0.3058], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 97 | Batch_idx: 0 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 97 | Batch_idx: 10 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 97 | Batch_idx: 20 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (2496/2688)
Epoch: 97 | Batch_idx: 30 |  Loss: (0.2168) |  Loss2: (0.0000) | Acc: (92.00%) (3672/3968)
Epoch: 97 | Batch_idx: 40 |  Loss: (0.2181) |  Loss2: (0.0000) | Acc: (92.00%) (4852/5248)
Epoch: 97 | Batch_idx: 50 |  Loss: (0.2239) |  Loss2: (0.0000) | Acc: (92.00%) (6031/6528)
Epoch: 97 | Batch_idx: 60 |  Loss: (0.2243) |  Loss2: (0.0000) | Acc: (92.00%) (7208/7808)
Epoch: 97 | Batch_idx: 70 |  Loss: (0.2249) |  Loss2: (0.0000) | Acc: (92.00%) (8386/9088)
Epoch: 97 | Batch_idx: 80 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (9561/10368)
Epoch: 97 | Batch_idx: 90 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (10748/11648)
Epoch: 97 | Batch_idx: 100 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (11912/12928)
Epoch: 97 | Batch_idx: 110 |  Loss: (0.2286) |  Loss2: (0.0000) | Acc: (92.00%) (13082/14208)
Epoch: 97 | Batch_idx: 120 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (14271/15488)
Epoch: 97 | Batch_idx: 130 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (92.00%) (15433/16768)
Epoch: 97 | Batch_idx: 140 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (92.00%) (16610/18048)
Epoch: 97 | Batch_idx: 150 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (17787/19328)
Epoch: 97 | Batch_idx: 160 |  Loss: (0.2349) |  Loss2: (0.0000) | Acc: (91.00%) (18946/20608)
Epoch: 97 | Batch_idx: 170 |  Loss: (0.2331) |  Loss2: (0.0000) | Acc: (91.00%) (20135/21888)
Epoch: 97 | Batch_idx: 180 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (92.00%) (21326/23168)
Epoch: 97 | Batch_idx: 190 |  Loss: (0.2325) |  Loss2: (0.0000) | Acc: (91.00%) (22489/24448)
Epoch: 97 | Batch_idx: 200 |  Loss: (0.2333) |  Loss2: (0.0000) | Acc: (91.00%) (23656/25728)
Epoch: 97 | Batch_idx: 210 |  Loss: (0.2341) |  Loss2: (0.0000) | Acc: (91.00%) (24831/27008)
Epoch: 97 | Batch_idx: 220 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (26005/28288)
Epoch: 97 | Batch_idx: 230 |  Loss: (0.2340) |  Loss2: (0.0000) | Acc: (91.00%) (27179/29568)
Epoch: 97 | Batch_idx: 240 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (28362/30848)
Epoch: 97 | Batch_idx: 250 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (29556/32128)
Epoch: 97 | Batch_idx: 260 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (91.00%) (30720/33408)
Epoch: 97 | Batch_idx: 270 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (31906/34688)
Epoch: 97 | Batch_idx: 280 |  Loss: (0.2344) |  Loss2: (0.0000) | Acc: (91.00%) (33084/35968)
Epoch: 97 | Batch_idx: 290 |  Loss: (0.2343) |  Loss2: (0.0000) | Acc: (91.00%) (34266/37248)
Epoch: 97 | Batch_idx: 300 |  Loss: (0.2345) |  Loss2: (0.0000) | Acc: (91.00%) (35425/38528)
Epoch: 97 | Batch_idx: 310 |  Loss: (0.2339) |  Loss2: (0.0000) | Acc: (91.00%) (36615/39808)
Epoch: 97 | Batch_idx: 320 |  Loss: (0.2336) |  Loss2: (0.0000) | Acc: (91.00%) (37793/41088)
Epoch: 97 | Batch_idx: 330 |  Loss: (0.2338) |  Loss2: (0.0000) | Acc: (91.00%) (38972/42368)
Epoch: 97 | Batch_idx: 340 |  Loss: (0.2335) |  Loss2: (0.0000) | Acc: (91.00%) (40156/43648)
Epoch: 97 | Batch_idx: 350 |  Loss: (0.2326) |  Loss2: (0.0000) | Acc: (92.00%) (41359/44928)
Epoch: 97 | Batch_idx: 360 |  Loss: (0.2319) |  Loss2: (0.0000) | Acc: (92.00%) (42543/46208)
Epoch: 97 | Batch_idx: 370 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (43716/47488)
Epoch: 97 | Batch_idx: 380 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (92.00%) (44899/48768)
Epoch: 97 | Batch_idx: 390 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (46017/50000)
# TEST : Loss: (0.4323) | Acc: (86.00%) (8628/10000)
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.5624, 0.4376], device='cuda:0')
percent tensor([0.5482, 0.4518], device='cuda:0')
percent tensor([0.6124, 0.3876], device='cuda:0')
percent tensor([0.5648, 0.4352], device='cuda:0')
percent tensor([0.6806, 0.3194], device='cuda:0')
percent tensor([0.7066, 0.2934], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 98 | Batch_idx: 0 |  Loss: (0.1826) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 98 | Batch_idx: 10 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (1300/1408)
Epoch: 98 | Batch_idx: 20 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (2481/2688)
Epoch: 98 | Batch_idx: 30 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (3654/3968)
Epoch: 98 | Batch_idx: 40 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (4830/5248)
Epoch: 98 | Batch_idx: 50 |  Loss: (0.2232) |  Loss2: (0.0000) | Acc: (92.00%) (6020/6528)
Epoch: 98 | Batch_idx: 60 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (7208/7808)
Epoch: 98 | Batch_idx: 70 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (8400/9088)
Epoch: 98 | Batch_idx: 80 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (9582/10368)
Epoch: 98 | Batch_idx: 90 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (10770/11648)
Epoch: 98 | Batch_idx: 100 |  Loss: (0.2218) |  Loss2: (0.0000) | Acc: (92.00%) (11957/12928)
Epoch: 98 | Batch_idx: 110 |  Loss: (0.2223) |  Loss2: (0.0000) | Acc: (92.00%) (13143/14208)
Epoch: 98 | Batch_idx: 120 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (14305/15488)
Epoch: 98 | Batch_idx: 130 |  Loss: (0.2256) |  Loss2: (0.0000) | Acc: (92.00%) (15477/16768)
Epoch: 98 | Batch_idx: 140 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (16663/18048)
Epoch: 98 | Batch_idx: 150 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (17846/19328)
Epoch: 98 | Batch_idx: 160 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (19031/20608)
Epoch: 98 | Batch_idx: 170 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (20198/21888)
Epoch: 98 | Batch_idx: 180 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (21382/23168)
Epoch: 98 | Batch_idx: 190 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (22569/24448)
Epoch: 98 | Batch_idx: 200 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (23746/25728)
Epoch: 98 | Batch_idx: 210 |  Loss: (0.2245) |  Loss2: (0.0000) | Acc: (92.00%) (24921/27008)
Epoch: 98 | Batch_idx: 220 |  Loss: (0.2246) |  Loss2: (0.0000) | Acc: (92.00%) (26117/28288)
Epoch: 98 | Batch_idx: 230 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (27293/29568)
Epoch: 98 | Batch_idx: 240 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (28458/30848)
Epoch: 98 | Batch_idx: 250 |  Loss: (0.2248) |  Loss2: (0.0000) | Acc: (92.00%) (29648/32128)
Epoch: 98 | Batch_idx: 260 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (30845/33408)
Epoch: 98 | Batch_idx: 270 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (32001/34688)
Epoch: 98 | Batch_idx: 280 |  Loss: (0.2253) |  Loss2: (0.0000) | Acc: (92.00%) (33169/35968)
Epoch: 98 | Batch_idx: 290 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (34341/37248)
Epoch: 98 | Batch_idx: 300 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (35528/38528)
Epoch: 98 | Batch_idx: 310 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (36711/39808)
Epoch: 98 | Batch_idx: 320 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (37896/41088)
Epoch: 98 | Batch_idx: 330 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (39074/42368)
Epoch: 98 | Batch_idx: 340 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (40248/43648)
Epoch: 98 | Batch_idx: 350 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (41416/44928)
Epoch: 98 | Batch_idx: 360 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (42598/46208)
Epoch: 98 | Batch_idx: 370 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (43784/47488)
Epoch: 98 | Batch_idx: 380 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (44975/48768)
Epoch: 98 | Batch_idx: 390 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (46103/50000)
# TEST : Loss: (0.4272) | Acc: (86.00%) (8642/10000)
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5494, 0.4506], device='cuda:0')
percent tensor([0.6104, 0.3896], device='cuda:0')
percent tensor([0.5697, 0.4303], device='cuda:0')
percent tensor([0.6863, 0.3137], device='cuda:0')
percent tensor([0.7055, 0.2945], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 99 | Batch_idx: 0 |  Loss: (0.2770) |  Loss2: (0.0000) | Acc: (88.00%) (113/128)
Epoch: 99 | Batch_idx: 10 |  Loss: (0.2422) |  Loss2: (0.0000) | Acc: (92.00%) (1297/1408)
Epoch: 99 | Batch_idx: 20 |  Loss: (0.2423) |  Loss2: (0.0000) | Acc: (91.00%) (2465/2688)
Epoch: 99 | Batch_idx: 30 |  Loss: (0.2328) |  Loss2: (0.0000) | Acc: (92.00%) (3653/3968)
Epoch: 99 | Batch_idx: 40 |  Loss: (0.2342) |  Loss2: (0.0000) | Acc: (92.00%) (4832/5248)
Epoch: 99 | Batch_idx: 50 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (6025/6528)
Epoch: 99 | Batch_idx: 60 |  Loss: (0.2293) |  Loss2: (0.0000) | Acc: (92.00%) (7199/7808)
Epoch: 99 | Batch_idx: 70 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (8382/9088)
Epoch: 99 | Batch_idx: 80 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (9569/10368)
Epoch: 99 | Batch_idx: 90 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (10737/11648)
Epoch: 99 | Batch_idx: 100 |  Loss: (0.2278) |  Loss2: (0.0000) | Acc: (92.00%) (11913/12928)
Epoch: 99 | Batch_idx: 110 |  Loss: (0.2281) |  Loss2: (0.0000) | Acc: (92.00%) (13099/14208)
Epoch: 99 | Batch_idx: 120 |  Loss: (0.2289) |  Loss2: (0.0000) | Acc: (92.00%) (14264/15488)
Epoch: 99 | Batch_idx: 130 |  Loss: (0.2305) |  Loss2: (0.0000) | Acc: (91.00%) (15425/16768)
Epoch: 99 | Batch_idx: 140 |  Loss: (0.2280) |  Loss2: (0.0000) | Acc: (92.00%) (16618/18048)
Epoch: 99 | Batch_idx: 150 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (17804/19328)
Epoch: 99 | Batch_idx: 160 |  Loss: (0.2270) |  Loss2: (0.0000) | Acc: (92.00%) (18982/20608)
Epoch: 99 | Batch_idx: 170 |  Loss: (0.2262) |  Loss2: (0.0000) | Acc: (92.00%) (20161/21888)
Epoch: 99 | Batch_idx: 180 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (21349/23168)
Epoch: 99 | Batch_idx: 190 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (22531/24448)
Epoch: 99 | Batch_idx: 200 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (23722/25728)
Epoch: 99 | Batch_idx: 210 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (24915/27008)
Epoch: 99 | Batch_idx: 220 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (26109/28288)
Epoch: 99 | Batch_idx: 230 |  Loss: (0.2224) |  Loss2: (0.0000) | Acc: (92.00%) (27303/29568)
Epoch: 99 | Batch_idx: 240 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (28484/30848)
Epoch: 99 | Batch_idx: 250 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (29643/32128)
Epoch: 99 | Batch_idx: 260 |  Loss: (0.2231) |  Loss2: (0.0000) | Acc: (92.00%) (30833/33408)
Epoch: 99 | Batch_idx: 270 |  Loss: (0.2237) |  Loss2: (0.0000) | Acc: (92.00%) (32006/34688)
Epoch: 99 | Batch_idx: 280 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (33187/35968)
Epoch: 99 | Batch_idx: 290 |  Loss: (0.2247) |  Loss2: (0.0000) | Acc: (92.00%) (34360/37248)
Epoch: 99 | Batch_idx: 300 |  Loss: (0.2240) |  Loss2: (0.0000) | Acc: (92.00%) (35552/38528)
Epoch: 99 | Batch_idx: 310 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (36739/39808)
Epoch: 99 | Batch_idx: 320 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (37933/41088)
Epoch: 99 | Batch_idx: 330 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (39122/42368)
Epoch: 99 | Batch_idx: 340 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (40314/43648)
Epoch: 99 | Batch_idx: 350 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (41512/44928)
Epoch: 99 | Batch_idx: 360 |  Loss: (0.2221) |  Loss2: (0.0000) | Acc: (92.00%) (42696/46208)
Epoch: 99 | Batch_idx: 370 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (43884/47488)
Epoch: 99 | Batch_idx: 380 |  Loss: (0.2220) |  Loss2: (0.0000) | Acc: (92.00%) (45061/48768)
Epoch: 99 | Batch_idx: 390 |  Loss: (0.2227) |  Loss2: (0.0000) | Acc: (92.00%) (46189/50000)
# TEST : Loss: (0.4183) | Acc: (86.00%) (8671/10000)
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5597, 0.4403], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.7078, 0.2922], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 100 | Batch_idx: 0 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 100 | Batch_idx: 10 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (91.00%) (1287/1408)
Epoch: 100 | Batch_idx: 20 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (2493/2688)
Epoch: 100 | Batch_idx: 30 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (92.00%) (3680/3968)
Epoch: 100 | Batch_idx: 40 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (4849/5248)
Epoch: 100 | Batch_idx: 50 |  Loss: (0.2173) |  Loss2: (0.0000) | Acc: (92.00%) (6032/6528)
Epoch: 100 | Batch_idx: 60 |  Loss: (0.2214) |  Loss2: (0.0000) | Acc: (92.00%) (7202/7808)
Epoch: 100 | Batch_idx: 70 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (8387/9088)
Epoch: 100 | Batch_idx: 80 |  Loss: (0.2193) |  Loss2: (0.0000) | Acc: (92.00%) (9575/10368)
Epoch: 100 | Batch_idx: 90 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (10768/11648)
Epoch: 100 | Batch_idx: 100 |  Loss: (0.2205) |  Loss2: (0.0000) | Acc: (92.00%) (11959/12928)
Epoch: 100 | Batch_idx: 110 |  Loss: (0.2194) |  Loss2: (0.0000) | Acc: (92.00%) (13147/14208)
Epoch: 100 | Batch_idx: 120 |  Loss: (0.2200) |  Loss2: (0.0000) | Acc: (92.00%) (14319/15488)
Epoch: 100 | Batch_idx: 130 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (15504/16768)
Epoch: 100 | Batch_idx: 140 |  Loss: (0.2202) |  Loss2: (0.0000) | Acc: (92.00%) (16689/18048)
Epoch: 100 | Batch_idx: 150 |  Loss: (0.2201) |  Loss2: (0.0000) | Acc: (92.00%) (17876/19328)
Epoch: 100 | Batch_idx: 160 |  Loss: (0.2204) |  Loss2: (0.0000) | Acc: (92.00%) (19039/20608)
Epoch: 100 | Batch_idx: 170 |  Loss: (0.2203) |  Loss2: (0.0000) | Acc: (92.00%) (20222/21888)
Epoch: 100 | Batch_idx: 180 |  Loss: (0.2209) |  Loss2: (0.0000) | Acc: (92.00%) (21389/23168)
Epoch: 100 | Batch_idx: 190 |  Loss: (0.2215) |  Loss2: (0.0000) | Acc: (92.00%) (22567/24448)
Epoch: 100 | Batch_idx: 200 |  Loss: (0.2228) |  Loss2: (0.0000) | Acc: (92.00%) (23735/25728)
Epoch: 100 | Batch_idx: 210 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (24907/27008)
Epoch: 100 | Batch_idx: 220 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (26078/28288)
Epoch: 100 | Batch_idx: 230 |  Loss: (0.2236) |  Loss2: (0.0000) | Acc: (92.00%) (27265/29568)
Epoch: 100 | Batch_idx: 240 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (28438/30848)
Epoch: 100 | Batch_idx: 250 |  Loss: (0.2252) |  Loss2: (0.0000) | Acc: (92.00%) (29612/32128)
Epoch: 100 | Batch_idx: 260 |  Loss: (0.2250) |  Loss2: (0.0000) | Acc: (92.00%) (30799/33408)
Epoch: 100 | Batch_idx: 270 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (31967/34688)
Epoch: 100 | Batch_idx: 280 |  Loss: (0.2257) |  Loss2: (0.0000) | Acc: (92.00%) (33156/35968)
Epoch: 100 | Batch_idx: 290 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (34324/37248)
Epoch: 100 | Batch_idx: 300 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (92.00%) (35517/38528)
Epoch: 100 | Batch_idx: 310 |  Loss: (0.2269) |  Loss2: (0.0000) | Acc: (92.00%) (36686/39808)
Epoch: 100 | Batch_idx: 320 |  Loss: (0.2265) |  Loss2: (0.0000) | Acc: (92.00%) (37882/41088)
Epoch: 100 | Batch_idx: 330 |  Loss: (0.2266) |  Loss2: (0.0000) | Acc: (92.00%) (39058/42368)
Epoch: 100 | Batch_idx: 340 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (40241/43648)
Epoch: 100 | Batch_idx: 350 |  Loss: (0.2261) |  Loss2: (0.0000) | Acc: (92.00%) (41413/44928)
Epoch: 100 | Batch_idx: 360 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (92.00%) (42599/46208)
Epoch: 100 | Batch_idx: 370 |  Loss: (0.2258) |  Loss2: (0.0000) | Acc: (92.00%) (43785/47488)
Epoch: 100 | Batch_idx: 380 |  Loss: (0.2264) |  Loss2: (0.0000) | Acc: (92.00%) (44953/48768)
Epoch: 100 | Batch_idx: 390 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (92.00%) (46061/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_100.pth.tar'
# TEST : Loss: (0.5097) | Acc: (84.00%) (8491/10000)
percent tensor([0.5696, 0.4304], device='cuda:0')
percent tensor([0.5594, 0.4406], device='cuda:0')
percent tensor([0.5490, 0.4510], device='cuda:0')
percent tensor([0.6099, 0.3901], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.6881, 0.3119], device='cuda:0')
percent tensor([0.7105, 0.2895], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.2844, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.4030, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(811.3518, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1524.3298, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(493.6793, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2244.6790, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4281.6304, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1393.6150, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6155.2432, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11871.3867, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3931.6592, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16639.9258, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 101 | Batch_idx: 0 |  Loss: (0.2492) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 101 | Batch_idx: 10 |  Loss: (0.2208) |  Loss2: (0.0000) | Acc: (92.00%) (1301/1408)
Epoch: 101 | Batch_idx: 20 |  Loss: (0.2089) |  Loss2: (0.0000) | Acc: (92.00%) (2492/2688)
Epoch: 101 | Batch_idx: 30 |  Loss: (0.2170) |  Loss2: (0.0000) | Acc: (92.00%) (3683/3968)
Epoch: 101 | Batch_idx: 40 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 101 | Batch_idx: 50 |  Loss: (0.2167) |  Loss2: (0.0000) | Acc: (92.00%) (6044/6528)
Epoch: 101 | Batch_idx: 60 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (7234/7808)
Epoch: 101 | Batch_idx: 70 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (8426/9088)
Epoch: 101 | Batch_idx: 80 |  Loss: (0.2129) |  Loss2: (0.0000) | Acc: (92.00%) (9615/10368)
Epoch: 101 | Batch_idx: 90 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 101 | Batch_idx: 100 |  Loss: (0.2114) |  Loss2: (0.0000) | Acc: (92.00%) (12005/12928)
Epoch: 101 | Batch_idx: 110 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (13195/14208)
Epoch: 101 | Batch_idx: 120 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (14367/15488)
Epoch: 101 | Batch_idx: 130 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (15559/16768)
Epoch: 101 | Batch_idx: 140 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (16766/18048)
Epoch: 101 | Batch_idx: 150 |  Loss: (0.2057) |  Loss2: (0.0000) | Acc: (92.00%) (17961/19328)
Epoch: 101 | Batch_idx: 160 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (92.00%) (19144/20608)
Epoch: 101 | Batch_idx: 170 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (20331/21888)
Epoch: 101 | Batch_idx: 180 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (21527/23168)
Epoch: 101 | Batch_idx: 190 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (22718/24448)
Epoch: 101 | Batch_idx: 200 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (23914/25728)
Epoch: 101 | Batch_idx: 210 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (25108/27008)
Epoch: 101 | Batch_idx: 220 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (26284/28288)
Epoch: 101 | Batch_idx: 230 |  Loss: (0.2079) |  Loss2: (0.0000) | Acc: (92.00%) (27463/29568)
Epoch: 101 | Batch_idx: 240 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (28652/30848)
Epoch: 101 | Batch_idx: 250 |  Loss: (0.2086) |  Loss2: (0.0000) | Acc: (92.00%) (29831/32128)
Epoch: 101 | Batch_idx: 260 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (30991/33408)
Epoch: 101 | Batch_idx: 270 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (32169/34688)
Epoch: 101 | Batch_idx: 280 |  Loss: (0.2139) |  Loss2: (0.0000) | Acc: (92.00%) (33332/35968)
Epoch: 101 | Batch_idx: 290 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (34513/37248)
Epoch: 101 | Batch_idx: 300 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (35701/38528)
Epoch: 101 | Batch_idx: 310 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (36886/39808)
Epoch: 101 | Batch_idx: 320 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (38077/41088)
Epoch: 101 | Batch_idx: 330 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (39255/42368)
Epoch: 101 | Batch_idx: 340 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (40445/43648)
Epoch: 101 | Batch_idx: 350 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (41627/44928)
Epoch: 101 | Batch_idx: 360 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (42824/46208)
Epoch: 101 | Batch_idx: 370 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (44010/47488)
Epoch: 101 | Batch_idx: 380 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (45200/48768)
Epoch: 101 | Batch_idx: 390 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (46324/50000)
# TEST : Loss: (0.4416) | Acc: (86.00%) (8633/10000)
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5492, 0.4508], device='cuda:0')
percent tensor([0.6118, 0.3882], device='cuda:0')
percent tensor([0.5743, 0.4257], device='cuda:0')
percent tensor([0.6890, 0.3110], device='cuda:0')
percent tensor([0.7082, 0.2918], device='cuda:0')
percent tensor([0.9994, 0.0006], device='cuda:0')
Epoch: 102 | Batch_idx: 0 |  Loss: (0.2495) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 102 | Batch_idx: 10 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 102 | Batch_idx: 20 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (2484/2688)
Epoch: 102 | Batch_idx: 30 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (3663/3968)
Epoch: 102 | Batch_idx: 40 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (4847/5248)
Epoch: 102 | Batch_idx: 50 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (6036/6528)
Epoch: 102 | Batch_idx: 60 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (7231/7808)
Epoch: 102 | Batch_idx: 70 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (8424/9088)
Epoch: 102 | Batch_idx: 80 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (9611/10368)
Epoch: 102 | Batch_idx: 90 |  Loss: (0.2062) |  Loss2: (0.0000) | Acc: (92.00%) (10794/11648)
Epoch: 102 | Batch_idx: 100 |  Loss: (0.2067) |  Loss2: (0.0000) | Acc: (92.00%) (11978/12928)
Epoch: 102 | Batch_idx: 110 |  Loss: (0.2063) |  Loss2: (0.0000) | Acc: (92.00%) (13177/14208)
Epoch: 102 | Batch_idx: 120 |  Loss: (0.2065) |  Loss2: (0.0000) | Acc: (92.00%) (14369/15488)
Epoch: 102 | Batch_idx: 130 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (15549/16768)
Epoch: 102 | Batch_idx: 140 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (16743/18048)
Epoch: 102 | Batch_idx: 150 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (17934/19328)
Epoch: 102 | Batch_idx: 160 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (19114/20608)
Epoch: 102 | Batch_idx: 170 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (20301/21888)
Epoch: 102 | Batch_idx: 180 |  Loss: (0.2110) |  Loss2: (0.0000) | Acc: (92.00%) (21486/23168)
Epoch: 102 | Batch_idx: 190 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (22686/24448)
Epoch: 102 | Batch_idx: 200 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (23879/25728)
Epoch: 102 | Batch_idx: 210 |  Loss: (0.2104) |  Loss2: (0.0000) | Acc: (92.00%) (25064/27008)
Epoch: 102 | Batch_idx: 220 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (26223/28288)
Epoch: 102 | Batch_idx: 230 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (27406/29568)
Epoch: 102 | Batch_idx: 240 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (28586/30848)
Epoch: 102 | Batch_idx: 250 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (29764/32128)
Epoch: 102 | Batch_idx: 260 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (30941/33408)
Epoch: 102 | Batch_idx: 270 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (32127/34688)
Epoch: 102 | Batch_idx: 280 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (33317/35968)
Epoch: 102 | Batch_idx: 290 |  Loss: (0.2149) |  Loss2: (0.0000) | Acc: (92.00%) (34505/37248)
Epoch: 102 | Batch_idx: 300 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (35694/38528)
Epoch: 102 | Batch_idx: 310 |  Loss: (0.2141) |  Loss2: (0.0000) | Acc: (92.00%) (36879/39808)
Epoch: 102 | Batch_idx: 320 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (38066/41088)
Epoch: 102 | Batch_idx: 330 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (39255/42368)
Epoch: 102 | Batch_idx: 340 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (40439/43648)
Epoch: 102 | Batch_idx: 350 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (41611/44928)
Epoch: 102 | Batch_idx: 360 |  Loss: (0.2142) |  Loss2: (0.0000) | Acc: (92.00%) (42804/46208)
Epoch: 102 | Batch_idx: 370 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (43989/47488)
Epoch: 102 | Batch_idx: 380 |  Loss: (0.2147) |  Loss2: (0.0000) | Acc: (92.00%) (45168/48768)
Epoch: 102 | Batch_idx: 390 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (46307/50000)
# TEST : Loss: (0.5002) | Acc: (84.00%) (8480/10000)
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.5599, 0.4401], device='cuda:0')
percent tensor([0.5495, 0.4505], device='cuda:0')
percent tensor([0.6112, 0.3888], device='cuda:0')
percent tensor([0.5738, 0.4262], device='cuda:0')
percent tensor([0.6888, 0.3112], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 103 | Batch_idx: 0 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 103 | Batch_idx: 10 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 103 | Batch_idx: 20 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (2507/2688)
Epoch: 103 | Batch_idx: 30 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (3704/3968)
Epoch: 103 | Batch_idx: 40 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 103 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6086/6528)
Epoch: 103 | Batch_idx: 60 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (7276/7808)
Epoch: 103 | Batch_idx: 70 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (8480/9088)
Epoch: 103 | Batch_idx: 80 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (93.00%) (9663/10368)
Epoch: 103 | Batch_idx: 90 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (10853/11648)
Epoch: 103 | Batch_idx: 100 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (12061/12928)
Epoch: 103 | Batch_idx: 110 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (13256/14208)
Epoch: 103 | Batch_idx: 120 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (14459/15488)
Epoch: 103 | Batch_idx: 130 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (15663/16768)
Epoch: 103 | Batch_idx: 140 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (16862/18048)
Epoch: 103 | Batch_idx: 150 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (18045/19328)
Epoch: 103 | Batch_idx: 160 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (19235/20608)
Epoch: 103 | Batch_idx: 170 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (20432/21888)
Epoch: 103 | Batch_idx: 180 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (21622/23168)
Epoch: 103 | Batch_idx: 190 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (22818/24448)
Epoch: 103 | Batch_idx: 200 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (24009/25728)
Epoch: 103 | Batch_idx: 210 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (25204/27008)
Epoch: 103 | Batch_idx: 220 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (93.00%) (26395/28288)
Epoch: 103 | Batch_idx: 230 |  Loss: (0.2008) |  Loss2: (0.0000) | Acc: (93.00%) (27586/29568)
Epoch: 103 | Batch_idx: 240 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (28788/30848)
Epoch: 103 | Batch_idx: 250 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (29982/32128)
Epoch: 103 | Batch_idx: 260 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (31165/33408)
Epoch: 103 | Batch_idx: 270 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (32343/34688)
Epoch: 103 | Batch_idx: 280 |  Loss: (0.2025) |  Loss2: (0.0000) | Acc: (93.00%) (33518/35968)
Epoch: 103 | Batch_idx: 290 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (93.00%) (34702/37248)
Epoch: 103 | Batch_idx: 300 |  Loss: (0.2039) |  Loss2: (0.0000) | Acc: (93.00%) (35887/38528)
Epoch: 103 | Batch_idx: 310 |  Loss: (0.2050) |  Loss2: (0.0000) | Acc: (93.00%) (37052/39808)
Epoch: 103 | Batch_idx: 320 |  Loss: (0.2060) |  Loss2: (0.0000) | Acc: (93.00%) (38239/41088)
Epoch: 103 | Batch_idx: 330 |  Loss: (0.2069) |  Loss2: (0.0000) | Acc: (93.00%) (39415/42368)
Epoch: 103 | Batch_idx: 340 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (93.00%) (40602/43648)
Epoch: 103 | Batch_idx: 350 |  Loss: (0.2071) |  Loss2: (0.0000) | Acc: (93.00%) (41784/44928)
Epoch: 103 | Batch_idx: 360 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (42965/46208)
Epoch: 103 | Batch_idx: 370 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (44148/47488)
Epoch: 103 | Batch_idx: 380 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (45339/48768)
Epoch: 103 | Batch_idx: 390 |  Loss: (0.2078) |  Loss2: (0.0000) | Acc: (92.00%) (46472/50000)
# TEST : Loss: (0.4583) | Acc: (85.00%) (8598/10000)
percent tensor([0.5694, 0.4306], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6108, 0.3892], device='cuda:0')
percent tensor([0.5730, 0.4270], device='cuda:0')
percent tensor([0.6884, 0.3116], device='cuda:0')
percent tensor([0.7096, 0.2904], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 104 | Batch_idx: 0 |  Loss: (0.1658) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 104 | Batch_idx: 10 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 104 | Batch_idx: 20 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (2509/2688)
Epoch: 104 | Batch_idx: 30 |  Loss: (0.1941) |  Loss2: (0.0000) | Acc: (93.00%) (3713/3968)
Epoch: 104 | Batch_idx: 40 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (4908/5248)
Epoch: 104 | Batch_idx: 50 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (6104/6528)
Epoch: 104 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 104 | Batch_idx: 70 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (8513/9088)
Epoch: 104 | Batch_idx: 80 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (9700/10368)
Epoch: 104 | Batch_idx: 90 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (10889/11648)
Epoch: 104 | Batch_idx: 100 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (12066/12928)
Epoch: 104 | Batch_idx: 110 |  Loss: (0.1924) |  Loss2: (0.0000) | Acc: (93.00%) (13270/14208)
Epoch: 104 | Batch_idx: 120 |  Loss: (0.1940) |  Loss2: (0.0000) | Acc: (93.00%) (14463/15488)
Epoch: 104 | Batch_idx: 130 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (15646/16768)
Epoch: 104 | Batch_idx: 140 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (16838/18048)
Epoch: 104 | Batch_idx: 150 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (18029/19328)
Epoch: 104 | Batch_idx: 160 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (19225/20608)
Epoch: 104 | Batch_idx: 170 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (20429/21888)
Epoch: 104 | Batch_idx: 180 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (21612/23168)
Epoch: 104 | Batch_idx: 190 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (22808/24448)
Epoch: 104 | Batch_idx: 200 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (23996/25728)
Epoch: 104 | Batch_idx: 210 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (93.00%) (25189/27008)
Epoch: 104 | Batch_idx: 220 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (26389/28288)
Epoch: 104 | Batch_idx: 230 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (27586/29568)
Epoch: 104 | Batch_idx: 240 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (28781/30848)
Epoch: 104 | Batch_idx: 250 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (29974/32128)
Epoch: 104 | Batch_idx: 260 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (31150/33408)
Epoch: 104 | Batch_idx: 270 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (32357/34688)
Epoch: 104 | Batch_idx: 280 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (33532/35968)
Epoch: 104 | Batch_idx: 290 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (34713/37248)
Epoch: 104 | Batch_idx: 300 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (35905/38528)
Epoch: 104 | Batch_idx: 310 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (37097/39808)
Epoch: 104 | Batch_idx: 320 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (38287/41088)
Epoch: 104 | Batch_idx: 330 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (39459/42368)
Epoch: 104 | Batch_idx: 340 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (40643/43648)
Epoch: 104 | Batch_idx: 350 |  Loss: (0.1996) |  Loss2: (0.0000) | Acc: (93.00%) (41822/44928)
Epoch: 104 | Batch_idx: 360 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (43013/46208)
Epoch: 104 | Batch_idx: 370 |  Loss: (0.1998) |  Loss2: (0.0000) | Acc: (93.00%) (44200/47488)
Epoch: 104 | Batch_idx: 380 |  Loss: (0.2002) |  Loss2: (0.0000) | Acc: (93.00%) (45384/48768)
Epoch: 104 | Batch_idx: 390 |  Loss: (0.2009) |  Loss2: (0.0000) | Acc: (93.00%) (46524/50000)
# TEST : Loss: (0.4244) | Acc: (86.00%) (8646/10000)
percent tensor([0.5703, 0.4297], device='cuda:0')
percent tensor([0.5602, 0.4398], device='cuda:0')
percent tensor([0.5496, 0.4504], device='cuda:0')
percent tensor([0.6111, 0.3889], device='cuda:0')
percent tensor([0.5734, 0.4266], device='cuda:0')
percent tensor([0.6889, 0.3111], device='cuda:0')
percent tensor([0.7053, 0.2947], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 105 | Batch_idx: 0 |  Loss: (0.2210) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 105 | Batch_idx: 10 |  Loss: (0.1955) |  Loss2: (0.0000) | Acc: (93.00%) (1315/1408)
Epoch: 105 | Batch_idx: 20 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (2487/2688)
Epoch: 105 | Batch_idx: 30 |  Loss: (0.2152) |  Loss2: (0.0000) | Acc: (92.00%) (3655/3968)
Epoch: 105 | Batch_idx: 40 |  Loss: (0.2316) |  Loss2: (0.0000) | Acc: (91.00%) (4812/5248)
Epoch: 105 | Batch_idx: 50 |  Loss: (0.2272) |  Loss2: (0.0000) | Acc: (92.00%) (6008/6528)
Epoch: 105 | Batch_idx: 60 |  Loss: (0.2323) |  Loss2: (0.0000) | Acc: (91.00%) (7165/7808)
Epoch: 105 | Batch_idx: 70 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (8329/9088)
Epoch: 105 | Batch_idx: 80 |  Loss: (0.2322) |  Loss2: (0.0000) | Acc: (91.00%) (9507/10368)
Epoch: 105 | Batch_idx: 90 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (10683/11648)
Epoch: 105 | Batch_idx: 100 |  Loss: (0.2307) |  Loss2: (0.0000) | Acc: (91.00%) (11858/12928)
Epoch: 105 | Batch_idx: 110 |  Loss: (0.2324) |  Loss2: (0.0000) | Acc: (91.00%) (13025/14208)
Epoch: 105 | Batch_idx: 120 |  Loss: (0.2329) |  Loss2: (0.0000) | Acc: (91.00%) (14189/15488)
Epoch: 105 | Batch_idx: 130 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (15376/16768)
Epoch: 105 | Batch_idx: 140 |  Loss: (0.2313) |  Loss2: (0.0000) | Acc: (91.00%) (16550/18048)
Epoch: 105 | Batch_idx: 150 |  Loss: (0.2315) |  Loss2: (0.0000) | Acc: (91.00%) (17728/19328)
Epoch: 105 | Batch_idx: 160 |  Loss: (0.2320) |  Loss2: (0.0000) | Acc: (91.00%) (18897/20608)
Epoch: 105 | Batch_idx: 170 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (20082/21888)
Epoch: 105 | Batch_idx: 180 |  Loss: (0.2306) |  Loss2: (0.0000) | Acc: (91.00%) (21274/23168)
Epoch: 105 | Batch_idx: 190 |  Loss: (0.2310) |  Loss2: (0.0000) | Acc: (91.00%) (22438/24448)
Epoch: 105 | Batch_idx: 200 |  Loss: (0.2302) |  Loss2: (0.0000) | Acc: (91.00%) (23629/25728)
Epoch: 105 | Batch_idx: 210 |  Loss: (0.2295) |  Loss2: (0.0000) | Acc: (91.00%) (24820/27008)
Epoch: 105 | Batch_idx: 220 |  Loss: (0.2304) |  Loss2: (0.0000) | Acc: (91.00%) (25981/28288)
Epoch: 105 | Batch_idx: 230 |  Loss: (0.2299) |  Loss2: (0.0000) | Acc: (91.00%) (27151/29568)
Epoch: 105 | Batch_idx: 240 |  Loss: (0.2290) |  Loss2: (0.0000) | Acc: (91.00%) (28338/30848)
Epoch: 105 | Batch_idx: 250 |  Loss: (0.2285) |  Loss2: (0.0000) | Acc: (91.00%) (29515/32128)
Epoch: 105 | Batch_idx: 260 |  Loss: (0.2275) |  Loss2: (0.0000) | Acc: (91.00%) (30700/33408)
Epoch: 105 | Batch_idx: 270 |  Loss: (0.2274) |  Loss2: (0.0000) | Acc: (91.00%) (31881/34688)
Epoch: 105 | Batch_idx: 280 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (91.00%) (33060/35968)
Epoch: 105 | Batch_idx: 290 |  Loss: (0.2263) |  Loss2: (0.0000) | Acc: (91.00%) (34249/37248)
Epoch: 105 | Batch_idx: 300 |  Loss: (0.2260) |  Loss2: (0.0000) | Acc: (91.00%) (35431/38528)
Epoch: 105 | Batch_idx: 310 |  Loss: (0.2259) |  Loss2: (0.0000) | Acc: (91.00%) (36618/39808)
Epoch: 105 | Batch_idx: 320 |  Loss: (0.2254) |  Loss2: (0.0000) | Acc: (92.00%) (37819/41088)
Epoch: 105 | Batch_idx: 330 |  Loss: (0.2251) |  Loss2: (0.0000) | Acc: (92.00%) (39002/42368)
Epoch: 105 | Batch_idx: 340 |  Loss: (0.2241) |  Loss2: (0.0000) | Acc: (92.00%) (40189/43648)
Epoch: 105 | Batch_idx: 350 |  Loss: (0.2233) |  Loss2: (0.0000) | Acc: (92.00%) (41386/44928)
Epoch: 105 | Batch_idx: 360 |  Loss: (0.2238) |  Loss2: (0.0000) | Acc: (92.00%) (42555/46208)
Epoch: 105 | Batch_idx: 370 |  Loss: (0.2242) |  Loss2: (0.0000) | Acc: (92.00%) (43717/47488)
Epoch: 105 | Batch_idx: 380 |  Loss: (0.2235) |  Loss2: (0.0000) | Acc: (92.00%) (44901/48768)
Epoch: 105 | Batch_idx: 390 |  Loss: (0.2244) |  Loss2: (0.0000) | Acc: (92.00%) (46015/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_105.pth.tar'
# TEST : Loss: (0.4269) | Acc: (86.00%) (8647/10000)
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.5667, 0.4333], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.6239, 0.3761], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 106 | Batch_idx: 0 |  Loss: (0.3481) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 106 | Batch_idx: 10 |  Loss: (0.2179) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 106 | Batch_idx: 20 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (2495/2688)
Epoch: 106 | Batch_idx: 30 |  Loss: (0.2051) |  Loss2: (0.0000) | Acc: (92.00%) (3690/3968)
Epoch: 106 | Batch_idx: 40 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (92.00%) (4875/5248)
Epoch: 106 | Batch_idx: 50 |  Loss: (0.2146) |  Loss2: (0.0000) | Acc: (92.00%) (6049/6528)
Epoch: 106 | Batch_idx: 60 |  Loss: (0.2119) |  Loss2: (0.0000) | Acc: (92.00%) (7239/7808)
Epoch: 106 | Batch_idx: 70 |  Loss: (0.2124) |  Loss2: (0.0000) | Acc: (92.00%) (8421/9088)
Epoch: 106 | Batch_idx: 80 |  Loss: (0.2102) |  Loss2: (0.0000) | Acc: (92.00%) (9616/10368)
Epoch: 106 | Batch_idx: 90 |  Loss: (0.2076) |  Loss2: (0.0000) | Acc: (92.00%) (10810/11648)
Epoch: 106 | Batch_idx: 100 |  Loss: (0.2068) |  Loss2: (0.0000) | Acc: (92.00%) (11999/12928)
Epoch: 106 | Batch_idx: 110 |  Loss: (0.2075) |  Loss2: (0.0000) | Acc: (92.00%) (13183/14208)
Epoch: 106 | Batch_idx: 120 |  Loss: (0.2073) |  Loss2: (0.0000) | Acc: (92.00%) (14362/15488)
Epoch: 106 | Batch_idx: 130 |  Loss: (0.2064) |  Loss2: (0.0000) | Acc: (92.00%) (15550/16768)
Epoch: 106 | Batch_idx: 140 |  Loss: (0.2059) |  Loss2: (0.0000) | Acc: (92.00%) (16740/18048)
Epoch: 106 | Batch_idx: 150 |  Loss: (0.2081) |  Loss2: (0.0000) | Acc: (92.00%) (17922/19328)
Epoch: 106 | Batch_idx: 160 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (19124/20608)
Epoch: 106 | Batch_idx: 170 |  Loss: (0.2056) |  Loss2: (0.0000) | Acc: (92.00%) (20321/21888)
Epoch: 106 | Batch_idx: 180 |  Loss: (0.2072) |  Loss2: (0.0000) | Acc: (92.00%) (21488/23168)
Epoch: 106 | Batch_idx: 190 |  Loss: (0.2080) |  Loss2: (0.0000) | Acc: (92.00%) (22668/24448)
Epoch: 106 | Batch_idx: 200 |  Loss: (0.2085) |  Loss2: (0.0000) | Acc: (92.00%) (23844/25728)
Epoch: 106 | Batch_idx: 210 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (25036/27008)
Epoch: 106 | Batch_idx: 220 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (26217/28288)
Epoch: 106 | Batch_idx: 230 |  Loss: (0.2097) |  Loss2: (0.0000) | Acc: (92.00%) (27411/29568)
Epoch: 106 | Batch_idx: 240 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (28596/30848)
Epoch: 106 | Batch_idx: 250 |  Loss: (0.2101) |  Loss2: (0.0000) | Acc: (92.00%) (29786/32128)
Epoch: 106 | Batch_idx: 260 |  Loss: (0.2103) |  Loss2: (0.0000) | Acc: (92.00%) (30973/33408)
Epoch: 106 | Batch_idx: 270 |  Loss: (0.2099) |  Loss2: (0.0000) | Acc: (92.00%) (32160/34688)
Epoch: 106 | Batch_idx: 280 |  Loss: (0.2100) |  Loss2: (0.0000) | Acc: (92.00%) (33345/35968)
Epoch: 106 | Batch_idx: 290 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (34542/37248)
Epoch: 106 | Batch_idx: 300 |  Loss: (0.2095) |  Loss2: (0.0000) | Acc: (92.00%) (35728/38528)
Epoch: 106 | Batch_idx: 310 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (36917/39808)
Epoch: 106 | Batch_idx: 320 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (38094/41088)
Epoch: 106 | Batch_idx: 330 |  Loss: (0.2094) |  Loss2: (0.0000) | Acc: (92.00%) (39286/42368)
Epoch: 106 | Batch_idx: 340 |  Loss: (0.2096) |  Loss2: (0.0000) | Acc: (92.00%) (40476/43648)
Epoch: 106 | Batch_idx: 350 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (41667/44928)
Epoch: 106 | Batch_idx: 360 |  Loss: (0.2091) |  Loss2: (0.0000) | Acc: (92.00%) (42851/46208)
Epoch: 106 | Batch_idx: 370 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (44043/47488)
Epoch: 106 | Batch_idx: 380 |  Loss: (0.2088) |  Loss2: (0.0000) | Acc: (92.00%) (45239/48768)
Epoch: 106 | Batch_idx: 390 |  Loss: (0.2087) |  Loss2: (0.0000) | Acc: (92.00%) (46390/50000)
# TEST : Loss: (0.4134) | Acc: (86.00%) (8670/10000)
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5716, 0.4284], device='cuda:0')
percent tensor([0.5438, 0.4562], device='cuda:0')
percent tensor([0.6216, 0.3784], device='cuda:0')
percent tensor([0.5878, 0.4122], device='cuda:0')
percent tensor([0.6710, 0.3290], device='cuda:0')
percent tensor([0.6668, 0.3332], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 107 | Batch_idx: 0 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 107 | Batch_idx: 10 |  Loss: (0.2148) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 107 | Batch_idx: 20 |  Loss: (0.2084) |  Loss2: (0.0000) | Acc: (92.00%) (2497/2688)
Epoch: 107 | Batch_idx: 30 |  Loss: (0.2004) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 107 | Batch_idx: 40 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (4891/5248)
Epoch: 107 | Batch_idx: 50 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (6083/6528)
Epoch: 107 | Batch_idx: 60 |  Loss: (0.2026) |  Loss2: (0.0000) | Acc: (93.00%) (7263/7808)
Epoch: 107 | Batch_idx: 70 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (8469/9088)
Epoch: 107 | Batch_idx: 80 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (9660/10368)
Epoch: 107 | Batch_idx: 90 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (93.00%) (10839/11648)
Epoch: 107 | Batch_idx: 100 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (12045/12928)
Epoch: 107 | Batch_idx: 110 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (13233/14208)
Epoch: 107 | Batch_idx: 120 |  Loss: (0.1964) |  Loss2: (0.0000) | Acc: (93.00%) (14437/15488)
Epoch: 107 | Batch_idx: 130 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (15627/16768)
Epoch: 107 | Batch_idx: 140 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (16813/18048)
Epoch: 107 | Batch_idx: 150 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (17993/19328)
Epoch: 107 | Batch_idx: 160 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (19189/20608)
Epoch: 107 | Batch_idx: 170 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (20390/21888)
Epoch: 107 | Batch_idx: 180 |  Loss: (0.1975) |  Loss2: (0.0000) | Acc: (93.00%) (21602/23168)
Epoch: 107 | Batch_idx: 190 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (22792/24448)
Epoch: 107 | Batch_idx: 200 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (23981/25728)
Epoch: 107 | Batch_idx: 210 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (25185/27008)
Epoch: 107 | Batch_idx: 220 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (26372/28288)
Epoch: 107 | Batch_idx: 230 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (27563/29568)
Epoch: 107 | Batch_idx: 240 |  Loss: (0.1962) |  Loss2: (0.0000) | Acc: (93.00%) (28776/30848)
Epoch: 107 | Batch_idx: 250 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (29967/32128)
Epoch: 107 | Batch_idx: 260 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (31148/33408)
Epoch: 107 | Batch_idx: 270 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (32351/34688)
Epoch: 107 | Batch_idx: 280 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (33543/35968)
Epoch: 107 | Batch_idx: 290 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (34728/37248)
Epoch: 107 | Batch_idx: 300 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (35920/38528)
Epoch: 107 | Batch_idx: 310 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (37125/39808)
Epoch: 107 | Batch_idx: 320 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (38295/41088)
Epoch: 107 | Batch_idx: 330 |  Loss: (0.1989) |  Loss2: (0.0000) | Acc: (93.00%) (39483/42368)
Epoch: 107 | Batch_idx: 340 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (40670/43648)
Epoch: 107 | Batch_idx: 350 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (41865/44928)
Epoch: 107 | Batch_idx: 360 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (43060/46208)
Epoch: 107 | Batch_idx: 370 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (44260/47488)
Epoch: 107 | Batch_idx: 380 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (45448/48768)
Epoch: 107 | Batch_idx: 390 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (46599/50000)
# TEST : Loss: (0.4025) | Acc: (87.00%) (8715/10000)
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5708, 0.4292], device='cuda:0')
percent tensor([0.5439, 0.4561], device='cuda:0')
percent tensor([0.6271, 0.3729], device='cuda:0')
percent tensor([0.5876, 0.4124], device='cuda:0')
percent tensor([0.6675, 0.3325], device='cuda:0')
percent tensor([0.6745, 0.3255], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 108 | Batch_idx: 0 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 108 | Batch_idx: 10 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (1310/1408)
Epoch: 108 | Batch_idx: 20 |  Loss: (0.2151) |  Loss2: (0.0000) | Acc: (92.00%) (2480/2688)
Epoch: 108 | Batch_idx: 30 |  Loss: (0.2092) |  Loss2: (0.0000) | Acc: (92.00%) (3671/3968)
Epoch: 108 | Batch_idx: 40 |  Loss: (0.2027) |  Loss2: (0.0000) | Acc: (92.00%) (4866/5248)
Epoch: 108 | Batch_idx: 50 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (92.00%) (6060/6528)
Epoch: 108 | Batch_idx: 60 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (7249/7808)
Epoch: 108 | Batch_idx: 70 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (92.00%) (8423/9088)
Epoch: 108 | Batch_idx: 80 |  Loss: (0.2029) |  Loss2: (0.0000) | Acc: (92.00%) (9617/10368)
Epoch: 108 | Batch_idx: 90 |  Loss: (0.2001) |  Loss2: (0.0000) | Acc: (92.00%) (10819/11648)
Epoch: 108 | Batch_idx: 100 |  Loss: (0.2005) |  Loss2: (0.0000) | Acc: (92.00%) (12011/12928)
Epoch: 108 | Batch_idx: 110 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (93.00%) (13219/14208)
Epoch: 108 | Batch_idx: 120 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (14408/15488)
Epoch: 108 | Batch_idx: 130 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (15595/16768)
Epoch: 108 | Batch_idx: 140 |  Loss: (0.2010) |  Loss2: (0.0000) | Acc: (92.00%) (16772/18048)
Epoch: 108 | Batch_idx: 150 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (92.00%) (17967/19328)
Epoch: 108 | Batch_idx: 160 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (92.00%) (19164/20608)
Epoch: 108 | Batch_idx: 170 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (92.00%) (20351/21888)
Epoch: 108 | Batch_idx: 180 |  Loss: (0.1994) |  Loss2: (0.0000) | Acc: (92.00%) (21538/23168)
Epoch: 108 | Batch_idx: 190 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (22750/24448)
Epoch: 108 | Batch_idx: 200 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (23938/25728)
Epoch: 108 | Batch_idx: 210 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (25128/27008)
Epoch: 108 | Batch_idx: 220 |  Loss: (0.1990) |  Loss2: (0.0000) | Acc: (93.00%) (26314/28288)
Epoch: 108 | Batch_idx: 230 |  Loss: (0.1984) |  Loss2: (0.0000) | Acc: (93.00%) (27504/29568)
Epoch: 108 | Batch_idx: 240 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (28698/30848)
Epoch: 108 | Batch_idx: 250 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (29895/32128)
Epoch: 108 | Batch_idx: 260 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (31096/33408)
Epoch: 108 | Batch_idx: 270 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (32293/34688)
Epoch: 108 | Batch_idx: 280 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (33491/35968)
Epoch: 108 | Batch_idx: 290 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (34677/37248)
Epoch: 108 | Batch_idx: 300 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (35878/38528)
Epoch: 108 | Batch_idx: 310 |  Loss: (0.1979) |  Loss2: (0.0000) | Acc: (93.00%) (37064/39808)
Epoch: 108 | Batch_idx: 320 |  Loss: (0.1981) |  Loss2: (0.0000) | Acc: (93.00%) (38252/41088)
Epoch: 108 | Batch_idx: 330 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (39448/42368)
Epoch: 108 | Batch_idx: 340 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (40653/43648)
Epoch: 108 | Batch_idx: 350 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (41851/44928)
Epoch: 108 | Batch_idx: 360 |  Loss: (0.1982) |  Loss2: (0.0000) | Acc: (93.00%) (43033/46208)
Epoch: 108 | Batch_idx: 370 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (44237/47488)
Epoch: 108 | Batch_idx: 380 |  Loss: (0.1978) |  Loss2: (0.0000) | Acc: (93.00%) (45430/48768)
Epoch: 108 | Batch_idx: 390 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (46579/50000)
# TEST : Loss: (0.3934) | Acc: (87.00%) (8742/10000)
percent tensor([0.5834, 0.4166], device='cuda:0')
percent tensor([0.5693, 0.4307], device='cuda:0')
percent tensor([0.5437, 0.4563], device='cuda:0')
percent tensor([0.6234, 0.3766], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6802, 0.3198], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 109 | Batch_idx: 0 |  Loss: (0.2478) |  Loss2: (0.0000) | Acc: (89.00%) (114/128)
Epoch: 109 | Batch_idx: 10 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (92.00%) (1308/1408)
Epoch: 109 | Batch_idx: 20 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (2516/2688)
Epoch: 109 | Batch_idx: 30 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (3708/3968)
Epoch: 109 | Batch_idx: 40 |  Loss: (0.1874) |  Loss2: (0.0000) | Acc: (93.00%) (4897/5248)
Epoch: 109 | Batch_idx: 50 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (6085/6528)
Epoch: 109 | Batch_idx: 60 |  Loss: (0.1928) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 109 | Batch_idx: 70 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (8467/9088)
Epoch: 109 | Batch_idx: 80 |  Loss: (0.1939) |  Loss2: (0.0000) | Acc: (93.00%) (9649/10368)
Epoch: 109 | Batch_idx: 90 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (10857/11648)
Epoch: 109 | Batch_idx: 100 |  Loss: (0.1922) |  Loss2: (0.0000) | Acc: (93.00%) (12050/12928)
Epoch: 109 | Batch_idx: 110 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (13255/14208)
Epoch: 109 | Batch_idx: 120 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (14472/15488)
Epoch: 109 | Batch_idx: 130 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (15679/16768)
Epoch: 109 | Batch_idx: 140 |  Loss: (0.1869) |  Loss2: (0.0000) | Acc: (93.00%) (16871/18048)
Epoch: 109 | Batch_idx: 150 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (18071/19328)
Epoch: 109 | Batch_idx: 160 |  Loss: (0.1871) |  Loss2: (0.0000) | Acc: (93.00%) (19265/20608)
Epoch: 109 | Batch_idx: 170 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (20442/21888)
Epoch: 109 | Batch_idx: 180 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (21647/23168)
Epoch: 109 | Batch_idx: 190 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (93.00%) (22843/24448)
Epoch: 109 | Batch_idx: 200 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (24038/25728)
Epoch: 109 | Batch_idx: 210 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (25219/27008)
Epoch: 109 | Batch_idx: 220 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (26413/28288)
Epoch: 109 | Batch_idx: 230 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (27607/29568)
Epoch: 109 | Batch_idx: 240 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (28800/30848)
Epoch: 109 | Batch_idx: 250 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (30008/32128)
Epoch: 109 | Batch_idx: 260 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (31194/33408)
Epoch: 109 | Batch_idx: 270 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (32384/34688)
Epoch: 109 | Batch_idx: 280 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (33587/35968)
Epoch: 109 | Batch_idx: 290 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (34790/37248)
Epoch: 109 | Batch_idx: 300 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (35973/38528)
Epoch: 109 | Batch_idx: 310 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (37181/39808)
Epoch: 109 | Batch_idx: 320 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (38377/41088)
Epoch: 109 | Batch_idx: 330 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (39583/42368)
Epoch: 109 | Batch_idx: 340 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (40771/43648)
Epoch: 109 | Batch_idx: 350 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (41960/44928)
Epoch: 109 | Batch_idx: 360 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (43148/46208)
Epoch: 109 | Batch_idx: 370 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (44349/47488)
Epoch: 109 | Batch_idx: 380 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (45544/48768)
Epoch: 109 | Batch_idx: 390 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (46694/50000)
# TEST : Loss: (0.3933) | Acc: (87.00%) (8725/10000)
percent tensor([0.5848, 0.4152], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5463, 0.4537], device='cuda:0')
percent tensor([0.6259, 0.3741], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6727, 0.3273], device='cuda:0')
percent tensor([0.6868, 0.3132], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 110 | Batch_idx: 0 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 110 | Batch_idx: 10 |  Loss: (0.1677) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 110 | Batch_idx: 20 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 110 | Batch_idx: 30 |  Loss: (0.1859) |  Loss2: (0.0000) | Acc: (93.00%) (3711/3968)
Epoch: 110 | Batch_idx: 40 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (4902/5248)
Epoch: 110 | Batch_idx: 50 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 110 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7290/7808)
Epoch: 110 | Batch_idx: 70 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (8472/9088)
Epoch: 110 | Batch_idx: 80 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (9674/10368)
Epoch: 110 | Batch_idx: 90 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (10857/11648)
Epoch: 110 | Batch_idx: 100 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (93.00%) (12053/12928)
Epoch: 110 | Batch_idx: 110 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (13254/14208)
Epoch: 110 | Batch_idx: 120 |  Loss: (0.1926) |  Loss2: (0.0000) | Acc: (93.00%) (14442/15488)
Epoch: 110 | Batch_idx: 130 |  Loss: (0.1931) |  Loss2: (0.0000) | Acc: (93.00%) (15625/16768)
Epoch: 110 | Batch_idx: 140 |  Loss: (0.1954) |  Loss2: (0.0000) | Acc: (93.00%) (16789/18048)
Epoch: 110 | Batch_idx: 150 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (92.00%) (17973/19328)
Epoch: 110 | Batch_idx: 160 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (92.00%) (19151/20608)
Epoch: 110 | Batch_idx: 170 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (92.00%) (20346/21888)
Epoch: 110 | Batch_idx: 180 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (21532/23168)
Epoch: 110 | Batch_idx: 190 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (92.00%) (22721/24448)
Epoch: 110 | Batch_idx: 200 |  Loss: (0.2007) |  Loss2: (0.0000) | Acc: (92.00%) (23902/25728)
Epoch: 110 | Batch_idx: 210 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (25093/27008)
Epoch: 110 | Batch_idx: 220 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (26290/28288)
Epoch: 110 | Batch_idx: 230 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (27479/29568)
Epoch: 110 | Batch_idx: 240 |  Loss: (0.2028) |  Loss2: (0.0000) | Acc: (92.00%) (28654/30848)
Epoch: 110 | Batch_idx: 250 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (29849/32128)
Epoch: 110 | Batch_idx: 260 |  Loss: (0.2023) |  Loss2: (0.0000) | Acc: (92.00%) (31044/33408)
Epoch: 110 | Batch_idx: 270 |  Loss: (0.2011) |  Loss2: (0.0000) | Acc: (92.00%) (32250/34688)
Epoch: 110 | Batch_idx: 280 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (33438/35968)
Epoch: 110 | Batch_idx: 290 |  Loss: (0.2018) |  Loss2: (0.0000) | Acc: (92.00%) (34618/37248)
Epoch: 110 | Batch_idx: 300 |  Loss: (0.2021) |  Loss2: (0.0000) | Acc: (92.00%) (35796/38528)
Epoch: 110 | Batch_idx: 310 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (36999/39808)
Epoch: 110 | Batch_idx: 320 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (38187/41088)
Epoch: 110 | Batch_idx: 330 |  Loss: (0.2015) |  Loss2: (0.0000) | Acc: (92.00%) (39382/42368)
Epoch: 110 | Batch_idx: 340 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (40568/43648)
Epoch: 110 | Batch_idx: 350 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (92.00%) (41759/44928)
Epoch: 110 | Batch_idx: 360 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (42949/46208)
Epoch: 110 | Batch_idx: 370 |  Loss: (0.2013) |  Loss2: (0.0000) | Acc: (92.00%) (44145/47488)
Epoch: 110 | Batch_idx: 380 |  Loss: (0.2016) |  Loss2: (0.0000) | Acc: (92.00%) (45323/48768)
Epoch: 110 | Batch_idx: 390 |  Loss: (0.2017) |  Loss2: (0.0000) | Acc: (92.00%) (46467/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_110.pth.tar'
# TEST : Loss: (0.4289) | Acc: (86.00%) (8696/10000)
percent tensor([0.5847, 0.4153], device='cuda:0')
percent tensor([0.5700, 0.4300], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.6256, 0.3744], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6868, 0.3132], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(179.9081, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.4470, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(814.3657, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.6339, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(492.0988, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2252.2583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4278.4106, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1388.8553, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6168.2754, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11836.4551, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3916.5063, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16572.6699, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 111 | Batch_idx: 0 |  Loss: (0.2645) |  Loss2: (0.0000) | Acc: (89.00%) (115/128)
Epoch: 111 | Batch_idx: 10 |  Loss: (0.1925) |  Loss2: (0.0000) | Acc: (92.00%) (1307/1408)
Epoch: 111 | Batch_idx: 20 |  Loss: (0.1932) |  Loss2: (0.0000) | Acc: (93.00%) (2503/2688)
Epoch: 111 | Batch_idx: 30 |  Loss: (0.1921) |  Loss2: (0.0000) | Acc: (93.00%) (3703/3968)
Epoch: 111 | Batch_idx: 40 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (4918/5248)
Epoch: 111 | Batch_idx: 50 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (93.00%) (6112/6528)
Epoch: 111 | Batch_idx: 60 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (7301/7808)
Epoch: 111 | Batch_idx: 70 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (8491/9088)
Epoch: 111 | Batch_idx: 80 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (9684/10368)
Epoch: 111 | Batch_idx: 90 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (10882/11648)
Epoch: 111 | Batch_idx: 100 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (12091/12928)
Epoch: 111 | Batch_idx: 110 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (13287/14208)
Epoch: 111 | Batch_idx: 120 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (14490/15488)
Epoch: 111 | Batch_idx: 130 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (15684/16768)
Epoch: 111 | Batch_idx: 140 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (16883/18048)
Epoch: 111 | Batch_idx: 150 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (18076/19328)
Epoch: 111 | Batch_idx: 160 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (19276/20608)
Epoch: 111 | Batch_idx: 170 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (20473/21888)
Epoch: 111 | Batch_idx: 180 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (21678/23168)
Epoch: 111 | Batch_idx: 190 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (22875/24448)
Epoch: 111 | Batch_idx: 200 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (24074/25728)
Epoch: 111 | Batch_idx: 210 |  Loss: (0.1887) |  Loss2: (0.0000) | Acc: (93.00%) (25275/27008)
Epoch: 111 | Batch_idx: 220 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (26472/28288)
Epoch: 111 | Batch_idx: 230 |  Loss: (0.1882) |  Loss2: (0.0000) | Acc: (93.00%) (27677/29568)
Epoch: 111 | Batch_idx: 240 |  Loss: (0.1883) |  Loss2: (0.0000) | Acc: (93.00%) (28868/30848)
Epoch: 111 | Batch_idx: 250 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (30072/32128)
Epoch: 111 | Batch_idx: 260 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (31265/33408)
Epoch: 111 | Batch_idx: 270 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (32459/34688)
Epoch: 111 | Batch_idx: 280 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (33660/35968)
Epoch: 111 | Batch_idx: 290 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (34835/37248)
Epoch: 111 | Batch_idx: 300 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (36025/38528)
Epoch: 111 | Batch_idx: 310 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (37215/39808)
Epoch: 111 | Batch_idx: 320 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (38406/41088)
Epoch: 111 | Batch_idx: 330 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (39606/42368)
Epoch: 111 | Batch_idx: 340 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (40807/43648)
Epoch: 111 | Batch_idx: 350 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (42005/44928)
Epoch: 111 | Batch_idx: 360 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (43196/46208)
Epoch: 111 | Batch_idx: 370 |  Loss: (0.1912) |  Loss2: (0.0000) | Acc: (93.00%) (44387/47488)
Epoch: 111 | Batch_idx: 380 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (45590/48768)
Epoch: 111 | Batch_idx: 390 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (46733/50000)
# TEST : Loss: (0.4527) | Acc: (86.00%) (8646/10000)
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.5699, 0.4301], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.6721, 0.3279], device='cuda:0')
percent tensor([0.6857, 0.3143], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 112 | Batch_idx: 0 |  Loss: (0.2003) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 112 | Batch_idx: 10 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 112 | Batch_idx: 20 |  Loss: (0.1853) |  Loss2: (0.0000) | Acc: (93.00%) (2518/2688)
Epoch: 112 | Batch_idx: 30 |  Loss: (0.1860) |  Loss2: (0.0000) | Acc: (93.00%) (3712/3968)
Epoch: 112 | Batch_idx: 40 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (4899/5248)
Epoch: 112 | Batch_idx: 50 |  Loss: (0.1917) |  Loss2: (0.0000) | Acc: (93.00%) (6105/6528)
Epoch: 112 | Batch_idx: 60 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (7304/7808)
Epoch: 112 | Batch_idx: 70 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (8500/9088)
Epoch: 112 | Batch_idx: 80 |  Loss: (0.1895) |  Loss2: (0.0000) | Acc: (93.00%) (9704/10368)
Epoch: 112 | Batch_idx: 90 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (10899/11648)
Epoch: 112 | Batch_idx: 100 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (12090/12928)
Epoch: 112 | Batch_idx: 110 |  Loss: (0.1875) |  Loss2: (0.0000) | Acc: (93.00%) (13284/14208)
Epoch: 112 | Batch_idx: 120 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (14464/15488)
Epoch: 112 | Batch_idx: 130 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (15666/16768)
Epoch: 112 | Batch_idx: 140 |  Loss: (0.1884) |  Loss2: (0.0000) | Acc: (93.00%) (16858/18048)
Epoch: 112 | Batch_idx: 150 |  Loss: (0.1905) |  Loss2: (0.0000) | Acc: (93.00%) (18043/19328)
Epoch: 112 | Batch_idx: 160 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (19240/20608)
Epoch: 112 | Batch_idx: 170 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (20438/21888)
Epoch: 112 | Batch_idx: 180 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (21631/23168)
Epoch: 112 | Batch_idx: 190 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (22833/24448)
Epoch: 112 | Batch_idx: 200 |  Loss: (0.1906) |  Loss2: (0.0000) | Acc: (93.00%) (24031/25728)
Epoch: 112 | Batch_idx: 210 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (25232/27008)
Epoch: 112 | Batch_idx: 220 |  Loss: (0.1896) |  Loss2: (0.0000) | Acc: (93.00%) (26427/28288)
Epoch: 112 | Batch_idx: 230 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (27619/29568)
Epoch: 112 | Batch_idx: 240 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (28826/30848)
Epoch: 112 | Batch_idx: 250 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (30018/32128)
Epoch: 112 | Batch_idx: 260 |  Loss: (0.1897) |  Loss2: (0.0000) | Acc: (93.00%) (31219/33408)
Epoch: 112 | Batch_idx: 270 |  Loss: (0.1904) |  Loss2: (0.0000) | Acc: (93.00%) (32404/34688)
Epoch: 112 | Batch_idx: 280 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (33598/35968)
Epoch: 112 | Batch_idx: 290 |  Loss: (0.1909) |  Loss2: (0.0000) | Acc: (93.00%) (34787/37248)
Epoch: 112 | Batch_idx: 300 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (35980/38528)
Epoch: 112 | Batch_idx: 310 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (37175/39808)
Epoch: 112 | Batch_idx: 320 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (38370/41088)
Epoch: 112 | Batch_idx: 330 |  Loss: (0.1918) |  Loss2: (0.0000) | Acc: (93.00%) (39562/42368)
Epoch: 112 | Batch_idx: 340 |  Loss: (0.1911) |  Loss2: (0.0000) | Acc: (93.00%) (40774/43648)
Epoch: 112 | Batch_idx: 350 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (41960/44928)
Epoch: 112 | Batch_idx: 360 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (43155/46208)
Epoch: 112 | Batch_idx: 370 |  Loss: (0.1910) |  Loss2: (0.0000) | Acc: (93.00%) (44350/47488)
Epoch: 112 | Batch_idx: 380 |  Loss: (0.1913) |  Loss2: (0.0000) | Acc: (93.00%) (45546/48768)
Epoch: 112 | Batch_idx: 390 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (46689/50000)
# TEST : Loss: (0.4007) | Acc: (87.00%) (8781/10000)
percent tensor([0.5858, 0.4142], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5467, 0.4533], device='cuda:0')
percent tensor([0.6235, 0.3765], device='cuda:0')
percent tensor([0.5915, 0.4085], device='cuda:0')
percent tensor([0.6718, 0.3282], device='cuda:0')
percent tensor([0.6915, 0.3085], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 113 | Batch_idx: 0 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 113 | Batch_idx: 10 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (1335/1408)
Epoch: 113 | Batch_idx: 20 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (2544/2688)
Epoch: 113 | Batch_idx: 30 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (3734/3968)
Epoch: 113 | Batch_idx: 40 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (4935/5248)
Epoch: 113 | Batch_idx: 50 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (6132/6528)
Epoch: 113 | Batch_idx: 60 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (7327/7808)
Epoch: 113 | Batch_idx: 70 |  Loss: (0.1787) |  Loss2: (0.0000) | Acc: (93.00%) (8524/9088)
Epoch: 113 | Batch_idx: 80 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (9719/10368)
Epoch: 113 | Batch_idx: 90 |  Loss: (0.1812) |  Loss2: (0.0000) | Acc: (93.00%) (10921/11648)
Epoch: 113 | Batch_idx: 100 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (12129/12928)
Epoch: 113 | Batch_idx: 110 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (13317/14208)
Epoch: 113 | Batch_idx: 120 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (14513/15488)
Epoch: 113 | Batch_idx: 130 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (15708/16768)
Epoch: 113 | Batch_idx: 140 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (16901/18048)
Epoch: 113 | Batch_idx: 150 |  Loss: (0.1848) |  Loss2: (0.0000) | Acc: (93.00%) (18109/19328)
Epoch: 113 | Batch_idx: 160 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (19312/20608)
Epoch: 113 | Batch_idx: 170 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (20514/21888)
Epoch: 113 | Batch_idx: 180 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (21722/23168)
Epoch: 113 | Batch_idx: 190 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (22926/24448)
Epoch: 113 | Batch_idx: 200 |  Loss: (0.1845) |  Loss2: (0.0000) | Acc: (93.00%) (24107/25728)
Epoch: 113 | Batch_idx: 210 |  Loss: (0.1842) |  Loss2: (0.0000) | Acc: (93.00%) (25312/27008)
Epoch: 113 | Batch_idx: 220 |  Loss: (0.1851) |  Loss2: (0.0000) | Acc: (93.00%) (26505/28288)
Epoch: 113 | Batch_idx: 230 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (27715/29568)
Epoch: 113 | Batch_idx: 240 |  Loss: (0.1841) |  Loss2: (0.0000) | Acc: (93.00%) (28917/30848)
Epoch: 113 | Batch_idx: 250 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (30119/32128)
Epoch: 113 | Batch_idx: 260 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (31316/33408)
Epoch: 113 | Batch_idx: 270 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (32511/34688)
Epoch: 113 | Batch_idx: 280 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (33722/35968)
Epoch: 113 | Batch_idx: 290 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (34920/37248)
Epoch: 113 | Batch_idx: 300 |  Loss: (0.1827) |  Loss2: (0.0000) | Acc: (93.00%) (36129/38528)
Epoch: 113 | Batch_idx: 310 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (37320/39808)
Epoch: 113 | Batch_idx: 320 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (38524/41088)
Epoch: 113 | Batch_idx: 330 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (39735/42368)
Epoch: 113 | Batch_idx: 340 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (40931/43648)
Epoch: 113 | Batch_idx: 350 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (42125/44928)
Epoch: 113 | Batch_idx: 360 |  Loss: (0.1823) |  Loss2: (0.0000) | Acc: (93.00%) (43321/46208)
Epoch: 113 | Batch_idx: 370 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (44503/47488)
Epoch: 113 | Batch_idx: 380 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (45711/48768)
Epoch: 113 | Batch_idx: 390 |  Loss: (0.1836) |  Loss2: (0.0000) | Acc: (93.00%) (46853/50000)
# TEST : Loss: (0.4444) | Acc: (86.00%) (8652/10000)
percent tensor([0.5853, 0.4147], device='cuda:0')
percent tensor([0.5701, 0.4299], device='cuda:0')
percent tensor([0.5465, 0.4535], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6730, 0.3270], device='cuda:0')
percent tensor([0.6910, 0.3090], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 114 | Batch_idx: 0 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 114 | Batch_idx: 10 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (1317/1408)
Epoch: 114 | Batch_idx: 20 |  Loss: (0.1714) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 114 | Batch_idx: 30 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 114 | Batch_idx: 40 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (4929/5248)
Epoch: 114 | Batch_idx: 50 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (6117/6528)
Epoch: 114 | Batch_idx: 60 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (7332/7808)
Epoch: 114 | Batch_idx: 70 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (8538/9088)
Epoch: 114 | Batch_idx: 80 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (9744/10368)
Epoch: 114 | Batch_idx: 90 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 114 | Batch_idx: 100 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (12140/12928)
Epoch: 114 | Batch_idx: 110 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (13343/14208)
Epoch: 114 | Batch_idx: 120 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 114 | Batch_idx: 130 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (15744/16768)
Epoch: 114 | Batch_idx: 140 |  Loss: (0.1713) |  Loss2: (0.0000) | Acc: (93.00%) (16961/18048)
Epoch: 114 | Batch_idx: 150 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (93.00%) (18161/19328)
Epoch: 114 | Batch_idx: 160 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (19368/20608)
Epoch: 114 | Batch_idx: 170 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 114 | Batch_idx: 180 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (21764/23168)
Epoch: 114 | Batch_idx: 190 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (22976/24448)
Epoch: 114 | Batch_idx: 200 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (93.00%) (24178/25728)
Epoch: 114 | Batch_idx: 210 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (25372/27008)
Epoch: 114 | Batch_idx: 220 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (26579/28288)
Epoch: 114 | Batch_idx: 230 |  Loss: (0.1744) |  Loss2: (0.0000) | Acc: (93.00%) (27784/29568)
Epoch: 114 | Batch_idx: 240 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (28991/30848)
Epoch: 114 | Batch_idx: 250 |  Loss: (0.1751) |  Loss2: (0.0000) | Acc: (93.00%) (30196/32128)
Epoch: 114 | Batch_idx: 260 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (31403/33408)
Epoch: 114 | Batch_idx: 270 |  Loss: (0.1754) |  Loss2: (0.0000) | Acc: (93.00%) (32602/34688)
Epoch: 114 | Batch_idx: 280 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (33803/35968)
Epoch: 114 | Batch_idx: 290 |  Loss: (0.1759) |  Loss2: (0.0000) | Acc: (93.00%) (35004/37248)
Epoch: 114 | Batch_idx: 300 |  Loss: (0.1764) |  Loss2: (0.0000) | Acc: (93.00%) (36199/38528)
Epoch: 114 | Batch_idx: 310 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (37402/39808)
Epoch: 114 | Batch_idx: 320 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (38603/41088)
Epoch: 114 | Batch_idx: 330 |  Loss: (0.1773) |  Loss2: (0.0000) | Acc: (93.00%) (39796/42368)
Epoch: 114 | Batch_idx: 340 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (41006/43648)
Epoch: 114 | Batch_idx: 350 |  Loss: (0.1772) |  Loss2: (0.0000) | Acc: (93.00%) (42203/44928)
Epoch: 114 | Batch_idx: 360 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (43397/46208)
Epoch: 114 | Batch_idx: 370 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (44594/47488)
Epoch: 114 | Batch_idx: 380 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (45782/48768)
Epoch: 114 | Batch_idx: 390 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (46943/50000)
# TEST : Loss: (0.4608) | Acc: (85.00%) (8583/10000)
percent tensor([0.5851, 0.4149], device='cuda:0')
percent tensor([0.5692, 0.4308], device='cuda:0')
percent tensor([0.5458, 0.4542], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5911, 0.4089], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6819, 0.3181], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 115 | Batch_idx: 0 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 115 | Batch_idx: 10 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 115 | Batch_idx: 20 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (2515/2688)
Epoch: 115 | Batch_idx: 30 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (3694/3968)
Epoch: 115 | Batch_idx: 40 |  Loss: (0.2024) |  Loss2: (0.0000) | Acc: (92.00%) (4877/5248)
Epoch: 115 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (92.00%) (6056/6528)
Epoch: 115 | Batch_idx: 60 |  Loss: (0.2107) |  Loss2: (0.0000) | Acc: (92.00%) (7245/7808)
Epoch: 115 | Batch_idx: 70 |  Loss: (0.2121) |  Loss2: (0.0000) | Acc: (92.00%) (8426/9088)
Epoch: 115 | Batch_idx: 80 |  Loss: (0.2126) |  Loss2: (0.0000) | Acc: (92.00%) (9609/10368)
Epoch: 115 | Batch_idx: 90 |  Loss: (0.2134) |  Loss2: (0.0000) | Acc: (92.00%) (10792/11648)
Epoch: 115 | Batch_idx: 100 |  Loss: (0.2133) |  Loss2: (0.0000) | Acc: (92.00%) (11972/12928)
Epoch: 115 | Batch_idx: 110 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (13167/14208)
Epoch: 115 | Batch_idx: 120 |  Loss: (0.2117) |  Loss2: (0.0000) | Acc: (92.00%) (14353/15488)
Epoch: 115 | Batch_idx: 130 |  Loss: (0.2144) |  Loss2: (0.0000) | Acc: (92.00%) (15526/16768)
Epoch: 115 | Batch_idx: 140 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (16696/18048)
Epoch: 115 | Batch_idx: 150 |  Loss: (0.2155) |  Loss2: (0.0000) | Acc: (92.00%) (17873/19328)
Epoch: 115 | Batch_idx: 160 |  Loss: (0.2171) |  Loss2: (0.0000) | Acc: (92.00%) (19046/20608)
Epoch: 115 | Batch_idx: 170 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (20236/21888)
Epoch: 115 | Batch_idx: 180 |  Loss: (0.2145) |  Loss2: (0.0000) | Acc: (92.00%) (21441/23168)
Epoch: 115 | Batch_idx: 190 |  Loss: (0.2150) |  Loss2: (0.0000) | Acc: (92.00%) (22625/24448)
Epoch: 115 | Batch_idx: 200 |  Loss: (0.2164) |  Loss2: (0.0000) | Acc: (92.00%) (23789/25728)
Epoch: 115 | Batch_idx: 210 |  Loss: (0.2157) |  Loss2: (0.0000) | Acc: (92.00%) (24973/27008)
Epoch: 115 | Batch_idx: 220 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (26149/28288)
Epoch: 115 | Batch_idx: 230 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (27332/29568)
Epoch: 115 | Batch_idx: 240 |  Loss: (0.2156) |  Loss2: (0.0000) | Acc: (92.00%) (28518/30848)
Epoch: 115 | Batch_idx: 250 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (29709/32128)
Epoch: 115 | Batch_idx: 260 |  Loss: (0.2160) |  Loss2: (0.0000) | Acc: (92.00%) (30891/33408)
Epoch: 115 | Batch_idx: 270 |  Loss: (0.2154) |  Loss2: (0.0000) | Acc: (92.00%) (32090/34688)
Epoch: 115 | Batch_idx: 280 |  Loss: (0.2153) |  Loss2: (0.0000) | Acc: (92.00%) (33274/35968)
Epoch: 115 | Batch_idx: 290 |  Loss: (0.2143) |  Loss2: (0.0000) | Acc: (92.00%) (34466/37248)
Epoch: 115 | Batch_idx: 300 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (35656/38528)
Epoch: 115 | Batch_idx: 310 |  Loss: (0.2140) |  Loss2: (0.0000) | Acc: (92.00%) (36846/39808)
Epoch: 115 | Batch_idx: 320 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (38036/41088)
Epoch: 115 | Batch_idx: 330 |  Loss: (0.2131) |  Loss2: (0.0000) | Acc: (92.00%) (39224/42368)
Epoch: 115 | Batch_idx: 340 |  Loss: (0.2136) |  Loss2: (0.0000) | Acc: (92.00%) (40399/43648)
Epoch: 115 | Batch_idx: 350 |  Loss: (0.2123) |  Loss2: (0.0000) | Acc: (92.00%) (41604/44928)
Epoch: 115 | Batch_idx: 360 |  Loss: (0.2115) |  Loss2: (0.0000) | Acc: (92.00%) (42805/46208)
Epoch: 115 | Batch_idx: 370 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (44001/47488)
Epoch: 115 | Batch_idx: 380 |  Loss: (0.2108) |  Loss2: (0.0000) | Acc: (92.00%) (45194/48768)
Epoch: 115 | Batch_idx: 390 |  Loss: (0.2109) |  Loss2: (0.0000) | Acc: (92.00%) (46338/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_115.pth.tar'
# TEST : Loss: (0.4362) | Acc: (86.00%) (8682/10000)
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5812, 0.4188], device='cuda:0')
percent tensor([0.5430, 0.4570], device='cuda:0')
percent tensor([0.6367, 0.3633], device='cuda:0')
percent tensor([0.5964, 0.4036], device='cuda:0')
percent tensor([0.6346, 0.3654], device='cuda:0')
percent tensor([0.6925, 0.3075], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 116 | Batch_idx: 0 |  Loss: (0.2311) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 116 | Batch_idx: 10 |  Loss: (0.2122) |  Loss2: (0.0000) | Acc: (92.00%) (1309/1408)
Epoch: 116 | Batch_idx: 20 |  Loss: (0.2255) |  Loss2: (0.0000) | Acc: (92.00%) (2489/2688)
Epoch: 116 | Batch_idx: 30 |  Loss: (0.2113) |  Loss2: (0.0000) | Acc: (92.00%) (3684/3968)
Epoch: 116 | Batch_idx: 40 |  Loss: (0.2093) |  Loss2: (0.0000) | Acc: (93.00%) (4883/5248)
Epoch: 116 | Batch_idx: 50 |  Loss: (0.2070) |  Loss2: (0.0000) | Acc: (93.00%) (6078/6528)
Epoch: 116 | Batch_idx: 60 |  Loss: (0.2034) |  Loss2: (0.0000) | Acc: (93.00%) (7273/7808)
Epoch: 116 | Batch_idx: 70 |  Loss: (0.2019) |  Loss2: (0.0000) | Acc: (93.00%) (8467/9088)
Epoch: 116 | Batch_idx: 80 |  Loss: (0.2014) |  Loss2: (0.0000) | Acc: (93.00%) (9652/10368)
Epoch: 116 | Batch_idx: 90 |  Loss: (0.2006) |  Loss2: (0.0000) | Acc: (93.00%) (10854/11648)
Epoch: 116 | Batch_idx: 100 |  Loss: (0.1986) |  Loss2: (0.0000) | Acc: (93.00%) (12051/12928)
Epoch: 116 | Batch_idx: 110 |  Loss: (0.1966) |  Loss2: (0.0000) | Acc: (93.00%) (13254/14208)
Epoch: 116 | Batch_idx: 120 |  Loss: (0.2000) |  Loss2: (0.0000) | Acc: (93.00%) (14435/15488)
Epoch: 116 | Batch_idx: 130 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (15623/16768)
Epoch: 116 | Batch_idx: 140 |  Loss: (0.1999) |  Loss2: (0.0000) | Acc: (93.00%) (16805/18048)
Epoch: 116 | Batch_idx: 150 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (17999/19328)
Epoch: 116 | Batch_idx: 160 |  Loss: (0.1991) |  Loss2: (0.0000) | Acc: (93.00%) (19190/20608)
Epoch: 116 | Batch_idx: 170 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (20395/21888)
Epoch: 116 | Batch_idx: 180 |  Loss: (0.1988) |  Loss2: (0.0000) | Acc: (93.00%) (21584/23168)
Epoch: 116 | Batch_idx: 190 |  Loss: (0.1983) |  Loss2: (0.0000) | Acc: (93.00%) (22783/24448)
Epoch: 116 | Batch_idx: 200 |  Loss: (0.1993) |  Loss2: (0.0000) | Acc: (93.00%) (23968/25728)
Epoch: 116 | Batch_idx: 210 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (25153/27008)
Epoch: 116 | Batch_idx: 220 |  Loss: (0.1987) |  Loss2: (0.0000) | Acc: (93.00%) (26360/28288)
Epoch: 116 | Batch_idx: 230 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (27569/29568)
Epoch: 116 | Batch_idx: 240 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (28752/30848)
Epoch: 116 | Batch_idx: 250 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (29950/32128)
Epoch: 116 | Batch_idx: 260 |  Loss: (0.1980) |  Loss2: (0.0000) | Acc: (93.00%) (31137/33408)
Epoch: 116 | Batch_idx: 270 |  Loss: (0.1972) |  Loss2: (0.0000) | Acc: (93.00%) (32339/34688)
Epoch: 116 | Batch_idx: 280 |  Loss: (0.1973) |  Loss2: (0.0000) | Acc: (93.00%) (33534/35968)
Epoch: 116 | Batch_idx: 290 |  Loss: (0.1977) |  Loss2: (0.0000) | Acc: (93.00%) (34713/37248)
Epoch: 116 | Batch_idx: 300 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (35905/38528)
Epoch: 116 | Batch_idx: 310 |  Loss: (0.1976) |  Loss2: (0.0000) | Acc: (93.00%) (37092/39808)
Epoch: 116 | Batch_idx: 320 |  Loss: (0.1974) |  Loss2: (0.0000) | Acc: (93.00%) (38291/41088)
Epoch: 116 | Batch_idx: 330 |  Loss: (0.1968) |  Loss2: (0.0000) | Acc: (93.00%) (39491/42368)
Epoch: 116 | Batch_idx: 340 |  Loss: (0.1967) |  Loss2: (0.0000) | Acc: (93.00%) (40688/43648)
Epoch: 116 | Batch_idx: 350 |  Loss: (0.1969) |  Loss2: (0.0000) | Acc: (93.00%) (41885/44928)
Epoch: 116 | Batch_idx: 360 |  Loss: (0.1963) |  Loss2: (0.0000) | Acc: (93.00%) (43087/46208)
Epoch: 116 | Batch_idx: 370 |  Loss: (0.1960) |  Loss2: (0.0000) | Acc: (93.00%) (44284/47488)
Epoch: 116 | Batch_idx: 380 |  Loss: (0.1965) |  Loss2: (0.0000) | Acc: (93.00%) (45467/48768)
Epoch: 116 | Batch_idx: 390 |  Loss: (0.1961) |  Loss2: (0.0000) | Acc: (93.00%) (46626/50000)
# TEST : Loss: (0.4177) | Acc: (87.00%) (8742/10000)
percent tensor([0.5756, 0.4244], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6373, 0.3627], device='cuda:0')
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.6430, 0.3570], device='cuda:0')
percent tensor([0.6937, 0.3063], device='cuda:0')
percent tensor([0.9995, 0.0005], device='cuda:0')
Epoch: 117 | Batch_idx: 0 |  Loss: (0.1881) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 117 | Batch_idx: 10 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (1321/1408)
Epoch: 117 | Batch_idx: 20 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (2540/2688)
Epoch: 117 | Batch_idx: 30 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 117 | Batch_idx: 40 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 117 | Batch_idx: 50 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (6143/6528)
Epoch: 117 | Batch_idx: 60 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (7348/7808)
Epoch: 117 | Batch_idx: 70 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (8547/9088)
Epoch: 117 | Batch_idx: 80 |  Loss: (0.1770) |  Loss2: (0.0000) | Acc: (93.00%) (9737/10368)
Epoch: 117 | Batch_idx: 90 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (10927/11648)
Epoch: 117 | Batch_idx: 100 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (12128/12928)
Epoch: 117 | Batch_idx: 110 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (13328/14208)
Epoch: 117 | Batch_idx: 120 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (14530/15488)
Epoch: 117 | Batch_idx: 130 |  Loss: (0.1795) |  Loss2: (0.0000) | Acc: (93.00%) (15722/16768)
Epoch: 117 | Batch_idx: 140 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (16933/18048)
Epoch: 117 | Batch_idx: 150 |  Loss: (0.1771) |  Loss2: (0.0000) | Acc: (93.00%) (18151/19328)
Epoch: 117 | Batch_idx: 160 |  Loss: (0.1774) |  Loss2: (0.0000) | Acc: (93.00%) (19346/20608)
Epoch: 117 | Batch_idx: 170 |  Loss: (0.1762) |  Loss2: (0.0000) | Acc: (93.00%) (20557/21888)
Epoch: 117 | Batch_idx: 180 |  Loss: (0.1758) |  Loss2: (0.0000) | Acc: (93.00%) (21764/23168)
Epoch: 117 | Batch_idx: 190 |  Loss: (0.1780) |  Loss2: (0.0000) | Acc: (93.00%) (22944/24448)
Epoch: 117 | Batch_idx: 200 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (24155/25728)
Epoch: 117 | Batch_idx: 210 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (25349/27008)
Epoch: 117 | Batch_idx: 220 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (26540/28288)
Epoch: 117 | Batch_idx: 230 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (27733/29568)
Epoch: 117 | Batch_idx: 240 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (28934/30848)
Epoch: 117 | Batch_idx: 250 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (30142/32128)
Epoch: 117 | Batch_idx: 260 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (31352/33408)
Epoch: 117 | Batch_idx: 270 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (32561/34688)
Epoch: 117 | Batch_idx: 280 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (33751/35968)
Epoch: 117 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34955/37248)
Epoch: 117 | Batch_idx: 300 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (36150/38528)
Epoch: 117 | Batch_idx: 310 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (37355/39808)
Epoch: 117 | Batch_idx: 320 |  Loss: (0.1807) |  Loss2: (0.0000) | Acc: (93.00%) (38549/41088)
Epoch: 117 | Batch_idx: 330 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (39745/42368)
Epoch: 117 | Batch_idx: 340 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (40917/43648)
Epoch: 117 | Batch_idx: 350 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (42096/44928)
Epoch: 117 | Batch_idx: 360 |  Loss: (0.1834) |  Loss2: (0.0000) | Acc: (93.00%) (43293/46208)
Epoch: 117 | Batch_idx: 370 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (44496/47488)
Epoch: 117 | Batch_idx: 380 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (45680/48768)
Epoch: 117 | Batch_idx: 390 |  Loss: (0.1840) |  Loss2: (0.0000) | Acc: (93.00%) (46837/50000)
# TEST : Loss: (0.4097) | Acc: (87.00%) (8766/10000)
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.5846, 0.4154], device='cuda:0')
percent tensor([0.5458, 0.4542], device='cuda:0')
percent tensor([0.6343, 0.3657], device='cuda:0')
percent tensor([0.5947, 0.4053], device='cuda:0')
percent tensor([0.6441, 0.3559], device='cuda:0')
percent tensor([0.7062, 0.2938], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 118 | Batch_idx: 0 |  Loss: (0.1886) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 118 | Batch_idx: 10 |  Loss: (0.2082) |  Loss2: (0.0000) | Acc: (92.00%) (1306/1408)
Epoch: 118 | Batch_idx: 20 |  Loss: (0.1985) |  Loss2: (0.0000) | Acc: (93.00%) (2501/2688)
Epoch: 118 | Batch_idx: 30 |  Loss: (0.1971) |  Loss2: (0.0000) | Acc: (93.00%) (3697/3968)
Epoch: 118 | Batch_idx: 40 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (4905/5248)
Epoch: 118 | Batch_idx: 50 |  Loss: (0.1873) |  Loss2: (0.0000) | Acc: (93.00%) (6113/6528)
Epoch: 118 | Batch_idx: 60 |  Loss: (0.1870) |  Loss2: (0.0000) | Acc: (93.00%) (7305/7808)
Epoch: 118 | Batch_idx: 70 |  Loss: (0.1877) |  Loss2: (0.0000) | Acc: (93.00%) (8511/9088)
Epoch: 118 | Batch_idx: 80 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (9724/10368)
Epoch: 118 | Batch_idx: 90 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (10934/11648)
Epoch: 118 | Batch_idx: 100 |  Loss: (0.1833) |  Loss2: (0.0000) | Acc: (93.00%) (12137/12928)
Epoch: 118 | Batch_idx: 110 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (13336/14208)
Epoch: 118 | Batch_idx: 120 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (14541/15488)
Epoch: 118 | Batch_idx: 130 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (15760/16768)
Epoch: 118 | Batch_idx: 140 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (94.00%) (16970/18048)
Epoch: 118 | Batch_idx: 150 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (18164/19328)
Epoch: 118 | Batch_idx: 160 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (19358/20608)
Epoch: 118 | Batch_idx: 170 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (20553/21888)
Epoch: 118 | Batch_idx: 180 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (21753/23168)
Epoch: 118 | Batch_idx: 190 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (22957/24448)
Epoch: 118 | Batch_idx: 200 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (24159/25728)
Epoch: 118 | Batch_idx: 210 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (25363/27008)
Epoch: 118 | Batch_idx: 220 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (26560/28288)
Epoch: 118 | Batch_idx: 230 |  Loss: (0.1809) |  Loss2: (0.0000) | Acc: (93.00%) (27760/29568)
Epoch: 118 | Batch_idx: 240 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (28960/30848)
Epoch: 118 | Batch_idx: 250 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (30158/32128)
Epoch: 118 | Batch_idx: 260 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (31363/33408)
Epoch: 118 | Batch_idx: 270 |  Loss: (0.1810) |  Loss2: (0.0000) | Acc: (93.00%) (32545/34688)
Epoch: 118 | Batch_idx: 280 |  Loss: (0.1806) |  Loss2: (0.0000) | Acc: (93.00%) (33753/35968)
Epoch: 118 | Batch_idx: 290 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (34956/37248)
Epoch: 118 | Batch_idx: 300 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (36151/38528)
Epoch: 118 | Batch_idx: 310 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (37367/39808)
Epoch: 118 | Batch_idx: 320 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (38545/41088)
Epoch: 118 | Batch_idx: 330 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (39748/42368)
Epoch: 118 | Batch_idx: 340 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (40961/43648)
Epoch: 118 | Batch_idx: 350 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (42163/44928)
Epoch: 118 | Batch_idx: 360 |  Loss: (0.1792) |  Loss2: (0.0000) | Acc: (93.00%) (43364/46208)
Epoch: 118 | Batch_idx: 370 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (44575/47488)
Epoch: 118 | Batch_idx: 380 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (45778/48768)
Epoch: 118 | Batch_idx: 390 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (46942/50000)
# TEST : Loss: (0.4041) | Acc: (87.00%) (8769/10000)
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.5459, 0.4541], device='cuda:0')
percent tensor([0.6364, 0.3636], device='cuda:0')
percent tensor([0.5946, 0.4054], device='cuda:0')
percent tensor([0.6421, 0.3579], device='cuda:0')
percent tensor([0.7099, 0.2901], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 119 | Batch_idx: 0 |  Loss: (0.1825) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 119 | Batch_idx: 10 |  Loss: (0.1992) |  Loss2: (0.0000) | Acc: (93.00%) (1314/1408)
Epoch: 119 | Batch_idx: 20 |  Loss: (0.1878) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 119 | Batch_idx: 30 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (3724/3968)
Epoch: 119 | Batch_idx: 40 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (4930/5248)
Epoch: 119 | Batch_idx: 50 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (6123/6528)
Epoch: 119 | Batch_idx: 60 |  Loss: (0.1777) |  Loss2: (0.0000) | Acc: (93.00%) (7335/7808)
Epoch: 119 | Batch_idx: 70 |  Loss: (0.1761) |  Loss2: (0.0000) | Acc: (93.00%) (8541/9088)
Epoch: 119 | Batch_idx: 80 |  Loss: (0.1760) |  Loss2: (0.0000) | Acc: (94.00%) (9749/10368)
Epoch: 119 | Batch_idx: 90 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (10957/11648)
Epoch: 119 | Batch_idx: 100 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (12170/12928)
Epoch: 119 | Batch_idx: 110 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (13379/14208)
Epoch: 119 | Batch_idx: 120 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (14586/15488)
Epoch: 119 | Batch_idx: 130 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (15775/16768)
Epoch: 119 | Batch_idx: 140 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (16995/18048)
Epoch: 119 | Batch_idx: 150 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (18194/19328)
Epoch: 119 | Batch_idx: 160 |  Loss: (0.1747) |  Loss2: (0.0000) | Acc: (94.00%) (19381/20608)
Epoch: 119 | Batch_idx: 170 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (94.00%) (20577/21888)
Epoch: 119 | Batch_idx: 180 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (94.00%) (21793/23168)
Epoch: 119 | Batch_idx: 190 |  Loss: (0.1740) |  Loss2: (0.0000) | Acc: (94.00%) (22998/24448)
Epoch: 119 | Batch_idx: 200 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (24220/25728)
Epoch: 119 | Batch_idx: 210 |  Loss: (0.1719) |  Loss2: (0.0000) | Acc: (94.00%) (25436/27008)
Epoch: 119 | Batch_idx: 220 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (26634/28288)
Epoch: 119 | Batch_idx: 230 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (27847/29568)
Epoch: 119 | Batch_idx: 240 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (29057/30848)
Epoch: 119 | Batch_idx: 250 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (94.00%) (30249/32128)
Epoch: 119 | Batch_idx: 260 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (94.00%) (31457/33408)
Epoch: 119 | Batch_idx: 270 |  Loss: (0.1723) |  Loss2: (0.0000) | Acc: (94.00%) (32665/34688)
Epoch: 119 | Batch_idx: 280 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (33865/35968)
Epoch: 119 | Batch_idx: 290 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (35069/37248)
Epoch: 119 | Batch_idx: 300 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (36269/38528)
Epoch: 119 | Batch_idx: 310 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (37467/39808)
Epoch: 119 | Batch_idx: 320 |  Loss: (0.1737) |  Loss2: (0.0000) | Acc: (94.00%) (38656/41088)
Epoch: 119 | Batch_idx: 330 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (39867/42368)
Epoch: 119 | Batch_idx: 340 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (41075/43648)
Epoch: 119 | Batch_idx: 350 |  Loss: (0.1733) |  Loss2: (0.0000) | Acc: (94.00%) (42277/44928)
Epoch: 119 | Batch_idx: 360 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (43493/46208)
Epoch: 119 | Batch_idx: 370 |  Loss: (0.1735) |  Loss2: (0.0000) | Acc: (94.00%) (44682/47488)
Epoch: 119 | Batch_idx: 380 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (45909/48768)
Epoch: 119 | Batch_idx: 390 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (47083/50000)
# TEST : Loss: (0.4025) | Acc: (87.00%) (8770/10000)
percent tensor([0.5766, 0.4234], device='cuda:0')
percent tensor([0.5806, 0.4194], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.5937, 0.4063], device='cuda:0')
percent tensor([0.6510, 0.3490], device='cuda:0')
percent tensor([0.7150, 0.2850], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 120 | Batch_idx: 0 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 120 | Batch_idx: 10 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 120 | Batch_idx: 20 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (2545/2688)
Epoch: 120 | Batch_idx: 30 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 120 | Batch_idx: 40 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (4956/5248)
Epoch: 120 | Batch_idx: 50 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 120 | Batch_idx: 60 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (7369/7808)
Epoch: 120 | Batch_idx: 70 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (8577/9088)
Epoch: 120 | Batch_idx: 80 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (9784/10368)
Epoch: 120 | Batch_idx: 90 |  Loss: (0.1680) |  Loss2: (0.0000) | Acc: (94.00%) (10979/11648)
Epoch: 120 | Batch_idx: 100 |  Loss: (0.1686) |  Loss2: (0.0000) | Acc: (94.00%) (12184/12928)
Epoch: 120 | Batch_idx: 110 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (13389/14208)
Epoch: 120 | Batch_idx: 120 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (14588/15488)
Epoch: 120 | Batch_idx: 130 |  Loss: (0.1720) |  Loss2: (0.0000) | Acc: (94.00%) (15769/16768)
Epoch: 120 | Batch_idx: 140 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (16964/18048)
Epoch: 120 | Batch_idx: 150 |  Loss: (0.1745) |  Loss2: (0.0000) | Acc: (93.00%) (18155/19328)
Epoch: 120 | Batch_idx: 160 |  Loss: (0.1743) |  Loss2: (0.0000) | Acc: (93.00%) (19357/20608)
Epoch: 120 | Batch_idx: 170 |  Loss: (0.1738) |  Loss2: (0.0000) | Acc: (93.00%) (20561/21888)
Epoch: 120 | Batch_idx: 180 |  Loss: (0.1739) |  Loss2: (0.0000) | Acc: (93.00%) (21773/23168)
Epoch: 120 | Batch_idx: 190 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (22969/24448)
Epoch: 120 | Batch_idx: 200 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (24180/25728)
Epoch: 120 | Batch_idx: 210 |  Loss: (0.1755) |  Loss2: (0.0000) | Acc: (93.00%) (25370/27008)
Epoch: 120 | Batch_idx: 220 |  Loss: (0.1752) |  Loss2: (0.0000) | Acc: (93.00%) (26571/28288)
Epoch: 120 | Batch_idx: 230 |  Loss: (0.1765) |  Loss2: (0.0000) | Acc: (93.00%) (27760/29568)
Epoch: 120 | Batch_idx: 240 |  Loss: (0.1775) |  Loss2: (0.0000) | Acc: (93.00%) (28958/30848)
Epoch: 120 | Batch_idx: 250 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (30136/32128)
Epoch: 120 | Batch_idx: 260 |  Loss: (0.1786) |  Loss2: (0.0000) | Acc: (93.00%) (31346/33408)
Epoch: 120 | Batch_idx: 270 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (32531/34688)
Epoch: 120 | Batch_idx: 280 |  Loss: (0.1796) |  Loss2: (0.0000) | Acc: (93.00%) (33738/35968)
Epoch: 120 | Batch_idx: 290 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (34935/37248)
Epoch: 120 | Batch_idx: 300 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (36131/38528)
Epoch: 120 | Batch_idx: 310 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (37324/39808)
Epoch: 120 | Batch_idx: 320 |  Loss: (0.1800) |  Loss2: (0.0000) | Acc: (93.00%) (38539/41088)
Epoch: 120 | Batch_idx: 330 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (39759/42368)
Epoch: 120 | Batch_idx: 340 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (40960/43648)
Epoch: 120 | Batch_idx: 350 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (42147/44928)
Epoch: 120 | Batch_idx: 360 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (43359/46208)
Epoch: 120 | Batch_idx: 370 |  Loss: (0.1798) |  Loss2: (0.0000) | Acc: (93.00%) (44543/47488)
Epoch: 120 | Batch_idx: 380 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (45726/48768)
Epoch: 120 | Batch_idx: 390 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (46889/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_120.pth.tar'
# TEST : Loss: (0.4365) | Acc: (86.00%) (8696/10000)
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5475, 0.4525], device='cuda:0')
percent tensor([0.6358, 0.3642], device='cuda:0')
percent tensor([0.5941, 0.4059], device='cuda:0')
percent tensor([0.6524, 0.3476], device='cuda:0')
percent tensor([0.7171, 0.2829], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(180.4536, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(818.5090, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(816.9260, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1523.0393, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(490.3608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2259.7393, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4275.3574, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1383.9344, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6180.3467, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11801.0225, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3901.3828, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16505.6094, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 121 | Batch_idx: 0 |  Loss: (0.2271) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 121 | Batch_idx: 10 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 121 | Batch_idx: 20 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (94.00%) (2529/2688)
Epoch: 121 | Batch_idx: 30 |  Loss: (0.1666) |  Loss2: (0.0000) | Acc: (94.00%) (3741/3968)
Epoch: 121 | Batch_idx: 40 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (93.00%) (4933/5248)
Epoch: 121 | Batch_idx: 50 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (93.00%) (6135/6528)
Epoch: 121 | Batch_idx: 60 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (7351/7808)
Epoch: 121 | Batch_idx: 70 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 121 | Batch_idx: 80 |  Loss: (0.1703) |  Loss2: (0.0000) | Acc: (94.00%) (9760/10368)
Epoch: 121 | Batch_idx: 90 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (10975/11648)
Epoch: 121 | Batch_idx: 100 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (12177/12928)
Epoch: 121 | Batch_idx: 110 |  Loss: (0.1715) |  Loss2: (0.0000) | Acc: (94.00%) (13366/14208)
Epoch: 121 | Batch_idx: 120 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (14561/15488)
Epoch: 121 | Batch_idx: 130 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (15768/16768)
Epoch: 121 | Batch_idx: 140 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (94.00%) (16969/18048)
Epoch: 121 | Batch_idx: 150 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (18177/19328)
Epoch: 121 | Batch_idx: 160 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (94.00%) (19379/20608)
Epoch: 121 | Batch_idx: 170 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (20579/21888)
Epoch: 121 | Batch_idx: 180 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (21795/23168)
Epoch: 121 | Batch_idx: 190 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (22990/24448)
Epoch: 121 | Batch_idx: 200 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (94.00%) (24197/25728)
Epoch: 121 | Batch_idx: 210 |  Loss: (0.1718) |  Loss2: (0.0000) | Acc: (94.00%) (25397/27008)
Epoch: 121 | Batch_idx: 220 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (94.00%) (26607/28288)
Epoch: 121 | Batch_idx: 230 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (94.00%) (27796/29568)
Epoch: 121 | Batch_idx: 240 |  Loss: (0.1736) |  Loss2: (0.0000) | Acc: (93.00%) (28994/30848)
Epoch: 121 | Batch_idx: 250 |  Loss: (0.1731) |  Loss2: (0.0000) | Acc: (94.00%) (30202/32128)
Epoch: 121 | Batch_idx: 260 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (94.00%) (31409/33408)
Epoch: 121 | Batch_idx: 270 |  Loss: (0.1727) |  Loss2: (0.0000) | Acc: (94.00%) (32607/34688)
Epoch: 121 | Batch_idx: 280 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (33807/35968)
Epoch: 121 | Batch_idx: 290 |  Loss: (0.1724) |  Loss2: (0.0000) | Acc: (93.00%) (35009/37248)
Epoch: 121 | Batch_idx: 300 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (36198/38528)
Epoch: 121 | Batch_idx: 310 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (37399/39808)
Epoch: 121 | Batch_idx: 320 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (38610/41088)
Epoch: 121 | Batch_idx: 330 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (39807/42368)
Epoch: 121 | Batch_idx: 340 |  Loss: (0.1732) |  Loss2: (0.0000) | Acc: (93.00%) (41008/43648)
Epoch: 121 | Batch_idx: 350 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (42192/44928)
Epoch: 121 | Batch_idx: 360 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (43389/46208)
Epoch: 121 | Batch_idx: 370 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (44584/47488)
Epoch: 121 | Batch_idx: 380 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (45788/48768)
Epoch: 121 | Batch_idx: 390 |  Loss: (0.1746) |  Loss2: (0.0000) | Acc: (93.00%) (46946/50000)
# TEST : Loss: (0.4210) | Acc: (87.00%) (8733/10000)
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5478, 0.4522], device='cuda:0')
percent tensor([0.6348, 0.3652], device='cuda:0')
percent tensor([0.5936, 0.4064], device='cuda:0')
percent tensor([0.6515, 0.3485], device='cuda:0')
percent tensor([0.7168, 0.2832], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 122 | Batch_idx: 0 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 122 | Batch_idx: 10 |  Loss: (0.1864) |  Loss2: (0.0000) | Acc: (93.00%) (1313/1408)
Epoch: 122 | Batch_idx: 20 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 122 | Batch_idx: 30 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (3736/3968)
Epoch: 122 | Batch_idx: 40 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (4952/5248)
Epoch: 122 | Batch_idx: 50 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (6145/6528)
Epoch: 122 | Batch_idx: 60 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (7349/7808)
Epoch: 122 | Batch_idx: 70 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (8557/9088)
Epoch: 122 | Batch_idx: 80 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (9778/10368)
Epoch: 122 | Batch_idx: 90 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (10995/11648)
Epoch: 122 | Batch_idx: 100 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (12200/12928)
Epoch: 122 | Batch_idx: 110 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (13408/14208)
Epoch: 122 | Batch_idx: 120 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (14590/15488)
Epoch: 122 | Batch_idx: 130 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (15788/16768)
Epoch: 122 | Batch_idx: 140 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (16987/18048)
Epoch: 122 | Batch_idx: 150 |  Loss: (0.1691) |  Loss2: (0.0000) | Acc: (94.00%) (18187/19328)
Epoch: 122 | Batch_idx: 160 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (19385/20608)
Epoch: 122 | Batch_idx: 170 |  Loss: (0.1675) |  Loss2: (0.0000) | Acc: (94.00%) (20605/21888)
Epoch: 122 | Batch_idx: 180 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (21814/23168)
Epoch: 122 | Batch_idx: 190 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (23027/24448)
Epoch: 122 | Batch_idx: 200 |  Loss: (0.1669) |  Loss2: (0.0000) | Acc: (94.00%) (24230/25728)
Epoch: 122 | Batch_idx: 210 |  Loss: (0.1673) |  Loss2: (0.0000) | Acc: (94.00%) (25431/27008)
Epoch: 122 | Batch_idx: 220 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (26635/28288)
Epoch: 122 | Batch_idx: 230 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (27822/29568)
Epoch: 122 | Batch_idx: 240 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (29039/30848)
Epoch: 122 | Batch_idx: 250 |  Loss: (0.1695) |  Loss2: (0.0000) | Acc: (94.00%) (30228/32128)
Epoch: 122 | Batch_idx: 260 |  Loss: (0.1693) |  Loss2: (0.0000) | Acc: (94.00%) (31441/33408)
Epoch: 122 | Batch_idx: 270 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (32633/34688)
Epoch: 122 | Batch_idx: 280 |  Loss: (0.1698) |  Loss2: (0.0000) | Acc: (94.00%) (33853/35968)
Epoch: 122 | Batch_idx: 290 |  Loss: (0.1694) |  Loss2: (0.0000) | Acc: (94.00%) (35061/37248)
Epoch: 122 | Batch_idx: 300 |  Loss: (0.1704) |  Loss2: (0.0000) | Acc: (94.00%) (36248/38528)
Epoch: 122 | Batch_idx: 310 |  Loss: (0.1701) |  Loss2: (0.0000) | Acc: (94.00%) (37456/39808)
Epoch: 122 | Batch_idx: 320 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (38648/41088)
Epoch: 122 | Batch_idx: 330 |  Loss: (0.1707) |  Loss2: (0.0000) | Acc: (94.00%) (39854/42368)
Epoch: 122 | Batch_idx: 340 |  Loss: (0.1705) |  Loss2: (0.0000) | Acc: (94.00%) (41058/43648)
Epoch: 122 | Batch_idx: 350 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (42271/44928)
Epoch: 122 | Batch_idx: 360 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (43475/46208)
Epoch: 122 | Batch_idx: 370 |  Loss: (0.1700) |  Loss2: (0.0000) | Acc: (94.00%) (44678/47488)
Epoch: 122 | Batch_idx: 380 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (45874/48768)
Epoch: 122 | Batch_idx: 390 |  Loss: (0.1706) |  Loss2: (0.0000) | Acc: (94.00%) (47020/50000)
# TEST : Loss: (0.4735) | Acc: (85.00%) (8585/10000)
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5811, 0.4189], device='cuda:0')
percent tensor([0.5473, 0.4527], device='cuda:0')
percent tensor([0.6353, 0.3647], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6502, 0.3498], device='cuda:0')
percent tensor([0.7143, 0.2857], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 123 | Batch_idx: 0 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 123 | Batch_idx: 10 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 123 | Batch_idx: 20 |  Loss: (0.1482) |  Loss2: (0.0000) | Acc: (95.00%) (2562/2688)
Epoch: 123 | Batch_idx: 30 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (95.00%) (3779/3968)
Epoch: 123 | Batch_idx: 40 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (95.00%) (4986/5248)
Epoch: 123 | Batch_idx: 50 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (6198/6528)
Epoch: 123 | Batch_idx: 60 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 123 | Batch_idx: 70 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (8614/9088)
Epoch: 123 | Batch_idx: 80 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (9814/10368)
Epoch: 123 | Batch_idx: 90 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (11030/11648)
Epoch: 123 | Batch_idx: 100 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (12238/12928)
Epoch: 123 | Batch_idx: 110 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (13457/14208)
Epoch: 123 | Batch_idx: 120 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (14668/15488)
Epoch: 123 | Batch_idx: 130 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (15882/16768)
Epoch: 123 | Batch_idx: 140 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (17090/18048)
Epoch: 123 | Batch_idx: 150 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (18311/19328)
Epoch: 123 | Batch_idx: 160 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (19513/20608)
Epoch: 123 | Batch_idx: 170 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (20726/21888)
Epoch: 123 | Batch_idx: 180 |  Loss: (0.1606) |  Loss2: (0.0000) | Acc: (94.00%) (21933/23168)
Epoch: 123 | Batch_idx: 190 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 123 | Batch_idx: 200 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (24332/25728)
Epoch: 123 | Batch_idx: 210 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (25541/27008)
Epoch: 123 | Batch_idx: 220 |  Loss: (0.1633) |  Loss2: (0.0000) | Acc: (94.00%) (26748/28288)
Epoch: 123 | Batch_idx: 230 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (27965/29568)
Epoch: 123 | Batch_idx: 240 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (29160/30848)
Epoch: 123 | Batch_idx: 250 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (30362/32128)
Epoch: 123 | Batch_idx: 260 |  Loss: (0.1654) |  Loss2: (0.0000) | Acc: (94.00%) (31554/33408)
Epoch: 123 | Batch_idx: 270 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (32753/34688)
Epoch: 123 | Batch_idx: 280 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (33981/35968)
Epoch: 123 | Batch_idx: 290 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (35177/37248)
Epoch: 123 | Batch_idx: 300 |  Loss: (0.1667) |  Loss2: (0.0000) | Acc: (94.00%) (36365/38528)
Epoch: 123 | Batch_idx: 310 |  Loss: (0.1674) |  Loss2: (0.0000) | Acc: (94.00%) (37568/39808)
Epoch: 123 | Batch_idx: 320 |  Loss: (0.1672) |  Loss2: (0.0000) | Acc: (94.00%) (38776/41088)
Epoch: 123 | Batch_idx: 330 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (39964/42368)
Epoch: 123 | Batch_idx: 340 |  Loss: (0.1676) |  Loss2: (0.0000) | Acc: (94.00%) (41164/43648)
Epoch: 123 | Batch_idx: 350 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (42368/44928)
Epoch: 123 | Batch_idx: 360 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (43569/46208)
Epoch: 123 | Batch_idx: 370 |  Loss: (0.1679) |  Loss2: (0.0000) | Acc: (94.00%) (44784/47488)
Epoch: 123 | Batch_idx: 380 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (45992/48768)
Epoch: 123 | Batch_idx: 390 |  Loss: (0.1683) |  Loss2: (0.0000) | Acc: (94.00%) (47147/50000)
# TEST : Loss: (0.4836) | Acc: (86.00%) (8603/10000)
percent tensor([0.5763, 0.4237], device='cuda:0')
percent tensor([0.5812, 0.4188], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.5925, 0.4075], device='cuda:0')
percent tensor([0.6519, 0.3481], device='cuda:0')
percent tensor([0.7137, 0.2863], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 124 | Batch_idx: 0 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 124 | Batch_idx: 10 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 124 | Batch_idx: 20 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (93.00%) (2526/2688)
Epoch: 124 | Batch_idx: 30 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (93.00%) (3722/3968)
Epoch: 124 | Batch_idx: 40 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (4947/5248)
Epoch: 124 | Batch_idx: 50 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 124 | Batch_idx: 60 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (7382/7808)
Epoch: 124 | Batch_idx: 70 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (8602/9088)
Epoch: 124 | Batch_idx: 80 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (9824/10368)
Epoch: 124 | Batch_idx: 90 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (11026/11648)
Epoch: 124 | Batch_idx: 100 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (12230/12928)
Epoch: 124 | Batch_idx: 110 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (13439/14208)
Epoch: 124 | Batch_idx: 120 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (14631/15488)
Epoch: 124 | Batch_idx: 130 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (15838/16768)
Epoch: 124 | Batch_idx: 140 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (17046/18048)
Epoch: 124 | Batch_idx: 150 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (18260/19328)
Epoch: 124 | Batch_idx: 160 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (19473/20608)
Epoch: 124 | Batch_idx: 170 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (20674/21888)
Epoch: 124 | Batch_idx: 180 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (21875/23168)
Epoch: 124 | Batch_idx: 190 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (23094/24448)
Epoch: 124 | Batch_idx: 200 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (24309/25728)
Epoch: 124 | Batch_idx: 210 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (25502/27008)
Epoch: 124 | Batch_idx: 220 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (26721/28288)
Epoch: 124 | Batch_idx: 230 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (27942/29568)
Epoch: 124 | Batch_idx: 240 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (29154/30848)
Epoch: 124 | Batch_idx: 250 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (30355/32128)
Epoch: 124 | Batch_idx: 260 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (31568/33408)
Epoch: 124 | Batch_idx: 270 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (32771/34688)
Epoch: 124 | Batch_idx: 280 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (33989/35968)
Epoch: 124 | Batch_idx: 290 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (35206/37248)
Epoch: 124 | Batch_idx: 300 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (36409/38528)
Epoch: 124 | Batch_idx: 310 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (37621/39808)
Epoch: 124 | Batch_idx: 320 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (38835/41088)
Epoch: 124 | Batch_idx: 330 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (40045/42368)
Epoch: 124 | Batch_idx: 340 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (41263/43648)
Epoch: 124 | Batch_idx: 350 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (42470/44928)
Epoch: 124 | Batch_idx: 360 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (43662/46208)
Epoch: 124 | Batch_idx: 370 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (44867/47488)
Epoch: 124 | Batch_idx: 380 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (46086/48768)
Epoch: 124 | Batch_idx: 390 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (47240/50000)
# TEST : Loss: (0.4315) | Acc: (87.00%) (8734/10000)
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.6349, 0.3651], device='cuda:0')
percent tensor([0.5929, 0.4071], device='cuda:0')
percent tensor([0.6532, 0.3468], device='cuda:0')
percent tensor([0.7173, 0.2827], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 125 | Batch_idx: 0 |  Loss: (0.1660) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 125 | Batch_idx: 10 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 125 | Batch_idx: 20 |  Loss: (0.1867) |  Loss2: (0.0000) | Acc: (93.00%) (2514/2688)
Epoch: 125 | Batch_idx: 30 |  Loss: (0.1858) |  Loss2: (0.0000) | Acc: (93.00%) (3709/3968)
Epoch: 125 | Batch_idx: 40 |  Loss: (0.1950) |  Loss2: (0.0000) | Acc: (93.00%) (4881/5248)
Epoch: 125 | Batch_idx: 50 |  Loss: (0.1970) |  Loss2: (0.0000) | Acc: (92.00%) (6068/6528)
Epoch: 125 | Batch_idx: 60 |  Loss: (0.1920) |  Loss2: (0.0000) | Acc: (93.00%) (7272/7808)
Epoch: 125 | Batch_idx: 70 |  Loss: (0.1915) |  Loss2: (0.0000) | Acc: (93.00%) (8455/9088)
Epoch: 125 | Batch_idx: 80 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (9656/10368)
Epoch: 125 | Batch_idx: 90 |  Loss: (0.1914) |  Loss2: (0.0000) | Acc: (93.00%) (10844/11648)
Epoch: 125 | Batch_idx: 100 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (12024/12928)
Epoch: 125 | Batch_idx: 110 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (92.00%) (13213/14208)
Epoch: 125 | Batch_idx: 120 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (14416/15488)
Epoch: 125 | Batch_idx: 130 |  Loss: (0.1949) |  Loss2: (0.0000) | Acc: (93.00%) (15598/16768)
Epoch: 125 | Batch_idx: 140 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (16791/18048)
Epoch: 125 | Batch_idx: 150 |  Loss: (0.1930) |  Loss2: (0.0000) | Acc: (93.00%) (18000/19328)
Epoch: 125 | Batch_idx: 160 |  Loss: (0.1948) |  Loss2: (0.0000) | Acc: (93.00%) (19180/20608)
Epoch: 125 | Batch_idx: 170 |  Loss: (0.1944) |  Loss2: (0.0000) | Acc: (93.00%) (20380/21888)
Epoch: 125 | Batch_idx: 180 |  Loss: (0.1938) |  Loss2: (0.0000) | Acc: (93.00%) (21579/23168)
Epoch: 125 | Batch_idx: 190 |  Loss: (0.1942) |  Loss2: (0.0000) | Acc: (93.00%) (22765/24448)
Epoch: 125 | Batch_idx: 200 |  Loss: (0.1933) |  Loss2: (0.0000) | Acc: (93.00%) (23968/25728)
Epoch: 125 | Batch_idx: 210 |  Loss: (0.1936) |  Loss2: (0.0000) | Acc: (93.00%) (25163/27008)
Epoch: 125 | Batch_idx: 220 |  Loss: (0.1934) |  Loss2: (0.0000) | Acc: (93.00%) (26349/28288)
Epoch: 125 | Batch_idx: 230 |  Loss: (0.1919) |  Loss2: (0.0000) | Acc: (93.00%) (27560/29568)
Epoch: 125 | Batch_idx: 240 |  Loss: (0.1907) |  Loss2: (0.0000) | Acc: (93.00%) (28771/30848)
Epoch: 125 | Batch_idx: 250 |  Loss: (0.1903) |  Loss2: (0.0000) | Acc: (93.00%) (29968/32128)
Epoch: 125 | Batch_idx: 260 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (31171/33408)
Epoch: 125 | Batch_idx: 270 |  Loss: (0.1908) |  Loss2: (0.0000) | Acc: (93.00%) (32360/34688)
Epoch: 125 | Batch_idx: 280 |  Loss: (0.1900) |  Loss2: (0.0000) | Acc: (93.00%) (33568/35968)
Epoch: 125 | Batch_idx: 290 |  Loss: (0.1902) |  Loss2: (0.0000) | Acc: (93.00%) (34763/37248)
Epoch: 125 | Batch_idx: 300 |  Loss: (0.1899) |  Loss2: (0.0000) | Acc: (93.00%) (35952/38528)
Epoch: 125 | Batch_idx: 310 |  Loss: (0.1901) |  Loss2: (0.0000) | Acc: (93.00%) (37142/39808)
Epoch: 125 | Batch_idx: 320 |  Loss: (0.1898) |  Loss2: (0.0000) | Acc: (93.00%) (38344/41088)
Epoch: 125 | Batch_idx: 330 |  Loss: (0.1891) |  Loss2: (0.0000) | Acc: (93.00%) (39559/42368)
Epoch: 125 | Batch_idx: 340 |  Loss: (0.1894) |  Loss2: (0.0000) | Acc: (93.00%) (40744/43648)
Epoch: 125 | Batch_idx: 350 |  Loss: (0.1885) |  Loss2: (0.0000) | Acc: (93.00%) (41957/44928)
Epoch: 125 | Batch_idx: 360 |  Loss: (0.1888) |  Loss2: (0.0000) | Acc: (93.00%) (43143/46208)
Epoch: 125 | Batch_idx: 370 |  Loss: (0.1889) |  Loss2: (0.0000) | Acc: (93.00%) (44340/47488)
Epoch: 125 | Batch_idx: 380 |  Loss: (0.1893) |  Loss2: (0.0000) | Acc: (93.00%) (45531/48768)
Epoch: 125 | Batch_idx: 390 |  Loss: (0.1890) |  Loss2: (0.0000) | Acc: (93.00%) (46689/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_125.pth.tar'
# TEST : Loss: (0.4350) | Acc: (87.00%) (8718/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5470, 0.4530], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.5958, 0.4042], device='cuda:0')
percent tensor([0.6575, 0.3425], device='cuda:0')
percent tensor([0.6887, 0.3113], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 126 | Batch_idx: 0 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 126 | Batch_idx: 10 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (94.00%) (1330/1408)
Epoch: 126 | Batch_idx: 20 |  Loss: (0.1838) |  Loss2: (0.0000) | Acc: (93.00%) (2520/2688)
Epoch: 126 | Batch_idx: 30 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (3716/3968)
Epoch: 126 | Batch_idx: 40 |  Loss: (0.1782) |  Loss2: (0.0000) | Acc: (93.00%) (4929/5248)
Epoch: 126 | Batch_idx: 50 |  Loss: (0.1831) |  Loss2: (0.0000) | Acc: (93.00%) (6118/6528)
Epoch: 126 | Batch_idx: 60 |  Loss: (0.1814) |  Loss2: (0.0000) | Acc: (93.00%) (7316/7808)
Epoch: 126 | Batch_idx: 70 |  Loss: (0.1805) |  Loss2: (0.0000) | Acc: (93.00%) (8517/9088)
Epoch: 126 | Batch_idx: 80 |  Loss: (0.1790) |  Loss2: (0.0000) | Acc: (93.00%) (9723/10368)
Epoch: 126 | Batch_idx: 90 |  Loss: (0.1801) |  Loss2: (0.0000) | Acc: (93.00%) (10910/11648)
Epoch: 126 | Batch_idx: 100 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (12097/12928)
Epoch: 126 | Batch_idx: 110 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (13295/14208)
Epoch: 126 | Batch_idx: 120 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (14488/15488)
Epoch: 126 | Batch_idx: 130 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (15689/16768)
Epoch: 126 | Batch_idx: 140 |  Loss: (0.1803) |  Loss2: (0.0000) | Acc: (93.00%) (16888/18048)
Epoch: 126 | Batch_idx: 150 |  Loss: (0.1799) |  Loss2: (0.0000) | Acc: (93.00%) (18085/19328)
Epoch: 126 | Batch_idx: 160 |  Loss: (0.1802) |  Loss2: (0.0000) | Acc: (93.00%) (19278/20608)
Epoch: 126 | Batch_idx: 170 |  Loss: (0.1793) |  Loss2: (0.0000) | Acc: (93.00%) (20481/21888)
Epoch: 126 | Batch_idx: 180 |  Loss: (0.1781) |  Loss2: (0.0000) | Acc: (93.00%) (21691/23168)
Epoch: 126 | Batch_idx: 190 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (22895/24448)
Epoch: 126 | Batch_idx: 200 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (24096/25728)
Epoch: 126 | Batch_idx: 210 |  Loss: (0.1768) |  Loss2: (0.0000) | Acc: (93.00%) (25308/27008)
Epoch: 126 | Batch_idx: 220 |  Loss: (0.1756) |  Loss2: (0.0000) | Acc: (93.00%) (26521/28288)
Epoch: 126 | Batch_idx: 230 |  Loss: (0.1749) |  Loss2: (0.0000) | Acc: (93.00%) (27722/29568)
Epoch: 126 | Batch_idx: 240 |  Loss: (0.1750) |  Loss2: (0.0000) | Acc: (93.00%) (28920/30848)
Epoch: 126 | Batch_idx: 250 |  Loss: (0.1757) |  Loss2: (0.0000) | Acc: (93.00%) (30115/32128)
Epoch: 126 | Batch_idx: 260 |  Loss: (0.1753) |  Loss2: (0.0000) | Acc: (93.00%) (31320/33408)
Epoch: 126 | Batch_idx: 270 |  Loss: (0.1748) |  Loss2: (0.0000) | Acc: (93.00%) (32531/34688)
Epoch: 126 | Batch_idx: 280 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (33741/35968)
Epoch: 126 | Batch_idx: 290 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (34939/37248)
Epoch: 126 | Batch_idx: 300 |  Loss: (0.1734) |  Loss2: (0.0000) | Acc: (93.00%) (36147/38528)
Epoch: 126 | Batch_idx: 310 |  Loss: (0.1729) |  Loss2: (0.0000) | Acc: (93.00%) (37350/39808)
Epoch: 126 | Batch_idx: 320 |  Loss: (0.1730) |  Loss2: (0.0000) | Acc: (93.00%) (38547/41088)
Epoch: 126 | Batch_idx: 330 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (39749/42368)
Epoch: 126 | Batch_idx: 340 |  Loss: (0.1726) |  Loss2: (0.0000) | Acc: (93.00%) (40950/43648)
Epoch: 126 | Batch_idx: 350 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (42163/44928)
Epoch: 126 | Batch_idx: 360 |  Loss: (0.1725) |  Loss2: (0.0000) | Acc: (93.00%) (43358/46208)
Epoch: 126 | Batch_idx: 370 |  Loss: (0.1717) |  Loss2: (0.0000) | Acc: (93.00%) (44575/47488)
Epoch: 126 | Batch_idx: 380 |  Loss: (0.1721) |  Loss2: (0.0000) | Acc: (93.00%) (45761/48768)
Epoch: 126 | Batch_idx: 390 |  Loss: (0.1722) |  Loss2: (0.0000) | Acc: (93.00%) (46915/50000)
# TEST : Loss: (0.4180) | Acc: (87.00%) (8749/10000)
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5471, 0.4529], device='cuda:0')
percent tensor([0.6228, 0.3772], device='cuda:0')
percent tensor([0.5982, 0.4018], device='cuda:0')
percent tensor([0.6603, 0.3397], device='cuda:0')
percent tensor([0.6918, 0.3082], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 127 | Batch_idx: 0 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 127 | Batch_idx: 10 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 127 | Batch_idx: 20 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (95.00%) (2554/2688)
Epoch: 127 | Batch_idx: 30 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (3756/3968)
Epoch: 127 | Batch_idx: 40 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (4967/5248)
Epoch: 127 | Batch_idx: 50 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (6179/6528)
Epoch: 127 | Batch_idx: 60 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (7374/7808)
Epoch: 127 | Batch_idx: 70 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (8587/9088)
Epoch: 127 | Batch_idx: 80 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (9787/10368)
Epoch: 127 | Batch_idx: 90 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (10983/11648)
Epoch: 127 | Batch_idx: 100 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (12176/12928)
Epoch: 127 | Batch_idx: 110 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (13391/14208)
Epoch: 127 | Batch_idx: 120 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (14599/15488)
Epoch: 127 | Batch_idx: 130 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (15789/16768)
Epoch: 127 | Batch_idx: 140 |  Loss: (0.1662) |  Loss2: (0.0000) | Acc: (94.00%) (16992/18048)
Epoch: 127 | Batch_idx: 150 |  Loss: (0.1653) |  Loss2: (0.0000) | Acc: (94.00%) (18201/19328)
Epoch: 127 | Batch_idx: 160 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (19409/20608)
Epoch: 127 | Batch_idx: 170 |  Loss: (0.1648) |  Loss2: (0.0000) | Acc: (94.00%) (20614/21888)
Epoch: 127 | Batch_idx: 180 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (21831/23168)
Epoch: 127 | Batch_idx: 190 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (23049/24448)
Epoch: 127 | Batch_idx: 200 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (24256/25728)
Epoch: 127 | Batch_idx: 210 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (25472/27008)
Epoch: 127 | Batch_idx: 220 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (26682/28288)
Epoch: 127 | Batch_idx: 230 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (27887/29568)
Epoch: 127 | Batch_idx: 240 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (29104/30848)
Epoch: 127 | Batch_idx: 250 |  Loss: (0.1626) |  Loss2: (0.0000) | Acc: (94.00%) (30331/32128)
Epoch: 127 | Batch_idx: 260 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (31540/33408)
Epoch: 127 | Batch_idx: 270 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (32743/34688)
Epoch: 127 | Batch_idx: 280 |  Loss: (0.1630) |  Loss2: (0.0000) | Acc: (94.00%) (33952/35968)
Epoch: 127 | Batch_idx: 290 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (35153/37248)
Epoch: 127 | Batch_idx: 300 |  Loss: (0.1634) |  Loss2: (0.0000) | Acc: (94.00%) (36360/38528)
Epoch: 127 | Batch_idx: 310 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (37556/39808)
Epoch: 127 | Batch_idx: 320 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (38754/41088)
Epoch: 127 | Batch_idx: 330 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (39974/42368)
Epoch: 127 | Batch_idx: 340 |  Loss: (0.1636) |  Loss2: (0.0000) | Acc: (94.00%) (41185/43648)
Epoch: 127 | Batch_idx: 350 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (42388/44928)
Epoch: 127 | Batch_idx: 360 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (43598/46208)
Epoch: 127 | Batch_idx: 370 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (44809/47488)
Epoch: 127 | Batch_idx: 380 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (46017/48768)
Epoch: 127 | Batch_idx: 390 |  Loss: (0.1632) |  Loss2: (0.0000) | Acc: (94.00%) (47175/50000)
# TEST : Loss: (0.4042) | Acc: (87.00%) (8765/10000)
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5477, 0.4523], device='cuda:0')
percent tensor([0.6273, 0.3727], device='cuda:0')
percent tensor([0.6025, 0.3975], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 128 | Batch_idx: 0 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 128 | Batch_idx: 10 |  Loss: (0.1423) |  Loss2: (0.0000) | Acc: (94.00%) (1334/1408)
Epoch: 128 | Batch_idx: 20 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 128 | Batch_idx: 30 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (3731/3968)
Epoch: 128 | Batch_idx: 40 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (4948/5248)
Epoch: 128 | Batch_idx: 50 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (6172/6528)
Epoch: 128 | Batch_idx: 60 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (7389/7808)
Epoch: 128 | Batch_idx: 70 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (8599/9088)
Epoch: 128 | Batch_idx: 80 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (9808/10368)
Epoch: 128 | Batch_idx: 90 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (11007/11648)
Epoch: 128 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12203/12928)
Epoch: 128 | Batch_idx: 110 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (13415/14208)
Epoch: 128 | Batch_idx: 120 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (14623/15488)
Epoch: 128 | Batch_idx: 130 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (15832/16768)
Epoch: 128 | Batch_idx: 140 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (17045/18048)
Epoch: 128 | Batch_idx: 150 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (18250/19328)
Epoch: 128 | Batch_idx: 160 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (19466/20608)
Epoch: 128 | Batch_idx: 170 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (20676/21888)
Epoch: 128 | Batch_idx: 180 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (21874/23168)
Epoch: 128 | Batch_idx: 190 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (23066/24448)
Epoch: 128 | Batch_idx: 200 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (24275/25728)
Epoch: 128 | Batch_idx: 210 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (25501/27008)
Epoch: 128 | Batch_idx: 220 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (26706/28288)
Epoch: 128 | Batch_idx: 230 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (27905/29568)
Epoch: 128 | Batch_idx: 240 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (29123/30848)
Epoch: 128 | Batch_idx: 250 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (30321/32128)
Epoch: 128 | Batch_idx: 260 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (31540/33408)
Epoch: 128 | Batch_idx: 270 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (32753/34688)
Epoch: 128 | Batch_idx: 280 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (33972/35968)
Epoch: 128 | Batch_idx: 290 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (35192/37248)
Epoch: 128 | Batch_idx: 300 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (36401/38528)
Epoch: 128 | Batch_idx: 310 |  Loss: (0.1593) |  Loss2: (0.0000) | Acc: (94.00%) (37612/39808)
Epoch: 128 | Batch_idx: 320 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (38827/41088)
Epoch: 128 | Batch_idx: 330 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (40047/42368)
Epoch: 128 | Batch_idx: 340 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (41261/43648)
Epoch: 128 | Batch_idx: 350 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (42472/44928)
Epoch: 128 | Batch_idx: 360 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (43697/46208)
Epoch: 128 | Batch_idx: 370 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (44901/47488)
Epoch: 128 | Batch_idx: 380 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (46104/48768)
Epoch: 128 | Batch_idx: 390 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (47259/50000)
# TEST : Loss: (0.3930) | Acc: (87.00%) (8774/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5774, 0.4226], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6263, 0.3737], device='cuda:0')
percent tensor([0.5996, 0.4004], device='cuda:0')
percent tensor([0.6652, 0.3348], device='cuda:0')
percent tensor([0.7020, 0.2980], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 129 | Batch_idx: 0 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 129 | Batch_idx: 10 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (93.00%) (1318/1408)
Epoch: 129 | Batch_idx: 20 |  Loss: (0.1697) |  Loss2: (0.0000) | Acc: (93.00%) (2523/2688)
Epoch: 129 | Batch_idx: 30 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (3754/3968)
Epoch: 129 | Batch_idx: 40 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 129 | Batch_idx: 50 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (6166/6528)
Epoch: 129 | Batch_idx: 60 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (7372/7808)
Epoch: 129 | Batch_idx: 70 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 129 | Batch_idx: 80 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (9790/10368)
Epoch: 129 | Batch_idx: 90 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (11002/11648)
Epoch: 129 | Batch_idx: 100 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (12217/12928)
Epoch: 129 | Batch_idx: 110 |  Loss: (0.1539) |  Loss2: (0.0000) | Acc: (94.00%) (13440/14208)
Epoch: 129 | Batch_idx: 120 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (14657/15488)
Epoch: 129 | Batch_idx: 130 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (15865/16768)
Epoch: 129 | Batch_idx: 140 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (17087/18048)
Epoch: 129 | Batch_idx: 150 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (18301/19328)
Epoch: 129 | Batch_idx: 160 |  Loss: (0.1523) |  Loss2: (0.0000) | Acc: (94.00%) (19514/20608)
Epoch: 129 | Batch_idx: 170 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (20721/21888)
Epoch: 129 | Batch_idx: 180 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (21937/23168)
Epoch: 129 | Batch_idx: 190 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (23149/24448)
Epoch: 129 | Batch_idx: 200 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (24368/25728)
Epoch: 129 | Batch_idx: 210 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (25575/27008)
Epoch: 129 | Batch_idx: 220 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (26788/28288)
Epoch: 129 | Batch_idx: 230 |  Loss: (0.1529) |  Loss2: (0.0000) | Acc: (94.00%) (28008/29568)
Epoch: 129 | Batch_idx: 240 |  Loss: (0.1535) |  Loss2: (0.0000) | Acc: (94.00%) (29211/30848)
Epoch: 129 | Batch_idx: 250 |  Loss: (0.1533) |  Loss2: (0.0000) | Acc: (94.00%) (30429/32128)
Epoch: 129 | Batch_idx: 260 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (31643/33408)
Epoch: 129 | Batch_idx: 270 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (32839/34688)
Epoch: 129 | Batch_idx: 280 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (34072/35968)
Epoch: 129 | Batch_idx: 290 |  Loss: (0.1537) |  Loss2: (0.0000) | Acc: (94.00%) (35286/37248)
Epoch: 129 | Batch_idx: 300 |  Loss: (0.1531) |  Loss2: (0.0000) | Acc: (94.00%) (36516/38528)
Epoch: 129 | Batch_idx: 310 |  Loss: (0.1527) |  Loss2: (0.0000) | Acc: (94.00%) (37728/39808)
Epoch: 129 | Batch_idx: 320 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (94.00%) (38951/41088)
Epoch: 129 | Batch_idx: 330 |  Loss: (0.1516) |  Loss2: (0.0000) | Acc: (94.00%) (40168/42368)
Epoch: 129 | Batch_idx: 340 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (41371/43648)
Epoch: 129 | Batch_idx: 350 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (42584/44928)
Epoch: 129 | Batch_idx: 360 |  Loss: (0.1521) |  Loss2: (0.0000) | Acc: (94.00%) (43800/46208)
Epoch: 129 | Batch_idx: 370 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (45017/47488)
Epoch: 129 | Batch_idx: 380 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (46224/48768)
Epoch: 129 | Batch_idx: 390 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (47378/50000)
# TEST : Loss: (0.3931) | Acc: (87.00%) (8795/10000)
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.5758, 0.4242], device='cuda:0')
percent tensor([0.5502, 0.4498], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.5999, 0.4001], device='cuda:0')
percent tensor([0.6648, 0.3352], device='cuda:0')
percent tensor([0.7042, 0.2958], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 130 | Batch_idx: 0 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 130 | Batch_idx: 10 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 130 | Batch_idx: 20 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (2552/2688)
Epoch: 130 | Batch_idx: 30 |  Loss: (0.1534) |  Loss2: (0.0000) | Acc: (94.00%) (3765/3968)
Epoch: 130 | Batch_idx: 40 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (4958/5248)
Epoch: 130 | Batch_idx: 50 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (6174/6528)
Epoch: 130 | Batch_idx: 60 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (7385/7808)
Epoch: 130 | Batch_idx: 70 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (8601/9088)
Epoch: 130 | Batch_idx: 80 |  Loss: (0.1551) |  Loss2: (0.0000) | Acc: (94.00%) (9823/10368)
Epoch: 130 | Batch_idx: 90 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (11018/11648)
Epoch: 130 | Batch_idx: 100 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (12231/12928)
Epoch: 130 | Batch_idx: 110 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (13442/14208)
Epoch: 130 | Batch_idx: 120 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (14647/15488)
Epoch: 130 | Batch_idx: 130 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (15861/16768)
Epoch: 130 | Batch_idx: 140 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (17065/18048)
Epoch: 130 | Batch_idx: 150 |  Loss: (0.1559) |  Loss2: (0.0000) | Acc: (94.00%) (18289/19328)
Epoch: 130 | Batch_idx: 160 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (19504/20608)
Epoch: 130 | Batch_idx: 170 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (20713/21888)
Epoch: 130 | Batch_idx: 180 |  Loss: (0.1565) |  Loss2: (0.0000) | Acc: (94.00%) (21926/23168)
Epoch: 130 | Batch_idx: 190 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (23132/24448)
Epoch: 130 | Batch_idx: 200 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (24342/25728)
Epoch: 130 | Batch_idx: 210 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (25549/27008)
Epoch: 130 | Batch_idx: 220 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (26757/28288)
Epoch: 130 | Batch_idx: 230 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (27966/29568)
Epoch: 130 | Batch_idx: 240 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (29180/30848)
Epoch: 130 | Batch_idx: 250 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (30395/32128)
Epoch: 130 | Batch_idx: 260 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (31598/33408)
Epoch: 130 | Batch_idx: 270 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (32796/34688)
Epoch: 130 | Batch_idx: 280 |  Loss: (0.1609) |  Loss2: (0.0000) | Acc: (94.00%) (33991/35968)
Epoch: 130 | Batch_idx: 290 |  Loss: (0.1618) |  Loss2: (0.0000) | Acc: (94.00%) (35190/37248)
Epoch: 130 | Batch_idx: 300 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (36399/38528)
Epoch: 130 | Batch_idx: 310 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (37611/39808)
Epoch: 130 | Batch_idx: 320 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (38800/41088)
Epoch: 130 | Batch_idx: 330 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (40002/42368)
Epoch: 130 | Batch_idx: 340 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (41227/43648)
Epoch: 130 | Batch_idx: 350 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (42422/44928)
Epoch: 130 | Batch_idx: 360 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (43628/46208)
Epoch: 130 | Batch_idx: 370 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (44837/47488)
Epoch: 130 | Batch_idx: 380 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (46038/48768)
Epoch: 130 | Batch_idx: 390 |  Loss: (0.1629) |  Loss2: (0.0000) | Acc: (94.00%) (47214/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_130.pth.tar'
# TEST : Loss: (0.4489) | Acc: (86.00%) (8679/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5758, 0.4242], device='cuda:0')
percent tensor([0.5504, 0.4496], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5985, 0.4015], device='cuda:0')
percent tensor([0.6631, 0.3369], device='cuda:0')
percent tensor([0.7031, 0.2969], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.0199, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(820.4230, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(819.4392, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.8595, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(488.6646, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2266.7656, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4272.3286, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1379.1045, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6193.2158, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11767.3730, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3886.2905, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16439.4121, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 131 | Batch_idx: 0 |  Loss: (0.2033) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 131 | Batch_idx: 10 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (1320/1408)
Epoch: 131 | Batch_idx: 20 |  Loss: (0.1682) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 131 | Batch_idx: 30 |  Loss: (0.1663) |  Loss2: (0.0000) | Acc: (93.00%) (3729/3968)
Epoch: 131 | Batch_idx: 40 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (4941/5248)
Epoch: 131 | Batch_idx: 50 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (6151/6528)
Epoch: 131 | Batch_idx: 60 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (7363/7808)
Epoch: 131 | Batch_idx: 70 |  Loss: (0.1588) |  Loss2: (0.0000) | Acc: (94.00%) (8566/9088)
Epoch: 131 | Batch_idx: 80 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (9781/10368)
Epoch: 131 | Batch_idx: 90 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (10987/11648)
Epoch: 131 | Batch_idx: 100 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (12205/12928)
Epoch: 131 | Batch_idx: 110 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (13427/14208)
Epoch: 131 | Batch_idx: 120 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (14629/15488)
Epoch: 131 | Batch_idx: 130 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (15832/16768)
Epoch: 131 | Batch_idx: 140 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (17031/18048)
Epoch: 131 | Batch_idx: 150 |  Loss: (0.1584) |  Loss2: (0.0000) | Acc: (94.00%) (18251/19328)
Epoch: 131 | Batch_idx: 160 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (19462/20608)
Epoch: 131 | Batch_idx: 170 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (20674/21888)
Epoch: 131 | Batch_idx: 180 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (21883/23168)
Epoch: 131 | Batch_idx: 190 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (23093/24448)
Epoch: 131 | Batch_idx: 200 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (24304/25728)
Epoch: 131 | Batch_idx: 210 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (25524/27008)
Epoch: 131 | Batch_idx: 220 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (26740/28288)
Epoch: 131 | Batch_idx: 230 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (27952/29568)
Epoch: 131 | Batch_idx: 240 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (29166/30848)
Epoch: 131 | Batch_idx: 250 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (30386/32128)
Epoch: 131 | Batch_idx: 260 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (31591/33408)
Epoch: 131 | Batch_idx: 270 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (32801/34688)
Epoch: 131 | Batch_idx: 280 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (34002/35968)
Epoch: 131 | Batch_idx: 290 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (35203/37248)
Epoch: 131 | Batch_idx: 300 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (36421/38528)
Epoch: 131 | Batch_idx: 310 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (37623/39808)
Epoch: 131 | Batch_idx: 320 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (38835/41088)
Epoch: 131 | Batch_idx: 330 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (40046/42368)
Epoch: 131 | Batch_idx: 340 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (41261/43648)
Epoch: 131 | Batch_idx: 350 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (42471/44928)
Epoch: 131 | Batch_idx: 360 |  Loss: (0.1571) |  Loss2: (0.0000) | Acc: (94.00%) (43677/46208)
Epoch: 131 | Batch_idx: 370 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (44893/47488)
Epoch: 131 | Batch_idx: 380 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (46087/48768)
Epoch: 131 | Batch_idx: 390 |  Loss: (0.1581) |  Loss2: (0.0000) | Acc: (94.00%) (47250/50000)
# TEST : Loss: (0.3997) | Acc: (87.00%) (8776/10000)
percent tensor([0.5814, 0.4186], device='cuda:0')
percent tensor([0.5754, 0.4246], device='cuda:0')
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.6246, 0.3754], device='cuda:0')
percent tensor([0.5994, 0.4006], device='cuda:0')
percent tensor([0.6653, 0.3347], device='cuda:0')
percent tensor([0.7027, 0.2973], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 132 | Batch_idx: 0 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 132 | Batch_idx: 10 |  Loss: (0.1741) |  Loss2: (0.0000) | Acc: (93.00%) (1322/1408)
Epoch: 132 | Batch_idx: 20 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (2536/2688)
Epoch: 132 | Batch_idx: 30 |  Loss: (0.1643) |  Loss2: (0.0000) | Acc: (94.00%) (3746/3968)
Epoch: 132 | Batch_idx: 40 |  Loss: (0.1655) |  Loss2: (0.0000) | Acc: (94.00%) (4945/5248)
Epoch: 132 | Batch_idx: 50 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (6161/6528)
Epoch: 132 | Batch_idx: 60 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (7386/7808)
Epoch: 132 | Batch_idx: 70 |  Loss: (0.1590) |  Loss2: (0.0000) | Acc: (94.00%) (8588/9088)
Epoch: 132 | Batch_idx: 80 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (9802/10368)
Epoch: 132 | Batch_idx: 90 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (11005/11648)
Epoch: 132 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12217/12928)
Epoch: 132 | Batch_idx: 110 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (13432/14208)
Epoch: 132 | Batch_idx: 120 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (14650/15488)
Epoch: 132 | Batch_idx: 130 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (15859/16768)
Epoch: 132 | Batch_idx: 140 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (17080/18048)
Epoch: 132 | Batch_idx: 150 |  Loss: (0.1568) |  Loss2: (0.0000) | Acc: (94.00%) (18275/19328)
Epoch: 132 | Batch_idx: 160 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (19480/20608)
Epoch: 132 | Batch_idx: 170 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (20676/21888)
Epoch: 132 | Batch_idx: 180 |  Loss: (0.1563) |  Loss2: (0.0000) | Acc: (94.00%) (21903/23168)
Epoch: 132 | Batch_idx: 190 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (23119/24448)
Epoch: 132 | Batch_idx: 200 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (24340/25728)
Epoch: 132 | Batch_idx: 210 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (25538/27008)
Epoch: 132 | Batch_idx: 220 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (26744/28288)
Epoch: 132 | Batch_idx: 230 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (27961/29568)
Epoch: 132 | Batch_idx: 240 |  Loss: (0.1567) |  Loss2: (0.0000) | Acc: (94.00%) (29178/30848)
Epoch: 132 | Batch_idx: 250 |  Loss: (0.1564) |  Loss2: (0.0000) | Acc: (94.00%) (30396/32128)
Epoch: 132 | Batch_idx: 260 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (31603/33408)
Epoch: 132 | Batch_idx: 270 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (32802/34688)
Epoch: 132 | Batch_idx: 280 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (34006/35968)
Epoch: 132 | Batch_idx: 290 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (35222/37248)
Epoch: 132 | Batch_idx: 300 |  Loss: (0.1579) |  Loss2: (0.0000) | Acc: (94.00%) (36432/38528)
Epoch: 132 | Batch_idx: 310 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (37646/39808)
Epoch: 132 | Batch_idx: 320 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (38858/41088)
Epoch: 132 | Batch_idx: 330 |  Loss: (0.1580) |  Loss2: (0.0000) | Acc: (94.00%) (40071/42368)
Epoch: 132 | Batch_idx: 340 |  Loss: (0.1585) |  Loss2: (0.0000) | Acc: (94.00%) (41269/43648)
Epoch: 132 | Batch_idx: 350 |  Loss: (0.1578) |  Loss2: (0.0000) | Acc: (94.00%) (42496/44928)
Epoch: 132 | Batch_idx: 360 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (43717/46208)
Epoch: 132 | Batch_idx: 370 |  Loss: (0.1572) |  Loss2: (0.0000) | Acc: (94.00%) (44926/47488)
Epoch: 132 | Batch_idx: 380 |  Loss: (0.1574) |  Loss2: (0.0000) | Acc: (94.00%) (46126/48768)
Epoch: 132 | Batch_idx: 390 |  Loss: (0.1575) |  Loss2: (0.0000) | Acc: (94.00%) (47289/50000)
# TEST : Loss: (0.4851) | Acc: (86.00%) (8605/10000)
percent tensor([0.5805, 0.4195], device='cuda:0')
percent tensor([0.5749, 0.4251], device='cuda:0')
percent tensor([0.5500, 0.4500], device='cuda:0')
percent tensor([0.6250, 0.3750], device='cuda:0')
percent tensor([0.5997, 0.4003], device='cuda:0')
percent tensor([0.6641, 0.3359], device='cuda:0')
percent tensor([0.7068, 0.2932], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 133 | Batch_idx: 0 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 133 | Batch_idx: 10 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (95.00%) (1345/1408)
Epoch: 133 | Batch_idx: 20 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (2569/2688)
Epoch: 133 | Batch_idx: 30 |  Loss: (0.1328) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 133 | Batch_idx: 40 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (5021/5248)
Epoch: 133 | Batch_idx: 50 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 133 | Batch_idx: 60 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 133 | Batch_idx: 70 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (8681/9088)
Epoch: 133 | Batch_idx: 80 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (9898/10368)
Epoch: 133 | Batch_idx: 90 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (95.00%) (11113/11648)
Epoch: 133 | Batch_idx: 100 |  Loss: (0.1420) |  Loss2: (0.0000) | Acc: (95.00%) (12332/12928)
Epoch: 133 | Batch_idx: 110 |  Loss: (0.1415) |  Loss2: (0.0000) | Acc: (95.00%) (13558/14208)
Epoch: 133 | Batch_idx: 120 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (95.00%) (14774/15488)
Epoch: 133 | Batch_idx: 130 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (15986/16768)
Epoch: 133 | Batch_idx: 140 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (17183/18048)
Epoch: 133 | Batch_idx: 150 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (18399/19328)
Epoch: 133 | Batch_idx: 160 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (19606/20608)
Epoch: 133 | Batch_idx: 170 |  Loss: (0.1463) |  Loss2: (0.0000) | Acc: (95.00%) (20821/21888)
Epoch: 133 | Batch_idx: 180 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (22022/23168)
Epoch: 133 | Batch_idx: 190 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (95.00%) (23255/24448)
Epoch: 133 | Batch_idx: 200 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (95.00%) (24473/25728)
Epoch: 133 | Batch_idx: 210 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (25685/27008)
Epoch: 133 | Batch_idx: 220 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (95.00%) (26905/28288)
Epoch: 133 | Batch_idx: 230 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (95.00%) (28108/29568)
Epoch: 133 | Batch_idx: 240 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (95.00%) (29325/30848)
Epoch: 133 | Batch_idx: 250 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (95.00%) (30547/32128)
Epoch: 133 | Batch_idx: 260 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (95.00%) (31761/33408)
Epoch: 133 | Batch_idx: 270 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (95.00%) (32970/34688)
Epoch: 133 | Batch_idx: 280 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (95.00%) (34191/35968)
Epoch: 133 | Batch_idx: 290 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (35399/37248)
Epoch: 133 | Batch_idx: 300 |  Loss: (0.1479) |  Loss2: (0.0000) | Acc: (95.00%) (36608/38528)
Epoch: 133 | Batch_idx: 310 |  Loss: (0.1483) |  Loss2: (0.0000) | Acc: (94.00%) (37815/39808)
Epoch: 133 | Batch_idx: 320 |  Loss: (0.1485) |  Loss2: (0.0000) | Acc: (94.00%) (39024/41088)
Epoch: 133 | Batch_idx: 330 |  Loss: (0.1492) |  Loss2: (0.0000) | Acc: (94.00%) (40227/42368)
Epoch: 133 | Batch_idx: 340 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (41425/43648)
Epoch: 133 | Batch_idx: 350 |  Loss: (0.1499) |  Loss2: (0.0000) | Acc: (94.00%) (42642/44928)
Epoch: 133 | Batch_idx: 360 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (43851/46208)
Epoch: 133 | Batch_idx: 370 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (45065/47488)
Epoch: 133 | Batch_idx: 380 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (46277/48768)
Epoch: 133 | Batch_idx: 390 |  Loss: (0.1507) |  Loss2: (0.0000) | Acc: (94.00%) (47433/50000)
# TEST : Loss: (0.4242) | Acc: (87.00%) (8763/10000)
percent tensor([0.5815, 0.4185], device='cuda:0')
percent tensor([0.5750, 0.4250], device='cuda:0')
percent tensor([0.5498, 0.4502], device='cuda:0')
percent tensor([0.6240, 0.3760], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.6651, 0.3349], device='cuda:0')
percent tensor([0.7073, 0.2927], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 134 | Batch_idx: 0 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 134 | Batch_idx: 10 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 134 | Batch_idx: 20 |  Loss: (0.1465) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 134 | Batch_idx: 30 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (3748/3968)
Epoch: 134 | Batch_idx: 40 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (4972/5248)
Epoch: 134 | Batch_idx: 50 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (94.00%) (6189/6528)
Epoch: 134 | Batch_idx: 60 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (7404/7808)
Epoch: 134 | Batch_idx: 70 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (8621/9088)
Epoch: 134 | Batch_idx: 80 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (9837/10368)
Epoch: 134 | Batch_idx: 90 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (94.00%) (11053/11648)
Epoch: 134 | Batch_idx: 100 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (12263/12928)
Epoch: 134 | Batch_idx: 110 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (13475/14208)
Epoch: 134 | Batch_idx: 120 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (14694/15488)
Epoch: 134 | Batch_idx: 130 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (94.00%) (15923/16768)
Epoch: 134 | Batch_idx: 140 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (94.00%) (17141/18048)
Epoch: 134 | Batch_idx: 150 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (18368/19328)
Epoch: 134 | Batch_idx: 160 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (19584/20608)
Epoch: 134 | Batch_idx: 170 |  Loss: (0.1427) |  Loss2: (0.0000) | Acc: (95.00%) (20803/21888)
Epoch: 134 | Batch_idx: 180 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (94.00%) (21995/23168)
Epoch: 134 | Batch_idx: 190 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (23217/24448)
Epoch: 134 | Batch_idx: 200 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (24445/25728)
Epoch: 134 | Batch_idx: 210 |  Loss: (0.1444) |  Loss2: (0.0000) | Acc: (94.00%) (25656/27008)
Epoch: 134 | Batch_idx: 220 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (26872/28288)
Epoch: 134 | Batch_idx: 230 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (28092/29568)
Epoch: 134 | Batch_idx: 240 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (29305/30848)
Epoch: 134 | Batch_idx: 250 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (30516/32128)
Epoch: 134 | Batch_idx: 260 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (94.00%) (31729/33408)
Epoch: 134 | Batch_idx: 270 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (94.00%) (32944/34688)
Epoch: 134 | Batch_idx: 280 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (34147/35968)
Epoch: 134 | Batch_idx: 290 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (35354/37248)
Epoch: 134 | Batch_idx: 300 |  Loss: (0.1477) |  Loss2: (0.0000) | Acc: (94.00%) (36566/38528)
Epoch: 134 | Batch_idx: 310 |  Loss: (0.1471) |  Loss2: (0.0000) | Acc: (94.00%) (37789/39808)
Epoch: 134 | Batch_idx: 320 |  Loss: (0.1473) |  Loss2: (0.0000) | Acc: (94.00%) (38997/41088)
Epoch: 134 | Batch_idx: 330 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (40219/42368)
Epoch: 134 | Batch_idx: 340 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (41442/43648)
Epoch: 134 | Batch_idx: 350 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (42663/44928)
Epoch: 134 | Batch_idx: 360 |  Loss: (0.1461) |  Loss2: (0.0000) | Acc: (94.00%) (43883/46208)
Epoch: 134 | Batch_idx: 370 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (45100/47488)
Epoch: 134 | Batch_idx: 380 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (46322/48768)
Epoch: 134 | Batch_idx: 390 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (47481/50000)
# TEST : Loss: (0.4392) | Acc: (87.00%) (8735/10000)
percent tensor([0.5810, 0.4190], device='cuda:0')
percent tensor([0.5748, 0.4252], device='cuda:0')
percent tensor([0.5489, 0.4511], device='cuda:0')
percent tensor([0.6251, 0.3749], device='cuda:0')
percent tensor([0.5995, 0.4005], device='cuda:0')
percent tensor([0.6646, 0.3354], device='cuda:0')
percent tensor([0.6999, 0.3001], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 135 | Batch_idx: 0 |  Loss: (0.2500) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 135 | Batch_idx: 10 |  Loss: (0.1684) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 135 | Batch_idx: 20 |  Loss: (0.1742) |  Loss2: (0.0000) | Acc: (93.00%) (2524/2688)
Epoch: 135 | Batch_idx: 30 |  Loss: (0.1785) |  Loss2: (0.0000) | Acc: (93.00%) (3718/3968)
Epoch: 135 | Batch_idx: 40 |  Loss: (0.1821) |  Loss2: (0.0000) | Acc: (93.00%) (4904/5248)
Epoch: 135 | Batch_idx: 50 |  Loss: (0.1829) |  Loss2: (0.0000) | Acc: (93.00%) (6097/6528)
Epoch: 135 | Batch_idx: 60 |  Loss: (0.1839) |  Loss2: (0.0000) | Acc: (93.00%) (7296/7808)
Epoch: 135 | Batch_idx: 70 |  Loss: (0.1816) |  Loss2: (0.0000) | Acc: (93.00%) (8501/9088)
Epoch: 135 | Batch_idx: 80 |  Loss: (0.1852) |  Loss2: (0.0000) | Acc: (93.00%) (9684/10368)
Epoch: 135 | Batch_idx: 90 |  Loss: (0.1843) |  Loss2: (0.0000) | Acc: (93.00%) (10877/11648)
Epoch: 135 | Batch_idx: 100 |  Loss: (0.1844) |  Loss2: (0.0000) | Acc: (93.00%) (12078/12928)
Epoch: 135 | Batch_idx: 110 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (13284/14208)
Epoch: 135 | Batch_idx: 120 |  Loss: (0.1847) |  Loss2: (0.0000) | Acc: (93.00%) (14474/15488)
Epoch: 135 | Batch_idx: 130 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (15654/16768)
Epoch: 135 | Batch_idx: 140 |  Loss: (0.1880) |  Loss2: (0.0000) | Acc: (93.00%) (16835/18048)
Epoch: 135 | Batch_idx: 150 |  Loss: (0.1866) |  Loss2: (0.0000) | Acc: (93.00%) (18035/19328)
Epoch: 135 | Batch_idx: 160 |  Loss: (0.1863) |  Loss2: (0.0000) | Acc: (93.00%) (19236/20608)
Epoch: 135 | Batch_idx: 170 |  Loss: (0.1868) |  Loss2: (0.0000) | Acc: (93.00%) (20424/21888)
Epoch: 135 | Batch_idx: 180 |  Loss: (0.1879) |  Loss2: (0.0000) | Acc: (93.00%) (21606/23168)
Epoch: 135 | Batch_idx: 190 |  Loss: (0.1865) |  Loss2: (0.0000) | Acc: (93.00%) (22818/24448)
Epoch: 135 | Batch_idx: 200 |  Loss: (0.1861) |  Loss2: (0.0000) | Acc: (93.00%) (24019/25728)
Epoch: 135 | Batch_idx: 210 |  Loss: (0.1862) |  Loss2: (0.0000) | Acc: (93.00%) (25218/27008)
Epoch: 135 | Batch_idx: 220 |  Loss: (0.1855) |  Loss2: (0.0000) | Acc: (93.00%) (26415/28288)
Epoch: 135 | Batch_idx: 230 |  Loss: (0.1846) |  Loss2: (0.0000) | Acc: (93.00%) (27622/29568)
Epoch: 135 | Batch_idx: 240 |  Loss: (0.1832) |  Loss2: (0.0000) | Acc: (93.00%) (28837/30848)
Epoch: 135 | Batch_idx: 250 |  Loss: (0.1835) |  Loss2: (0.0000) | Acc: (93.00%) (30036/32128)
Epoch: 135 | Batch_idx: 260 |  Loss: (0.1824) |  Loss2: (0.0000) | Acc: (93.00%) (31250/33408)
Epoch: 135 | Batch_idx: 270 |  Loss: (0.1822) |  Loss2: (0.0000) | Acc: (93.00%) (32445/34688)
Epoch: 135 | Batch_idx: 280 |  Loss: (0.1819) |  Loss2: (0.0000) | Acc: (93.00%) (33645/35968)
Epoch: 135 | Batch_idx: 290 |  Loss: (0.1818) |  Loss2: (0.0000) | Acc: (93.00%) (34847/37248)
Epoch: 135 | Batch_idx: 300 |  Loss: (0.1813) |  Loss2: (0.0000) | Acc: (93.00%) (36057/38528)
Epoch: 135 | Batch_idx: 310 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (37265/39808)
Epoch: 135 | Batch_idx: 320 |  Loss: (0.1804) |  Loss2: (0.0000) | Acc: (93.00%) (38459/41088)
Epoch: 135 | Batch_idx: 330 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (39667/42368)
Epoch: 135 | Batch_idx: 340 |  Loss: (0.1794) |  Loss2: (0.0000) | Acc: (93.00%) (40865/43648)
Epoch: 135 | Batch_idx: 350 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (42076/44928)
Epoch: 135 | Batch_idx: 360 |  Loss: (0.1789) |  Loss2: (0.0000) | Acc: (93.00%) (43284/46208)
Epoch: 135 | Batch_idx: 370 |  Loss: (0.1791) |  Loss2: (0.0000) | Acc: (93.00%) (44472/47488)
Epoch: 135 | Batch_idx: 380 |  Loss: (0.1783) |  Loss2: (0.0000) | Acc: (93.00%) (45686/48768)
Epoch: 135 | Batch_idx: 390 |  Loss: (0.1776) |  Loss2: (0.0000) | Acc: (93.00%) (46854/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_135.pth.tar'
# TEST : Loss: (0.4261) | Acc: (87.00%) (8733/10000)
percent tensor([0.5779, 0.4221], device='cuda:0')
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.6418, 0.3582], device='cuda:0')
percent tensor([0.5903, 0.4097], device='cuda:0')
percent tensor([0.6461, 0.3539], device='cuda:0')
percent tensor([0.7111, 0.2889], device='cuda:0')
percent tensor([0.9996, 0.0004], device='cuda:0')
Epoch: 136 | Batch_idx: 0 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 136 | Batch_idx: 10 |  Loss: (0.1668) |  Loss2: (0.0000) | Acc: (94.00%) (1329/1408)
Epoch: 136 | Batch_idx: 20 |  Loss: (0.1651) |  Loss2: (0.0000) | Acc: (94.00%) (2535/2688)
Epoch: 136 | Batch_idx: 30 |  Loss: (0.1711) |  Loss2: (0.0000) | Acc: (94.00%) (3739/3968)
Epoch: 136 | Batch_idx: 40 |  Loss: (0.1699) |  Loss2: (0.0000) | Acc: (94.00%) (4954/5248)
Epoch: 136 | Batch_idx: 50 |  Loss: (0.1685) |  Loss2: (0.0000) | Acc: (94.00%) (6159/6528)
Epoch: 136 | Batch_idx: 60 |  Loss: (0.1696) |  Loss2: (0.0000) | Acc: (94.00%) (7353/7808)
Epoch: 136 | Batch_idx: 70 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (8581/9088)
Epoch: 136 | Batch_idx: 80 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (9794/10368)
Epoch: 136 | Batch_idx: 90 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (11010/11648)
Epoch: 136 | Batch_idx: 100 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (12202/12928)
Epoch: 136 | Batch_idx: 110 |  Loss: (0.1620) |  Loss2: (0.0000) | Acc: (94.00%) (13411/14208)
Epoch: 136 | Batch_idx: 120 |  Loss: (0.1596) |  Loss2: (0.0000) | Acc: (94.00%) (14633/15488)
Epoch: 136 | Batch_idx: 130 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (15837/16768)
Epoch: 136 | Batch_idx: 140 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (17030/18048)
Epoch: 136 | Batch_idx: 150 |  Loss: (0.1613) |  Loss2: (0.0000) | Acc: (94.00%) (18229/19328)
Epoch: 136 | Batch_idx: 160 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (19433/20608)
Epoch: 136 | Batch_idx: 170 |  Loss: (0.1637) |  Loss2: (0.0000) | Acc: (94.00%) (20618/21888)
Epoch: 136 | Batch_idx: 180 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (21806/23168)
Epoch: 136 | Batch_idx: 190 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (23020/24448)
Epoch: 136 | Batch_idx: 200 |  Loss: (0.1652) |  Loss2: (0.0000) | Acc: (94.00%) (24232/25728)
Epoch: 136 | Batch_idx: 210 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (25448/27008)
Epoch: 136 | Batch_idx: 220 |  Loss: (0.1638) |  Loss2: (0.0000) | Acc: (94.00%) (26663/28288)
Epoch: 136 | Batch_idx: 230 |  Loss: (0.1640) |  Loss2: (0.0000) | Acc: (94.00%) (27870/29568)
Epoch: 136 | Batch_idx: 240 |  Loss: (0.1642) |  Loss2: (0.0000) | Acc: (94.00%) (29068/30848)
Epoch: 136 | Batch_idx: 250 |  Loss: (0.1647) |  Loss2: (0.0000) | Acc: (94.00%) (30267/32128)
Epoch: 136 | Batch_idx: 260 |  Loss: (0.1644) |  Loss2: (0.0000) | Acc: (94.00%) (31481/33408)
Epoch: 136 | Batch_idx: 270 |  Loss: (0.1646) |  Loss2: (0.0000) | Acc: (94.00%) (32678/34688)
Epoch: 136 | Batch_idx: 280 |  Loss: (0.1645) |  Loss2: (0.0000) | Acc: (94.00%) (33883/35968)
Epoch: 136 | Batch_idx: 290 |  Loss: (0.1641) |  Loss2: (0.0000) | Acc: (94.00%) (35090/37248)
Epoch: 136 | Batch_idx: 300 |  Loss: (0.1631) |  Loss2: (0.0000) | Acc: (94.00%) (36308/38528)
Epoch: 136 | Batch_idx: 310 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (37530/39808)
Epoch: 136 | Batch_idx: 320 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (38751/41088)
Epoch: 136 | Batch_idx: 330 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (39955/42368)
Epoch: 136 | Batch_idx: 340 |  Loss: (0.1619) |  Loss2: (0.0000) | Acc: (94.00%) (41166/43648)
Epoch: 136 | Batch_idx: 350 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (42388/44928)
Epoch: 136 | Batch_idx: 360 |  Loss: (0.1605) |  Loss2: (0.0000) | Acc: (94.00%) (43609/46208)
Epoch: 136 | Batch_idx: 370 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (44824/47488)
Epoch: 136 | Batch_idx: 380 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (46034/48768)
Epoch: 136 | Batch_idx: 390 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (47194/50000)
# TEST : Loss: (0.4075) | Acc: (87.00%) (8793/10000)
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5801, 0.4199], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.6414, 0.3586], device='cuda:0')
percent tensor([0.5937, 0.4063], device='cuda:0')
percent tensor([0.6483, 0.3517], device='cuda:0')
percent tensor([0.7261, 0.2739], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 137 | Batch_idx: 0 |  Loss: (0.1815) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 137 | Batch_idx: 10 |  Loss: (0.1639) |  Loss2: (0.0000) | Acc: (94.00%) (1326/1408)
Epoch: 137 | Batch_idx: 20 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (94.00%) (2528/2688)
Epoch: 137 | Batch_idx: 30 |  Loss: (0.1621) |  Loss2: (0.0000) | Acc: (94.00%) (3740/3968)
Epoch: 137 | Batch_idx: 40 |  Loss: (0.1678) |  Loss2: (0.0000) | Acc: (94.00%) (4939/5248)
Epoch: 137 | Batch_idx: 50 |  Loss: (0.1728) |  Loss2: (0.0000) | Acc: (94.00%) (6138/6528)
Epoch: 137 | Batch_idx: 60 |  Loss: (0.1716) |  Loss2: (0.0000) | Acc: (94.00%) (7344/7808)
Epoch: 137 | Batch_idx: 70 |  Loss: (0.1671) |  Loss2: (0.0000) | Acc: (94.00%) (8560/9088)
Epoch: 137 | Batch_idx: 80 |  Loss: (0.1661) |  Loss2: (0.0000) | Acc: (94.00%) (9767/10368)
Epoch: 137 | Batch_idx: 90 |  Loss: (0.1635) |  Loss2: (0.0000) | Acc: (94.00%) (10980/11648)
Epoch: 137 | Batch_idx: 100 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (12201/12928)
Epoch: 137 | Batch_idx: 110 |  Loss: (0.1622) |  Loss2: (0.0000) | Acc: (94.00%) (13404/14208)
Epoch: 137 | Batch_idx: 120 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (14609/15488)
Epoch: 137 | Batch_idx: 130 |  Loss: (0.1612) |  Loss2: (0.0000) | Acc: (94.00%) (15826/16768)
Epoch: 137 | Batch_idx: 140 |  Loss: (0.1608) |  Loss2: (0.0000) | Acc: (94.00%) (17035/18048)
Epoch: 137 | Batch_idx: 150 |  Loss: (0.1598) |  Loss2: (0.0000) | Acc: (94.00%) (18255/19328)
Epoch: 137 | Batch_idx: 160 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (19477/20608)
Epoch: 137 | Batch_idx: 170 |  Loss: (0.1573) |  Loss2: (0.0000) | Acc: (94.00%) (20699/21888)
Epoch: 137 | Batch_idx: 180 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (21923/23168)
Epoch: 137 | Batch_idx: 190 |  Loss: (0.1561) |  Loss2: (0.0000) | Acc: (94.00%) (23129/24448)
Epoch: 137 | Batch_idx: 200 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (24338/25728)
Epoch: 137 | Batch_idx: 210 |  Loss: (0.1548) |  Loss2: (0.0000) | Acc: (94.00%) (25562/27008)
Epoch: 137 | Batch_idx: 220 |  Loss: (0.1550) |  Loss2: (0.0000) | Acc: (94.00%) (26772/28288)
Epoch: 137 | Batch_idx: 230 |  Loss: (0.1549) |  Loss2: (0.0000) | Acc: (94.00%) (27993/29568)
Epoch: 137 | Batch_idx: 240 |  Loss: (0.1545) |  Loss2: (0.0000) | Acc: (94.00%) (29209/30848)
Epoch: 137 | Batch_idx: 250 |  Loss: (0.1540) |  Loss2: (0.0000) | Acc: (94.00%) (30430/32128)
Epoch: 137 | Batch_idx: 260 |  Loss: (0.1532) |  Loss2: (0.0000) | Acc: (94.00%) (31650/33408)
Epoch: 137 | Batch_idx: 270 |  Loss: (0.1524) |  Loss2: (0.0000) | Acc: (94.00%) (32870/34688)
Epoch: 137 | Batch_idx: 280 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (34088/35968)
Epoch: 137 | Batch_idx: 290 |  Loss: (0.1525) |  Loss2: (0.0000) | Acc: (94.00%) (35293/37248)
Epoch: 137 | Batch_idx: 300 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (36512/38528)
Epoch: 137 | Batch_idx: 310 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (37727/39808)
Epoch: 137 | Batch_idx: 320 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (38945/41088)
Epoch: 137 | Batch_idx: 330 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (40163/42368)
Epoch: 137 | Batch_idx: 340 |  Loss: (0.1510) |  Loss2: (0.0000) | Acc: (94.00%) (41386/43648)
Epoch: 137 | Batch_idx: 350 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (42613/44928)
Epoch: 137 | Batch_idx: 360 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (43815/46208)
Epoch: 137 | Batch_idx: 370 |  Loss: (0.1505) |  Loss2: (0.0000) | Acc: (94.00%) (45039/47488)
Epoch: 137 | Batch_idx: 380 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (46248/48768)
Epoch: 137 | Batch_idx: 390 |  Loss: (0.1503) |  Loss2: (0.0000) | Acc: (94.00%) (47425/50000)
# TEST : Loss: (0.4035) | Acc: (88.00%) (8807/10000)
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.5647, 0.4353], device='cuda:0')
percent tensor([0.6451, 0.3549], device='cuda:0')
percent tensor([0.5962, 0.4038], device='cuda:0')
percent tensor([0.6568, 0.3432], device='cuda:0')
percent tensor([0.7368, 0.2632], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 138 | Batch_idx: 0 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 138 | Batch_idx: 10 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 138 | Batch_idx: 20 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (96.00%) (2585/2688)
Epoch: 138 | Batch_idx: 30 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (3804/3968)
Epoch: 138 | Batch_idx: 40 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (5028/5248)
Epoch: 138 | Batch_idx: 50 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (6243/6528)
Epoch: 138 | Batch_idx: 60 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (7455/7808)
Epoch: 138 | Batch_idx: 70 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (8674/9088)
Epoch: 138 | Batch_idx: 80 |  Loss: (0.1377) |  Loss2: (0.0000) | Acc: (95.00%) (9904/10368)
Epoch: 138 | Batch_idx: 90 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (11112/11648)
Epoch: 138 | Batch_idx: 100 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (12338/12928)
Epoch: 138 | Batch_idx: 110 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (13555/14208)
Epoch: 138 | Batch_idx: 120 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (14778/15488)
Epoch: 138 | Batch_idx: 130 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (15996/16768)
Epoch: 138 | Batch_idx: 140 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (17207/18048)
Epoch: 138 | Batch_idx: 150 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (18431/19328)
Epoch: 138 | Batch_idx: 160 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (19652/20608)
Epoch: 138 | Batch_idx: 170 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (20877/21888)
Epoch: 138 | Batch_idx: 180 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (22089/23168)
Epoch: 138 | Batch_idx: 190 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (23314/24448)
Epoch: 138 | Batch_idx: 200 |  Loss: (0.1409) |  Loss2: (0.0000) | Acc: (95.00%) (24527/25728)
Epoch: 138 | Batch_idx: 210 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (25730/27008)
Epoch: 138 | Batch_idx: 220 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (26937/28288)
Epoch: 138 | Batch_idx: 230 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (28139/29568)
Epoch: 138 | Batch_idx: 240 |  Loss: (0.1425) |  Loss2: (0.0000) | Acc: (95.00%) (29356/30848)
Epoch: 138 | Batch_idx: 250 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (30566/32128)
Epoch: 138 | Batch_idx: 260 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (31768/33408)
Epoch: 138 | Batch_idx: 270 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (32986/34688)
Epoch: 138 | Batch_idx: 280 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (34209/35968)
Epoch: 138 | Batch_idx: 290 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (35430/37248)
Epoch: 138 | Batch_idx: 300 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (36642/38528)
Epoch: 138 | Batch_idx: 310 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (37839/39808)
Epoch: 138 | Batch_idx: 320 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (39060/41088)
Epoch: 138 | Batch_idx: 330 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (40275/42368)
Epoch: 138 | Batch_idx: 340 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (41488/43648)
Epoch: 138 | Batch_idx: 350 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (42712/44928)
Epoch: 138 | Batch_idx: 360 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (95.00%) (43935/46208)
Epoch: 138 | Batch_idx: 370 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (45154/47488)
Epoch: 138 | Batch_idx: 380 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (46381/48768)
Epoch: 138 | Batch_idx: 390 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (47555/50000)
# TEST : Loss: (0.3944) | Acc: (88.00%) (8829/10000)
percent tensor([0.5823, 0.4177], device='cuda:0')
percent tensor([0.5808, 0.4192], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.5912, 0.4088], device='cuda:0')
percent tensor([0.6580, 0.3420], device='cuda:0')
percent tensor([0.7324, 0.2676], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 139 | Batch_idx: 0 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 139 | Batch_idx: 10 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 139 | Batch_idx: 20 |  Loss: (0.1371) |  Loss2: (0.0000) | Acc: (95.00%) (2560/2688)
Epoch: 139 | Batch_idx: 30 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (3771/3968)
Epoch: 139 | Batch_idx: 40 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (4997/5248)
Epoch: 139 | Batch_idx: 50 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (94.00%) (6196/6528)
Epoch: 139 | Batch_idx: 60 |  Loss: (0.1417) |  Loss2: (0.0000) | Acc: (94.00%) (7409/7808)
Epoch: 139 | Batch_idx: 70 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (94.00%) (8633/9088)
Epoch: 139 | Batch_idx: 80 |  Loss: (0.1411) |  Loss2: (0.0000) | Acc: (94.00%) (9844/10368)
Epoch: 139 | Batch_idx: 90 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (11075/11648)
Epoch: 139 | Batch_idx: 100 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (12297/12928)
Epoch: 139 | Batch_idx: 110 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (13509/14208)
Epoch: 139 | Batch_idx: 120 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (14722/15488)
Epoch: 139 | Batch_idx: 130 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (15944/16768)
Epoch: 139 | Batch_idx: 140 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (17163/18048)
Epoch: 139 | Batch_idx: 150 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (18381/19328)
Epoch: 139 | Batch_idx: 160 |  Loss: (0.1393) |  Loss2: (0.0000) | Acc: (95.00%) (19601/20608)
Epoch: 139 | Batch_idx: 170 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (20806/21888)
Epoch: 139 | Batch_idx: 180 |  Loss: (0.1398) |  Loss2: (0.0000) | Acc: (95.00%) (22024/23168)
Epoch: 139 | Batch_idx: 190 |  Loss: (0.1407) |  Loss2: (0.0000) | Acc: (95.00%) (23231/24448)
Epoch: 139 | Batch_idx: 200 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (24469/25728)
Epoch: 139 | Batch_idx: 210 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (25691/27008)
Epoch: 139 | Batch_idx: 220 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (26903/28288)
Epoch: 139 | Batch_idx: 230 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (28120/29568)
Epoch: 139 | Batch_idx: 240 |  Loss: (0.1416) |  Loss2: (0.0000) | Acc: (95.00%) (29320/30848)
Epoch: 139 | Batch_idx: 250 |  Loss: (0.1410) |  Loss2: (0.0000) | Acc: (95.00%) (30547/32128)
Epoch: 139 | Batch_idx: 260 |  Loss: (0.1405) |  Loss2: (0.0000) | Acc: (95.00%) (31764/33408)
Epoch: 139 | Batch_idx: 270 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (32995/34688)
Epoch: 139 | Batch_idx: 280 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (34227/35968)
Epoch: 139 | Batch_idx: 290 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (35446/37248)
Epoch: 139 | Batch_idx: 300 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (36667/38528)
Epoch: 139 | Batch_idx: 310 |  Loss: (0.1395) |  Loss2: (0.0000) | Acc: (95.00%) (37888/39808)
Epoch: 139 | Batch_idx: 320 |  Loss: (0.1396) |  Loss2: (0.0000) | Acc: (95.00%) (39105/41088)
Epoch: 139 | Batch_idx: 330 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (95.00%) (40340/42368)
Epoch: 139 | Batch_idx: 340 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (41565/43648)
Epoch: 139 | Batch_idx: 350 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (42788/44928)
Epoch: 139 | Batch_idx: 360 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (44012/46208)
Epoch: 139 | Batch_idx: 370 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (45234/47488)
Epoch: 139 | Batch_idx: 380 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (46456/48768)
Epoch: 139 | Batch_idx: 390 |  Loss: (0.1388) |  Loss2: (0.0000) | Acc: (95.00%) (47621/50000)
# TEST : Loss: (0.3908) | Acc: (88.00%) (8825/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.5604, 0.4396], device='cuda:0')
percent tensor([0.6397, 0.3603], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6630, 0.3370], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 140 | Batch_idx: 0 |  Loss: (0.2125) |  Loss2: (0.0000) | Acc: (90.00%) (116/128)
Epoch: 140 | Batch_idx: 10 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 140 | Batch_idx: 20 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (94.00%) (2549/2688)
Epoch: 140 | Batch_idx: 30 |  Loss: (0.1426) |  Loss2: (0.0000) | Acc: (94.00%) (3762/3968)
Epoch: 140 | Batch_idx: 40 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (4988/5248)
Epoch: 140 | Batch_idx: 50 |  Loss: (0.1470) |  Loss2: (0.0000) | Acc: (94.00%) (6201/6528)
Epoch: 140 | Batch_idx: 60 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (7412/7808)
Epoch: 140 | Batch_idx: 70 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (8630/9088)
Epoch: 140 | Batch_idx: 80 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (9853/10368)
Epoch: 140 | Batch_idx: 90 |  Loss: (0.1441) |  Loss2: (0.0000) | Acc: (95.00%) (11075/11648)
Epoch: 140 | Batch_idx: 100 |  Loss: (0.1455) |  Loss2: (0.0000) | Acc: (95.00%) (12283/12928)
Epoch: 140 | Batch_idx: 110 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (94.00%) (13494/14208)
Epoch: 140 | Batch_idx: 120 |  Loss: (0.1433) |  Loss2: (0.0000) | Acc: (95.00%) (14729/15488)
Epoch: 140 | Batch_idx: 130 |  Loss: (0.1424) |  Loss2: (0.0000) | Acc: (95.00%) (15953/16768)
Epoch: 140 | Batch_idx: 140 |  Loss: (0.1408) |  Loss2: (0.0000) | Acc: (95.00%) (17182/18048)
Epoch: 140 | Batch_idx: 150 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (18384/19328)
Epoch: 140 | Batch_idx: 160 |  Loss: (0.1429) |  Loss2: (0.0000) | Acc: (95.00%) (19604/20608)
Epoch: 140 | Batch_idx: 170 |  Loss: (0.1438) |  Loss2: (0.0000) | Acc: (95.00%) (20809/21888)
Epoch: 140 | Batch_idx: 180 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (22029/23168)
Epoch: 140 | Batch_idx: 190 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (95.00%) (23238/24448)
Epoch: 140 | Batch_idx: 200 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (95.00%) (24450/25728)
Epoch: 140 | Batch_idx: 210 |  Loss: (0.1454) |  Loss2: (0.0000) | Acc: (95.00%) (25663/27008)
Epoch: 140 | Batch_idx: 220 |  Loss: (0.1445) |  Loss2: (0.0000) | Acc: (95.00%) (26887/28288)
Epoch: 140 | Batch_idx: 230 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (95.00%) (28103/29568)
Epoch: 140 | Batch_idx: 240 |  Loss: (0.1442) |  Loss2: (0.0000) | Acc: (95.00%) (29331/30848)
Epoch: 140 | Batch_idx: 250 |  Loss: (0.1451) |  Loss2: (0.0000) | Acc: (95.00%) (30538/32128)
Epoch: 140 | Batch_idx: 260 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (31743/33408)
Epoch: 140 | Batch_idx: 270 |  Loss: (0.1459) |  Loss2: (0.0000) | Acc: (95.00%) (32958/34688)
Epoch: 140 | Batch_idx: 280 |  Loss: (0.1456) |  Loss2: (0.0000) | Acc: (95.00%) (34177/35968)
Epoch: 140 | Batch_idx: 290 |  Loss: (0.1457) |  Loss2: (0.0000) | Acc: (95.00%) (35394/37248)
Epoch: 140 | Batch_idx: 300 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (36591/38528)
Epoch: 140 | Batch_idx: 310 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (37803/39808)
Epoch: 140 | Batch_idx: 320 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (39025/41088)
Epoch: 140 | Batch_idx: 330 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (40227/42368)
Epoch: 140 | Batch_idx: 340 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (41450/43648)
Epoch: 140 | Batch_idx: 350 |  Loss: (0.1469) |  Loss2: (0.0000) | Acc: (94.00%) (42659/44928)
Epoch: 140 | Batch_idx: 360 |  Loss: (0.1467) |  Loss2: (0.0000) | Acc: (94.00%) (43876/46208)
Epoch: 140 | Batch_idx: 370 |  Loss: (0.1466) |  Loss2: (0.0000) | Acc: (94.00%) (45090/47488)
Epoch: 140 | Batch_idx: 380 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (46307/48768)
Epoch: 140 | Batch_idx: 390 |  Loss: (0.1464) |  Loss2: (0.0000) | Acc: (94.00%) (47479/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_140.pth.tar'
# TEST : Loss: (0.4362) | Acc: (87.00%) (8774/10000)
percent tensor([0.5793, 0.4207], device='cuda:0')
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5607, 0.4393], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.6628, 0.3372], device='cuda:0')
percent tensor([0.7347, 0.2653], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.4400, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(821.9816, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(822.1249, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1521.1364, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(487.0274, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2272.9983, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4269.7402, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1374.2428, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6205.4424, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11733.7725, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3871.2607, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16373.6230, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 141 | Batch_idx: 0 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 141 | Batch_idx: 10 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (1346/1408)
Epoch: 141 | Batch_idx: 20 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 141 | Batch_idx: 30 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (3802/3968)
Epoch: 141 | Batch_idx: 40 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (5018/5248)
Epoch: 141 | Batch_idx: 50 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 141 | Batch_idx: 60 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (7468/7808)
Epoch: 141 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8703/9088)
Epoch: 141 | Batch_idx: 80 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (9926/10368)
Epoch: 141 | Batch_idx: 90 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (11141/11648)
Epoch: 141 | Batch_idx: 100 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (12366/12928)
Epoch: 141 | Batch_idx: 110 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (13589/14208)
Epoch: 141 | Batch_idx: 120 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (14799/15488)
Epoch: 141 | Batch_idx: 130 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (16024/16768)
Epoch: 141 | Batch_idx: 140 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (17251/18048)
Epoch: 141 | Batch_idx: 150 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (18476/19328)
Epoch: 141 | Batch_idx: 160 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (19692/20608)
Epoch: 141 | Batch_idx: 170 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (20911/21888)
Epoch: 141 | Batch_idx: 180 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (22120/23168)
Epoch: 141 | Batch_idx: 190 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (23336/24448)
Epoch: 141 | Batch_idx: 200 |  Loss: (0.1324) |  Loss2: (0.0000) | Acc: (95.00%) (24556/25728)
Epoch: 141 | Batch_idx: 210 |  Loss: (0.1325) |  Loss2: (0.0000) | Acc: (95.00%) (25771/27008)
Epoch: 141 | Batch_idx: 220 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (26983/28288)
Epoch: 141 | Batch_idx: 230 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (28212/29568)
Epoch: 141 | Batch_idx: 240 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (29416/30848)
Epoch: 141 | Batch_idx: 250 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (30632/32128)
Epoch: 141 | Batch_idx: 260 |  Loss: (0.1348) |  Loss2: (0.0000) | Acc: (95.00%) (31856/33408)
Epoch: 141 | Batch_idx: 270 |  Loss: (0.1356) |  Loss2: (0.0000) | Acc: (95.00%) (33067/34688)
Epoch: 141 | Batch_idx: 280 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (34286/35968)
Epoch: 141 | Batch_idx: 290 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (35513/37248)
Epoch: 141 | Batch_idx: 300 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (36727/38528)
Epoch: 141 | Batch_idx: 310 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (37934/39808)
Epoch: 141 | Batch_idx: 320 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (39150/41088)
Epoch: 141 | Batch_idx: 330 |  Loss: (0.1382) |  Loss2: (0.0000) | Acc: (95.00%) (40359/42368)
Epoch: 141 | Batch_idx: 340 |  Loss: (0.1386) |  Loss2: (0.0000) | Acc: (95.00%) (41577/43648)
Epoch: 141 | Batch_idx: 350 |  Loss: (0.1385) |  Loss2: (0.0000) | Acc: (95.00%) (42794/44928)
Epoch: 141 | Batch_idx: 360 |  Loss: (0.1392) |  Loss2: (0.0000) | Acc: (95.00%) (44003/46208)
Epoch: 141 | Batch_idx: 370 |  Loss: (0.1397) |  Loss2: (0.0000) | Acc: (95.00%) (45210/47488)
Epoch: 141 | Batch_idx: 380 |  Loss: (0.1399) |  Loss2: (0.0000) | Acc: (95.00%) (46424/48768)
Epoch: 141 | Batch_idx: 390 |  Loss: (0.1403) |  Loss2: (0.0000) | Acc: (95.00%) (47591/50000)
# TEST : Loss: (0.4210) | Acc: (88.00%) (8808/10000)
percent tensor([0.5791, 0.4209], device='cuda:0')
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5608, 0.4392], device='cuda:0')
percent tensor([0.6381, 0.3619], device='cuda:0')
percent tensor([0.5961, 0.4039], device='cuda:0')
percent tensor([0.6612, 0.3388], device='cuda:0')
percent tensor([0.7333, 0.2667], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 142 | Batch_idx: 0 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 142 | Batch_idx: 10 |  Loss: (0.1125) |  Loss2: (0.0000) | Acc: (96.00%) (1359/1408)
Epoch: 142 | Batch_idx: 20 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 142 | Batch_idx: 30 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (3827/3968)
Epoch: 142 | Batch_idx: 40 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 142 | Batch_idx: 50 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (6258/6528)
Epoch: 142 | Batch_idx: 60 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (7490/7808)
Epoch: 142 | Batch_idx: 70 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (8716/9088)
Epoch: 142 | Batch_idx: 80 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (9936/10368)
Epoch: 142 | Batch_idx: 90 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (95.00%) (11155/11648)
Epoch: 142 | Batch_idx: 100 |  Loss: (0.1229) |  Loss2: (0.0000) | Acc: (95.00%) (12379/12928)
Epoch: 142 | Batch_idx: 110 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (13611/14208)
Epoch: 142 | Batch_idx: 120 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (14832/15488)
Epoch: 142 | Batch_idx: 130 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (16051/16768)
Epoch: 142 | Batch_idx: 140 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (17274/18048)
Epoch: 142 | Batch_idx: 150 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (18497/19328)
Epoch: 142 | Batch_idx: 160 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (19720/20608)
Epoch: 142 | Batch_idx: 170 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (20928/21888)
Epoch: 142 | Batch_idx: 180 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (22152/23168)
Epoch: 142 | Batch_idx: 190 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (23362/24448)
Epoch: 142 | Batch_idx: 200 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (24570/25728)
Epoch: 142 | Batch_idx: 210 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (25778/27008)
Epoch: 142 | Batch_idx: 220 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (26984/28288)
Epoch: 142 | Batch_idx: 230 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (28206/29568)
Epoch: 142 | Batch_idx: 240 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (29418/30848)
Epoch: 142 | Batch_idx: 250 |  Loss: (0.1336) |  Loss2: (0.0000) | Acc: (95.00%) (30635/32128)
Epoch: 142 | Batch_idx: 260 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (31861/33408)
Epoch: 142 | Batch_idx: 270 |  Loss: (0.1337) |  Loss2: (0.0000) | Acc: (95.00%) (33090/34688)
Epoch: 142 | Batch_idx: 280 |  Loss: (0.1339) |  Loss2: (0.0000) | Acc: (95.00%) (34308/35968)
Epoch: 142 | Batch_idx: 290 |  Loss: (0.1344) |  Loss2: (0.0000) | Acc: (95.00%) (35512/37248)
Epoch: 142 | Batch_idx: 300 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (36717/38528)
Epoch: 142 | Batch_idx: 310 |  Loss: (0.1355) |  Loss2: (0.0000) | Acc: (95.00%) (37923/39808)
Epoch: 142 | Batch_idx: 320 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (39118/41088)
Epoch: 142 | Batch_idx: 330 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (40340/42368)
Epoch: 142 | Batch_idx: 340 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (41561/43648)
Epoch: 142 | Batch_idx: 350 |  Loss: (0.1367) |  Loss2: (0.0000) | Acc: (95.00%) (42784/44928)
Epoch: 142 | Batch_idx: 360 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (44000/46208)
Epoch: 142 | Batch_idx: 370 |  Loss: (0.1372) |  Loss2: (0.0000) | Acc: (95.00%) (45220/47488)
Epoch: 142 | Batch_idx: 380 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (46437/48768)
Epoch: 142 | Batch_idx: 390 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (47606/50000)
# TEST : Loss: (0.4860) | Acc: (86.00%) (8648/10000)
percent tensor([0.5794, 0.4206], device='cuda:0')
percent tensor([0.5807, 0.4193], device='cuda:0')
percent tensor([0.5605, 0.4395], device='cuda:0')
percent tensor([0.6383, 0.3617], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.7348, 0.2652], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 143 | Batch_idx: 0 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 143 | Batch_idx: 10 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 143 | Batch_idx: 20 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (2586/2688)
Epoch: 143 | Batch_idx: 30 |  Loss: (0.1264) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 143 | Batch_idx: 40 |  Loss: (0.1225) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 143 | Batch_idx: 50 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (6255/6528)
Epoch: 143 | Batch_idx: 60 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (7477/7808)
Epoch: 143 | Batch_idx: 70 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (8708/9088)
Epoch: 143 | Batch_idx: 80 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (9932/10368)
Epoch: 143 | Batch_idx: 90 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (11168/11648)
Epoch: 143 | Batch_idx: 100 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (12395/12928)
Epoch: 143 | Batch_idx: 110 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (13629/14208)
Epoch: 143 | Batch_idx: 120 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (14854/15488)
Epoch: 143 | Batch_idx: 130 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (16076/16768)
Epoch: 143 | Batch_idx: 140 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (17308/18048)
Epoch: 143 | Batch_idx: 150 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (18528/19328)
Epoch: 143 | Batch_idx: 160 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (19747/20608)
Epoch: 143 | Batch_idx: 170 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (20973/21888)
Epoch: 143 | Batch_idx: 180 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (22189/23168)
Epoch: 143 | Batch_idx: 190 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (23424/24448)
Epoch: 143 | Batch_idx: 200 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (24646/25728)
Epoch: 143 | Batch_idx: 210 |  Loss: (0.1257) |  Loss2: (0.0000) | Acc: (95.00%) (25868/27008)
Epoch: 143 | Batch_idx: 220 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (27094/28288)
Epoch: 143 | Batch_idx: 230 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (28318/29568)
Epoch: 143 | Batch_idx: 240 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (29522/30848)
Epoch: 143 | Batch_idx: 250 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (30737/32128)
Epoch: 143 | Batch_idx: 260 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (31959/33408)
Epoch: 143 | Batch_idx: 270 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (33180/34688)
Epoch: 143 | Batch_idx: 280 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (34401/35968)
Epoch: 143 | Batch_idx: 290 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (35619/37248)
Epoch: 143 | Batch_idx: 300 |  Loss: (0.1283) |  Loss2: (0.0000) | Acc: (95.00%) (36856/38528)
Epoch: 143 | Batch_idx: 310 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (38072/39808)
Epoch: 143 | Batch_idx: 320 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (39290/41088)
Epoch: 143 | Batch_idx: 330 |  Loss: (0.1300) |  Loss2: (0.0000) | Acc: (95.00%) (40504/42368)
Epoch: 143 | Batch_idx: 340 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (41715/43648)
Epoch: 143 | Batch_idx: 350 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (42934/44928)
Epoch: 143 | Batch_idx: 360 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (44158/46208)
Epoch: 143 | Batch_idx: 370 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (45360/47488)
Epoch: 143 | Batch_idx: 380 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (46590/48768)
Epoch: 143 | Batch_idx: 390 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (47759/50000)
# TEST : Loss: (0.4464) | Acc: (87.00%) (8739/10000)
percent tensor([0.5789, 0.4211], device='cuda:0')
percent tensor([0.5797, 0.4203], device='cuda:0')
percent tensor([0.5614, 0.4386], device='cuda:0')
percent tensor([0.6379, 0.3621], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6606, 0.3394], device='cuda:0')
percent tensor([0.7352, 0.2648], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 144 | Batch_idx: 0 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 144 | Batch_idx: 10 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (1347/1408)
Epoch: 144 | Batch_idx: 20 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 144 | Batch_idx: 30 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (3805/3968)
Epoch: 144 | Batch_idx: 40 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (5034/5248)
Epoch: 144 | Batch_idx: 50 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (6251/6528)
Epoch: 144 | Batch_idx: 60 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (7483/7808)
Epoch: 144 | Batch_idx: 70 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (8706/9088)
Epoch: 144 | Batch_idx: 80 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (9941/10368)
Epoch: 144 | Batch_idx: 90 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (11161/11648)
Epoch: 144 | Batch_idx: 100 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (12373/12928)
Epoch: 144 | Batch_idx: 110 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (13599/14208)
Epoch: 144 | Batch_idx: 120 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (14829/15488)
Epoch: 144 | Batch_idx: 130 |  Loss: (0.1226) |  Loss2: (0.0000) | Acc: (95.00%) (16058/16768)
Epoch: 144 | Batch_idx: 140 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (17275/18048)
Epoch: 144 | Batch_idx: 150 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (18499/19328)
Epoch: 144 | Batch_idx: 160 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (19714/20608)
Epoch: 144 | Batch_idx: 170 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (20918/21888)
Epoch: 144 | Batch_idx: 180 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (22137/23168)
Epoch: 144 | Batch_idx: 190 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (23348/24448)
Epoch: 144 | Batch_idx: 200 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (24567/25728)
Epoch: 144 | Batch_idx: 210 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (25791/27008)
Epoch: 144 | Batch_idx: 220 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (27018/28288)
Epoch: 144 | Batch_idx: 230 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (28253/29568)
Epoch: 144 | Batch_idx: 240 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (29482/30848)
Epoch: 144 | Batch_idx: 250 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (30696/32128)
Epoch: 144 | Batch_idx: 260 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (31910/33408)
Epoch: 144 | Batch_idx: 270 |  Loss: (0.1287) |  Loss2: (0.0000) | Acc: (95.00%) (33134/34688)
Epoch: 144 | Batch_idx: 280 |  Loss: (0.1289) |  Loss2: (0.0000) | Acc: (95.00%) (34357/35968)
Epoch: 144 | Batch_idx: 290 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (35578/37248)
Epoch: 144 | Batch_idx: 300 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (36795/38528)
Epoch: 144 | Batch_idx: 310 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (38011/39808)
Epoch: 144 | Batch_idx: 320 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (39233/41088)
Epoch: 144 | Batch_idx: 330 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (40457/42368)
Epoch: 144 | Batch_idx: 340 |  Loss: (0.1308) |  Loss2: (0.0000) | Acc: (95.00%) (41660/43648)
Epoch: 144 | Batch_idx: 350 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (42874/44928)
Epoch: 144 | Batch_idx: 360 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (44108/46208)
Epoch: 144 | Batch_idx: 370 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (45325/47488)
Epoch: 144 | Batch_idx: 380 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (46552/48768)
Epoch: 144 | Batch_idx: 390 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (47718/50000)
# TEST : Loss: (0.4528) | Acc: (87.00%) (8740/10000)
percent tensor([0.5787, 0.4213], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.5598, 0.4402], device='cuda:0')
percent tensor([0.6392, 0.3608], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.6604, 0.3396], device='cuda:0')
percent tensor([0.7336, 0.2664], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 145 | Batch_idx: 0 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 145 | Batch_idx: 10 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 145 | Batch_idx: 20 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (95.00%) (2564/2688)
Epoch: 145 | Batch_idx: 30 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (3764/3968)
Epoch: 145 | Batch_idx: 40 |  Loss: (0.1544) |  Loss2: (0.0000) | Acc: (94.00%) (4969/5248)
Epoch: 145 | Batch_idx: 50 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (6176/6528)
Epoch: 145 | Batch_idx: 60 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (7368/7808)
Epoch: 145 | Batch_idx: 70 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (8579/9088)
Epoch: 145 | Batch_idx: 80 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (9786/10368)
Epoch: 145 | Batch_idx: 90 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (94.00%) (10997/11648)
Epoch: 145 | Batch_idx: 100 |  Loss: (0.1607) |  Loss2: (0.0000) | Acc: (94.00%) (12205/12928)
Epoch: 145 | Batch_idx: 110 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (13411/14208)
Epoch: 145 | Batch_idx: 120 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (14622/15488)
Epoch: 145 | Batch_idx: 130 |  Loss: (0.1610) |  Loss2: (0.0000) | Acc: (94.00%) (15825/16768)
Epoch: 145 | Batch_idx: 140 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (94.00%) (17027/18048)
Epoch: 145 | Batch_idx: 150 |  Loss: (0.1614) |  Loss2: (0.0000) | Acc: (94.00%) (18238/19328)
Epoch: 145 | Batch_idx: 160 |  Loss: (0.1617) |  Loss2: (0.0000) | Acc: (94.00%) (19442/20608)
Epoch: 145 | Batch_idx: 170 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (20651/21888)
Epoch: 145 | Batch_idx: 180 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (21854/23168)
Epoch: 145 | Batch_idx: 190 |  Loss: (0.1611) |  Loss2: (0.0000) | Acc: (94.00%) (23061/24448)
Epoch: 145 | Batch_idx: 200 |  Loss: (0.1628) |  Loss2: (0.0000) | Acc: (94.00%) (24247/25728)
Epoch: 145 | Batch_idx: 210 |  Loss: (0.1627) |  Loss2: (0.0000) | Acc: (94.00%) (25460/27008)
Epoch: 145 | Batch_idx: 220 |  Loss: (0.1623) |  Loss2: (0.0000) | Acc: (94.00%) (26669/28288)
Epoch: 145 | Batch_idx: 230 |  Loss: (0.1625) |  Loss2: (0.0000) | Acc: (94.00%) (27878/29568)
Epoch: 145 | Batch_idx: 240 |  Loss: (0.1616) |  Loss2: (0.0000) | Acc: (94.00%) (29096/30848)
Epoch: 145 | Batch_idx: 250 |  Loss: (0.1603) |  Loss2: (0.0000) | Acc: (94.00%) (30330/32128)
Epoch: 145 | Batch_idx: 260 |  Loss: (0.1602) |  Loss2: (0.0000) | Acc: (94.00%) (31530/33408)
Epoch: 145 | Batch_idx: 270 |  Loss: (0.1599) |  Loss2: (0.0000) | Acc: (94.00%) (32740/34688)
Epoch: 145 | Batch_idx: 280 |  Loss: (0.1594) |  Loss2: (0.0000) | Acc: (94.00%) (33957/35968)
Epoch: 145 | Batch_idx: 290 |  Loss: (0.1595) |  Loss2: (0.0000) | Acc: (94.00%) (35155/37248)
Epoch: 145 | Batch_idx: 300 |  Loss: (0.1589) |  Loss2: (0.0000) | Acc: (94.00%) (36375/38528)
Epoch: 145 | Batch_idx: 310 |  Loss: (0.1583) |  Loss2: (0.0000) | Acc: (94.00%) (37587/39808)
Epoch: 145 | Batch_idx: 320 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (38800/41088)
Epoch: 145 | Batch_idx: 330 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (40016/42368)
Epoch: 145 | Batch_idx: 340 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (41240/43648)
Epoch: 145 | Batch_idx: 350 |  Loss: (0.1569) |  Loss2: (0.0000) | Acc: (94.00%) (42454/44928)
Epoch: 145 | Batch_idx: 360 |  Loss: (0.1566) |  Loss2: (0.0000) | Acc: (94.00%) (43665/46208)
Epoch: 145 | Batch_idx: 370 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (44890/47488)
Epoch: 145 | Batch_idx: 380 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (46103/48768)
Epoch: 145 | Batch_idx: 390 |  Loss: (0.1562) |  Loss2: (0.0000) | Acc: (94.00%) (47265/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_145.pth.tar'
# TEST : Loss: (0.4289) | Acc: (87.00%) (8756/10000)
percent tensor([0.5660, 0.4340], device='cuda:0')
percent tensor([0.5778, 0.4222], device='cuda:0')
percent tensor([0.5653, 0.4347], device='cuda:0')
percent tensor([0.6571, 0.3429], device='cuda:0')
percent tensor([0.5992, 0.4008], device='cuda:0')
percent tensor([0.6719, 0.3281], device='cuda:0')
percent tensor([0.7433, 0.2567], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 146 | Batch_idx: 0 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 146 | Batch_idx: 10 |  Loss: (0.1528) |  Loss2: (0.0000) | Acc: (94.00%) (1327/1408)
Epoch: 146 | Batch_idx: 20 |  Loss: (0.1546) |  Loss2: (0.0000) | Acc: (94.00%) (2529/2688)
Epoch: 146 | Batch_idx: 30 |  Loss: (0.1526) |  Loss2: (0.0000) | Acc: (94.00%) (3749/3968)
Epoch: 146 | Batch_idx: 40 |  Loss: (0.1500) |  Loss2: (0.0000) | Acc: (94.00%) (4960/5248)
Epoch: 146 | Batch_idx: 50 |  Loss: (0.1518) |  Loss2: (0.0000) | Acc: (94.00%) (6170/6528)
Epoch: 146 | Batch_idx: 60 |  Loss: (0.1508) |  Loss2: (0.0000) | Acc: (94.00%) (7383/7808)
Epoch: 146 | Batch_idx: 70 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (8603/9088)
Epoch: 146 | Batch_idx: 80 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (9817/10368)
Epoch: 146 | Batch_idx: 90 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (11043/11648)
Epoch: 146 | Batch_idx: 100 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (12251/12928)
Epoch: 146 | Batch_idx: 110 |  Loss: (0.1490) |  Loss2: (0.0000) | Acc: (94.00%) (13469/14208)
Epoch: 146 | Batch_idx: 120 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (14688/15488)
Epoch: 146 | Batch_idx: 130 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (15902/16768)
Epoch: 146 | Batch_idx: 140 |  Loss: (0.1496) |  Loss2: (0.0000) | Acc: (94.00%) (17113/18048)
Epoch: 146 | Batch_idx: 150 |  Loss: (0.1480) |  Loss2: (0.0000) | Acc: (94.00%) (18342/19328)
Epoch: 146 | Batch_idx: 160 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (19560/20608)
Epoch: 146 | Batch_idx: 170 |  Loss: (0.1474) |  Loss2: (0.0000) | Acc: (94.00%) (20773/21888)
Epoch: 146 | Batch_idx: 180 |  Loss: (0.1481) |  Loss2: (0.0000) | Acc: (94.00%) (21989/23168)
Epoch: 146 | Batch_idx: 190 |  Loss: (0.1478) |  Loss2: (0.0000) | Acc: (94.00%) (23202/24448)
Epoch: 146 | Batch_idx: 200 |  Loss: (0.1475) |  Loss2: (0.0000) | Acc: (94.00%) (24420/25728)
Epoch: 146 | Batch_idx: 210 |  Loss: (0.1472) |  Loss2: (0.0000) | Acc: (94.00%) (25636/27008)
Epoch: 146 | Batch_idx: 220 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (26853/28288)
Epoch: 146 | Batch_idx: 230 |  Loss: (0.1468) |  Loss2: (0.0000) | Acc: (94.00%) (28064/29568)
Epoch: 146 | Batch_idx: 240 |  Loss: (0.1462) |  Loss2: (0.0000) | Acc: (94.00%) (29280/30848)
Epoch: 146 | Batch_idx: 250 |  Loss: (0.1458) |  Loss2: (0.0000) | Acc: (94.00%) (30500/32128)
Epoch: 146 | Batch_idx: 260 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (31726/33408)
Epoch: 146 | Batch_idx: 270 |  Loss: (0.1446) |  Loss2: (0.0000) | Acc: (94.00%) (32946/34688)
Epoch: 146 | Batch_idx: 280 |  Loss: (0.1449) |  Loss2: (0.0000) | Acc: (94.00%) (34162/35968)
Epoch: 146 | Batch_idx: 290 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (35383/37248)
Epoch: 146 | Batch_idx: 300 |  Loss: (0.1448) |  Loss2: (0.0000) | Acc: (94.00%) (36589/38528)
Epoch: 146 | Batch_idx: 310 |  Loss: (0.1440) |  Loss2: (0.0000) | Acc: (94.00%) (37816/39808)
Epoch: 146 | Batch_idx: 320 |  Loss: (0.1443) |  Loss2: (0.0000) | Acc: (94.00%) (39026/41088)
Epoch: 146 | Batch_idx: 330 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (40258/42368)
Epoch: 146 | Batch_idx: 340 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (41483/43648)
Epoch: 146 | Batch_idx: 350 |  Loss: (0.1435) |  Loss2: (0.0000) | Acc: (95.00%) (42698/44928)
Epoch: 146 | Batch_idx: 360 |  Loss: (0.1439) |  Loss2: (0.0000) | Acc: (95.00%) (43903/46208)
Epoch: 146 | Batch_idx: 370 |  Loss: (0.1436) |  Loss2: (0.0000) | Acc: (95.00%) (45121/47488)
Epoch: 146 | Batch_idx: 380 |  Loss: (0.1434) |  Loss2: (0.0000) | Acc: (95.00%) (46341/48768)
Epoch: 146 | Batch_idx: 390 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (47514/50000)
# TEST : Loss: (0.4182) | Acc: (87.00%) (8788/10000)
percent tensor([0.5715, 0.4285], device='cuda:0')
percent tensor([0.5769, 0.4231], device='cuda:0')
percent tensor([0.5664, 0.4336], device='cuda:0')
percent tensor([0.6581, 0.3419], device='cuda:0')
percent tensor([0.5974, 0.4026], device='cuda:0')
percent tensor([0.6693, 0.3307], device='cuda:0')
percent tensor([0.7550, 0.2450], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 147 | Batch_idx: 0 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 147 | Batch_idx: 10 |  Loss: (0.1390) |  Loss2: (0.0000) | Acc: (94.00%) (1337/1408)
Epoch: 147 | Batch_idx: 20 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (2557/2688)
Epoch: 147 | Batch_idx: 30 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (3775/3968)
Epoch: 147 | Batch_idx: 40 |  Loss: (0.1394) |  Loss2: (0.0000) | Acc: (95.00%) (4993/5248)
Epoch: 147 | Batch_idx: 50 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (6205/6528)
Epoch: 147 | Batch_idx: 60 |  Loss: (0.1419) |  Loss2: (0.0000) | Acc: (95.00%) (7426/7808)
Epoch: 147 | Batch_idx: 70 |  Loss: (0.1437) |  Loss2: (0.0000) | Acc: (95.00%) (8638/9088)
Epoch: 147 | Batch_idx: 80 |  Loss: (0.1447) |  Loss2: (0.0000) | Acc: (95.00%) (9850/10368)
Epoch: 147 | Batch_idx: 90 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (11083/11648)
Epoch: 147 | Batch_idx: 100 |  Loss: (0.1430) |  Loss2: (0.0000) | Acc: (95.00%) (12300/12928)
Epoch: 147 | Batch_idx: 110 |  Loss: (0.1432) |  Loss2: (0.0000) | Acc: (95.00%) (13516/14208)
Epoch: 147 | Batch_idx: 120 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (14751/15488)
Epoch: 147 | Batch_idx: 130 |  Loss: (0.1414) |  Loss2: (0.0000) | Acc: (95.00%) (15961/16768)
Epoch: 147 | Batch_idx: 140 |  Loss: (0.1422) |  Loss2: (0.0000) | Acc: (95.00%) (17171/18048)
Epoch: 147 | Batch_idx: 150 |  Loss: (0.1406) |  Loss2: (0.0000) | Acc: (95.00%) (18395/19328)
Epoch: 147 | Batch_idx: 160 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (19621/20608)
Epoch: 147 | Batch_idx: 170 |  Loss: (0.1384) |  Loss2: (0.0000) | Acc: (95.00%) (20848/21888)
Epoch: 147 | Batch_idx: 180 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (22072/23168)
Epoch: 147 | Batch_idx: 190 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (23305/24448)
Epoch: 147 | Batch_idx: 200 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (24533/25728)
Epoch: 147 | Batch_idx: 210 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (25758/27008)
Epoch: 147 | Batch_idx: 220 |  Loss: (0.1365) |  Loss2: (0.0000) | Acc: (95.00%) (26981/28288)
Epoch: 147 | Batch_idx: 230 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (28206/29568)
Epoch: 147 | Batch_idx: 240 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (29421/30848)
Epoch: 147 | Batch_idx: 250 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (30632/32128)
Epoch: 147 | Batch_idx: 260 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (31862/33408)
Epoch: 147 | Batch_idx: 270 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (33077/34688)
Epoch: 147 | Batch_idx: 280 |  Loss: (0.1358) |  Loss2: (0.0000) | Acc: (95.00%) (34302/35968)
Epoch: 147 | Batch_idx: 290 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (35521/37248)
Epoch: 147 | Batch_idx: 300 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (36746/38528)
Epoch: 147 | Batch_idx: 310 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (37987/39808)
Epoch: 147 | Batch_idx: 320 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (39199/41088)
Epoch: 147 | Batch_idx: 330 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (40423/42368)
Epoch: 147 | Batch_idx: 340 |  Loss: (0.1349) |  Loss2: (0.0000) | Acc: (95.00%) (41649/43648)
Epoch: 147 | Batch_idx: 350 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (42871/44928)
Epoch: 147 | Batch_idx: 360 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (44090/46208)
Epoch: 147 | Batch_idx: 370 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (45310/47488)
Epoch: 147 | Batch_idx: 380 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (46526/48768)
Epoch: 147 | Batch_idx: 390 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (47710/50000)
# TEST : Loss: (0.4107) | Acc: (88.00%) (8803/10000)
percent tensor([0.5718, 0.4282], device='cuda:0')
percent tensor([0.5757, 0.4243], device='cuda:0')
percent tensor([0.5630, 0.4370], device='cuda:0')
percent tensor([0.6594, 0.3406], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.6727, 0.3273], device='cuda:0')
percent tensor([0.7570, 0.2430], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 148 | Batch_idx: 0 |  Loss: (0.1712) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 148 | Batch_idx: 10 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 148 | Batch_idx: 20 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 148 | Batch_idx: 30 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 148 | Batch_idx: 40 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (4995/5248)
Epoch: 148 | Batch_idx: 50 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (6209/6528)
Epoch: 148 | Batch_idx: 60 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (7440/7808)
Epoch: 148 | Batch_idx: 70 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (8665/9088)
Epoch: 148 | Batch_idx: 80 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (9893/10368)
Epoch: 148 | Batch_idx: 90 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (11118/11648)
Epoch: 148 | Batch_idx: 100 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (12338/12928)
Epoch: 148 | Batch_idx: 110 |  Loss: (0.1302) |  Loss2: (0.0000) | Acc: (95.00%) (13565/14208)
Epoch: 148 | Batch_idx: 120 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (14794/15488)
Epoch: 148 | Batch_idx: 130 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (16018/16768)
Epoch: 148 | Batch_idx: 140 |  Loss: (0.1295) |  Loss2: (0.0000) | Acc: (95.00%) (17236/18048)
Epoch: 148 | Batch_idx: 150 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (18451/19328)
Epoch: 148 | Batch_idx: 160 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (19681/20608)
Epoch: 148 | Batch_idx: 170 |  Loss: (0.1301) |  Loss2: (0.0000) | Acc: (95.00%) (20900/21888)
Epoch: 148 | Batch_idx: 180 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (22129/23168)
Epoch: 148 | Batch_idx: 190 |  Loss: (0.1298) |  Loss2: (0.0000) | Acc: (95.00%) (23350/24448)
Epoch: 148 | Batch_idx: 200 |  Loss: (0.1294) |  Loss2: (0.0000) | Acc: (95.00%) (24582/25728)
Epoch: 148 | Batch_idx: 210 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (25821/27008)
Epoch: 148 | Batch_idx: 220 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (27035/28288)
Epoch: 148 | Batch_idx: 230 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (28255/29568)
Epoch: 148 | Batch_idx: 240 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (29484/30848)
Epoch: 148 | Batch_idx: 250 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (30700/32128)
Epoch: 148 | Batch_idx: 260 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (31921/33408)
Epoch: 148 | Batch_idx: 270 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (33145/34688)
Epoch: 148 | Batch_idx: 280 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (34367/35968)
Epoch: 148 | Batch_idx: 290 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (35591/37248)
Epoch: 148 | Batch_idx: 300 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (36813/38528)
Epoch: 148 | Batch_idx: 310 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (38027/39808)
Epoch: 148 | Batch_idx: 320 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (39259/41088)
Epoch: 148 | Batch_idx: 330 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (40481/42368)
Epoch: 148 | Batch_idx: 340 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (41706/43648)
Epoch: 148 | Batch_idx: 350 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (42935/44928)
Epoch: 148 | Batch_idx: 360 |  Loss: (0.1286) |  Loss2: (0.0000) | Acc: (95.00%) (44162/46208)
Epoch: 148 | Batch_idx: 370 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (45386/47488)
Epoch: 148 | Batch_idx: 380 |  Loss: (0.1290) |  Loss2: (0.0000) | Acc: (95.00%) (46605/48768)
Epoch: 148 | Batch_idx: 390 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (47787/50000)
# TEST : Loss: (0.4067) | Acc: (88.00%) (8829/10000)
percent tensor([0.5719, 0.4281], device='cuda:0')
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5640, 0.4360], device='cuda:0')
percent tensor([0.6582, 0.3418], device='cuda:0')
percent tensor([0.5976, 0.4024], device='cuda:0')
percent tensor([0.6708, 0.3292], device='cuda:0')
percent tensor([0.7593, 0.2407], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 149 | Batch_idx: 0 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 149 | Batch_idx: 10 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (1338/1408)
Epoch: 149 | Batch_idx: 20 |  Loss: (0.1261) |  Loss2: (0.0000) | Acc: (95.00%) (2563/2688)
Epoch: 149 | Batch_idx: 30 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (3786/3968)
Epoch: 149 | Batch_idx: 40 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 149 | Batch_idx: 50 |  Loss: (0.1260) |  Loss2: (0.0000) | Acc: (95.00%) (6233/6528)
Epoch: 149 | Batch_idx: 60 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (7463/7808)
Epoch: 149 | Batch_idx: 70 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (8692/9088)
Epoch: 149 | Batch_idx: 80 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (9923/10368)
Epoch: 149 | Batch_idx: 90 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (11137/11648)
Epoch: 149 | Batch_idx: 100 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (12375/12928)
Epoch: 149 | Batch_idx: 110 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (13599/14208)
Epoch: 149 | Batch_idx: 120 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (14827/15488)
Epoch: 149 | Batch_idx: 130 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (16043/16768)
Epoch: 149 | Batch_idx: 140 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (17266/18048)
Epoch: 149 | Batch_idx: 150 |  Loss: (0.1262) |  Loss2: (0.0000) | Acc: (95.00%) (18484/19328)
Epoch: 149 | Batch_idx: 160 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (19703/20608)
Epoch: 149 | Batch_idx: 170 |  Loss: (0.1267) |  Loss2: (0.0000) | Acc: (95.00%) (20927/21888)
Epoch: 149 | Batch_idx: 180 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (22153/23168)
Epoch: 149 | Batch_idx: 190 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (23361/24448)
Epoch: 149 | Batch_idx: 200 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (24577/25728)
Epoch: 149 | Batch_idx: 210 |  Loss: (0.1272) |  Loss2: (0.0000) | Acc: (95.00%) (25808/27008)
Epoch: 149 | Batch_idx: 220 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (27044/28288)
Epoch: 149 | Batch_idx: 230 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (28282/29568)
Epoch: 149 | Batch_idx: 240 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (29502/30848)
Epoch: 149 | Batch_idx: 250 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (30729/32128)
Epoch: 149 | Batch_idx: 260 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (31970/33408)
Epoch: 149 | Batch_idx: 270 |  Loss: (0.1242) |  Loss2: (0.0000) | Acc: (95.00%) (33199/34688)
Epoch: 149 | Batch_idx: 280 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (34426/35968)
Epoch: 149 | Batch_idx: 290 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (35651/37248)
Epoch: 149 | Batch_idx: 300 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (36866/38528)
Epoch: 149 | Batch_idx: 310 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (38096/39808)
Epoch: 149 | Batch_idx: 320 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (39322/41088)
Epoch: 149 | Batch_idx: 330 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (40537/42368)
Epoch: 149 | Batch_idx: 340 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (41751/43648)
Epoch: 149 | Batch_idx: 350 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (42973/44928)
Epoch: 149 | Batch_idx: 360 |  Loss: (0.1256) |  Loss2: (0.0000) | Acc: (95.00%) (44200/46208)
Epoch: 149 | Batch_idx: 370 |  Loss: (0.1254) |  Loss2: (0.0000) | Acc: (95.00%) (45429/47488)
Epoch: 149 | Batch_idx: 380 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (46658/48768)
Epoch: 149 | Batch_idx: 390 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (47850/50000)
# TEST : Loss: (0.4054) | Acc: (88.00%) (8829/10000)
percent tensor([0.5714, 0.4286], device='cuda:0')
percent tensor([0.5764, 0.4236], device='cuda:0')
percent tensor([0.5673, 0.4327], device='cuda:0')
percent tensor([0.6560, 0.3440], device='cuda:0')
percent tensor([0.5983, 0.4017], device='cuda:0')
percent tensor([0.6620, 0.3380], device='cuda:0')
percent tensor([0.7583, 0.2417], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 150 | Batch_idx: 0 |  Loss: (0.1709) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 150 | Batch_idx: 10 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (1355/1408)
Epoch: 150 | Batch_idx: 20 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (95.00%) (2577/2688)
Epoch: 150 | Batch_idx: 30 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (3806/3968)
Epoch: 150 | Batch_idx: 40 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (5032/5248)
Epoch: 150 | Batch_idx: 50 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (6263/6528)
Epoch: 150 | Batch_idx: 60 |  Loss: (0.1213) |  Loss2: (0.0000) | Acc: (95.00%) (7493/7808)
Epoch: 150 | Batch_idx: 70 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (8714/9088)
Epoch: 150 | Batch_idx: 80 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (9932/10368)
Epoch: 150 | Batch_idx: 90 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (11159/11648)
Epoch: 150 | Batch_idx: 100 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (12386/12928)
Epoch: 150 | Batch_idx: 110 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (13609/14208)
Epoch: 150 | Batch_idx: 120 |  Loss: (0.1263) |  Loss2: (0.0000) | Acc: (95.00%) (14828/15488)
Epoch: 150 | Batch_idx: 130 |  Loss: (0.1255) |  Loss2: (0.0000) | Acc: (95.00%) (16054/16768)
Epoch: 150 | Batch_idx: 140 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (17273/18048)
Epoch: 150 | Batch_idx: 150 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (18490/19328)
Epoch: 150 | Batch_idx: 160 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (19703/20608)
Epoch: 150 | Batch_idx: 170 |  Loss: (0.1297) |  Loss2: (0.0000) | Acc: (95.00%) (20920/21888)
Epoch: 150 | Batch_idx: 180 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (95.00%) (22139/23168)
Epoch: 150 | Batch_idx: 190 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (23355/24448)
Epoch: 150 | Batch_idx: 200 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (24580/25728)
Epoch: 150 | Batch_idx: 210 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (25806/27008)
Epoch: 150 | Batch_idx: 220 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (27026/28288)
Epoch: 150 | Batch_idx: 230 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (28247/29568)
Epoch: 150 | Batch_idx: 240 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (29465/30848)
Epoch: 150 | Batch_idx: 250 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (30681/32128)
Epoch: 150 | Batch_idx: 260 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (31908/33408)
Epoch: 150 | Batch_idx: 270 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (33131/34688)
Epoch: 150 | Batch_idx: 280 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (34346/35968)
Epoch: 150 | Batch_idx: 290 |  Loss: (0.1314) |  Loss2: (0.0000) | Acc: (95.00%) (35572/37248)
Epoch: 150 | Batch_idx: 300 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (36794/38528)
Epoch: 150 | Batch_idx: 310 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (38017/39808)
Epoch: 150 | Batch_idx: 320 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (39232/41088)
Epoch: 150 | Batch_idx: 330 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (40451/42368)
Epoch: 150 | Batch_idx: 340 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (41656/43648)
Epoch: 150 | Batch_idx: 350 |  Loss: (0.1330) |  Loss2: (0.0000) | Acc: (95.00%) (42878/44928)
Epoch: 150 | Batch_idx: 360 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (44105/46208)
Epoch: 150 | Batch_idx: 370 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (45323/47488)
Epoch: 150 | Batch_idx: 380 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (46547/48768)
Epoch: 150 | Batch_idx: 390 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (47721/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
# TEST : Loss: (0.4636) | Acc: (87.00%) (8723/10000)
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5768, 0.4232], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.6557, 0.3443], device='cuda:0')
percent tensor([0.5979, 0.4021], device='cuda:0')
percent tensor([0.6618, 0.3382], device='cuda:0')
percent tensor([0.7645, 0.2355], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(181.8157, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.4608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(823.8359, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1520.4581, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(485.3317, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2279.1204, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4266.6323, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1369.4167, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6217.1787, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11700.4521, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3856.3081, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16307.9277, device='cuda:0', grad_fn=<NormBackward0>)



Files already downloaded and verified
USE 1 GPUs!
=> loading checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_150.pth.tar'
Epoch: 151 | Batch_idx: 0 |  Loss: (0.1304) |  Loss2: (0.0000) | Acc: (93.00%) (120/128)
Epoch: 151 | Batch_idx: 10 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (1343/1408)
Epoch: 151 | Batch_idx: 20 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (2579/2688)
Epoch: 151 | Batch_idx: 30 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (3795/3968)
Epoch: 151 | Batch_idx: 40 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (5027/5248)
Epoch: 151 | Batch_idx: 50 |  Loss: (0.1216) |  Loss2: (0.0000) | Acc: (95.00%) (6251/6528)
Epoch: 151 | Batch_idx: 60 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (7471/7808)
Epoch: 151 | Batch_idx: 70 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (8704/9088)
Epoch: 151 | Batch_idx: 80 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (9913/10368)
Epoch: 151 | Batch_idx: 90 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (11143/11648)
Epoch: 151 | Batch_idx: 100 |  Loss: (0.1233) |  Loss2: (0.0000) | Acc: (95.00%) (12367/12928)
Epoch: 151 | Batch_idx: 110 |  Loss: (0.1234) |  Loss2: (0.0000) | Acc: (95.00%) (13593/14208)
Epoch: 151 | Batch_idx: 120 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (14825/15488)
Epoch: 151 | Batch_idx: 130 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (16047/16768)
Epoch: 151 | Batch_idx: 140 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (17281/18048)
Epoch: 151 | Batch_idx: 150 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (18516/19328)
Epoch: 151 | Batch_idx: 160 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (19737/20608)
Epoch: 151 | Batch_idx: 170 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (20967/21888)
Epoch: 151 | Batch_idx: 180 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (22197/23168)
Epoch: 151 | Batch_idx: 190 |  Loss: (0.1220) |  Loss2: (0.0000) | Acc: (95.00%) (23434/24448)
Epoch: 151 | Batch_idx: 200 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (24654/25728)
Epoch: 151 | Batch_idx: 210 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (25882/27008)
Epoch: 151 | Batch_idx: 220 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (27096/28288)
Epoch: 151 | Batch_idx: 230 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (28319/29568)
Epoch: 151 | Batch_idx: 240 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (29527/30848)
Epoch: 151 | Batch_idx: 250 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (30752/32128)
Epoch: 151 | Batch_idx: 260 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (31964/33408)
Epoch: 151 | Batch_idx: 270 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (33183/34688)
Epoch: 151 | Batch_idx: 280 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (34407/35968)
Epoch: 151 | Batch_idx: 290 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (35617/37248)
Epoch: 151 | Batch_idx: 300 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (36839/38528)
Epoch: 151 | Batch_idx: 310 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (38070/39808)
Epoch: 151 | Batch_idx: 320 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (39292/41088)
Epoch: 151 | Batch_idx: 330 |  Loss: (0.1280) |  Loss2: (0.0000) | Acc: (95.00%) (40516/42368)
Epoch: 151 | Batch_idx: 340 |  Loss: (0.1281) |  Loss2: (0.0000) | Acc: (95.00%) (41741/43648)
Epoch: 151 | Batch_idx: 350 |  Loss: (0.1285) |  Loss2: (0.0000) | Acc: (95.00%) (42954/44928)
Epoch: 151 | Batch_idx: 360 |  Loss: (0.1291) |  Loss2: (0.0000) | Acc: (95.00%) (44169/46208)
Epoch: 151 | Batch_idx: 370 |  Loss: (0.1292) |  Loss2: (0.0000) | Acc: (95.00%) (45395/47488)
Epoch: 151 | Batch_idx: 380 |  Loss: (0.1293) |  Loss2: (0.0000) | Acc: (95.00%) (46610/48768)
Epoch: 151 | Batch_idx: 390 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (47775/50000)
# TEST : Loss: (0.4281) | Acc: (88.00%) (8813/10000)
percent tensor([0.5723, 0.4277], device='cuda:0')
percent tensor([0.5772, 0.4228], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.6550, 0.3450], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6597, 0.3403], device='cuda:0')
percent tensor([0.7619, 0.2381], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 152 | Batch_idx: 0 |  Loss: (0.1601) |  Loss2: (0.0000) | Acc: (92.00%) (119/128)
Epoch: 152 | Batch_idx: 10 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (1343/1408)
Epoch: 152 | Batch_idx: 20 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (2565/2688)
Epoch: 152 | Batch_idx: 30 |  Loss: (0.1269) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 152 | Batch_idx: 40 |  Loss: (0.1266) |  Loss2: (0.0000) | Acc: (95.00%) (5015/5248)
Epoch: 152 | Batch_idx: 50 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (6241/6528)
Epoch: 152 | Batch_idx: 60 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (7471/7808)
Epoch: 152 | Batch_idx: 70 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (8695/9088)
Epoch: 152 | Batch_idx: 80 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (9917/10368)
Epoch: 152 | Batch_idx: 90 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (11145/11648)
Epoch: 152 | Batch_idx: 100 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (12378/12928)
Epoch: 152 | Batch_idx: 110 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (13610/14208)
Epoch: 152 | Batch_idx: 120 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (14837/15488)
Epoch: 152 | Batch_idx: 130 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (16064/16768)
Epoch: 152 | Batch_idx: 140 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (17288/18048)
Epoch: 152 | Batch_idx: 150 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (18516/19328)
Epoch: 152 | Batch_idx: 160 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (19751/20608)
Epoch: 152 | Batch_idx: 170 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (20970/21888)
Epoch: 152 | Batch_idx: 180 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (22203/23168)
Epoch: 152 | Batch_idx: 190 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (23430/24448)
Epoch: 152 | Batch_idx: 200 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (24660/25728)
Epoch: 152 | Batch_idx: 210 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (25887/27008)
Epoch: 152 | Batch_idx: 220 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (27108/28288)
Epoch: 152 | Batch_idx: 230 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (28336/29568)
Epoch: 152 | Batch_idx: 240 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (29557/30848)
Epoch: 152 | Batch_idx: 250 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (30775/32128)
Epoch: 152 | Batch_idx: 260 |  Loss: (0.1211) |  Loss2: (0.0000) | Acc: (95.00%) (32000/33408)
Epoch: 152 | Batch_idx: 270 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (33217/34688)
Epoch: 152 | Batch_idx: 280 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (34444/35968)
Epoch: 152 | Batch_idx: 290 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (35659/37248)
Epoch: 152 | Batch_idx: 300 |  Loss: (0.1224) |  Loss2: (0.0000) | Acc: (95.00%) (36878/38528)
Epoch: 152 | Batch_idx: 310 |  Loss: (0.1231) |  Loss2: (0.0000) | Acc: (95.00%) (38097/39808)
Epoch: 152 | Batch_idx: 320 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (39326/41088)
Epoch: 152 | Batch_idx: 330 |  Loss: (0.1232) |  Loss2: (0.0000) | Acc: (95.00%) (40549/42368)
Epoch: 152 | Batch_idx: 340 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (41762/43648)
Epoch: 152 | Batch_idx: 350 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (42982/44928)
Epoch: 152 | Batch_idx: 360 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (44211/46208)
Epoch: 152 | Batch_idx: 370 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (45443/47488)
Epoch: 152 | Batch_idx: 380 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (46665/48768)
Epoch: 152 | Batch_idx: 390 |  Loss: (0.1241) |  Loss2: (0.0000) | Acc: (95.00%) (47845/50000)
# TEST : Loss: (0.4595) | Acc: (87.00%) (8719/10000)
percent tensor([0.5724, 0.4276], device='cuda:0')
percent tensor([0.5770, 0.4230], device='cuda:0')
percent tensor([0.5675, 0.4325], device='cuda:0')
percent tensor([0.6552, 0.3448], device='cuda:0')
percent tensor([0.5969, 0.4031], device='cuda:0')
percent tensor([0.6592, 0.3408], device='cuda:0')
percent tensor([0.7606, 0.2394], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 153 | Batch_idx: 0 |  Loss: (0.1350) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 153 | Batch_idx: 10 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 153 | Batch_idx: 20 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (95.00%) (2573/2688)
Epoch: 153 | Batch_idx: 30 |  Loss: (0.1227) |  Loss2: (0.0000) | Acc: (95.00%) (3789/3968)
Epoch: 153 | Batch_idx: 40 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (5023/5248)
Epoch: 153 | Batch_idx: 50 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (6257/6528)
Epoch: 153 | Batch_idx: 60 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (95.00%) (7488/7808)
Epoch: 153 | Batch_idx: 70 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (8720/9088)
Epoch: 153 | Batch_idx: 80 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (9956/10368)
Epoch: 153 | Batch_idx: 90 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (96.00%) (11186/11648)
Epoch: 153 | Batch_idx: 100 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (96.00%) (12412/12928)
Epoch: 153 | Batch_idx: 110 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (13635/14208)
Epoch: 153 | Batch_idx: 120 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (14868/15488)
Epoch: 153 | Batch_idx: 130 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (16092/16768)
Epoch: 153 | Batch_idx: 140 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (96.00%) (17327/18048)
Epoch: 153 | Batch_idx: 150 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (96.00%) (18565/19328)
Epoch: 153 | Batch_idx: 160 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (96.00%) (19785/20608)
Epoch: 153 | Batch_idx: 170 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (21003/21888)
Epoch: 153 | Batch_idx: 180 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (22239/23168)
Epoch: 153 | Batch_idx: 190 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (23467/24448)
Epoch: 153 | Batch_idx: 200 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (24695/25728)
Epoch: 153 | Batch_idx: 210 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (25931/27008)
Epoch: 153 | Batch_idx: 220 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (27149/28288)
Epoch: 153 | Batch_idx: 230 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (28380/29568)
Epoch: 153 | Batch_idx: 240 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (29614/30848)
Epoch: 153 | Batch_idx: 250 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (96.00%) (30852/32128)
Epoch: 153 | Batch_idx: 260 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (32080/33408)
Epoch: 153 | Batch_idx: 270 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (96.00%) (33303/34688)
Epoch: 153 | Batch_idx: 280 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (34519/35968)
Epoch: 153 | Batch_idx: 290 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (35744/37248)
Epoch: 153 | Batch_idx: 300 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (36980/38528)
Epoch: 153 | Batch_idx: 310 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (38203/39808)
Epoch: 153 | Batch_idx: 320 |  Loss: (0.1215) |  Loss2: (0.0000) | Acc: (95.00%) (39415/41088)
Epoch: 153 | Batch_idx: 330 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (40634/42368)
Epoch: 153 | Batch_idx: 340 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (41859/43648)
Epoch: 153 | Batch_idx: 350 |  Loss: (0.1219) |  Loss2: (0.0000) | Acc: (95.00%) (43077/44928)
Epoch: 153 | Batch_idx: 360 |  Loss: (0.1221) |  Loss2: (0.0000) | Acc: (95.00%) (44307/46208)
Epoch: 153 | Batch_idx: 370 |  Loss: (0.1218) |  Loss2: (0.0000) | Acc: (95.00%) (45536/47488)
Epoch: 153 | Batch_idx: 380 |  Loss: (0.1223) |  Loss2: (0.0000) | Acc: (95.00%) (46753/48768)
Epoch: 153 | Batch_idx: 390 |  Loss: (0.1228) |  Loss2: (0.0000) | Acc: (95.00%) (47929/50000)
# TEST : Loss: (0.4527) | Acc: (87.00%) (8796/10000)
percent tensor([0.5720, 0.4280], device='cuda:0')
percent tensor([0.5766, 0.4234], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.6541, 0.3459], device='cuda:0')
percent tensor([0.5960, 0.4040], device='cuda:0')
percent tensor([0.6590, 0.3410], device='cuda:0')
percent tensor([0.7624, 0.2376], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 154 | Batch_idx: 0 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 154 | Batch_idx: 10 |  Loss: (0.1222) |  Loss2: (0.0000) | Acc: (95.00%) (1351/1408)
Epoch: 154 | Batch_idx: 20 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 154 | Batch_idx: 30 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 154 | Batch_idx: 40 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (5058/5248)
Epoch: 154 | Batch_idx: 50 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (6286/6528)
Epoch: 154 | Batch_idx: 60 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (7528/7808)
Epoch: 154 | Batch_idx: 70 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (8752/9088)
Epoch: 154 | Batch_idx: 80 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (9971/10368)
Epoch: 154 | Batch_idx: 90 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (11191/11648)
Epoch: 154 | Batch_idx: 100 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (95.00%) (12406/12928)
Epoch: 154 | Batch_idx: 110 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (95.00%) (13633/14208)
Epoch: 154 | Batch_idx: 120 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (95.00%) (14859/15488)
Epoch: 154 | Batch_idx: 130 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (16091/16768)
Epoch: 154 | Batch_idx: 140 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (95.00%) (17305/18048)
Epoch: 154 | Batch_idx: 150 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (18523/19328)
Epoch: 154 | Batch_idx: 160 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (19747/20608)
Epoch: 154 | Batch_idx: 170 |  Loss: (0.1188) |  Loss2: (0.0000) | Acc: (95.00%) (20975/21888)
Epoch: 154 | Batch_idx: 180 |  Loss: (0.1198) |  Loss2: (0.0000) | Acc: (95.00%) (22196/23168)
Epoch: 154 | Batch_idx: 190 |  Loss: (0.1196) |  Loss2: (0.0000) | Acc: (95.00%) (23422/24448)
Epoch: 154 | Batch_idx: 200 |  Loss: (0.1199) |  Loss2: (0.0000) | Acc: (95.00%) (24643/25728)
Epoch: 154 | Batch_idx: 210 |  Loss: (0.1203) |  Loss2: (0.0000) | Acc: (95.00%) (25873/27008)
Epoch: 154 | Batch_idx: 220 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (27100/28288)
Epoch: 154 | Batch_idx: 230 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (28332/29568)
Epoch: 154 | Batch_idx: 240 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (29570/30848)
Epoch: 154 | Batch_idx: 250 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (30798/32128)
Epoch: 154 | Batch_idx: 260 |  Loss: (0.1204) |  Loss2: (0.0000) | Acc: (95.00%) (32017/33408)
Epoch: 154 | Batch_idx: 270 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (33242/34688)
Epoch: 154 | Batch_idx: 280 |  Loss: (0.1200) |  Loss2: (0.0000) | Acc: (95.00%) (34483/35968)
Epoch: 154 | Batch_idx: 290 |  Loss: (0.1207) |  Loss2: (0.0000) | Acc: (95.00%) (35714/37248)
Epoch: 154 | Batch_idx: 300 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (36935/38528)
Epoch: 154 | Batch_idx: 310 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (38161/39808)
Epoch: 154 | Batch_idx: 320 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (39388/41088)
Epoch: 154 | Batch_idx: 330 |  Loss: (0.1214) |  Loss2: (0.0000) | Acc: (95.00%) (40612/42368)
Epoch: 154 | Batch_idx: 340 |  Loss: (0.1212) |  Loss2: (0.0000) | Acc: (95.00%) (41840/43648)
Epoch: 154 | Batch_idx: 350 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (43068/44928)
Epoch: 154 | Batch_idx: 360 |  Loss: (0.1210) |  Loss2: (0.0000) | Acc: (95.00%) (44294/46208)
Epoch: 154 | Batch_idx: 370 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (45517/47488)
Epoch: 154 | Batch_idx: 380 |  Loss: (0.1206) |  Loss2: (0.0000) | Acc: (95.00%) (46751/48768)
Epoch: 154 | Batch_idx: 390 |  Loss: (0.1208) |  Loss2: (0.0000) | Acc: (95.00%) (47926/50000)
# TEST : Loss: (0.4269) | Acc: (88.00%) (8837/10000)
percent tensor([0.5725, 0.4275], device='cuda:0')
percent tensor([0.5769, 0.4231], device='cuda:0')
percent tensor([0.5674, 0.4326], device='cuda:0')
percent tensor([0.6551, 0.3449], device='cuda:0')
percent tensor([0.5975, 0.4025], device='cuda:0')
percent tensor([0.6593, 0.3407], device='cuda:0')
percent tensor([0.7582, 0.2418], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 155 | Batch_idx: 0 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 155 | Batch_idx: 10 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (1348/1408)
Epoch: 155 | Batch_idx: 20 |  Loss: (0.1259) |  Loss2: (0.0000) | Acc: (95.00%) (2566/2688)
Epoch: 155 | Batch_idx: 30 |  Loss: (0.1354) |  Loss2: (0.0000) | Acc: (95.00%) (3778/3968)
Epoch: 155 | Batch_idx: 40 |  Loss: (0.1450) |  Loss2: (0.0000) | Acc: (94.00%) (4972/5248)
Epoch: 155 | Batch_idx: 50 |  Loss: (0.1506) |  Loss2: (0.0000) | Acc: (94.00%) (6168/6528)
Epoch: 155 | Batch_idx: 60 |  Loss: (0.1543) |  Loss2: (0.0000) | Acc: (94.00%) (7358/7808)
Epoch: 155 | Batch_idx: 70 |  Loss: (0.1570) |  Loss2: (0.0000) | Acc: (94.00%) (8565/9088)
Epoch: 155 | Batch_idx: 80 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (9772/10368)
Epoch: 155 | Batch_idx: 90 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (10975/11648)
Epoch: 155 | Batch_idx: 100 |  Loss: (0.1586) |  Loss2: (0.0000) | Acc: (94.00%) (12187/12928)
Epoch: 155 | Batch_idx: 110 |  Loss: (0.1597) |  Loss2: (0.0000) | Acc: (94.00%) (13394/14208)
Epoch: 155 | Batch_idx: 120 |  Loss: (0.1556) |  Loss2: (0.0000) | Acc: (94.00%) (14627/15488)
Epoch: 155 | Batch_idx: 130 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (15826/16768)
Epoch: 155 | Batch_idx: 140 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (17040/18048)
Epoch: 155 | Batch_idx: 150 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (18256/19328)
Epoch: 155 | Batch_idx: 160 |  Loss: (0.1576) |  Loss2: (0.0000) | Acc: (94.00%) (19460/20608)
Epoch: 155 | Batch_idx: 170 |  Loss: (0.1582) |  Loss2: (0.0000) | Acc: (94.00%) (20665/21888)
Epoch: 155 | Batch_idx: 180 |  Loss: (0.1577) |  Loss2: (0.0000) | Acc: (94.00%) (21876/23168)
Epoch: 155 | Batch_idx: 190 |  Loss: (0.1560) |  Loss2: (0.0000) | Acc: (94.00%) (23093/24448)
Epoch: 155 | Batch_idx: 200 |  Loss: (0.1553) |  Loss2: (0.0000) | Acc: (94.00%) (24303/25728)
Epoch: 155 | Batch_idx: 210 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (94.00%) (25519/27008)
Epoch: 155 | Batch_idx: 220 |  Loss: (0.1555) |  Loss2: (0.0000) | Acc: (94.00%) (26736/28288)
Epoch: 155 | Batch_idx: 230 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (27940/29568)
Epoch: 155 | Batch_idx: 240 |  Loss: (0.1558) |  Loss2: (0.0000) | Acc: (94.00%) (29147/30848)
Epoch: 155 | Batch_idx: 250 |  Loss: (0.1557) |  Loss2: (0.0000) | Acc: (94.00%) (30356/32128)
Epoch: 155 | Batch_idx: 260 |  Loss: (0.1547) |  Loss2: (0.0000) | Acc: (94.00%) (31582/33408)
Epoch: 155 | Batch_idx: 270 |  Loss: (0.1536) |  Loss2: (0.0000) | Acc: (94.00%) (32800/34688)
Epoch: 155 | Batch_idx: 280 |  Loss: (0.1530) |  Loss2: (0.0000) | Acc: (94.00%) (34023/35968)
Epoch: 155 | Batch_idx: 290 |  Loss: (0.1519) |  Loss2: (0.0000) | Acc: (94.00%) (35247/37248)
Epoch: 155 | Batch_idx: 300 |  Loss: (0.1517) |  Loss2: (0.0000) | Acc: (94.00%) (36470/38528)
Epoch: 155 | Batch_idx: 310 |  Loss: (0.1514) |  Loss2: (0.0000) | Acc: (94.00%) (37689/39808)
Epoch: 155 | Batch_idx: 320 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (38912/41088)
Epoch: 155 | Batch_idx: 330 |  Loss: (0.1504) |  Loss2: (0.0000) | Acc: (94.00%) (40124/42368)
Epoch: 155 | Batch_idx: 340 |  Loss: (0.1501) |  Loss2: (0.0000) | Acc: (94.00%) (41348/43648)
Epoch: 155 | Batch_idx: 350 |  Loss: (0.1493) |  Loss2: (0.0000) | Acc: (94.00%) (42576/44928)
Epoch: 155 | Batch_idx: 360 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (43795/46208)
Epoch: 155 | Batch_idx: 370 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (45007/47488)
Epoch: 155 | Batch_idx: 380 |  Loss: (0.1489) |  Loss2: (0.0000) | Acc: (94.00%) (46216/48768)
Epoch: 155 | Batch_idx: 390 |  Loss: (0.1487) |  Loss2: (0.0000) | Acc: (94.00%) (47391/50000)
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type _Gate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_155.pth.tar'
# TEST : Loss: (0.4617) | Acc: (87.00%) (8740/10000)
percent tensor([0.5869, 0.4131], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.6692, 0.3308], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6257, 0.3743], device='cuda:0')
percent tensor([0.7548, 0.2452], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 156 | Batch_idx: 0 |  Loss: (0.1520) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 156 | Batch_idx: 10 |  Loss: (0.1305) |  Loss2: (0.0000) | Acc: (95.00%) (1342/1408)
Epoch: 156 | Batch_idx: 20 |  Loss: (0.1236) |  Loss2: (0.0000) | Acc: (95.00%) (2574/2688)
Epoch: 156 | Batch_idx: 30 |  Loss: (0.1327) |  Loss2: (0.0000) | Acc: (95.00%) (3793/3968)
Epoch: 156 | Batch_idx: 40 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (4995/5248)
Epoch: 156 | Batch_idx: 50 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (6218/6528)
Epoch: 156 | Batch_idx: 60 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (7445/7808)
Epoch: 156 | Batch_idx: 70 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (8672/9088)
Epoch: 156 | Batch_idx: 80 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (9898/10368)
Epoch: 156 | Batch_idx: 90 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (11111/11648)
Epoch: 156 | Batch_idx: 100 |  Loss: (0.1331) |  Loss2: (0.0000) | Acc: (95.00%) (12332/12928)
Epoch: 156 | Batch_idx: 110 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (13552/14208)
Epoch: 156 | Batch_idx: 120 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (95.00%) (14776/15488)
Epoch: 156 | Batch_idx: 130 |  Loss: (0.1334) |  Loss2: (0.0000) | Acc: (95.00%) (15998/16768)
Epoch: 156 | Batch_idx: 140 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (17230/18048)
Epoch: 156 | Batch_idx: 150 |  Loss: (0.1311) |  Loss2: (0.0000) | Acc: (95.00%) (18460/19328)
Epoch: 156 | Batch_idx: 160 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (19674/20608)
Epoch: 156 | Batch_idx: 170 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (20887/21888)
Epoch: 156 | Batch_idx: 180 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (22121/23168)
Epoch: 156 | Batch_idx: 190 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (23334/24448)
Epoch: 156 | Batch_idx: 200 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (24565/25728)
Epoch: 156 | Batch_idx: 210 |  Loss: (0.1316) |  Loss2: (0.0000) | Acc: (95.00%) (25787/27008)
Epoch: 156 | Batch_idx: 220 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (27022/28288)
Epoch: 156 | Batch_idx: 230 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (28238/29568)
Epoch: 156 | Batch_idx: 240 |  Loss: (0.1306) |  Loss2: (0.0000) | Acc: (95.00%) (29467/30848)
Epoch: 156 | Batch_idx: 250 |  Loss: (0.1310) |  Loss2: (0.0000) | Acc: (95.00%) (30686/32128)
Epoch: 156 | Batch_idx: 260 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (31899/33408)
Epoch: 156 | Batch_idx: 270 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (33115/34688)
Epoch: 156 | Batch_idx: 280 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (34335/35968)
Epoch: 156 | Batch_idx: 290 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (35561/37248)
Epoch: 156 | Batch_idx: 300 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (36781/38528)
Epoch: 156 | Batch_idx: 310 |  Loss: (0.1313) |  Loss2: (0.0000) | Acc: (95.00%) (38013/39808)
Epoch: 156 | Batch_idx: 320 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (39231/41088)
Epoch: 156 | Batch_idx: 330 |  Loss: (0.1315) |  Loss2: (0.0000) | Acc: (95.00%) (40453/42368)
Epoch: 156 | Batch_idx: 340 |  Loss: (0.1319) |  Loss2: (0.0000) | Acc: (95.00%) (41671/43648)
Epoch: 156 | Batch_idx: 350 |  Loss: (0.1323) |  Loss2: (0.0000) | Acc: (95.00%) (42881/44928)
Epoch: 156 | Batch_idx: 360 |  Loss: (0.1318) |  Loss2: (0.0000) | Acc: (95.00%) (44119/46208)
Epoch: 156 | Batch_idx: 370 |  Loss: (0.1321) |  Loss2: (0.0000) | Acc: (95.00%) (45336/47488)
Epoch: 156 | Batch_idx: 380 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (46566/48768)
Epoch: 156 | Batch_idx: 390 |  Loss: (0.1309) |  Loss2: (0.0000) | Acc: (95.00%) (47757/50000)
# TEST : Loss: (0.4407) | Acc: (87.00%) (8789/10000)
percent tensor([0.5845, 0.4155], device='cuda:0')
percent tensor([0.5820, 0.4180], device='cuda:0')
percent tensor([0.5648, 0.4352], device='cuda:0')
percent tensor([0.6739, 0.3261], device='cuda:0')
percent tensor([0.6069, 0.3931], device='cuda:0')
percent tensor([0.6334, 0.3666], device='cuda:0')
percent tensor([0.7618, 0.2382], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 157 | Batch_idx: 0 |  Loss: (0.2045) |  Loss2: (0.0000) | Acc: (91.00%) (117/128)
Epoch: 157 | Batch_idx: 10 |  Loss: (0.1412) |  Loss2: (0.0000) | Acc: (94.00%) (1333/1408)
Epoch: 157 | Batch_idx: 20 |  Loss: (0.1312) |  Loss2: (0.0000) | Acc: (95.00%) (2558/2688)
Epoch: 157 | Batch_idx: 30 |  Loss: (0.1307) |  Loss2: (0.0000) | Acc: (95.00%) (3787/3968)
Epoch: 157 | Batch_idx: 40 |  Loss: (0.1322) |  Loss2: (0.0000) | Acc: (95.00%) (5014/5248)
Epoch: 157 | Batch_idx: 50 |  Loss: (0.1299) |  Loss2: (0.0000) | Acc: (95.00%) (6245/6528)
Epoch: 157 | Batch_idx: 60 |  Loss: (0.1273) |  Loss2: (0.0000) | Acc: (95.00%) (7472/7808)
Epoch: 157 | Batch_idx: 70 |  Loss: (0.1265) |  Loss2: (0.0000) | Acc: (95.00%) (8698/9088)
Epoch: 157 | Batch_idx: 80 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (9916/10368)
Epoch: 157 | Batch_idx: 90 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (11150/11648)
Epoch: 157 | Batch_idx: 100 |  Loss: (0.1275) |  Loss2: (0.0000) | Acc: (95.00%) (12369/12928)
Epoch: 157 | Batch_idx: 110 |  Loss: (0.1288) |  Loss2: (0.0000) | Acc: (95.00%) (13587/14208)
Epoch: 157 | Batch_idx: 120 |  Loss: (0.1284) |  Loss2: (0.0000) | Acc: (95.00%) (14811/15488)
Epoch: 157 | Batch_idx: 130 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (16036/16768)
Epoch: 157 | Batch_idx: 140 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (17255/18048)
Epoch: 157 | Batch_idx: 150 |  Loss: (0.1279) |  Loss2: (0.0000) | Acc: (95.00%) (18478/19328)
Epoch: 157 | Batch_idx: 160 |  Loss: (0.1277) |  Loss2: (0.0000) | Acc: (95.00%) (19703/20608)
Epoch: 157 | Batch_idx: 170 |  Loss: (0.1278) |  Loss2: (0.0000) | Acc: (95.00%) (20924/21888)
Epoch: 157 | Batch_idx: 180 |  Loss: (0.1270) |  Loss2: (0.0000) | Acc: (95.00%) (22157/23168)
Epoch: 157 | Batch_idx: 190 |  Loss: (0.1282) |  Loss2: (0.0000) | Acc: (95.00%) (23369/24448)
Epoch: 157 | Batch_idx: 200 |  Loss: (0.1271) |  Loss2: (0.0000) | Acc: (95.00%) (24603/25728)
Epoch: 157 | Batch_idx: 210 |  Loss: (0.1268) |  Loss2: (0.0000) | Acc: (95.00%) (25829/27008)
Epoch: 157 | Batch_idx: 220 |  Loss: (0.1258) |  Loss2: (0.0000) | Acc: (95.00%) (27062/28288)
Epoch: 157 | Batch_idx: 230 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (28296/29568)
Epoch: 157 | Batch_idx: 240 |  Loss: (0.1250) |  Loss2: (0.0000) | Acc: (95.00%) (29521/30848)
Epoch: 157 | Batch_idx: 250 |  Loss: (0.1243) |  Loss2: (0.0000) | Acc: (95.00%) (30751/32128)
Epoch: 157 | Batch_idx: 260 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (31981/33408)
Epoch: 157 | Batch_idx: 270 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (33192/34688)
Epoch: 157 | Batch_idx: 280 |  Loss: (0.1251) |  Loss2: (0.0000) | Acc: (95.00%) (34415/35968)
Epoch: 157 | Batch_idx: 290 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (35645/37248)
Epoch: 157 | Batch_idx: 300 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (36875/38528)
Epoch: 157 | Batch_idx: 310 |  Loss: (0.1244) |  Loss2: (0.0000) | Acc: (95.00%) (38091/39808)
Epoch: 157 | Batch_idx: 320 |  Loss: (0.1239) |  Loss2: (0.0000) | Acc: (95.00%) (39322/41088)
Epoch: 157 | Batch_idx: 330 |  Loss: (0.1240) |  Loss2: (0.0000) | Acc: (95.00%) (40544/42368)
Epoch: 157 | Batch_idx: 340 |  Loss: (0.1237) |  Loss2: (0.0000) | Acc: (95.00%) (41779/43648)
Epoch: 157 | Batch_idx: 350 |  Loss: (0.1247) |  Loss2: (0.0000) | Acc: (95.00%) (42988/44928)
Epoch: 157 | Batch_idx: 360 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (44210/46208)
Epoch: 157 | Batch_idx: 370 |  Loss: (0.1249) |  Loss2: (0.0000) | Acc: (95.00%) (45440/47488)
Epoch: 157 | Batch_idx: 380 |  Loss: (0.1248) |  Loss2: (0.0000) | Acc: (95.00%) (46670/48768)
Epoch: 157 | Batch_idx: 390 |  Loss: (0.1246) |  Loss2: (0.0000) | Acc: (95.00%) (47849/50000)
# TEST : Loss: (0.4293) | Acc: (88.00%) (8804/10000)
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.5836, 0.4164], device='cuda:0')
percent tensor([0.5659, 0.4341], device='cuda:0')
percent tensor([0.6698, 0.3302], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6332, 0.3668], device='cuda:0')
percent tensor([0.7610, 0.2390], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 158 | Batch_idx: 0 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 158 | Batch_idx: 10 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (1356/1408)
Epoch: 158 | Batch_idx: 20 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (2590/2688)
Epoch: 158 | Batch_idx: 30 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (3823/3968)
Epoch: 158 | Batch_idx: 40 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (96.00%) (5039/5248)
Epoch: 158 | Batch_idx: 50 |  Loss: (0.1145) |  Loss2: (0.0000) | Acc: (96.00%) (6274/6528)
Epoch: 158 | Batch_idx: 60 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (96.00%) (7502/7808)
Epoch: 158 | Batch_idx: 70 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (8739/9088)
Epoch: 158 | Batch_idx: 80 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (96.00%) (9967/10368)
Epoch: 158 | Batch_idx: 90 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (11201/11648)
Epoch: 158 | Batch_idx: 100 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (96.00%) (12419/12928)
Epoch: 158 | Batch_idx: 110 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (13651/14208)
Epoch: 158 | Batch_idx: 120 |  Loss: (0.1177) |  Loss2: (0.0000) | Acc: (96.00%) (14883/15488)
Epoch: 158 | Batch_idx: 130 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (16115/16768)
Epoch: 158 | Batch_idx: 140 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (96.00%) (17342/18048)
Epoch: 158 | Batch_idx: 150 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (18574/19328)
Epoch: 158 | Batch_idx: 160 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (19816/20608)
Epoch: 158 | Batch_idx: 170 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (21049/21888)
Epoch: 158 | Batch_idx: 180 |  Loss: (0.1168) |  Loss2: (0.0000) | Acc: (96.00%) (22266/23168)
Epoch: 158 | Batch_idx: 190 |  Loss: (0.1174) |  Loss2: (0.0000) | Acc: (96.00%) (23488/24448)
Epoch: 158 | Batch_idx: 200 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (24713/25728)
Epoch: 158 | Batch_idx: 210 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (25923/27008)
Epoch: 158 | Batch_idx: 220 |  Loss: (0.1195) |  Loss2: (0.0000) | Acc: (95.00%) (27148/28288)
Epoch: 158 | Batch_idx: 230 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (28381/29568)
Epoch: 158 | Batch_idx: 240 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (29607/30848)
Epoch: 158 | Batch_idx: 250 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (30830/32128)
Epoch: 158 | Batch_idx: 260 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (32050/33408)
Epoch: 158 | Batch_idx: 270 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (33276/34688)
Epoch: 158 | Batch_idx: 280 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (34501/35968)
Epoch: 158 | Batch_idx: 290 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (35730/37248)
Epoch: 158 | Batch_idx: 300 |  Loss: (0.1189) |  Loss2: (0.0000) | Acc: (95.00%) (36960/38528)
Epoch: 158 | Batch_idx: 310 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (38186/39808)
Epoch: 158 | Batch_idx: 320 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (95.00%) (39417/41088)
Epoch: 158 | Batch_idx: 330 |  Loss: (0.1190) |  Loss2: (0.0000) | Acc: (95.00%) (40644/42368)
Epoch: 158 | Batch_idx: 340 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (41872/43648)
Epoch: 158 | Batch_idx: 350 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (43105/44928)
Epoch: 158 | Batch_idx: 360 |  Loss: (0.1185) |  Loss2: (0.0000) | Acc: (95.00%) (44339/46208)
Epoch: 158 | Batch_idx: 370 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (45564/47488)
Epoch: 158 | Batch_idx: 380 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (46802/48768)
Epoch: 158 | Batch_idx: 390 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (47978/50000)
# TEST : Loss: (0.4223) | Acc: (88.00%) (8822/10000)
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.5844, 0.4156], device='cuda:0')
percent tensor([0.5661, 0.4339], device='cuda:0')
percent tensor([0.6662, 0.3338], device='cuda:0')
percent tensor([0.6058, 0.3942], device='cuda:0')
percent tensor([0.6344, 0.3656], device='cuda:0')
percent tensor([0.7674, 0.2326], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 159 | Batch_idx: 0 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 159 | Batch_idx: 10 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (1350/1408)
Epoch: 159 | Batch_idx: 20 |  Loss: (0.1197) |  Loss2: (0.0000) | Acc: (95.00%) (2575/2688)
Epoch: 159 | Batch_idx: 30 |  Loss: (0.1217) |  Loss2: (0.0000) | Acc: (95.00%) (3803/3968)
Epoch: 159 | Batch_idx: 40 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (5037/5248)
Epoch: 159 | Batch_idx: 50 |  Loss: (0.1179) |  Loss2: (0.0000) | Acc: (95.00%) (6264/6528)
Epoch: 159 | Batch_idx: 60 |  Loss: (0.1201) |  Loss2: (0.0000) | Acc: (95.00%) (7489/7808)
Epoch: 159 | Batch_idx: 70 |  Loss: (0.1187) |  Loss2: (0.0000) | Acc: (95.00%) (8719/9088)
Epoch: 159 | Batch_idx: 80 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (9963/10368)
Epoch: 159 | Batch_idx: 90 |  Loss: (0.1178) |  Loss2: (0.0000) | Acc: (95.00%) (11182/11648)
Epoch: 159 | Batch_idx: 100 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (12415/12928)
Epoch: 159 | Batch_idx: 110 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (13650/14208)
Epoch: 159 | Batch_idx: 120 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (14881/15488)
Epoch: 159 | Batch_idx: 130 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (16105/16768)
Epoch: 159 | Batch_idx: 140 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (17343/18048)
Epoch: 159 | Batch_idx: 150 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (18565/19328)
Epoch: 159 | Batch_idx: 160 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (19790/20608)
Epoch: 159 | Batch_idx: 170 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (21016/21888)
Epoch: 159 | Batch_idx: 180 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (22245/23168)
Epoch: 159 | Batch_idx: 190 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (23473/24448)
Epoch: 159 | Batch_idx: 200 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (24701/25728)
Epoch: 159 | Batch_idx: 210 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (25927/27008)
Epoch: 159 | Batch_idx: 220 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (27151/28288)
Epoch: 159 | Batch_idx: 230 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (28386/29568)
Epoch: 159 | Batch_idx: 240 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (96.00%) (29615/30848)
Epoch: 159 | Batch_idx: 250 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (30850/32128)
Epoch: 159 | Batch_idx: 260 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (32077/33408)
Epoch: 159 | Batch_idx: 270 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (33313/34688)
Epoch: 159 | Batch_idx: 280 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (34534/35968)
Epoch: 159 | Batch_idx: 290 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (35755/37248)
Epoch: 159 | Batch_idx: 300 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (95.00%) (36981/38528)
Epoch: 159 | Batch_idx: 310 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (38209/39808)
Epoch: 159 | Batch_idx: 320 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (95.00%) (39435/41088)
Epoch: 159 | Batch_idx: 330 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (95.00%) (40669/42368)
Epoch: 159 | Batch_idx: 340 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (41908/43648)
Epoch: 159 | Batch_idx: 350 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (43147/44928)
Epoch: 159 | Batch_idx: 360 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (44375/46208)
Epoch: 159 | Batch_idx: 370 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (45610/47488)
Epoch: 159 | Batch_idx: 380 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (46846/48768)
Epoch: 159 | Batch_idx: 390 |  Loss: (0.1144) |  Loss2: (0.0000) | Acc: (96.00%) (48043/50000)
# TEST : Loss: (0.4161) | Acc: (88.00%) (8825/10000)
percent tensor([0.5829, 0.4171], device='cuda:0')
percent tensor([0.5837, 0.4163], device='cuda:0')
percent tensor([0.5680, 0.4320], device='cuda:0')
percent tensor([0.6691, 0.3309], device='cuda:0')
percent tensor([0.6035, 0.3965], device='cuda:0')
percent tensor([0.6391, 0.3609], device='cuda:0')
percent tensor([0.7712, 0.2288], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 160 | Batch_idx: 0 |  Loss: (0.1708) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 160 | Batch_idx: 10 |  Loss: (0.1245) |  Loss2: (0.0000) | Acc: (95.00%) (1339/1408)
Epoch: 160 | Batch_idx: 20 |  Loss: (0.1164) |  Loss2: (0.0000) | Acc: (95.00%) (2578/2688)
Epoch: 160 | Batch_idx: 30 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (96.00%) (3810/3968)
Epoch: 160 | Batch_idx: 40 |  Loss: (0.1170) |  Loss2: (0.0000) | Acc: (95.00%) (5030/5248)
Epoch: 160 | Batch_idx: 50 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (6266/6528)
Epoch: 160 | Batch_idx: 60 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (7504/7808)
Epoch: 160 | Batch_idx: 70 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (95.00%) (8723/9088)
Epoch: 160 | Batch_idx: 80 |  Loss: (0.1163) |  Loss2: (0.0000) | Acc: (96.00%) (9955/10368)
Epoch: 160 | Batch_idx: 90 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (95.00%) (11180/11648)
Epoch: 160 | Batch_idx: 100 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (95.00%) (12410/12928)
Epoch: 160 | Batch_idx: 110 |  Loss: (0.1166) |  Loss2: (0.0000) | Acc: (96.00%) (13645/14208)
Epoch: 160 | Batch_idx: 120 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (14883/15488)
Epoch: 160 | Batch_idx: 130 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (16113/16768)
Epoch: 160 | Batch_idx: 140 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (17337/18048)
Epoch: 160 | Batch_idx: 150 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (18569/19328)
Epoch: 160 | Batch_idx: 160 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (19804/20608)
Epoch: 160 | Batch_idx: 170 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (21029/21888)
Epoch: 160 | Batch_idx: 180 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (22260/23168)
Epoch: 160 | Batch_idx: 190 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (23484/24448)
Epoch: 160 | Batch_idx: 200 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (24709/25728)
Epoch: 160 | Batch_idx: 210 |  Loss: (0.1152) |  Loss2: (0.0000) | Acc: (96.00%) (25948/27008)
Epoch: 160 | Batch_idx: 220 |  Loss: (0.1155) |  Loss2: (0.0000) | Acc: (96.00%) (27177/28288)
Epoch: 160 | Batch_idx: 230 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (28415/29568)
Epoch: 160 | Batch_idx: 240 |  Loss: (0.1154) |  Loss2: (0.0000) | Acc: (96.00%) (29639/30848)
Epoch: 160 | Batch_idx: 250 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (30875/32128)
Epoch: 160 | Batch_idx: 260 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (32099/33408)
Epoch: 160 | Batch_idx: 270 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (33321/34688)
Epoch: 160 | Batch_idx: 280 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (34550/35968)
Epoch: 160 | Batch_idx: 290 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (35780/37248)
Epoch: 160 | Batch_idx: 300 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (37004/38528)
Epoch: 160 | Batch_idx: 310 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (38220/39808)
Epoch: 160 | Batch_idx: 320 |  Loss: (0.1176) |  Loss2: (0.0000) | Acc: (96.00%) (39449/41088)
Epoch: 160 | Batch_idx: 330 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (40679/42368)
Epoch: 160 | Batch_idx: 340 |  Loss: (0.1169) |  Loss2: (0.0000) | Acc: (96.00%) (41916/43648)
Epoch: 160 | Batch_idx: 350 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (96.00%) (43143/44928)
Epoch: 160 | Batch_idx: 360 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (96.00%) (44365/46208)
Epoch: 160 | Batch_idx: 370 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (45582/47488)
Epoch: 160 | Batch_idx: 380 |  Loss: (0.1183) |  Loss2: (0.0000) | Acc: (95.00%) (46804/48768)
Epoch: 160 | Batch_idx: 390 |  Loss: (0.1180) |  Loss2: (0.0000) | Acc: (95.00%) (47990/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_160.pth.tar'
# TEST : Loss: (0.4195) | Acc: (88.00%) (8824/10000)
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.5830, 0.4170], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.6695, 0.3305], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6409, 0.3591], device='cuda:0')
percent tensor([0.7697, 0.2303], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.1788, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(824.8174, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(825.8727, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1519.2159, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(483.6387, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2284.7847, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4263.6445, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1364.6675, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6229.1411, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11667.3232, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3841.3528, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16242.6094, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 161 | Batch_idx: 0 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 161 | Batch_idx: 10 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 161 | Batch_idx: 20 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 161 | Batch_idx: 30 |  Loss: (0.1033) |  Loss2: (0.0000) | Acc: (96.00%) (3822/3968)
Epoch: 161 | Batch_idx: 40 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (5053/5248)
Epoch: 161 | Batch_idx: 50 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (96.00%) (6286/6528)
Epoch: 161 | Batch_idx: 60 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (7514/7808)
Epoch: 161 | Batch_idx: 70 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (8745/9088)
Epoch: 161 | Batch_idx: 80 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (9976/10368)
Epoch: 161 | Batch_idx: 90 |  Loss: (0.1118) |  Loss2: (0.0000) | Acc: (96.00%) (11196/11648)
Epoch: 161 | Batch_idx: 100 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (12428/12928)
Epoch: 161 | Batch_idx: 110 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (13666/14208)
Epoch: 161 | Batch_idx: 120 |  Loss: (0.1119) |  Loss2: (0.0000) | Acc: (96.00%) (14895/15488)
Epoch: 161 | Batch_idx: 130 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (16125/16768)
Epoch: 161 | Batch_idx: 140 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (17355/18048)
Epoch: 161 | Batch_idx: 150 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (18575/19328)
Epoch: 161 | Batch_idx: 160 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (19804/20608)
Epoch: 161 | Batch_idx: 170 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (21029/21888)
Epoch: 161 | Batch_idx: 180 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (22268/23168)
Epoch: 161 | Batch_idx: 190 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (23485/24448)
Epoch: 161 | Batch_idx: 200 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (96.00%) (24712/25728)
Epoch: 161 | Batch_idx: 210 |  Loss: (0.1135) |  Loss2: (0.0000) | Acc: (96.00%) (25943/27008)
Epoch: 161 | Batch_idx: 220 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (27175/28288)
Epoch: 161 | Batch_idx: 230 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (28400/29568)
Epoch: 161 | Batch_idx: 240 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (29629/30848)
Epoch: 161 | Batch_idx: 250 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (30853/32128)
Epoch: 161 | Batch_idx: 260 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (32078/33408)
Epoch: 161 | Batch_idx: 270 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (33301/34688)
Epoch: 161 | Batch_idx: 280 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (34535/35968)
Epoch: 161 | Batch_idx: 290 |  Loss: (0.1151) |  Loss2: (0.0000) | Acc: (96.00%) (35768/37248)
Epoch: 161 | Batch_idx: 300 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (37008/38528)
Epoch: 161 | Batch_idx: 310 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (38235/39808)
Epoch: 161 | Batch_idx: 320 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (39465/41088)
Epoch: 161 | Batch_idx: 330 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (96.00%) (40692/42368)
Epoch: 161 | Batch_idx: 340 |  Loss: (0.1157) |  Loss2: (0.0000) | Acc: (96.00%) (41911/43648)
Epoch: 161 | Batch_idx: 350 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (43131/44928)
Epoch: 161 | Batch_idx: 360 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (44360/46208)
Epoch: 161 | Batch_idx: 370 |  Loss: (0.1162) |  Loss2: (0.0000) | Acc: (96.00%) (45594/47488)
Epoch: 161 | Batch_idx: 380 |  Loss: (0.1172) |  Loss2: (0.0000) | Acc: (95.00%) (46812/48768)
Epoch: 161 | Batch_idx: 390 |  Loss: (0.1171) |  Loss2: (0.0000) | Acc: (95.00%) (47992/50000)
# TEST : Loss: (0.4317) | Acc: (88.00%) (8821/10000)
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.5671, 0.4329], device='cuda:0')
percent tensor([0.6690, 0.3310], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6384, 0.3616], device='cuda:0')
percent tensor([0.7693, 0.2307], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 162 | Batch_idx: 0 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 162 | Batch_idx: 10 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 162 | Batch_idx: 20 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (2587/2688)
Epoch: 162 | Batch_idx: 30 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 162 | Batch_idx: 40 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (5044/5248)
Epoch: 162 | Batch_idx: 50 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (6274/6528)
Epoch: 162 | Batch_idx: 60 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (7518/7808)
Epoch: 162 | Batch_idx: 70 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (8739/9088)
Epoch: 162 | Batch_idx: 80 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (96.00%) (9966/10368)
Epoch: 162 | Batch_idx: 90 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (11207/11648)
Epoch: 162 | Batch_idx: 100 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (12443/12928)
Epoch: 162 | Batch_idx: 110 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (13678/14208)
Epoch: 162 | Batch_idx: 120 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (14914/15488)
Epoch: 162 | Batch_idx: 130 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (16135/16768)
Epoch: 162 | Batch_idx: 140 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (17382/18048)
Epoch: 162 | Batch_idx: 150 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (18611/19328)
Epoch: 162 | Batch_idx: 160 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (19841/20608)
Epoch: 162 | Batch_idx: 170 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (21074/21888)
Epoch: 162 | Batch_idx: 180 |  Loss: (0.1082) |  Loss2: (0.0000) | Acc: (96.00%) (22318/23168)
Epoch: 162 | Batch_idx: 190 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (23558/24448)
Epoch: 162 | Batch_idx: 200 |  Loss: (0.1083) |  Loss2: (0.0000) | Acc: (96.00%) (24785/25728)
Epoch: 162 | Batch_idx: 210 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (26002/27008)
Epoch: 162 | Batch_idx: 220 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (27231/28288)
Epoch: 162 | Batch_idx: 230 |  Loss: (0.1096) |  Loss2: (0.0000) | Acc: (96.00%) (28462/29568)
Epoch: 162 | Batch_idx: 240 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (29685/30848)
Epoch: 162 | Batch_idx: 250 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (30915/32128)
Epoch: 162 | Batch_idx: 260 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (32140/33408)
Epoch: 162 | Batch_idx: 270 |  Loss: (0.1109) |  Loss2: (0.0000) | Acc: (96.00%) (33379/34688)
Epoch: 162 | Batch_idx: 280 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (34619/35968)
Epoch: 162 | Batch_idx: 290 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (35854/37248)
Epoch: 162 | Batch_idx: 300 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (37085/38528)
Epoch: 162 | Batch_idx: 310 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (38329/39808)
Epoch: 162 | Batch_idx: 320 |  Loss: (0.1100) |  Loss2: (0.0000) | Acc: (96.00%) (39565/41088)
Epoch: 162 | Batch_idx: 330 |  Loss: (0.1102) |  Loss2: (0.0000) | Acc: (96.00%) (40784/42368)
Epoch: 162 | Batch_idx: 340 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (42006/43648)
Epoch: 162 | Batch_idx: 350 |  Loss: (0.1104) |  Loss2: (0.0000) | Acc: (96.00%) (43240/44928)
Epoch: 162 | Batch_idx: 360 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (44461/46208)
Epoch: 162 | Batch_idx: 370 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (45701/47488)
Epoch: 162 | Batch_idx: 380 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (46925/48768)
Epoch: 162 | Batch_idx: 390 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (48102/50000)
# TEST : Loss: (0.4499) | Acc: (87.00%) (8776/10000)
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.5682, 0.4318], device='cuda:0')
percent tensor([0.6685, 0.3315], device='cuda:0')
percent tensor([0.6024, 0.3976], device='cuda:0')
percent tensor([0.6376, 0.3624], device='cuda:0')
percent tensor([0.7628, 0.2372], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 163 | Batch_idx: 0 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 163 | Batch_idx: 10 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 163 | Batch_idx: 20 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 163 | Batch_idx: 30 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (3824/3968)
Epoch: 163 | Batch_idx: 40 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (5057/5248)
Epoch: 163 | Batch_idx: 50 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (6290/6528)
Epoch: 163 | Batch_idx: 60 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (7524/7808)
Epoch: 163 | Batch_idx: 70 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (8755/9088)
Epoch: 163 | Batch_idx: 80 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (9979/10368)
Epoch: 163 | Batch_idx: 90 |  Loss: (0.1097) |  Loss2: (0.0000) | Acc: (96.00%) (11200/11648)
Epoch: 163 | Batch_idx: 100 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (12438/12928)
Epoch: 163 | Batch_idx: 110 |  Loss: (0.1076) |  Loss2: (0.0000) | Acc: (96.00%) (13681/14208)
Epoch: 163 | Batch_idx: 120 |  Loss: (0.1073) |  Loss2: (0.0000) | Acc: (96.00%) (14915/15488)
Epoch: 163 | Batch_idx: 130 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (16160/16768)
Epoch: 163 | Batch_idx: 140 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (17386/18048)
Epoch: 163 | Batch_idx: 150 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (18621/19328)
Epoch: 163 | Batch_idx: 160 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (19849/20608)
Epoch: 163 | Batch_idx: 170 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (21084/21888)
Epoch: 163 | Batch_idx: 180 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (22309/23168)
Epoch: 163 | Batch_idx: 190 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (23549/24448)
Epoch: 163 | Batch_idx: 200 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (24790/25728)
Epoch: 163 | Batch_idx: 210 |  Loss: (0.1063) |  Loss2: (0.0000) | Acc: (96.00%) (26030/27008)
Epoch: 163 | Batch_idx: 220 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (27250/28288)
Epoch: 163 | Batch_idx: 230 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (28486/29568)
Epoch: 163 | Batch_idx: 240 |  Loss: (0.1074) |  Loss2: (0.0000) | Acc: (96.00%) (29708/30848)
Epoch: 163 | Batch_idx: 250 |  Loss: (0.1075) |  Loss2: (0.0000) | Acc: (96.00%) (30939/32128)
Epoch: 163 | Batch_idx: 260 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (32172/33408)
Epoch: 163 | Batch_idx: 270 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (33398/34688)
Epoch: 163 | Batch_idx: 280 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (34626/35968)
Epoch: 163 | Batch_idx: 290 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (35864/37248)
Epoch: 163 | Batch_idx: 300 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (37094/38528)
Epoch: 163 | Batch_idx: 310 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (38333/39808)
Epoch: 163 | Batch_idx: 320 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (39576/41088)
Epoch: 163 | Batch_idx: 330 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (96.00%) (40813/42368)
Epoch: 163 | Batch_idx: 340 |  Loss: (0.1078) |  Loss2: (0.0000) | Acc: (96.00%) (42043/43648)
Epoch: 163 | Batch_idx: 350 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (43268/44928)
Epoch: 163 | Batch_idx: 360 |  Loss: (0.1080) |  Loss2: (0.0000) | Acc: (96.00%) (44492/46208)
Epoch: 163 | Batch_idx: 370 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (45705/47488)
Epoch: 163 | Batch_idx: 380 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (46934/48768)
Epoch: 163 | Batch_idx: 390 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (48113/50000)
# TEST : Loss: (0.4923) | Acc: (86.00%) (8681/10000)
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.5826, 0.4174], device='cuda:0')
percent tensor([0.5677, 0.4323], device='cuda:0')
percent tensor([0.6689, 0.3311], device='cuda:0')
percent tensor([0.6045, 0.3955], device='cuda:0')
percent tensor([0.6398, 0.3602], device='cuda:0')
percent tensor([0.7666, 0.2334], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 164 | Batch_idx: 0 |  Loss: (0.1522) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 164 | Batch_idx: 10 |  Loss: (0.1359) |  Loss2: (0.0000) | Acc: (95.00%) (1341/1408)
Epoch: 164 | Batch_idx: 20 |  Loss: (0.1253) |  Loss2: (0.0000) | Acc: (95.00%) (2570/2688)
Epoch: 164 | Batch_idx: 30 |  Loss: (0.1175) |  Loss2: (0.0000) | Acc: (95.00%) (3808/3968)
Epoch: 164 | Batch_idx: 40 |  Loss: (0.1153) |  Loss2: (0.0000) | Acc: (95.00%) (5038/5248)
Epoch: 164 | Batch_idx: 50 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (6278/6528)
Epoch: 164 | Batch_idx: 60 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (7519/7808)
Epoch: 164 | Batch_idx: 70 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (8754/9088)
Epoch: 164 | Batch_idx: 80 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (9993/10368)
Epoch: 164 | Batch_idx: 90 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (11233/11648)
Epoch: 164 | Batch_idx: 100 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (12477/12928)
Epoch: 164 | Batch_idx: 110 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (13708/14208)
Epoch: 164 | Batch_idx: 120 |  Loss: (0.1036) |  Loss2: (0.0000) | Acc: (96.00%) (14956/15488)
Epoch: 164 | Batch_idx: 130 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (16193/16768)
Epoch: 164 | Batch_idx: 140 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (17429/18048)
Epoch: 164 | Batch_idx: 150 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (18660/19328)
Epoch: 164 | Batch_idx: 160 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (19899/20608)
Epoch: 164 | Batch_idx: 170 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (21132/21888)
Epoch: 164 | Batch_idx: 180 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (22363/23168)
Epoch: 164 | Batch_idx: 190 |  Loss: (0.1028) |  Loss2: (0.0000) | Acc: (96.00%) (23598/24448)
Epoch: 164 | Batch_idx: 200 |  Loss: (0.1039) |  Loss2: (0.0000) | Acc: (96.00%) (24823/25728)
Epoch: 164 | Batch_idx: 210 |  Loss: (0.1041) |  Loss2: (0.0000) | Acc: (96.00%) (26057/27008)
Epoch: 164 | Batch_idx: 220 |  Loss: (0.1045) |  Loss2: (0.0000) | Acc: (96.00%) (27283/28288)
Epoch: 164 | Batch_idx: 230 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (96.00%) (28509/29568)
Epoch: 164 | Batch_idx: 240 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (29736/30848)
Epoch: 164 | Batch_idx: 250 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (30972/32128)
Epoch: 164 | Batch_idx: 260 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (32205/33408)
Epoch: 164 | Batch_idx: 270 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (33435/34688)
Epoch: 164 | Batch_idx: 280 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (34675/35968)
Epoch: 164 | Batch_idx: 290 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (35911/37248)
Epoch: 164 | Batch_idx: 300 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (37130/38528)
Epoch: 164 | Batch_idx: 310 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (38359/39808)
Epoch: 164 | Batch_idx: 320 |  Loss: (0.1072) |  Loss2: (0.0000) | Acc: (96.00%) (39593/41088)
Epoch: 164 | Batch_idx: 330 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (40823/42368)
Epoch: 164 | Batch_idx: 340 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (42058/43648)
Epoch: 164 | Batch_idx: 350 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (43285/44928)
Epoch: 164 | Batch_idx: 360 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (44533/46208)
Epoch: 164 | Batch_idx: 370 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (45762/47488)
Epoch: 164 | Batch_idx: 380 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (46992/48768)
Epoch: 164 | Batch_idx: 390 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (48174/50000)
# TEST : Loss: (0.4239) | Acc: (88.00%) (8820/10000)
percent tensor([0.5824, 0.4176], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.5679, 0.4321], device='cuda:0')
percent tensor([0.6683, 0.3317], device='cuda:0')
percent tensor([0.6049, 0.3951], device='cuda:0')
percent tensor([0.6380, 0.3620], device='cuda:0')
percent tensor([0.7673, 0.2327], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 165 | Batch_idx: 0 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 165 | Batch_idx: 10 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 165 | Batch_idx: 20 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (96.00%) (2593/2688)
Epoch: 165 | Batch_idx: 30 |  Loss: (0.1238) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 165 | Batch_idx: 40 |  Loss: (0.1252) |  Loss2: (0.0000) | Acc: (95.00%) (5023/5248)
Epoch: 165 | Batch_idx: 50 |  Loss: (0.1333) |  Loss2: (0.0000) | Acc: (95.00%) (6234/6528)
Epoch: 165 | Batch_idx: 60 |  Loss: (0.1317) |  Loss2: (0.0000) | Acc: (95.00%) (7454/7808)
Epoch: 165 | Batch_idx: 70 |  Loss: (0.1335) |  Loss2: (0.0000) | Acc: (95.00%) (8675/9088)
Epoch: 165 | Batch_idx: 80 |  Loss: (0.1351) |  Loss2: (0.0000) | Acc: (95.00%) (9883/10368)
Epoch: 165 | Batch_idx: 90 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (11105/11648)
Epoch: 165 | Batch_idx: 100 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (12320/12928)
Epoch: 165 | Batch_idx: 110 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (13538/14208)
Epoch: 165 | Batch_idx: 120 |  Loss: (0.1381) |  Loss2: (0.0000) | Acc: (95.00%) (14745/15488)
Epoch: 165 | Batch_idx: 130 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (15962/16768)
Epoch: 165 | Batch_idx: 140 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (17177/18048)
Epoch: 165 | Batch_idx: 150 |  Loss: (0.1368) |  Loss2: (0.0000) | Acc: (95.00%) (18394/19328)
Epoch: 165 | Batch_idx: 160 |  Loss: (0.1366) |  Loss2: (0.0000) | Acc: (95.00%) (19616/20608)
Epoch: 165 | Batch_idx: 170 |  Loss: (0.1364) |  Loss2: (0.0000) | Acc: (95.00%) (20843/21888)
Epoch: 165 | Batch_idx: 180 |  Loss: (0.1369) |  Loss2: (0.0000) | Acc: (95.00%) (22059/23168)
Epoch: 165 | Batch_idx: 190 |  Loss: (0.1375) |  Loss2: (0.0000) | Acc: (95.00%) (23272/24448)
Epoch: 165 | Batch_idx: 200 |  Loss: (0.1374) |  Loss2: (0.0000) | Acc: (95.00%) (24494/25728)
Epoch: 165 | Batch_idx: 210 |  Loss: (0.1387) |  Loss2: (0.0000) | Acc: (95.00%) (25699/27008)
Epoch: 165 | Batch_idx: 220 |  Loss: (0.1379) |  Loss2: (0.0000) | Acc: (95.00%) (26920/28288)
Epoch: 165 | Batch_idx: 230 |  Loss: (0.1389) |  Loss2: (0.0000) | Acc: (95.00%) (28128/29568)
Epoch: 165 | Batch_idx: 240 |  Loss: (0.1380) |  Loss2: (0.0000) | Acc: (95.00%) (29357/30848)
Epoch: 165 | Batch_idx: 250 |  Loss: (0.1373) |  Loss2: (0.0000) | Acc: (95.00%) (30586/32128)
Epoch: 165 | Batch_idx: 260 |  Loss: (0.1370) |  Loss2: (0.0000) | Acc: (95.00%) (31808/33408)
Epoch: 165 | Batch_idx: 270 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (33044/34688)
Epoch: 165 | Batch_idx: 280 |  Loss: (0.1362) |  Loss2: (0.0000) | Acc: (95.00%) (34260/35968)
Epoch: 165 | Batch_idx: 290 |  Loss: (0.1360) |  Loss2: (0.0000) | Acc: (95.00%) (35483/37248)
Epoch: 165 | Batch_idx: 300 |  Loss: (0.1361) |  Loss2: (0.0000) | Acc: (95.00%) (36701/38528)
Epoch: 165 | Batch_idx: 310 |  Loss: (0.1357) |  Loss2: (0.0000) | Acc: (95.00%) (37924/39808)
Epoch: 165 | Batch_idx: 320 |  Loss: (0.1353) |  Loss2: (0.0000) | Acc: (95.00%) (39145/41088)
Epoch: 165 | Batch_idx: 330 |  Loss: (0.1352) |  Loss2: (0.0000) | Acc: (95.00%) (40370/42368)
Epoch: 165 | Batch_idx: 340 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (41594/43648)
Epoch: 165 | Batch_idx: 350 |  Loss: (0.1342) |  Loss2: (0.0000) | Acc: (95.00%) (42817/44928)
Epoch: 165 | Batch_idx: 360 |  Loss: (0.1347) |  Loss2: (0.0000) | Acc: (95.00%) (44024/46208)
Epoch: 165 | Batch_idx: 370 |  Loss: (0.1345) |  Loss2: (0.0000) | Acc: (95.00%) (45243/47488)
Epoch: 165 | Batch_idx: 380 |  Loss: (0.1338) |  Loss2: (0.0000) | Acc: (95.00%) (46479/48768)
Epoch: 165 | Batch_idx: 390 |  Loss: (0.1332) |  Loss2: (0.0000) | Acc: (95.00%) (47669/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_165.pth.tar'
# TEST : Loss: (0.4369) | Acc: (87.00%) (8787/10000)
percent tensor([0.5904, 0.4096], device='cuda:0')
percent tensor([0.5766, 0.4234], device='cuda:0')
percent tensor([0.5684, 0.4316], device='cuda:0')
percent tensor([0.6715, 0.3285], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6507, 0.3493], device='cuda:0')
percent tensor([0.7322, 0.2678], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 166 | Batch_idx: 0 |  Loss: (0.1552) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 166 | Batch_idx: 10 |  Loss: (0.1230) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 166 | Batch_idx: 20 |  Loss: (0.1192) |  Loss2: (0.0000) | Acc: (96.00%) (2583/2688)
Epoch: 166 | Batch_idx: 30 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (3813/3968)
Epoch: 166 | Batch_idx: 40 |  Loss: (0.1193) |  Loss2: (0.0000) | Acc: (95.00%) (5032/5248)
Epoch: 166 | Batch_idx: 50 |  Loss: (0.1173) |  Loss2: (0.0000) | Acc: (96.00%) (6268/6528)
Epoch: 166 | Batch_idx: 60 |  Loss: (0.1186) |  Loss2: (0.0000) | Acc: (95.00%) (7488/7808)
Epoch: 166 | Batch_idx: 70 |  Loss: (0.1202) |  Loss2: (0.0000) | Acc: (95.00%) (8711/9088)
Epoch: 166 | Batch_idx: 80 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (9947/10368)
Epoch: 166 | Batch_idx: 90 |  Loss: (0.1182) |  Loss2: (0.0000) | Acc: (95.00%) (11181/11648)
Epoch: 166 | Batch_idx: 100 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (95.00%) (12406/12928)
Epoch: 166 | Batch_idx: 110 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (13654/14208)
Epoch: 166 | Batch_idx: 120 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (14873/15488)
Epoch: 166 | Batch_idx: 130 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (16108/16768)
Epoch: 166 | Batch_idx: 140 |  Loss: (0.1140) |  Loss2: (0.0000) | Acc: (96.00%) (17348/18048)
Epoch: 166 | Batch_idx: 150 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (18568/19328)
Epoch: 166 | Batch_idx: 160 |  Loss: (0.1160) |  Loss2: (0.0000) | Acc: (96.00%) (19802/20608)
Epoch: 166 | Batch_idx: 170 |  Loss: (0.1161) |  Loss2: (0.0000) | Acc: (96.00%) (21028/21888)
Epoch: 166 | Batch_idx: 180 |  Loss: (0.1158) |  Loss2: (0.0000) | Acc: (96.00%) (22265/23168)
Epoch: 166 | Batch_idx: 190 |  Loss: (0.1159) |  Loss2: (0.0000) | Acc: (96.00%) (23488/24448)
Epoch: 166 | Batch_idx: 200 |  Loss: (0.1148) |  Loss2: (0.0000) | Acc: (96.00%) (24728/25728)
Epoch: 166 | Batch_idx: 210 |  Loss: (0.1149) |  Loss2: (0.0000) | Acc: (96.00%) (25960/27008)
Epoch: 166 | Batch_idx: 220 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (27190/28288)
Epoch: 166 | Batch_idx: 230 |  Loss: (0.1147) |  Loss2: (0.0000) | Acc: (96.00%) (28420/29568)
Epoch: 166 | Batch_idx: 240 |  Loss: (0.1143) |  Loss2: (0.0000) | Acc: (96.00%) (29655/30848)
Epoch: 166 | Batch_idx: 250 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (30890/32128)
Epoch: 166 | Batch_idx: 260 |  Loss: (0.1137) |  Loss2: (0.0000) | Acc: (96.00%) (32114/33408)
Epoch: 166 | Batch_idx: 270 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (33356/34688)
Epoch: 166 | Batch_idx: 280 |  Loss: (0.1136) |  Loss2: (0.0000) | Acc: (96.00%) (34587/35968)
Epoch: 166 | Batch_idx: 290 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (35824/37248)
Epoch: 166 | Batch_idx: 300 |  Loss: (0.1130) |  Loss2: (0.0000) | Acc: (96.00%) (37061/38528)
Epoch: 166 | Batch_idx: 310 |  Loss: (0.1129) |  Loss2: (0.0000) | Acc: (96.00%) (38293/39808)
Epoch: 166 | Batch_idx: 320 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (39527/41088)
Epoch: 166 | Batch_idx: 330 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (40749/42368)
Epoch: 166 | Batch_idx: 340 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (96.00%) (41970/43648)
Epoch: 166 | Batch_idx: 350 |  Loss: (0.1133) |  Loss2: (0.0000) | Acc: (96.00%) (43202/44928)
Epoch: 166 | Batch_idx: 360 |  Loss: (0.1131) |  Loss2: (0.0000) | Acc: (96.00%) (44434/46208)
Epoch: 166 | Batch_idx: 370 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (45676/47488)
Epoch: 166 | Batch_idx: 380 |  Loss: (0.1126) |  Loss2: (0.0000) | Acc: (96.00%) (46904/48768)
Epoch: 166 | Batch_idx: 390 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (96.00%) (48098/50000)
# TEST : Loss: (0.4189) | Acc: (88.00%) (8829/10000)
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5687, 0.4313], device='cuda:0')
percent tensor([0.6747, 0.3253], device='cuda:0')
percent tensor([0.6104, 0.3896], device='cuda:0')
percent tensor([0.6509, 0.3491], device='cuda:0')
percent tensor([0.7431, 0.2569], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 167 | Batch_idx: 0 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 167 | Batch_idx: 10 |  Loss: (0.1191) |  Loss2: (0.0000) | Acc: (96.00%) (1353/1408)
Epoch: 167 | Batch_idx: 20 |  Loss: (0.1184) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 167 | Batch_idx: 30 |  Loss: (0.1150) |  Loss2: (0.0000) | Acc: (96.00%) (3814/3968)
Epoch: 167 | Batch_idx: 40 |  Loss: (0.1165) |  Loss2: (0.0000) | Acc: (96.00%) (5040/5248)
Epoch: 167 | Batch_idx: 50 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (96.00%) (6279/6528)
Epoch: 167 | Batch_idx: 60 |  Loss: (0.1112) |  Loss2: (0.0000) | Acc: (96.00%) (7508/7808)
Epoch: 167 | Batch_idx: 70 |  Loss: (0.1128) |  Loss2: (0.0000) | Acc: (96.00%) (8730/9088)
Epoch: 167 | Batch_idx: 80 |  Loss: (0.1134) |  Loss2: (0.0000) | Acc: (95.00%) (9951/10368)
Epoch: 167 | Batch_idx: 90 |  Loss: (0.1121) |  Loss2: (0.0000) | Acc: (96.00%) (11186/11648)
Epoch: 167 | Batch_idx: 100 |  Loss: (0.1115) |  Loss2: (0.0000) | Acc: (96.00%) (12413/12928)
Epoch: 167 | Batch_idx: 110 |  Loss: (0.1116) |  Loss2: (0.0000) | Acc: (96.00%) (13643/14208)
Epoch: 167 | Batch_idx: 120 |  Loss: (0.1127) |  Loss2: (0.0000) | Acc: (96.00%) (14871/15488)
Epoch: 167 | Batch_idx: 130 |  Loss: (0.1111) |  Loss2: (0.0000) | Acc: (96.00%) (16115/16768)
Epoch: 167 | Batch_idx: 140 |  Loss: (0.1113) |  Loss2: (0.0000) | Acc: (96.00%) (17342/18048)
Epoch: 167 | Batch_idx: 150 |  Loss: (0.1120) |  Loss2: (0.0000) | Acc: (96.00%) (18573/19328)
Epoch: 167 | Batch_idx: 160 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (19812/20608)
Epoch: 167 | Batch_idx: 170 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (21044/21888)
Epoch: 167 | Batch_idx: 180 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (96.00%) (22265/23168)
Epoch: 167 | Batch_idx: 190 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (23505/24448)
Epoch: 167 | Batch_idx: 200 |  Loss: (0.1105) |  Loss2: (0.0000) | Acc: (96.00%) (24734/25728)
Epoch: 167 | Batch_idx: 210 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (25962/27008)
Epoch: 167 | Batch_idx: 220 |  Loss: (0.1103) |  Loss2: (0.0000) | Acc: (96.00%) (27201/28288)
Epoch: 167 | Batch_idx: 230 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (28441/29568)
Epoch: 167 | Batch_idx: 240 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (96.00%) (29668/30848)
Epoch: 167 | Batch_idx: 250 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (96.00%) (30913/32128)
Epoch: 167 | Batch_idx: 260 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (32149/33408)
Epoch: 167 | Batch_idx: 270 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (33381/34688)
Epoch: 167 | Batch_idx: 280 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (34616/35968)
Epoch: 167 | Batch_idx: 290 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (35869/37248)
Epoch: 167 | Batch_idx: 300 |  Loss: (0.1071) |  Loss2: (0.0000) | Acc: (96.00%) (37095/38528)
Epoch: 167 | Batch_idx: 310 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (38326/39808)
Epoch: 167 | Batch_idx: 320 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (39571/41088)
Epoch: 167 | Batch_idx: 330 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (40796/42368)
Epoch: 167 | Batch_idx: 340 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (42030/43648)
Epoch: 167 | Batch_idx: 350 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (43262/44928)
Epoch: 167 | Batch_idx: 360 |  Loss: (0.1069) |  Loss2: (0.0000) | Acc: (96.00%) (44499/46208)
Epoch: 167 | Batch_idx: 370 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (45735/47488)
Epoch: 167 | Batch_idx: 380 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (46965/48768)
Epoch: 167 | Batch_idx: 390 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (48158/50000)
# TEST : Loss: (0.4094) | Acc: (88.00%) (8848/10000)
percent tensor([0.5877, 0.4123], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.5722, 0.4278], device='cuda:0')
percent tensor([0.6776, 0.3224], device='cuda:0')
percent tensor([0.6143, 0.3857], device='cuda:0')
percent tensor([0.6523, 0.3477], device='cuda:0')
percent tensor([0.7405, 0.2595], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 168 | Batch_idx: 0 |  Loss: (0.1541) |  Loss2: (0.0000) | Acc: (92.00%) (118/128)
Epoch: 168 | Batch_idx: 10 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (1360/1408)
Epoch: 168 | Batch_idx: 20 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (2594/2688)
Epoch: 168 | Batch_idx: 30 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (3837/3968)
Epoch: 168 | Batch_idx: 40 |  Loss: (0.1023) |  Loss2: (0.0000) | Acc: (96.00%) (5072/5248)
Epoch: 168 | Batch_idx: 50 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (6318/6528)
Epoch: 168 | Batch_idx: 60 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (7557/7808)
Epoch: 168 | Batch_idx: 70 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (8789/9088)
Epoch: 168 | Batch_idx: 80 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (10021/10368)
Epoch: 168 | Batch_idx: 90 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (11255/11648)
Epoch: 168 | Batch_idx: 100 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (12487/12928)
Epoch: 168 | Batch_idx: 110 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (13722/14208)
Epoch: 168 | Batch_idx: 120 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (14966/15488)
Epoch: 168 | Batch_idx: 130 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (16203/16768)
Epoch: 168 | Batch_idx: 140 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (17445/18048)
Epoch: 168 | Batch_idx: 150 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (18669/19328)
Epoch: 168 | Batch_idx: 160 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (19910/20608)
Epoch: 168 | Batch_idx: 170 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (21151/21888)
Epoch: 168 | Batch_idx: 180 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (22383/23168)
Epoch: 168 | Batch_idx: 190 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (23619/24448)
Epoch: 168 | Batch_idx: 200 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (24863/25728)
Epoch: 168 | Batch_idx: 210 |  Loss: (0.1010) |  Loss2: (0.0000) | Acc: (96.00%) (26105/27008)
Epoch: 168 | Batch_idx: 220 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (27333/28288)
Epoch: 168 | Batch_idx: 230 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (28575/29568)
Epoch: 168 | Batch_idx: 240 |  Loss: (0.1006) |  Loss2: (0.0000) | Acc: (96.00%) (29816/30848)
Epoch: 168 | Batch_idx: 250 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (31052/32128)
Epoch: 168 | Batch_idx: 260 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (32286/33408)
Epoch: 168 | Batch_idx: 270 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (33531/34688)
Epoch: 168 | Batch_idx: 280 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (34772/35968)
Epoch: 168 | Batch_idx: 290 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (36011/37248)
Epoch: 168 | Batch_idx: 300 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (37252/38528)
Epoch: 168 | Batch_idx: 310 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (38485/39808)
Epoch: 168 | Batch_idx: 320 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (39723/41088)
Epoch: 168 | Batch_idx: 330 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (40955/42368)
Epoch: 168 | Batch_idx: 340 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (42190/43648)
Epoch: 168 | Batch_idx: 350 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (43435/44928)
Epoch: 168 | Batch_idx: 360 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (44676/46208)
Epoch: 168 | Batch_idx: 370 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (45914/47488)
Epoch: 168 | Batch_idx: 380 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (47154/48768)
Epoch: 168 | Batch_idx: 390 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (48352/50000)
# TEST : Loss: (0.4033) | Acc: (88.00%) (8880/10000)
percent tensor([0.5878, 0.4122], device='cuda:0')
percent tensor([0.5771, 0.4229], device='cuda:0')
percent tensor([0.5746, 0.4254], device='cuda:0')
percent tensor([0.6761, 0.3239], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.6443, 0.3557], device='cuda:0')
percent tensor([0.7464, 0.2536], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 169 | Batch_idx: 0 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 169 | Batch_idx: 10 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 169 | Batch_idx: 20 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 169 | Batch_idx: 30 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (97.00%) (3854/3968)
Epoch: 169 | Batch_idx: 40 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (97.00%) (5096/5248)
Epoch: 169 | Batch_idx: 50 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (97.00%) (6339/6528)
Epoch: 169 | Batch_idx: 60 |  Loss: (0.0923) |  Loss2: (0.0000) | Acc: (97.00%) (7582/7808)
Epoch: 169 | Batch_idx: 70 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (8830/9088)
Epoch: 169 | Batch_idx: 80 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (97.00%) (10079/10368)
Epoch: 169 | Batch_idx: 90 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (97.00%) (11309/11648)
Epoch: 169 | Batch_idx: 100 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (97.00%) (12550/12928)
Epoch: 169 | Batch_idx: 110 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (13778/14208)
Epoch: 169 | Batch_idx: 120 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (15010/15488)
Epoch: 169 | Batch_idx: 130 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (16252/16768)
Epoch: 169 | Batch_idx: 140 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (17486/18048)
Epoch: 169 | Batch_idx: 150 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (18719/19328)
Epoch: 169 | Batch_idx: 160 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (19963/20608)
Epoch: 169 | Batch_idx: 170 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (21197/21888)
Epoch: 169 | Batch_idx: 180 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (22442/23168)
Epoch: 169 | Batch_idx: 190 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (23673/24448)
Epoch: 169 | Batch_idx: 200 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (24912/25728)
Epoch: 169 | Batch_idx: 210 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (26151/27008)
Epoch: 169 | Batch_idx: 220 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (27397/28288)
Epoch: 169 | Batch_idx: 230 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (28628/29568)
Epoch: 169 | Batch_idx: 240 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (29863/30848)
Epoch: 169 | Batch_idx: 250 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (31105/32128)
Epoch: 169 | Batch_idx: 260 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (32338/33408)
Epoch: 169 | Batch_idx: 270 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (33581/34688)
Epoch: 169 | Batch_idx: 280 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (34820/35968)
Epoch: 169 | Batch_idx: 290 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (36058/37248)
Epoch: 169 | Batch_idx: 300 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (37293/38528)
Epoch: 169 | Batch_idx: 310 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (38529/39808)
Epoch: 169 | Batch_idx: 320 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (39766/41088)
Epoch: 169 | Batch_idx: 330 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (41015/42368)
Epoch: 169 | Batch_idx: 340 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (42245/43648)
Epoch: 169 | Batch_idx: 350 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (43492/44928)
Epoch: 169 | Batch_idx: 360 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (44737/46208)
Epoch: 169 | Batch_idx: 370 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (45969/47488)
Epoch: 169 | Batch_idx: 380 |  Loss: (0.0963) |  Loss2: (0.0000) | Acc: (96.00%) (47215/48768)
Epoch: 169 | Batch_idx: 390 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (48402/50000)
# TEST : Loss: (0.3968) | Acc: (89.00%) (8903/10000)
percent tensor([0.5889, 0.4111], device='cuda:0')
percent tensor([0.5811, 0.4189], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.6773, 0.3227], device='cuda:0')
percent tensor([0.6119, 0.3881], device='cuda:0')
percent tensor([0.6446, 0.3554], device='cuda:0')
percent tensor([0.7577, 0.2423], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 170 | Batch_idx: 0 |  Loss: (0.1296) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 170 | Batch_idx: 10 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 170 | Batch_idx: 20 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 170 | Batch_idx: 30 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (3842/3968)
Epoch: 170 | Batch_idx: 40 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (5079/5248)
Epoch: 170 | Batch_idx: 50 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (6323/6528)
Epoch: 170 | Batch_idx: 60 |  Loss: (0.1005) |  Loss2: (0.0000) | Acc: (96.00%) (7537/7808)
Epoch: 170 | Batch_idx: 70 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (8782/9088)
Epoch: 170 | Batch_idx: 80 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (10034/10368)
Epoch: 170 | Batch_idx: 90 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (11270/11648)
Epoch: 170 | Batch_idx: 100 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (12496/12928)
Epoch: 170 | Batch_idx: 110 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (13740/14208)
Epoch: 170 | Batch_idx: 120 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (14968/15488)
Epoch: 170 | Batch_idx: 130 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (16204/16768)
Epoch: 170 | Batch_idx: 140 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (17447/18048)
Epoch: 170 | Batch_idx: 150 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (18679/19328)
Epoch: 170 | Batch_idx: 160 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (19908/20608)
Epoch: 170 | Batch_idx: 170 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (21145/21888)
Epoch: 170 | Batch_idx: 180 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (22389/23168)
Epoch: 170 | Batch_idx: 190 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (23635/24448)
Epoch: 170 | Batch_idx: 200 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (24866/25728)
Epoch: 170 | Batch_idx: 210 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (26102/27008)
Epoch: 170 | Batch_idx: 220 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (27340/28288)
Epoch: 170 | Batch_idx: 230 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (28563/29568)
Epoch: 170 | Batch_idx: 240 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (29794/30848)
Epoch: 170 | Batch_idx: 250 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (31032/32128)
Epoch: 170 | Batch_idx: 260 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (32265/33408)
Epoch: 170 | Batch_idx: 270 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (33505/34688)
Epoch: 170 | Batch_idx: 280 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (34734/35968)
Epoch: 170 | Batch_idx: 290 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (35963/37248)
Epoch: 170 | Batch_idx: 300 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (37190/38528)
Epoch: 170 | Batch_idx: 310 |  Loss: (0.1043) |  Loss2: (0.0000) | Acc: (96.00%) (38410/39808)
Epoch: 170 | Batch_idx: 320 |  Loss: (0.1048) |  Loss2: (0.0000) | Acc: (96.00%) (39629/41088)
Epoch: 170 | Batch_idx: 330 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (40862/42368)
Epoch: 170 | Batch_idx: 340 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (42087/43648)
Epoch: 170 | Batch_idx: 350 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (43311/44928)
Epoch: 170 | Batch_idx: 360 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (44533/46208)
Epoch: 170 | Batch_idx: 370 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (45759/47488)
Epoch: 170 | Batch_idx: 380 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (46994/48768)
Epoch: 170 | Batch_idx: 390 |  Loss: (0.1064) |  Loss2: (0.0000) | Acc: (96.00%) (48178/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_170.pth.tar'
# TEST : Loss: (0.4547) | Acc: (87.00%) (8722/10000)
percent tensor([0.5880, 0.4120], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.5742, 0.4258], device='cuda:0')
percent tensor([0.6778, 0.3222], device='cuda:0')
percent tensor([0.6115, 0.3885], device='cuda:0')
percent tensor([0.6444, 0.3556], device='cuda:0')
percent tensor([0.7611, 0.2389], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.4986, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(826.1711, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.6033, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1517.3453, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(481.9604, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2289.8464, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4260.2422, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1359.8381, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6238.5176, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11634.1045, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3826.4583, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16177.5615, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 171 | Batch_idx: 0 |  Loss: (0.0651) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 171 | Batch_idx: 10 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 171 | Batch_idx: 20 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (2612/2688)
Epoch: 171 | Batch_idx: 30 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 171 | Batch_idx: 40 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (5095/5248)
Epoch: 171 | Batch_idx: 50 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (6321/6528)
Epoch: 171 | Batch_idx: 60 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (7560/7808)
Epoch: 171 | Batch_idx: 70 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (8798/9088)
Epoch: 171 | Batch_idx: 80 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (10025/10368)
Epoch: 171 | Batch_idx: 90 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (11261/11648)
Epoch: 171 | Batch_idx: 100 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (12497/12928)
Epoch: 171 | Batch_idx: 110 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (13745/14208)
Epoch: 171 | Batch_idx: 120 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (14983/15488)
Epoch: 171 | Batch_idx: 130 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (16217/16768)
Epoch: 171 | Batch_idx: 140 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (17457/18048)
Epoch: 171 | Batch_idx: 150 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (18695/19328)
Epoch: 171 | Batch_idx: 160 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (19938/20608)
Epoch: 171 | Batch_idx: 170 |  Loss: (0.0973) |  Loss2: (0.0000) | Acc: (96.00%) (21178/21888)
Epoch: 171 | Batch_idx: 180 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (22415/23168)
Epoch: 171 | Batch_idx: 190 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (23652/24448)
Epoch: 171 | Batch_idx: 200 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (24878/25728)
Epoch: 171 | Batch_idx: 210 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (26114/27008)
Epoch: 171 | Batch_idx: 220 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (27357/28288)
Epoch: 171 | Batch_idx: 230 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (28592/29568)
Epoch: 171 | Batch_idx: 240 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (29823/30848)
Epoch: 171 | Batch_idx: 250 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (31050/32128)
Epoch: 171 | Batch_idx: 260 |  Loss: (0.0999) |  Loss2: (0.0000) | Acc: (96.00%) (32296/33408)
Epoch: 171 | Batch_idx: 270 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (33522/34688)
Epoch: 171 | Batch_idx: 280 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (34750/35968)
Epoch: 171 | Batch_idx: 290 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (35984/37248)
Epoch: 171 | Batch_idx: 300 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (37226/38528)
Epoch: 171 | Batch_idx: 310 |  Loss: (0.1017) |  Loss2: (0.0000) | Acc: (96.00%) (38463/39808)
Epoch: 171 | Batch_idx: 320 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (39704/41088)
Epoch: 171 | Batch_idx: 330 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (40931/42368)
Epoch: 171 | Batch_idx: 340 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (42168/43648)
Epoch: 171 | Batch_idx: 350 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (43406/44928)
Epoch: 171 | Batch_idx: 360 |  Loss: (0.1024) |  Loss2: (0.0000) | Acc: (96.00%) (44634/46208)
Epoch: 171 | Batch_idx: 370 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (45860/47488)
Epoch: 171 | Batch_idx: 380 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (47097/48768)
Epoch: 171 | Batch_idx: 390 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (48287/50000)
# TEST : Loss: (0.4805) | Acc: (87.00%) (8741/10000)
percent tensor([0.5880, 0.4120], device='cuda:0')
percent tensor([0.5817, 0.4183], device='cuda:0')
percent tensor([0.5739, 0.4261], device='cuda:0')
percent tensor([0.6786, 0.3214], device='cuda:0')
percent tensor([0.6108, 0.3892], device='cuda:0')
percent tensor([0.6448, 0.3552], device='cuda:0')
percent tensor([0.7602, 0.2398], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 172 | Batch_idx: 0 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 172 | Batch_idx: 10 |  Loss: (0.1057) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 172 | Batch_idx: 20 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (2597/2688)
Epoch: 172 | Batch_idx: 30 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (3839/3968)
Epoch: 172 | Batch_idx: 40 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (5081/5248)
Epoch: 172 | Batch_idx: 50 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (6312/6528)
Epoch: 172 | Batch_idx: 60 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (7542/7808)
Epoch: 172 | Batch_idx: 70 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (8777/9088)
Epoch: 172 | Batch_idx: 80 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (10020/10368)
Epoch: 172 | Batch_idx: 90 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (11269/11648)
Epoch: 172 | Batch_idx: 100 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (12507/12928)
Epoch: 172 | Batch_idx: 110 |  Loss: (0.0978) |  Loss2: (0.0000) | Acc: (96.00%) (13745/14208)
Epoch: 172 | Batch_idx: 120 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (14994/15488)
Epoch: 172 | Batch_idx: 130 |  Loss: (0.0967) |  Loss2: (0.0000) | Acc: (96.00%) (16230/16768)
Epoch: 172 | Batch_idx: 140 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (17471/18048)
Epoch: 172 | Batch_idx: 150 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (18707/19328)
Epoch: 172 | Batch_idx: 160 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (19938/20608)
Epoch: 172 | Batch_idx: 170 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (21177/21888)
Epoch: 172 | Batch_idx: 180 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (22421/23168)
Epoch: 172 | Batch_idx: 190 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (23658/24448)
Epoch: 172 | Batch_idx: 200 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (24890/25728)
Epoch: 172 | Batch_idx: 210 |  Loss: (0.0971) |  Loss2: (0.0000) | Acc: (96.00%) (26127/27008)
Epoch: 172 | Batch_idx: 220 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (27344/28288)
Epoch: 172 | Batch_idx: 230 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (28581/29568)
Epoch: 172 | Batch_idx: 240 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (29810/30848)
Epoch: 172 | Batch_idx: 250 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (31044/32128)
Epoch: 172 | Batch_idx: 260 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (32272/33408)
Epoch: 172 | Batch_idx: 270 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (33515/34688)
Epoch: 172 | Batch_idx: 280 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (34746/35968)
Epoch: 172 | Batch_idx: 290 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (35985/37248)
Epoch: 172 | Batch_idx: 300 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (37224/38528)
Epoch: 172 | Batch_idx: 310 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (38475/39808)
Epoch: 172 | Batch_idx: 320 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (39716/41088)
Epoch: 172 | Batch_idx: 330 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (40934/42368)
Epoch: 172 | Batch_idx: 340 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (42182/43648)
Epoch: 172 | Batch_idx: 350 |  Loss: (0.0992) |  Loss2: (0.0000) | Acc: (96.00%) (43413/44928)
Epoch: 172 | Batch_idx: 360 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (44654/46208)
Epoch: 172 | Batch_idx: 370 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (45869/47488)
Epoch: 172 | Batch_idx: 380 |  Loss: (0.1012) |  Loss2: (0.0000) | Acc: (96.00%) (47079/48768)
Epoch: 172 | Batch_idx: 390 |  Loss: (0.1013) |  Loss2: (0.0000) | Acc: (96.00%) (48274/50000)
# TEST : Loss: (0.4924) | Acc: (86.00%) (8677/10000)
percent tensor([0.5879, 0.4121], device='cuda:0')
percent tensor([0.5814, 0.4186], device='cuda:0')
percent tensor([0.5752, 0.4248], device='cuda:0')
percent tensor([0.6774, 0.3226], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6456, 0.3544], device='cuda:0')
percent tensor([0.7609, 0.2391], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 173 | Batch_idx: 0 |  Loss: (0.1132) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 173 | Batch_idx: 10 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (1354/1408)
Epoch: 173 | Batch_idx: 20 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (2592/2688)
Epoch: 173 | Batch_idx: 30 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (3834/3968)
Epoch: 173 | Batch_idx: 40 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (5073/5248)
Epoch: 173 | Batch_idx: 50 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (6313/6528)
Epoch: 173 | Batch_idx: 60 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (7557/7808)
Epoch: 173 | Batch_idx: 70 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 173 | Batch_idx: 80 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (10019/10368)
Epoch: 173 | Batch_idx: 90 |  Loss: (0.0996) |  Loss2: (0.0000) | Acc: (96.00%) (11260/11648)
Epoch: 173 | Batch_idx: 100 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (12499/12928)
Epoch: 173 | Batch_idx: 110 |  Loss: (0.0979) |  Loss2: (0.0000) | Acc: (96.00%) (13739/14208)
Epoch: 173 | Batch_idx: 120 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (14975/15488)
Epoch: 173 | Batch_idx: 130 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (16211/16768)
Epoch: 173 | Batch_idx: 140 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (17442/18048)
Epoch: 173 | Batch_idx: 150 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (18677/19328)
Epoch: 173 | Batch_idx: 160 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (19911/20608)
Epoch: 173 | Batch_idx: 170 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (21142/21888)
Epoch: 173 | Batch_idx: 180 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (22373/23168)
Epoch: 173 | Batch_idx: 190 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (23606/24448)
Epoch: 173 | Batch_idx: 200 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (24852/25728)
Epoch: 173 | Batch_idx: 210 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (26096/27008)
Epoch: 173 | Batch_idx: 220 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (27329/28288)
Epoch: 173 | Batch_idx: 230 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (28567/29568)
Epoch: 173 | Batch_idx: 240 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (29798/30848)
Epoch: 173 | Batch_idx: 250 |  Loss: (0.0985) |  Loss2: (0.0000) | Acc: (96.00%) (31036/32128)
Epoch: 173 | Batch_idx: 260 |  Loss: (0.0986) |  Loss2: (0.0000) | Acc: (96.00%) (32273/33408)
Epoch: 173 | Batch_idx: 270 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (33513/34688)
Epoch: 173 | Batch_idx: 280 |  Loss: (0.0982) |  Loss2: (0.0000) | Acc: (96.00%) (34753/35968)
Epoch: 173 | Batch_idx: 290 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (35985/37248)
Epoch: 173 | Batch_idx: 300 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (37214/38528)
Epoch: 173 | Batch_idx: 310 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (38457/39808)
Epoch: 173 | Batch_idx: 320 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (39697/41088)
Epoch: 173 | Batch_idx: 330 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (40931/42368)
Epoch: 173 | Batch_idx: 340 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (42157/43648)
Epoch: 173 | Batch_idx: 350 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (43395/44928)
Epoch: 173 | Batch_idx: 360 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (44636/46208)
Epoch: 173 | Batch_idx: 370 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (45867/47488)
Epoch: 173 | Batch_idx: 380 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (47096/48768)
Epoch: 173 | Batch_idx: 390 |  Loss: (0.1003) |  Loss2: (0.0000) | Acc: (96.00%) (48281/50000)
# TEST : Loss: (0.4872) | Acc: (86.00%) (8695/10000)
percent tensor([0.5875, 0.4125], device='cuda:0')
percent tensor([0.5808, 0.4192], device='cuda:0')
percent tensor([0.5729, 0.4271], device='cuda:0')
percent tensor([0.6787, 0.3213], device='cuda:0')
percent tensor([0.6120, 0.3880], device='cuda:0')
percent tensor([0.6457, 0.3543], device='cuda:0')
percent tensor([0.7576, 0.2424], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 174 | Batch_idx: 0 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 174 | Batch_idx: 10 |  Loss: (0.1117) |  Loss2: (0.0000) | Acc: (95.00%) (1344/1408)
Epoch: 174 | Batch_idx: 20 |  Loss: (0.1059) |  Loss2: (0.0000) | Acc: (96.00%) (2581/2688)
Epoch: 174 | Batch_idx: 30 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (3825/3968)
Epoch: 174 | Batch_idx: 40 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (5063/5248)
Epoch: 174 | Batch_idx: 50 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (6294/6528)
Epoch: 174 | Batch_idx: 60 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (7534/7808)
Epoch: 174 | Batch_idx: 70 |  Loss: (0.0989) |  Loss2: (0.0000) | Acc: (96.00%) (8781/9088)
Epoch: 174 | Batch_idx: 80 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (10021/10368)
Epoch: 174 | Batch_idx: 90 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (11268/11648)
Epoch: 174 | Batch_idx: 100 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (12518/12928)
Epoch: 174 | Batch_idx: 110 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (13761/14208)
Epoch: 174 | Batch_idx: 120 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (14998/15488)
Epoch: 174 | Batch_idx: 130 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (16235/16768)
Epoch: 174 | Batch_idx: 140 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (17476/18048)
Epoch: 174 | Batch_idx: 150 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (18711/19328)
Epoch: 174 | Batch_idx: 160 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (19957/20608)
Epoch: 174 | Batch_idx: 170 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (21186/21888)
Epoch: 174 | Batch_idx: 180 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (22428/23168)
Epoch: 174 | Batch_idx: 190 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (23652/24448)
Epoch: 174 | Batch_idx: 200 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (24891/25728)
Epoch: 174 | Batch_idx: 210 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (26130/27008)
Epoch: 174 | Batch_idx: 220 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (27375/28288)
Epoch: 174 | Batch_idx: 230 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (28618/29568)
Epoch: 174 | Batch_idx: 240 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (29850/30848)
Epoch: 174 | Batch_idx: 250 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (31094/32128)
Epoch: 174 | Batch_idx: 260 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (32340/33408)
Epoch: 174 | Batch_idx: 270 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (33575/34688)
Epoch: 174 | Batch_idx: 280 |  Loss: (0.0941) |  Loss2: (0.0000) | Acc: (96.00%) (34805/35968)
Epoch: 174 | Batch_idx: 290 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (36047/37248)
Epoch: 174 | Batch_idx: 300 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (37272/38528)
Epoch: 174 | Batch_idx: 310 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (38521/39808)
Epoch: 174 | Batch_idx: 320 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (39750/41088)
Epoch: 174 | Batch_idx: 330 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (40992/42368)
Epoch: 174 | Batch_idx: 340 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (42221/43648)
Epoch: 174 | Batch_idx: 350 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (43454/44928)
Epoch: 174 | Batch_idx: 360 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (44685/46208)
Epoch: 174 | Batch_idx: 370 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (45936/47488)
Epoch: 174 | Batch_idx: 380 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (47181/48768)
Epoch: 174 | Batch_idx: 390 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (48377/50000)
# TEST : Loss: (0.4335) | Acc: (88.00%) (8841/10000)
percent tensor([0.5883, 0.4117], device='cuda:0')
percent tensor([0.5809, 0.4191], device='cuda:0')
percent tensor([0.5736, 0.4264], device='cuda:0')
percent tensor([0.6765, 0.3235], device='cuda:0')
percent tensor([0.6096, 0.3904], device='cuda:0')
percent tensor([0.6431, 0.3569], device='cuda:0')
percent tensor([0.7556, 0.2444], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 175 | Batch_idx: 0 |  Loss: (0.1326) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 175 | Batch_idx: 10 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 175 | Batch_idx: 20 |  Loss: (0.1051) |  Loss2: (0.0000) | Acc: (95.00%) (2580/2688)
Epoch: 175 | Batch_idx: 30 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (95.00%) (3799/3968)
Epoch: 175 | Batch_idx: 40 |  Loss: (0.1122) |  Loss2: (0.0000) | Acc: (95.00%) (5025/5248)
Epoch: 175 | Batch_idx: 50 |  Loss: (0.1124) |  Loss2: (0.0000) | Acc: (95.00%) (6248/6528)
Epoch: 175 | Batch_idx: 60 |  Loss: (0.1138) |  Loss2: (0.0000) | Acc: (95.00%) (7467/7808)
Epoch: 175 | Batch_idx: 70 |  Loss: (0.1108) |  Loss2: (0.0000) | Acc: (95.00%) (8709/9088)
Epoch: 175 | Batch_idx: 80 |  Loss: (0.1094) |  Loss2: (0.0000) | Acc: (95.00%) (9948/10368)
Epoch: 175 | Batch_idx: 90 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (95.00%) (11178/11648)
Epoch: 175 | Batch_idx: 100 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (95.00%) (12402/12928)
Epoch: 175 | Batch_idx: 110 |  Loss: (0.1092) |  Loss2: (0.0000) | Acc: (95.00%) (13626/14208)
Epoch: 175 | Batch_idx: 120 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (95.00%) (14861/15488)
Epoch: 175 | Batch_idx: 130 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (95.00%) (16091/16768)
Epoch: 175 | Batch_idx: 140 |  Loss: (0.1099) |  Loss2: (0.0000) | Acc: (95.00%) (17316/18048)
Epoch: 175 | Batch_idx: 150 |  Loss: (0.1106) |  Loss2: (0.0000) | Acc: (95.00%) (18544/19328)
Epoch: 175 | Batch_idx: 160 |  Loss: (0.1098) |  Loss2: (0.0000) | Acc: (95.00%) (19783/20608)
Epoch: 175 | Batch_idx: 170 |  Loss: (0.1095) |  Loss2: (0.0000) | Acc: (96.00%) (21020/21888)
Epoch: 175 | Batch_idx: 180 |  Loss: (0.1090) |  Loss2: (0.0000) | Acc: (96.00%) (22255/23168)
Epoch: 175 | Batch_idx: 190 |  Loss: (0.1091) |  Loss2: (0.0000) | Acc: (96.00%) (23482/24448)
Epoch: 175 | Batch_idx: 200 |  Loss: (0.1087) |  Loss2: (0.0000) | Acc: (96.00%) (24719/25728)
Epoch: 175 | Batch_idx: 210 |  Loss: (0.1093) |  Loss2: (0.0000) | Acc: (96.00%) (25940/27008)
Epoch: 175 | Batch_idx: 220 |  Loss: (0.1088) |  Loss2: (0.0000) | Acc: (96.00%) (27179/28288)
Epoch: 175 | Batch_idx: 230 |  Loss: (0.1081) |  Loss2: (0.0000) | Acc: (96.00%) (28412/29568)
Epoch: 175 | Batch_idx: 240 |  Loss: (0.1085) |  Loss2: (0.0000) | Acc: (96.00%) (29639/30848)
Epoch: 175 | Batch_idx: 250 |  Loss: (0.1079) |  Loss2: (0.0000) | Acc: (96.00%) (30881/32128)
Epoch: 175 | Batch_idx: 260 |  Loss: (0.1067) |  Loss2: (0.0000) | Acc: (96.00%) (32132/33408)
Epoch: 175 | Batch_idx: 270 |  Loss: (0.1070) |  Loss2: (0.0000) | Acc: (96.00%) (33360/34688)
Epoch: 175 | Batch_idx: 280 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (34601/35968)
Epoch: 175 | Batch_idx: 290 |  Loss: (0.1065) |  Loss2: (0.0000) | Acc: (96.00%) (35834/37248)
Epoch: 175 | Batch_idx: 300 |  Loss: (0.1061) |  Loss2: (0.0000) | Acc: (96.00%) (37072/38528)
Epoch: 175 | Batch_idx: 310 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (38307/39808)
Epoch: 175 | Batch_idx: 320 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (39543/41088)
Epoch: 175 | Batch_idx: 330 |  Loss: (0.1055) |  Loss2: (0.0000) | Acc: (96.00%) (40780/42368)
Epoch: 175 | Batch_idx: 340 |  Loss: (0.1052) |  Loss2: (0.0000) | Acc: (96.00%) (42013/43648)
Epoch: 175 | Batch_idx: 350 |  Loss: (0.1056) |  Loss2: (0.0000) | Acc: (96.00%) (43242/44928)
Epoch: 175 | Batch_idx: 360 |  Loss: (0.1058) |  Loss2: (0.0000) | Acc: (96.00%) (44472/46208)
Epoch: 175 | Batch_idx: 370 |  Loss: (0.1054) |  Loss2: (0.0000) | Acc: (96.00%) (45710/47488)
Epoch: 175 | Batch_idx: 380 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (46945/48768)
Epoch: 175 | Batch_idx: 390 |  Loss: (0.1050) |  Loss2: (0.0000) | Acc: (96.00%) (48133/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_175.pth.tar'
# TEST : Loss: (0.4310) | Acc: (88.00%) (8848/10000)
percent tensor([0.5796, 0.4204], device='cuda:0')
percent tensor([0.5777, 0.4223], device='cuda:0')
percent tensor([0.5899, 0.4101], device='cuda:0')
percent tensor([0.6846, 0.3154], device='cuda:0')
percent tensor([0.6167, 0.3833], device='cuda:0')
percent tensor([0.6840, 0.3160], device='cuda:0')
percent tensor([0.7476, 0.2524], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 176 | Batch_idx: 0 |  Loss: (0.1101) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 176 | Batch_idx: 10 |  Loss: (0.1021) |  Loss2: (0.0000) | Acc: (96.00%) (1357/1408)
Epoch: 176 | Batch_idx: 20 |  Loss: (0.1089) |  Loss2: (0.0000) | Acc: (96.00%) (2589/2688)
Epoch: 176 | Batch_idx: 30 |  Loss: (0.1053) |  Loss2: (0.0000) | Acc: (96.00%) (3831/3968)
Epoch: 176 | Batch_idx: 40 |  Loss: (0.1062) |  Loss2: (0.0000) | Acc: (96.00%) (5065/5248)
Epoch: 176 | Batch_idx: 50 |  Loss: (0.1066) |  Loss2: (0.0000) | Acc: (96.00%) (6300/6528)
Epoch: 176 | Batch_idx: 60 |  Loss: (0.1026) |  Loss2: (0.0000) | Acc: (96.00%) (7544/7808)
Epoch: 176 | Batch_idx: 70 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (8784/9088)
Epoch: 176 | Batch_idx: 80 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (10023/10368)
Epoch: 176 | Batch_idx: 90 |  Loss: (0.0993) |  Loss2: (0.0000) | Acc: (96.00%) (11273/11648)
Epoch: 176 | Batch_idx: 100 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (12510/12928)
Epoch: 176 | Batch_idx: 110 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (13741/14208)
Epoch: 176 | Batch_idx: 120 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (14969/15488)
Epoch: 176 | Batch_idx: 130 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (16207/16768)
Epoch: 176 | Batch_idx: 140 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (17439/18048)
Epoch: 176 | Batch_idx: 150 |  Loss: (0.0983) |  Loss2: (0.0000) | Acc: (96.00%) (18682/19328)
Epoch: 176 | Batch_idx: 160 |  Loss: (0.0980) |  Loss2: (0.0000) | Acc: (96.00%) (19923/20608)
Epoch: 176 | Batch_idx: 170 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (21167/21888)
Epoch: 176 | Batch_idx: 180 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (22419/23168)
Epoch: 176 | Batch_idx: 190 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (23662/24448)
Epoch: 176 | Batch_idx: 200 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (24910/25728)
Epoch: 176 | Batch_idx: 210 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (26146/27008)
Epoch: 176 | Batch_idx: 220 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (27381/28288)
Epoch: 176 | Batch_idx: 230 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (28617/29568)
Epoch: 176 | Batch_idx: 240 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (29860/30848)
Epoch: 176 | Batch_idx: 250 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (31096/32128)
Epoch: 176 | Batch_idx: 260 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (32331/33408)
Epoch: 176 | Batch_idx: 270 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (33571/34688)
Epoch: 176 | Batch_idx: 280 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (34812/35968)
Epoch: 176 | Batch_idx: 290 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (36046/37248)
Epoch: 176 | Batch_idx: 300 |  Loss: (0.0961) |  Loss2: (0.0000) | Acc: (96.00%) (37287/38528)
Epoch: 176 | Batch_idx: 310 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (38528/39808)
Epoch: 176 | Batch_idx: 320 |  Loss: (0.0959) |  Loss2: (0.0000) | Acc: (96.00%) (39771/41088)
Epoch: 176 | Batch_idx: 330 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (41014/42368)
Epoch: 176 | Batch_idx: 340 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (42258/43648)
Epoch: 176 | Batch_idx: 350 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (43511/44928)
Epoch: 176 | Batch_idx: 360 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (44758/46208)
Epoch: 176 | Batch_idx: 370 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (46007/47488)
Epoch: 176 | Batch_idx: 380 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (47241/48768)
Epoch: 176 | Batch_idx: 390 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (48431/50000)
# TEST : Loss: (0.4182) | Acc: (88.00%) (8873/10000)
percent tensor([0.5798, 0.4202], device='cuda:0')
percent tensor([0.5773, 0.4227], device='cuda:0')
percent tensor([0.5931, 0.4069], device='cuda:0')
percent tensor([0.6829, 0.3171], device='cuda:0')
percent tensor([0.6218, 0.3782], device='cuda:0')
percent tensor([0.6859, 0.3141], device='cuda:0')
percent tensor([0.7537, 0.2463], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 177 | Batch_idx: 0 |  Loss: (0.0563) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 177 | Batch_idx: 10 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (1368/1408)
Epoch: 177 | Batch_idx: 20 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (2602/2688)
Epoch: 177 | Batch_idx: 30 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (3841/3968)
Epoch: 177 | Batch_idx: 40 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 177 | Batch_idx: 50 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (6321/6528)
Epoch: 177 | Batch_idx: 60 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (7574/7808)
Epoch: 177 | Batch_idx: 70 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (8819/9088)
Epoch: 177 | Batch_idx: 80 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (10055/10368)
Epoch: 177 | Batch_idx: 90 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (11293/11648)
Epoch: 177 | Batch_idx: 100 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (12539/12928)
Epoch: 177 | Batch_idx: 110 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (13779/14208)
Epoch: 177 | Batch_idx: 120 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (15022/15488)
Epoch: 177 | Batch_idx: 130 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (16273/16768)
Epoch: 177 | Batch_idx: 140 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (17515/18048)
Epoch: 177 | Batch_idx: 150 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (97.00%) (18751/19328)
Epoch: 177 | Batch_idx: 160 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (19991/20608)
Epoch: 177 | Batch_idx: 170 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (21221/21888)
Epoch: 177 | Batch_idx: 180 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (22462/23168)
Epoch: 177 | Batch_idx: 190 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (23711/24448)
Epoch: 177 | Batch_idx: 200 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (24956/25728)
Epoch: 177 | Batch_idx: 210 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (97.00%) (26208/27008)
Epoch: 177 | Batch_idx: 220 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (27451/28288)
Epoch: 177 | Batch_idx: 230 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (28682/29568)
Epoch: 177 | Batch_idx: 240 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (29929/30848)
Epoch: 177 | Batch_idx: 250 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (97.00%) (31179/32128)
Epoch: 177 | Batch_idx: 260 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (32414/33408)
Epoch: 177 | Batch_idx: 270 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (97.00%) (33656/34688)
Epoch: 177 | Batch_idx: 280 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (96.00%) (34883/35968)
Epoch: 177 | Batch_idx: 290 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (36130/37248)
Epoch: 177 | Batch_idx: 300 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (37383/38528)
Epoch: 177 | Batch_idx: 310 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (38632/39808)
Epoch: 177 | Batch_idx: 320 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (39872/41088)
Epoch: 177 | Batch_idx: 330 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (41112/42368)
Epoch: 177 | Batch_idx: 340 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (42354/43648)
Epoch: 177 | Batch_idx: 350 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (43590/44928)
Epoch: 177 | Batch_idx: 360 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (97.00%) (44842/46208)
Epoch: 177 | Batch_idx: 370 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (46086/47488)
Epoch: 177 | Batch_idx: 380 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (47329/48768)
Epoch: 177 | Batch_idx: 390 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (97.00%) (48527/50000)
# TEST : Loss: (0.4098) | Acc: (88.00%) (8893/10000)
percent tensor([0.5813, 0.4187], device='cuda:0')
percent tensor([0.5804, 0.4196], device='cuda:0')
percent tensor([0.5901, 0.4099], device='cuda:0')
percent tensor([0.6867, 0.3133], device='cuda:0')
percent tensor([0.6243, 0.3757], device='cuda:0')
percent tensor([0.6892, 0.3108], device='cuda:0')
percent tensor([0.7602, 0.2398], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 178 | Batch_idx: 0 |  Loss: (0.1068) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 178 | Batch_idx: 10 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 178 | Batch_idx: 20 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (2615/2688)
Epoch: 178 | Batch_idx: 30 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 178 | Batch_idx: 40 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (5096/5248)
Epoch: 178 | Batch_idx: 50 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (6339/6528)
Epoch: 178 | Batch_idx: 60 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (7583/7808)
Epoch: 178 | Batch_idx: 70 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (8828/9088)
Epoch: 178 | Batch_idx: 80 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (10064/10368)
Epoch: 178 | Batch_idx: 90 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (11316/11648)
Epoch: 178 | Batch_idx: 100 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (12557/12928)
Epoch: 178 | Batch_idx: 110 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (97.00%) (13798/14208)
Epoch: 178 | Batch_idx: 120 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (15046/15488)
Epoch: 178 | Batch_idx: 130 |  Loss: (0.0851) |  Loss2: (0.0000) | Acc: (97.00%) (16291/16768)
Epoch: 178 | Batch_idx: 140 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (17540/18048)
Epoch: 178 | Batch_idx: 150 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (18787/19328)
Epoch: 178 | Batch_idx: 160 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (20029/20608)
Epoch: 178 | Batch_idx: 170 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (21270/21888)
Epoch: 178 | Batch_idx: 180 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (22512/23168)
Epoch: 178 | Batch_idx: 190 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (23758/24448)
Epoch: 178 | Batch_idx: 200 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (25011/25728)
Epoch: 178 | Batch_idx: 210 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (26249/27008)
Epoch: 178 | Batch_idx: 220 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (27507/28288)
Epoch: 178 | Batch_idx: 230 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (28760/29568)
Epoch: 178 | Batch_idx: 240 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (30003/30848)
Epoch: 178 | Batch_idx: 250 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (31248/32128)
Epoch: 178 | Batch_idx: 260 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (32486/33408)
Epoch: 178 | Batch_idx: 270 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (33735/34688)
Epoch: 178 | Batch_idx: 280 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (34974/35968)
Epoch: 178 | Batch_idx: 290 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (36216/37248)
Epoch: 178 | Batch_idx: 300 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (37456/38528)
Epoch: 178 | Batch_idx: 310 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (38692/39808)
Epoch: 178 | Batch_idx: 320 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (39929/41088)
Epoch: 178 | Batch_idx: 330 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (41174/42368)
Epoch: 178 | Batch_idx: 340 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (42422/43648)
Epoch: 178 | Batch_idx: 350 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (43664/44928)
Epoch: 178 | Batch_idx: 360 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (44910/46208)
Epoch: 178 | Batch_idx: 370 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (46158/47488)
Epoch: 178 | Batch_idx: 380 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (47407/48768)
Epoch: 178 | Batch_idx: 390 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (48595/50000)
# TEST : Loss: (0.4034) | Acc: (89.00%) (8905/10000)
percent tensor([0.5825, 0.4175], device='cuda:0')
percent tensor([0.5831, 0.4169], device='cuda:0')
percent tensor([0.5928, 0.4072], device='cuda:0')
percent tensor([0.6867, 0.3133], device='cuda:0')
percent tensor([0.6247, 0.3753], device='cuda:0')
percent tensor([0.6848, 0.3152], device='cuda:0')
percent tensor([0.7620, 0.2380], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 179 | Batch_idx: 0 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 179 | Batch_idx: 10 |  Loss: (0.0849) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 179 | Batch_idx: 20 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 179 | Batch_idx: 30 |  Loss: (0.0801) |  Loss2: (0.0000) | Acc: (97.00%) (3855/3968)
Epoch: 179 | Batch_idx: 40 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (5101/5248)
Epoch: 179 | Batch_idx: 50 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (6347/6528)
Epoch: 179 | Batch_idx: 60 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (7585/7808)
Epoch: 179 | Batch_idx: 70 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (8836/9088)
Epoch: 179 | Batch_idx: 80 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 179 | Batch_idx: 90 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (11319/11648)
Epoch: 179 | Batch_idx: 100 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (12552/12928)
Epoch: 179 | Batch_idx: 110 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (13795/14208)
Epoch: 179 | Batch_idx: 120 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (15033/15488)
Epoch: 179 | Batch_idx: 130 |  Loss: (0.0845) |  Loss2: (0.0000) | Acc: (97.00%) (16271/16768)
Epoch: 179 | Batch_idx: 140 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (17521/18048)
Epoch: 179 | Batch_idx: 150 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (18767/19328)
Epoch: 179 | Batch_idx: 160 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (20013/20608)
Epoch: 179 | Batch_idx: 170 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (21246/21888)
Epoch: 179 | Batch_idx: 180 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (22489/23168)
Epoch: 179 | Batch_idx: 190 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (23731/24448)
Epoch: 179 | Batch_idx: 200 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (24971/25728)
Epoch: 179 | Batch_idx: 210 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (26217/27008)
Epoch: 179 | Batch_idx: 220 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (27475/28288)
Epoch: 179 | Batch_idx: 230 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (28718/29568)
Epoch: 179 | Batch_idx: 240 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (29968/30848)
Epoch: 179 | Batch_idx: 250 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (31215/32128)
Epoch: 179 | Batch_idx: 260 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (32451/33408)
Epoch: 179 | Batch_idx: 270 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (33701/34688)
Epoch: 179 | Batch_idx: 280 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (34940/35968)
Epoch: 179 | Batch_idx: 290 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (36184/37248)
Epoch: 179 | Batch_idx: 300 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (37430/38528)
Epoch: 179 | Batch_idx: 310 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (38674/39808)
Epoch: 179 | Batch_idx: 320 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (39925/41088)
Epoch: 179 | Batch_idx: 330 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (41178/42368)
Epoch: 179 | Batch_idx: 340 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (42421/43648)
Epoch: 179 | Batch_idx: 350 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (43663/44928)
Epoch: 179 | Batch_idx: 360 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (44906/46208)
Epoch: 179 | Batch_idx: 370 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (46138/47488)
Epoch: 179 | Batch_idx: 380 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (47378/48768)
Epoch: 179 | Batch_idx: 390 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (48585/50000)
# TEST : Loss: (0.4047) | Acc: (88.00%) (8893/10000)
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.5870, 0.4130], device='cuda:0')
percent tensor([0.6873, 0.3127], device='cuda:0')
percent tensor([0.6249, 0.3751], device='cuda:0')
percent tensor([0.6833, 0.3167], device='cuda:0')
percent tensor([0.7658, 0.2342], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 180 | Batch_idx: 0 |  Loss: (0.1107) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 180 | Batch_idx: 10 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 180 | Batch_idx: 20 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 180 | Batch_idx: 30 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (3870/3968)
Epoch: 180 | Batch_idx: 40 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (5113/5248)
Epoch: 180 | Batch_idx: 50 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (6361/6528)
Epoch: 180 | Batch_idx: 60 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (7594/7808)
Epoch: 180 | Batch_idx: 70 |  Loss: (0.0857) |  Loss2: (0.0000) | Acc: (97.00%) (8833/9088)
Epoch: 180 | Batch_idx: 80 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (10074/10368)
Epoch: 180 | Batch_idx: 90 |  Loss: (0.0875) |  Loss2: (0.0000) | Acc: (97.00%) (11313/11648)
Epoch: 180 | Batch_idx: 100 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (97.00%) (12554/12928)
Epoch: 180 | Batch_idx: 110 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (13790/14208)
Epoch: 180 | Batch_idx: 120 |  Loss: (0.0899) |  Loss2: (0.0000) | Acc: (97.00%) (15030/15488)
Epoch: 180 | Batch_idx: 130 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (16277/16768)
Epoch: 180 | Batch_idx: 140 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (17519/18048)
Epoch: 180 | Batch_idx: 150 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (18755/19328)
Epoch: 180 | Batch_idx: 160 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (97.00%) (20001/20608)
Epoch: 180 | Batch_idx: 170 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (21230/21888)
Epoch: 180 | Batch_idx: 180 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (22467/23168)
Epoch: 180 | Batch_idx: 190 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (23696/24448)
Epoch: 180 | Batch_idx: 200 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (24938/25728)
Epoch: 180 | Batch_idx: 210 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (26176/27008)
Epoch: 180 | Batch_idx: 220 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (27417/28288)
Epoch: 180 | Batch_idx: 230 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (28656/29568)
Epoch: 180 | Batch_idx: 240 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (29889/30848)
Epoch: 180 | Batch_idx: 250 |  Loss: (0.0947) |  Loss2: (0.0000) | Acc: (96.00%) (31130/32128)
Epoch: 180 | Batch_idx: 260 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (32365/33408)
Epoch: 180 | Batch_idx: 270 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (33603/34688)
Epoch: 180 | Batch_idx: 280 |  Loss: (0.0954) |  Loss2: (0.0000) | Acc: (96.00%) (34844/35968)
Epoch: 180 | Batch_idx: 290 |  Loss: (0.0948) |  Loss2: (0.0000) | Acc: (96.00%) (36098/37248)
Epoch: 180 | Batch_idx: 300 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (37328/38528)
Epoch: 180 | Batch_idx: 310 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (38570/39808)
Epoch: 180 | Batch_idx: 320 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (39815/41088)
Epoch: 180 | Batch_idx: 330 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (41049/42368)
Epoch: 180 | Batch_idx: 340 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (42283/43648)
Epoch: 180 | Batch_idx: 350 |  Loss: (0.0952) |  Loss2: (0.0000) | Acc: (96.00%) (43513/44928)
Epoch: 180 | Batch_idx: 360 |  Loss: (0.0951) |  Loss2: (0.0000) | Acc: (96.00%) (44756/46208)
Epoch: 180 | Batch_idx: 370 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (45977/47488)
Epoch: 180 | Batch_idx: 380 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (47216/48768)
Epoch: 180 | Batch_idx: 390 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (48400/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_180.pth.tar'
# TEST : Loss: (0.4654) | Acc: (87.00%) (8777/10000)
percent tensor([0.5827, 0.4173], device='cuda:0')
percent tensor([0.5839, 0.4161], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.6871, 0.3129], device='cuda:0')
percent tensor([0.6244, 0.3756], device='cuda:0')
percent tensor([0.6819, 0.3181], device='cuda:0')
percent tensor([0.7637, 0.2363], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.7515, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(827.3544, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(829.1219, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.5731, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(480.2336, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2294.4690, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4256.3408, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1354.9231, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6247.4932, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11600.2666, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3811.5964, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16112.9453, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 181 | Batch_idx: 0 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 181 | Batch_idx: 10 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 181 | Batch_idx: 20 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (2599/2688)
Epoch: 181 | Batch_idx: 30 |  Loss: (0.0850) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 181 | Batch_idx: 40 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (96.00%) (5087/5248)
Epoch: 181 | Batch_idx: 50 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (6326/6528)
Epoch: 181 | Batch_idx: 60 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (7574/7808)
Epoch: 181 | Batch_idx: 70 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (96.00%) (8808/9088)
Epoch: 181 | Batch_idx: 80 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (10059/10368)
Epoch: 181 | Batch_idx: 90 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (96.00%) (11298/11648)
Epoch: 181 | Batch_idx: 100 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (12533/12928)
Epoch: 181 | Batch_idx: 110 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (13777/14208)
Epoch: 181 | Batch_idx: 120 |  Loss: (0.0888) |  Loss2: (0.0000) | Acc: (96.00%) (15020/15488)
Epoch: 181 | Batch_idx: 130 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (16261/16768)
Epoch: 181 | Batch_idx: 140 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (17500/18048)
Epoch: 181 | Batch_idx: 150 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (18733/19328)
Epoch: 181 | Batch_idx: 160 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (19972/20608)
Epoch: 181 | Batch_idx: 170 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (96.00%) (21210/21888)
Epoch: 181 | Batch_idx: 180 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (22453/23168)
Epoch: 181 | Batch_idx: 190 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (23690/24448)
Epoch: 181 | Batch_idx: 200 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (24932/25728)
Epoch: 181 | Batch_idx: 210 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (26155/27008)
Epoch: 181 | Batch_idx: 220 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (96.00%) (27404/28288)
Epoch: 181 | Batch_idx: 230 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (28647/29568)
Epoch: 181 | Batch_idx: 240 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (29883/30848)
Epoch: 181 | Batch_idx: 250 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (31108/32128)
Epoch: 181 | Batch_idx: 260 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (32336/33408)
Epoch: 181 | Batch_idx: 270 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (33577/34688)
Epoch: 181 | Batch_idx: 280 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (34823/35968)
Epoch: 181 | Batch_idx: 290 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (36062/37248)
Epoch: 181 | Batch_idx: 300 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (37291/38528)
Epoch: 181 | Batch_idx: 310 |  Loss: (0.0937) |  Loss2: (0.0000) | Acc: (96.00%) (38522/39808)
Epoch: 181 | Batch_idx: 320 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (96.00%) (39770/41088)
Epoch: 181 | Batch_idx: 330 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (41017/42368)
Epoch: 181 | Batch_idx: 340 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (42257/43648)
Epoch: 181 | Batch_idx: 350 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (43487/44928)
Epoch: 181 | Batch_idx: 360 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (44727/46208)
Epoch: 181 | Batch_idx: 370 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (45963/47488)
Epoch: 181 | Batch_idx: 380 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (47205/48768)
Epoch: 181 | Batch_idx: 390 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (48394/50000)
# TEST : Loss: (0.4393) | Acc: (88.00%) (8831/10000)
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.5840, 0.4160], device='cuda:0')
percent tensor([0.5859, 0.4141], device='cuda:0')
percent tensor([0.6873, 0.3127], device='cuda:0')
percent tensor([0.6264, 0.3736], device='cuda:0')
percent tensor([0.6822, 0.3178], device='cuda:0')
percent tensor([0.7672, 0.2328], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 182 | Batch_idx: 0 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 182 | Batch_idx: 10 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 182 | Batch_idx: 20 |  Loss: (0.0925) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 182 | Batch_idx: 30 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (3847/3968)
Epoch: 182 | Batch_idx: 40 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (5083/5248)
Epoch: 182 | Batch_idx: 50 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (6321/6528)
Epoch: 182 | Batch_idx: 60 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (7564/7808)
Epoch: 182 | Batch_idx: 70 |  Loss: (0.0940) |  Loss2: (0.0000) | Acc: (96.00%) (8808/9088)
Epoch: 182 | Batch_idx: 80 |  Loss: (0.0935) |  Loss2: (0.0000) | Acc: (97.00%) (10058/10368)
Epoch: 182 | Batch_idx: 90 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (11294/11648)
Epoch: 182 | Batch_idx: 100 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (12533/12928)
Epoch: 182 | Batch_idx: 110 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (13772/14208)
Epoch: 182 | Batch_idx: 120 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (97.00%) (15026/15488)
Epoch: 182 | Batch_idx: 130 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (97.00%) (16269/16768)
Epoch: 182 | Batch_idx: 140 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (97.00%) (17516/18048)
Epoch: 182 | Batch_idx: 150 |  Loss: (0.0909) |  Loss2: (0.0000) | Acc: (97.00%) (18751/19328)
Epoch: 182 | Batch_idx: 160 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (19989/20608)
Epoch: 182 | Batch_idx: 170 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (21235/21888)
Epoch: 182 | Batch_idx: 180 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (97.00%) (22481/23168)
Epoch: 182 | Batch_idx: 190 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (23714/24448)
Epoch: 182 | Batch_idx: 200 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (97.00%) (24964/25728)
Epoch: 182 | Batch_idx: 210 |  Loss: (0.0907) |  Loss2: (0.0000) | Acc: (97.00%) (26206/27008)
Epoch: 182 | Batch_idx: 220 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (97.00%) (27461/28288)
Epoch: 182 | Batch_idx: 230 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (28690/29568)
Epoch: 182 | Batch_idx: 240 |  Loss: (0.0908) |  Loss2: (0.0000) | Acc: (97.00%) (29927/30848)
Epoch: 182 | Batch_idx: 250 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (31174/32128)
Epoch: 182 | Batch_idx: 260 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (32409/33408)
Epoch: 182 | Batch_idx: 270 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (33655/34688)
Epoch: 182 | Batch_idx: 280 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (34890/35968)
Epoch: 182 | Batch_idx: 290 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (36141/37248)
Epoch: 182 | Batch_idx: 300 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (97.00%) (37379/38528)
Epoch: 182 | Batch_idx: 310 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (97.00%) (38621/39808)
Epoch: 182 | Batch_idx: 320 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (97.00%) (39869/41088)
Epoch: 182 | Batch_idx: 330 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (97.00%) (41110/42368)
Epoch: 182 | Batch_idx: 340 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (42337/43648)
Epoch: 182 | Batch_idx: 350 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (43577/44928)
Epoch: 182 | Batch_idx: 360 |  Loss: (0.0906) |  Loss2: (0.0000) | Acc: (96.00%) (44810/46208)
Epoch: 182 | Batch_idx: 370 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (46044/47488)
Epoch: 182 | Batch_idx: 380 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (47276/48768)
Epoch: 182 | Batch_idx: 390 |  Loss: (0.0924) |  Loss2: (0.0000) | Acc: (96.00%) (48452/50000)
# TEST : Loss: (0.5081) | Acc: (87.00%) (8726/10000)
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.5841, 0.4159], device='cuda:0')
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.6862, 0.3138], device='cuda:0')
percent tensor([0.6252, 0.3748], device='cuda:0')
percent tensor([0.6818, 0.3182], device='cuda:0')
percent tensor([0.7667, 0.2333], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 183 | Batch_idx: 0 |  Loss: (0.1194) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 183 | Batch_idx: 10 |  Loss: (0.0904) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 183 | Batch_idx: 20 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (96.00%) (2607/2688)
Epoch: 183 | Batch_idx: 30 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 183 | Batch_idx: 40 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (5094/5248)
Epoch: 183 | Batch_idx: 50 |  Loss: (0.0882) |  Loss2: (0.0000) | Acc: (96.00%) (6331/6528)
Epoch: 183 | Batch_idx: 60 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (7580/7808)
Epoch: 183 | Batch_idx: 70 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (8843/9088)
Epoch: 183 | Batch_idx: 80 |  Loss: (0.0855) |  Loss2: (0.0000) | Acc: (97.00%) (10073/10368)
Epoch: 183 | Batch_idx: 90 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (11313/11648)
Epoch: 183 | Batch_idx: 100 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (12552/12928)
Epoch: 183 | Batch_idx: 110 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (97.00%) (13783/14208)
Epoch: 183 | Batch_idx: 120 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (15024/15488)
Epoch: 183 | Batch_idx: 130 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (16264/16768)
Epoch: 183 | Batch_idx: 140 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (97.00%) (17508/18048)
Epoch: 183 | Batch_idx: 150 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (97.00%) (18753/19328)
Epoch: 183 | Batch_idx: 160 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (19993/20608)
Epoch: 183 | Batch_idx: 170 |  Loss: (0.0901) |  Loss2: (0.0000) | Acc: (96.00%) (21225/21888)
Epoch: 183 | Batch_idx: 180 |  Loss: (0.0898) |  Loss2: (0.0000) | Acc: (96.00%) (22463/23168)
Epoch: 183 | Batch_idx: 190 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (23707/24448)
Epoch: 183 | Batch_idx: 200 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (24947/25728)
Epoch: 183 | Batch_idx: 210 |  Loss: (0.0885) |  Loss2: (0.0000) | Acc: (96.00%) (26193/27008)
Epoch: 183 | Batch_idx: 220 |  Loss: (0.0891) |  Loss2: (0.0000) | Acc: (96.00%) (27425/28288)
Epoch: 183 | Batch_idx: 230 |  Loss: (0.0893) |  Loss2: (0.0000) | Acc: (96.00%) (28671/29568)
Epoch: 183 | Batch_idx: 240 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (29912/30848)
Epoch: 183 | Batch_idx: 250 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (31157/32128)
Epoch: 183 | Batch_idx: 260 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (32404/33408)
Epoch: 183 | Batch_idx: 270 |  Loss: (0.0887) |  Loss2: (0.0000) | Acc: (96.00%) (33640/34688)
Epoch: 183 | Batch_idx: 280 |  Loss: (0.0894) |  Loss2: (0.0000) | Acc: (96.00%) (34877/35968)
Epoch: 183 | Batch_idx: 290 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (96.00%) (36119/37248)
Epoch: 183 | Batch_idx: 300 |  Loss: (0.0900) |  Loss2: (0.0000) | Acc: (96.00%) (37342/38528)
Epoch: 183 | Batch_idx: 310 |  Loss: (0.0902) |  Loss2: (0.0000) | Acc: (96.00%) (38576/39808)
Epoch: 183 | Batch_idx: 320 |  Loss: (0.0903) |  Loss2: (0.0000) | Acc: (96.00%) (39815/41088)
Epoch: 183 | Batch_idx: 330 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (41054/42368)
Epoch: 183 | Batch_idx: 340 |  Loss: (0.0905) |  Loss2: (0.0000) | Acc: (96.00%) (42292/43648)
Epoch: 183 | Batch_idx: 350 |  Loss: (0.0911) |  Loss2: (0.0000) | Acc: (96.00%) (43528/44928)
Epoch: 183 | Batch_idx: 360 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (44769/46208)
Epoch: 183 | Batch_idx: 370 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (46010/47488)
Epoch: 183 | Batch_idx: 380 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (47246/48768)
Epoch: 183 | Batch_idx: 390 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (48440/50000)
# TEST : Loss: (0.4647) | Acc: (87.00%) (8774/10000)
percent tensor([0.5821, 0.4179], device='cuda:0')
percent tensor([0.5843, 0.4157], device='cuda:0')
percent tensor([0.5872, 0.4128], device='cuda:0')
percent tensor([0.6863, 0.3137], device='cuda:0')
percent tensor([0.6247, 0.3753], device='cuda:0')
percent tensor([0.6809, 0.3191], device='cuda:0')
percent tensor([0.7663, 0.2337], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 184 | Batch_idx: 0 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 184 | Batch_idx: 10 |  Loss: (0.0910) |  Loss2: (0.0000) | Acc: (96.00%) (1361/1408)
Epoch: 184 | Batch_idx: 20 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 184 | Batch_idx: 30 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 184 | Batch_idx: 40 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 184 | Batch_idx: 50 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (6347/6528)
Epoch: 184 | Batch_idx: 60 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (7593/7808)
Epoch: 184 | Batch_idx: 70 |  Loss: (0.0796) |  Loss2: (0.0000) | Acc: (97.00%) (8847/9088)
Epoch: 184 | Batch_idx: 80 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 184 | Batch_idx: 90 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (11337/11648)
Epoch: 184 | Batch_idx: 100 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (12584/12928)
Epoch: 184 | Batch_idx: 110 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (13845/14208)
Epoch: 184 | Batch_idx: 120 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (15089/15488)
Epoch: 184 | Batch_idx: 130 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (16341/16768)
Epoch: 184 | Batch_idx: 140 |  Loss: (0.0800) |  Loss2: (0.0000) | Acc: (97.00%) (17577/18048)
Epoch: 184 | Batch_idx: 150 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (18821/19328)
Epoch: 184 | Batch_idx: 160 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (20058/20608)
Epoch: 184 | Batch_idx: 170 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (21299/21888)
Epoch: 184 | Batch_idx: 180 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (22540/23168)
Epoch: 184 | Batch_idx: 190 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (23777/24448)
Epoch: 184 | Batch_idx: 200 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (25014/25728)
Epoch: 184 | Batch_idx: 210 |  Loss: (0.0842) |  Loss2: (0.0000) | Acc: (97.00%) (26257/27008)
Epoch: 184 | Batch_idx: 220 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (27499/28288)
Epoch: 184 | Batch_idx: 230 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (28738/29568)
Epoch: 184 | Batch_idx: 240 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (29982/30848)
Epoch: 184 | Batch_idx: 250 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (31222/32128)
Epoch: 184 | Batch_idx: 260 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (32476/33408)
Epoch: 184 | Batch_idx: 270 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (33721/34688)
Epoch: 184 | Batch_idx: 280 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (34964/35968)
Epoch: 184 | Batch_idx: 290 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (36197/37248)
Epoch: 184 | Batch_idx: 300 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (37436/38528)
Epoch: 184 | Batch_idx: 310 |  Loss: (0.0848) |  Loss2: (0.0000) | Acc: (97.00%) (38675/39808)
Epoch: 184 | Batch_idx: 320 |  Loss: (0.0853) |  Loss2: (0.0000) | Acc: (97.00%) (39912/41088)
Epoch: 184 | Batch_idx: 330 |  Loss: (0.0867) |  Loss2: (0.0000) | Acc: (97.00%) (41130/42368)
Epoch: 184 | Batch_idx: 340 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (42362/43648)
Epoch: 184 | Batch_idx: 350 |  Loss: (0.0878) |  Loss2: (0.0000) | Acc: (97.00%) (43593/44928)
Epoch: 184 | Batch_idx: 360 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (44829/46208)
Epoch: 184 | Batch_idx: 370 |  Loss: (0.0883) |  Loss2: (0.0000) | Acc: (97.00%) (46066/47488)
Epoch: 184 | Batch_idx: 380 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (47315/48768)
Epoch: 184 | Batch_idx: 390 |  Loss: (0.0880) |  Loss2: (0.0000) | Acc: (97.00%) (48512/50000)
# TEST : Loss: (0.4789) | Acc: (87.00%) (8780/10000)
percent tensor([0.5829, 0.4171], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.5860, 0.4140], device='cuda:0')
percent tensor([0.6858, 0.3142], device='cuda:0')
percent tensor([0.6228, 0.3772], device='cuda:0')
percent tensor([0.6796, 0.3204], device='cuda:0')
percent tensor([0.7659, 0.2341], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 185 | Batch_idx: 0 |  Loss: (0.0889) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 185 | Batch_idx: 10 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 185 | Batch_idx: 20 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 185 | Batch_idx: 30 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 185 | Batch_idx: 40 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (5085/5248)
Epoch: 185 | Batch_idx: 50 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (6304/6528)
Epoch: 185 | Batch_idx: 60 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (7529/7808)
Epoch: 185 | Batch_idx: 70 |  Loss: (0.1034) |  Loss2: (0.0000) | Acc: (96.00%) (8756/9088)
Epoch: 185 | Batch_idx: 80 |  Loss: (0.1030) |  Loss2: (0.0000) | Acc: (96.00%) (9985/10368)
Epoch: 185 | Batch_idx: 90 |  Loss: (0.1020) |  Loss2: (0.0000) | Acc: (96.00%) (11220/11648)
Epoch: 185 | Batch_idx: 100 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (96.00%) (12469/12928)
Epoch: 185 | Batch_idx: 110 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (13697/14208)
Epoch: 185 | Batch_idx: 120 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (14932/15488)
Epoch: 185 | Batch_idx: 130 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (16161/16768)
Epoch: 185 | Batch_idx: 140 |  Loss: (0.1025) |  Loss2: (0.0000) | Acc: (96.00%) (17383/18048)
Epoch: 185 | Batch_idx: 150 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (18615/19328)
Epoch: 185 | Batch_idx: 160 |  Loss: (0.1032) |  Loss2: (0.0000) | Acc: (96.00%) (19850/20608)
Epoch: 185 | Batch_idx: 170 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (21083/21888)
Epoch: 185 | Batch_idx: 180 |  Loss: (0.1029) |  Loss2: (0.0000) | Acc: (96.00%) (22315/23168)
Epoch: 185 | Batch_idx: 190 |  Loss: (0.1027) |  Loss2: (0.0000) | Acc: (96.00%) (23548/24448)
Epoch: 185 | Batch_idx: 200 |  Loss: (0.1015) |  Loss2: (0.0000) | Acc: (96.00%) (24798/25728)
Epoch: 185 | Batch_idx: 210 |  Loss: (0.1022) |  Loss2: (0.0000) | Acc: (96.00%) (26024/27008)
Epoch: 185 | Batch_idx: 220 |  Loss: (0.1018) |  Loss2: (0.0000) | Acc: (96.00%) (27257/28288)
Epoch: 185 | Batch_idx: 230 |  Loss: (0.1014) |  Loss2: (0.0000) | Acc: (96.00%) (28502/29568)
Epoch: 185 | Batch_idx: 240 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (29746/30848)
Epoch: 185 | Batch_idx: 250 |  Loss: (0.1009) |  Loss2: (0.0000) | Acc: (96.00%) (30977/32128)
Epoch: 185 | Batch_idx: 260 |  Loss: (0.1011) |  Loss2: (0.0000) | Acc: (96.00%) (32214/33408)
Epoch: 185 | Batch_idx: 270 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (33455/34688)
Epoch: 185 | Batch_idx: 280 |  Loss: (0.1007) |  Loss2: (0.0000) | Acc: (96.00%) (34693/35968)
Epoch: 185 | Batch_idx: 290 |  Loss: (0.1008) |  Loss2: (0.0000) | Acc: (96.00%) (35920/37248)
Epoch: 185 | Batch_idx: 300 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (37167/38528)
Epoch: 185 | Batch_idx: 310 |  Loss: (0.1002) |  Loss2: (0.0000) | Acc: (96.00%) (38405/39808)
Epoch: 185 | Batch_idx: 320 |  Loss: (0.1000) |  Loss2: (0.0000) | Acc: (96.00%) (39646/41088)
Epoch: 185 | Batch_idx: 330 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (40883/42368)
Epoch: 185 | Batch_idx: 340 |  Loss: (0.0998) |  Loss2: (0.0000) | Acc: (96.00%) (42116/43648)
Epoch: 185 | Batch_idx: 350 |  Loss: (0.0997) |  Loss2: (0.0000) | Acc: (96.00%) (43357/44928)
Epoch: 185 | Batch_idx: 360 |  Loss: (0.0995) |  Loss2: (0.0000) | Acc: (96.00%) (44599/46208)
Epoch: 185 | Batch_idx: 370 |  Loss: (0.0994) |  Loss2: (0.0000) | Acc: (96.00%) (45835/47488)
Epoch: 185 | Batch_idx: 380 |  Loss: (0.0990) |  Loss2: (0.0000) | Acc: (96.00%) (47075/48768)
Epoch: 185 | Batch_idx: 390 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (48267/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_185.pth.tar'
# TEST : Loss: (0.4370) | Acc: (88.00%) (8837/10000)
percent tensor([0.5843, 0.4157], device='cuda:0')
percent tensor([0.5976, 0.4024], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.6670, 0.3330], device='cuda:0')
percent tensor([0.6206, 0.3794], device='cuda:0')
percent tensor([0.6733, 0.3267], device='cuda:0')
percent tensor([0.7335, 0.2665], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Epoch: 186 | Batch_idx: 0 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 186 | Batch_idx: 10 |  Loss: (0.1086) |  Loss2: (0.0000) | Acc: (96.00%) (1358/1408)
Epoch: 186 | Batch_idx: 20 |  Loss: (0.0949) |  Loss2: (0.0000) | Acc: (96.00%) (2602/2688)
Epoch: 186 | Batch_idx: 30 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (3839/3968)
Epoch: 186 | Batch_idx: 40 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (5078/5248)
Epoch: 186 | Batch_idx: 50 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (6314/6528)
Epoch: 186 | Batch_idx: 60 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (7556/7808)
Epoch: 186 | Batch_idx: 70 |  Loss: (0.0956) |  Loss2: (0.0000) | Acc: (96.00%) (8800/9088)
Epoch: 186 | Batch_idx: 80 |  Loss: (0.0939) |  Loss2: (0.0000) | Acc: (96.00%) (10049/10368)
Epoch: 186 | Batch_idx: 90 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (11289/11648)
Epoch: 186 | Batch_idx: 100 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (12529/12928)
Epoch: 186 | Batch_idx: 110 |  Loss: (0.0955) |  Loss2: (0.0000) | Acc: (96.00%) (13753/14208)
Epoch: 186 | Batch_idx: 120 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (15000/15488)
Epoch: 186 | Batch_idx: 130 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (16249/16768)
Epoch: 186 | Batch_idx: 140 |  Loss: (0.0938) |  Loss2: (0.0000) | Acc: (96.00%) (17491/18048)
Epoch: 186 | Batch_idx: 150 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (18738/19328)
Epoch: 186 | Batch_idx: 160 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (19980/20608)
Epoch: 186 | Batch_idx: 170 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (21222/21888)
Epoch: 186 | Batch_idx: 180 |  Loss: (0.0927) |  Loss2: (0.0000) | Acc: (96.00%) (22468/23168)
Epoch: 186 | Batch_idx: 190 |  Loss: (0.0929) |  Loss2: (0.0000) | Acc: (96.00%) (23706/24448)
Epoch: 186 | Batch_idx: 200 |  Loss: (0.0928) |  Loss2: (0.0000) | Acc: (96.00%) (24949/25728)
Epoch: 186 | Batch_idx: 210 |  Loss: (0.0926) |  Loss2: (0.0000) | Acc: (96.00%) (26186/27008)
Epoch: 186 | Batch_idx: 220 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (27434/28288)
Epoch: 186 | Batch_idx: 230 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (28674/29568)
Epoch: 186 | Batch_idx: 240 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (29899/30848)
Epoch: 186 | Batch_idx: 250 |  Loss: (0.0920) |  Loss2: (0.0000) | Acc: (96.00%) (31137/32128)
Epoch: 186 | Batch_idx: 260 |  Loss: (0.0921) |  Loss2: (0.0000) | Acc: (96.00%) (32382/33408)
Epoch: 186 | Batch_idx: 270 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (33628/34688)
Epoch: 186 | Batch_idx: 280 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (34875/35968)
Epoch: 186 | Batch_idx: 290 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (36108/37248)
Epoch: 186 | Batch_idx: 300 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (37349/38528)
Epoch: 186 | Batch_idx: 310 |  Loss: (0.0918) |  Loss2: (0.0000) | Acc: (96.00%) (38593/39808)
Epoch: 186 | Batch_idx: 320 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (39839/41088)
Epoch: 186 | Batch_idx: 330 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (41081/42368)
Epoch: 186 | Batch_idx: 340 |  Loss: (0.0915) |  Loss2: (0.0000) | Acc: (96.00%) (42329/43648)
Epoch: 186 | Batch_idx: 350 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (43572/44928)
Epoch: 186 | Batch_idx: 360 |  Loss: (0.0912) |  Loss2: (0.0000) | Acc: (96.00%) (44806/46208)
Epoch: 186 | Batch_idx: 370 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (46043/47488)
Epoch: 186 | Batch_idx: 380 |  Loss: (0.0914) |  Loss2: (0.0000) | Acc: (96.00%) (47280/48768)
Epoch: 186 | Batch_idx: 390 |  Loss: (0.0913) |  Loss2: (0.0000) | Acc: (96.00%) (48476/50000)
# TEST : Loss: (0.4200) | Acc: (88.00%) (8885/10000)
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.6006, 0.3994], device='cuda:0')
percent tensor([0.5776, 0.4224], device='cuda:0')
percent tensor([0.6681, 0.3319], device='cuda:0')
percent tensor([0.6248, 0.3752], device='cuda:0')
percent tensor([0.6756, 0.3244], device='cuda:0')
percent tensor([0.7378, 0.2622], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 187 | Batch_idx: 0 |  Loss: (0.0991) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 187 | Batch_idx: 10 |  Loss: (0.0916) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 187 | Batch_idx: 20 |  Loss: (0.0897) |  Loss2: (0.0000) | Acc: (96.00%) (2603/2688)
Epoch: 187 | Batch_idx: 30 |  Loss: (0.0886) |  Loss2: (0.0000) | Acc: (96.00%) (3843/3968)
Epoch: 187 | Batch_idx: 40 |  Loss: (0.0890) |  Loss2: (0.0000) | Acc: (96.00%) (5083/5248)
Epoch: 187 | Batch_idx: 50 |  Loss: (0.0877) |  Loss2: (0.0000) | Acc: (96.00%) (6331/6528)
Epoch: 187 | Batch_idx: 60 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (7576/7808)
Epoch: 187 | Batch_idx: 70 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (8817/9088)
Epoch: 187 | Batch_idx: 80 |  Loss: (0.0862) |  Loss2: (0.0000) | Acc: (97.00%) (10069/10368)
Epoch: 187 | Batch_idx: 90 |  Loss: (0.0859) |  Loss2: (0.0000) | Acc: (97.00%) (11319/11648)
Epoch: 187 | Batch_idx: 100 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (12561/12928)
Epoch: 187 | Batch_idx: 110 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (97.00%) (13795/14208)
Epoch: 187 | Batch_idx: 120 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (15050/15488)
Epoch: 187 | Batch_idx: 130 |  Loss: (0.0868) |  Loss2: (0.0000) | Acc: (97.00%) (16277/16768)
Epoch: 187 | Batch_idx: 140 |  Loss: (0.0879) |  Loss2: (0.0000) | Acc: (97.00%) (17517/18048)
Epoch: 187 | Batch_idx: 150 |  Loss: (0.0874) |  Loss2: (0.0000) | Acc: (97.00%) (18763/19328)
Epoch: 187 | Batch_idx: 160 |  Loss: (0.0869) |  Loss2: (0.0000) | Acc: (97.00%) (20007/20608)
Epoch: 187 | Batch_idx: 170 |  Loss: (0.0870) |  Loss2: (0.0000) | Acc: (97.00%) (21249/21888)
Epoch: 187 | Batch_idx: 180 |  Loss: (0.0872) |  Loss2: (0.0000) | Acc: (97.00%) (22497/23168)
Epoch: 187 | Batch_idx: 190 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (23747/24448)
Epoch: 187 | Batch_idx: 200 |  Loss: (0.0866) |  Loss2: (0.0000) | Acc: (97.00%) (24988/25728)
Epoch: 187 | Batch_idx: 210 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (97.00%) (26240/27008)
Epoch: 187 | Batch_idx: 220 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (27489/28288)
Epoch: 187 | Batch_idx: 230 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (28740/29568)
Epoch: 187 | Batch_idx: 240 |  Loss: (0.0841) |  Loss2: (0.0000) | Acc: (97.00%) (29989/30848)
Epoch: 187 | Batch_idx: 250 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (31235/32128)
Epoch: 187 | Batch_idx: 260 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (32489/33408)
Epoch: 187 | Batch_idx: 270 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (33738/34688)
Epoch: 187 | Batch_idx: 280 |  Loss: (0.0835) |  Loss2: (0.0000) | Acc: (97.00%) (34975/35968)
Epoch: 187 | Batch_idx: 290 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (36218/37248)
Epoch: 187 | Batch_idx: 300 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (37464/38528)
Epoch: 187 | Batch_idx: 310 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (38722/39808)
Epoch: 187 | Batch_idx: 320 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (39970/41088)
Epoch: 187 | Batch_idx: 330 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (41205/42368)
Epoch: 187 | Batch_idx: 340 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (42446/43648)
Epoch: 187 | Batch_idx: 350 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (43691/44928)
Epoch: 187 | Batch_idx: 360 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (44935/46208)
Epoch: 187 | Batch_idx: 370 |  Loss: (0.0836) |  Loss2: (0.0000) | Acc: (97.00%) (46178/47488)
Epoch: 187 | Batch_idx: 380 |  Loss: (0.0837) |  Loss2: (0.0000) | Acc: (97.00%) (47423/48768)
Epoch: 187 | Batch_idx: 390 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (48613/50000)
# TEST : Loss: (0.4114) | Acc: (88.00%) (8881/10000)
percent tensor([0.5852, 0.4148], device='cuda:0')
percent tensor([0.5999, 0.4001], device='cuda:0')
percent tensor([0.5788, 0.4212], device='cuda:0')
percent tensor([0.6724, 0.3276], device='cuda:0')
percent tensor([0.6285, 0.3715], device='cuda:0')
percent tensor([0.6685, 0.3315], device='cuda:0')
percent tensor([0.7415, 0.2585], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 188 | Batch_idx: 0 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 188 | Batch_idx: 10 |  Loss: (0.0647) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 188 | Batch_idx: 20 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (2622/2688)
Epoch: 188 | Batch_idx: 30 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 188 | Batch_idx: 40 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 188 | Batch_idx: 50 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (6358/6528)
Epoch: 188 | Batch_idx: 60 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (7592/7808)
Epoch: 188 | Batch_idx: 70 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (8838/9088)
Epoch: 188 | Batch_idx: 80 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (10083/10368)
Epoch: 188 | Batch_idx: 90 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (11332/11648)
Epoch: 188 | Batch_idx: 100 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (12589/12928)
Epoch: 188 | Batch_idx: 110 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (13836/14208)
Epoch: 188 | Batch_idx: 120 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (15079/15488)
Epoch: 188 | Batch_idx: 130 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (16332/16768)
Epoch: 188 | Batch_idx: 140 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (17582/18048)
Epoch: 188 | Batch_idx: 150 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (18827/19328)
Epoch: 188 | Batch_idx: 160 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (20069/20608)
Epoch: 188 | Batch_idx: 170 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (21320/21888)
Epoch: 188 | Batch_idx: 180 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (22558/23168)
Epoch: 188 | Batch_idx: 190 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (23798/24448)
Epoch: 188 | Batch_idx: 200 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (25051/25728)
Epoch: 188 | Batch_idx: 210 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (26297/27008)
Epoch: 188 | Batch_idx: 220 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (27550/28288)
Epoch: 188 | Batch_idx: 230 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (28795/29568)
Epoch: 188 | Batch_idx: 240 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (30046/30848)
Epoch: 188 | Batch_idx: 250 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (31294/32128)
Epoch: 188 | Batch_idx: 260 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (32534/33408)
Epoch: 188 | Batch_idx: 270 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (33786/34688)
Epoch: 188 | Batch_idx: 280 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (35029/35968)
Epoch: 188 | Batch_idx: 290 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (36275/37248)
Epoch: 188 | Batch_idx: 300 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (37520/38528)
Epoch: 188 | Batch_idx: 310 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (38771/39808)
Epoch: 188 | Batch_idx: 320 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (40008/41088)
Epoch: 188 | Batch_idx: 330 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (41253/42368)
Epoch: 188 | Batch_idx: 340 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (42495/43648)
Epoch: 188 | Batch_idx: 350 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (43744/44928)
Epoch: 188 | Batch_idx: 360 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (44979/46208)
Epoch: 188 | Batch_idx: 370 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (46225/47488)
Epoch: 188 | Batch_idx: 380 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (47476/48768)
Epoch: 188 | Batch_idx: 390 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (48670/50000)
# TEST : Loss: (0.4056) | Acc: (89.00%) (8902/10000)
percent tensor([0.5850, 0.4150], device='cuda:0')
percent tensor([0.5991, 0.4009], device='cuda:0')
percent tensor([0.5819, 0.4181], device='cuda:0')
percent tensor([0.6711, 0.3289], device='cuda:0')
percent tensor([0.6258, 0.3742], device='cuda:0')
percent tensor([0.6738, 0.3262], device='cuda:0')
percent tensor([0.7394, 0.2606], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 189 | Batch_idx: 0 |  Loss: (0.0639) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 189 | Batch_idx: 10 |  Loss: (0.0793) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 189 | Batch_idx: 20 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (2616/2688)
Epoch: 189 | Batch_idx: 30 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (3863/3968)
Epoch: 189 | Batch_idx: 40 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (5110/5248)
Epoch: 189 | Batch_idx: 50 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (6360/6528)
Epoch: 189 | Batch_idx: 60 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (7615/7808)
Epoch: 189 | Batch_idx: 70 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (8850/9088)
Epoch: 189 | Batch_idx: 80 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (10100/10368)
Epoch: 189 | Batch_idx: 90 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (11358/11648)
Epoch: 189 | Batch_idx: 100 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (12601/12928)
Epoch: 189 | Batch_idx: 110 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (13847/14208)
Epoch: 189 | Batch_idx: 120 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (15090/15488)
Epoch: 189 | Batch_idx: 130 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (16340/16768)
Epoch: 189 | Batch_idx: 140 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (17589/18048)
Epoch: 189 | Batch_idx: 150 |  Loss: (0.0804) |  Loss2: (0.0000) | Acc: (97.00%) (18832/19328)
Epoch: 189 | Batch_idx: 160 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (20081/20608)
Epoch: 189 | Batch_idx: 170 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (21329/21888)
Epoch: 189 | Batch_idx: 180 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (22578/23168)
Epoch: 189 | Batch_idx: 190 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (23833/24448)
Epoch: 189 | Batch_idx: 200 |  Loss: (0.0791) |  Loss2: (0.0000) | Acc: (97.00%) (25083/25728)
Epoch: 189 | Batch_idx: 210 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (26335/27008)
Epoch: 189 | Batch_idx: 220 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (27581/28288)
Epoch: 189 | Batch_idx: 230 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (28823/29568)
Epoch: 189 | Batch_idx: 240 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (30079/30848)
Epoch: 189 | Batch_idx: 250 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (31327/32128)
Epoch: 189 | Batch_idx: 260 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (32572/33408)
Epoch: 189 | Batch_idx: 270 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (33814/34688)
Epoch: 189 | Batch_idx: 280 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (35065/35968)
Epoch: 189 | Batch_idx: 290 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (36324/37248)
Epoch: 189 | Batch_idx: 300 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (37567/38528)
Epoch: 189 | Batch_idx: 310 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (38821/39808)
Epoch: 189 | Batch_idx: 320 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (40069/41088)
Epoch: 189 | Batch_idx: 330 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (41319/42368)
Epoch: 189 | Batch_idx: 340 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (42566/43648)
Epoch: 189 | Batch_idx: 350 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (43803/44928)
Epoch: 189 | Batch_idx: 360 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (45050/46208)
Epoch: 189 | Batch_idx: 370 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (46302/47488)
Epoch: 189 | Batch_idx: 380 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (47547/48768)
Epoch: 189 | Batch_idx: 390 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (48738/50000)
# TEST : Loss: (0.4067) | Acc: (88.00%) (8893/10000)
percent tensor([0.5851, 0.4149], device='cuda:0')
percent tensor([0.5961, 0.4039], device='cuda:0')
percent tensor([0.5799, 0.4201], device='cuda:0')
percent tensor([0.6690, 0.3310], device='cuda:0')
percent tensor([0.6270, 0.3730], device='cuda:0')
percent tensor([0.6750, 0.3250], device='cuda:0')
percent tensor([0.7418, 0.2582], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 190 | Batch_idx: 0 |  Loss: (0.0943) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 190 | Batch_idx: 10 |  Loss: (0.0858) |  Loss2: (0.0000) | Acc: (96.00%) (1364/1408)
Epoch: 190 | Batch_idx: 20 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (2613/2688)
Epoch: 190 | Batch_idx: 30 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (3860/3968)
Epoch: 190 | Batch_idx: 40 |  Loss: (0.0798) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 190 | Batch_idx: 50 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (6351/6528)
Epoch: 190 | Batch_idx: 60 |  Loss: (0.0852) |  Loss2: (0.0000) | Acc: (97.00%) (7590/7808)
Epoch: 190 | Batch_idx: 70 |  Loss: (0.0864) |  Loss2: (0.0000) | Acc: (97.00%) (8828/9088)
Epoch: 190 | Batch_idx: 80 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (10083/10368)
Epoch: 190 | Batch_idx: 90 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (11334/11648)
Epoch: 190 | Batch_idx: 100 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (12577/12928)
Epoch: 190 | Batch_idx: 110 |  Loss: (0.0832) |  Loss2: (0.0000) | Acc: (97.00%) (13823/14208)
Epoch: 190 | Batch_idx: 120 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (15065/15488)
Epoch: 190 | Batch_idx: 130 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (16309/16768)
Epoch: 190 | Batch_idx: 140 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (17549/18048)
Epoch: 190 | Batch_idx: 150 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (18799/19328)
Epoch: 190 | Batch_idx: 160 |  Loss: (0.0829) |  Loss2: (0.0000) | Acc: (97.00%) (20045/20608)
Epoch: 190 | Batch_idx: 170 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (21290/21888)
Epoch: 190 | Batch_idx: 180 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (22543/23168)
Epoch: 190 | Batch_idx: 190 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (23789/24448)
Epoch: 190 | Batch_idx: 200 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (25035/25728)
Epoch: 190 | Batch_idx: 210 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (26278/27008)
Epoch: 190 | Batch_idx: 220 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (27517/28288)
Epoch: 190 | Batch_idx: 230 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (28757/29568)
Epoch: 190 | Batch_idx: 240 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (30004/30848)
Epoch: 190 | Batch_idx: 250 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (31247/32128)
Epoch: 190 | Batch_idx: 260 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (32484/33408)
Epoch: 190 | Batch_idx: 270 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (33720/34688)
Epoch: 190 | Batch_idx: 280 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (34965/35968)
Epoch: 190 | Batch_idx: 290 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (36214/37248)
Epoch: 190 | Batch_idx: 300 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (37464/38528)
Epoch: 190 | Batch_idx: 310 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (38698/39808)
Epoch: 190 | Batch_idx: 320 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (39930/41088)
Epoch: 190 | Batch_idx: 330 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (41166/42368)
Epoch: 190 | Batch_idx: 340 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (42409/43648)
Epoch: 190 | Batch_idx: 350 |  Loss: (0.0839) |  Loss2: (0.0000) | Acc: (97.00%) (43654/44928)
Epoch: 190 | Batch_idx: 360 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (44896/46208)
Epoch: 190 | Batch_idx: 370 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (97.00%) (46137/47488)
Epoch: 190 | Batch_idx: 380 |  Loss: (0.0847) |  Loss2: (0.0000) | Acc: (97.00%) (47379/48768)
Epoch: 190 | Batch_idx: 390 |  Loss: (0.0854) |  Loss2: (0.0000) | Acc: (97.00%) (48565/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_190.pth.tar'
# TEST : Loss: (0.4136) | Acc: (88.00%) (8851/10000)
percent tensor([0.5850, 0.4150], device='cuda:0')
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.5814, 0.4186], device='cuda:0')
percent tensor([0.6694, 0.3306], device='cuda:0')
percent tensor([0.6284, 0.3716], device='cuda:0')
percent tensor([0.6755, 0.3245], device='cuda:0')
percent tensor([0.7430, 0.2570], device='cuda:0')
percent tensor([0.9997, 0.0003], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(182.8572, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.0843, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(830.3807, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1514.2419, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(478.5973, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2298.4221, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4251.8389, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1350.0903, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6256.7695, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11566.8086, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3796.8552, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(16048.8574, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 191 | Batch_idx: 0 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 191 | Batch_idx: 10 |  Loss: (0.0860) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 191 | Batch_idx: 20 |  Loss: (0.0895) |  Loss2: (0.0000) | Acc: (96.00%) (2605/2688)
Epoch: 191 | Batch_idx: 30 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (97.00%) (3852/3968)
Epoch: 191 | Batch_idx: 40 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (5104/5248)
Epoch: 191 | Batch_idx: 50 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (6359/6528)
Epoch: 191 | Batch_idx: 60 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (7604/7808)
Epoch: 191 | Batch_idx: 70 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (8854/9088)
Epoch: 191 | Batch_idx: 80 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (10104/10368)
Epoch: 191 | Batch_idx: 90 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (11349/11648)
Epoch: 191 | Batch_idx: 100 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (12594/12928)
Epoch: 191 | Batch_idx: 110 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (13834/14208)
Epoch: 191 | Batch_idx: 120 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (15093/15488)
Epoch: 191 | Batch_idx: 130 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (16337/16768)
Epoch: 191 | Batch_idx: 140 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (17573/18048)
Epoch: 191 | Batch_idx: 150 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (18813/19328)
Epoch: 191 | Batch_idx: 160 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (20062/20608)
Epoch: 191 | Batch_idx: 170 |  Loss: (0.0785) |  Loss2: (0.0000) | Acc: (97.00%) (21310/21888)
Epoch: 191 | Batch_idx: 180 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (22559/23168)
Epoch: 191 | Batch_idx: 190 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (23801/24448)
Epoch: 191 | Batch_idx: 200 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (25044/25728)
Epoch: 191 | Batch_idx: 210 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (26286/27008)
Epoch: 191 | Batch_idx: 220 |  Loss: (0.0790) |  Loss2: (0.0000) | Acc: (97.00%) (27538/28288)
Epoch: 191 | Batch_idx: 230 |  Loss: (0.0788) |  Loss2: (0.0000) | Acc: (97.00%) (28784/29568)
Epoch: 191 | Batch_idx: 240 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (30035/30848)
Epoch: 191 | Batch_idx: 250 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (31288/32128)
Epoch: 191 | Batch_idx: 260 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (32533/33408)
Epoch: 191 | Batch_idx: 270 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (33784/34688)
Epoch: 191 | Batch_idx: 280 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (97.00%) (35023/35968)
Epoch: 191 | Batch_idx: 290 |  Loss: (0.0789) |  Loss2: (0.0000) | Acc: (97.00%) (36260/37248)
Epoch: 191 | Batch_idx: 300 |  Loss: (0.0795) |  Loss2: (0.0000) | Acc: (97.00%) (37497/38528)
Epoch: 191 | Batch_idx: 310 |  Loss: (0.0794) |  Loss2: (0.0000) | Acc: (97.00%) (38744/39808)
Epoch: 191 | Batch_idx: 320 |  Loss: (0.0799) |  Loss2: (0.0000) | Acc: (97.00%) (39987/41088)
Epoch: 191 | Batch_idx: 330 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (41223/42368)
Epoch: 191 | Batch_idx: 340 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (42464/43648)
Epoch: 191 | Batch_idx: 350 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (43704/44928)
Epoch: 191 | Batch_idx: 360 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (44952/46208)
Epoch: 191 | Batch_idx: 370 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (46194/47488)
Epoch: 191 | Batch_idx: 380 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (47440/48768)
Epoch: 191 | Batch_idx: 390 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (48636/50000)
# TEST : Loss: (0.4461) | Acc: (88.00%) (8870/10000)
percent tensor([0.5849, 0.4151], device='cuda:0')
percent tensor([0.5961, 0.4039], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6695, 0.3305], device='cuda:0')
percent tensor([0.6284, 0.3716], device='cuda:0')
percent tensor([0.6750, 0.3250], device='cuda:0')
percent tensor([0.7400, 0.2600], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 192 | Batch_idx: 0 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 192 | Batch_idx: 10 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 192 | Batch_idx: 20 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (2633/2688)
Epoch: 192 | Batch_idx: 30 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (3881/3968)
Epoch: 192 | Batch_idx: 40 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (5134/5248)
Epoch: 192 | Batch_idx: 50 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (6381/6528)
Epoch: 192 | Batch_idx: 60 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (7626/7808)
Epoch: 192 | Batch_idx: 70 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (8868/9088)
Epoch: 192 | Batch_idx: 80 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (10116/10368)
Epoch: 192 | Batch_idx: 90 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (11369/11648)
Epoch: 192 | Batch_idx: 100 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (12608/12928)
Epoch: 192 | Batch_idx: 110 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (13851/14208)
Epoch: 192 | Batch_idx: 120 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (15096/15488)
Epoch: 192 | Batch_idx: 130 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (16344/16768)
Epoch: 192 | Batch_idx: 140 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (17595/18048)
Epoch: 192 | Batch_idx: 150 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (18833/19328)
Epoch: 192 | Batch_idx: 160 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (20081/20608)
Epoch: 192 | Batch_idx: 170 |  Loss: (0.0778) |  Loss2: (0.0000) | Acc: (97.00%) (21329/21888)
Epoch: 192 | Batch_idx: 180 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (22579/23168)
Epoch: 192 | Batch_idx: 190 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (23827/24448)
Epoch: 192 | Batch_idx: 200 |  Loss: (0.0787) |  Loss2: (0.0000) | Acc: (97.00%) (25066/25728)
Epoch: 192 | Batch_idx: 210 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (26307/27008)
Epoch: 192 | Batch_idx: 220 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (27527/28288)
Epoch: 192 | Batch_idx: 230 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (28768/29568)
Epoch: 192 | Batch_idx: 240 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (30008/30848)
Epoch: 192 | Batch_idx: 250 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (31243/32128)
Epoch: 192 | Batch_idx: 260 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (32485/33408)
Epoch: 192 | Batch_idx: 270 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (33721/34688)
Epoch: 192 | Batch_idx: 280 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (34969/35968)
Epoch: 192 | Batch_idx: 290 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (36212/37248)
Epoch: 192 | Batch_idx: 300 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (37458/38528)
Epoch: 192 | Batch_idx: 310 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (38699/39808)
Epoch: 192 | Batch_idx: 320 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (39943/41088)
Epoch: 192 | Batch_idx: 330 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (41187/42368)
Epoch: 192 | Batch_idx: 340 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (42424/43648)
Epoch: 192 | Batch_idx: 350 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (43669/44928)
Epoch: 192 | Batch_idx: 360 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (44917/46208)
Epoch: 192 | Batch_idx: 370 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (46162/47488)
Epoch: 192 | Batch_idx: 380 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (47402/48768)
Epoch: 192 | Batch_idx: 390 |  Loss: (0.0827) |  Loss2: (0.0000) | Acc: (97.00%) (48591/50000)
# TEST : Loss: (0.4508) | Acc: (88.00%) (8844/10000)
percent tensor([0.5861, 0.4139], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.5792, 0.4208], device='cuda:0')
percent tensor([0.6683, 0.3317], device='cuda:0')
percent tensor([0.6276, 0.3724], device='cuda:0')
percent tensor([0.6751, 0.3249], device='cuda:0')
percent tensor([0.7446, 0.2554], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 193 | Batch_idx: 0 |  Loss: (0.1624) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 193 | Batch_idx: 10 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 193 | Batch_idx: 20 |  Loss: (0.0881) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 193 | Batch_idx: 30 |  Loss: (0.0838) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 193 | Batch_idx: 40 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (5110/5248)
Epoch: 193 | Batch_idx: 50 |  Loss: (0.0828) |  Loss2: (0.0000) | Acc: (97.00%) (6353/6528)
Epoch: 193 | Batch_idx: 60 |  Loss: (0.0834) |  Loss2: (0.0000) | Acc: (97.00%) (7592/7808)
Epoch: 193 | Batch_idx: 70 |  Loss: (0.0830) |  Loss2: (0.0000) | Acc: (97.00%) (8837/9088)
Epoch: 193 | Batch_idx: 80 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (10090/10368)
Epoch: 193 | Batch_idx: 90 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (11338/11648)
Epoch: 193 | Batch_idx: 100 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (12592/12928)
Epoch: 193 | Batch_idx: 110 |  Loss: (0.0803) |  Loss2: (0.0000) | Acc: (97.00%) (13835/14208)
Epoch: 193 | Batch_idx: 120 |  Loss: (0.0805) |  Loss2: (0.0000) | Acc: (97.00%) (15077/15488)
Epoch: 193 | Batch_idx: 130 |  Loss: (0.0797) |  Loss2: (0.0000) | Acc: (97.00%) (16325/16768)
Epoch: 193 | Batch_idx: 140 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (17569/18048)
Epoch: 193 | Batch_idx: 150 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (18811/19328)
Epoch: 193 | Batch_idx: 160 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (20054/20608)
Epoch: 193 | Batch_idx: 170 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (21297/21888)
Epoch: 193 | Batch_idx: 180 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (22546/23168)
Epoch: 193 | Batch_idx: 190 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (23790/24448)
Epoch: 193 | Batch_idx: 200 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (25036/25728)
Epoch: 193 | Batch_idx: 210 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (26278/27008)
Epoch: 193 | Batch_idx: 220 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (27529/28288)
Epoch: 193 | Batch_idx: 230 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (28779/29568)
Epoch: 193 | Batch_idx: 240 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (30033/30848)
Epoch: 193 | Batch_idx: 250 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (31277/32128)
Epoch: 193 | Batch_idx: 260 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (32523/33408)
Epoch: 193 | Batch_idx: 270 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (33771/34688)
Epoch: 193 | Batch_idx: 280 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (35018/35968)
Epoch: 193 | Batch_idx: 290 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (36271/37248)
Epoch: 193 | Batch_idx: 300 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (37519/38528)
Epoch: 193 | Batch_idx: 310 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (38758/39808)
Epoch: 193 | Batch_idx: 320 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (40004/41088)
Epoch: 193 | Batch_idx: 330 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (41253/42368)
Epoch: 193 | Batch_idx: 340 |  Loss: (0.0810) |  Loss2: (0.0000) | Acc: (97.00%) (42491/43648)
Epoch: 193 | Batch_idx: 350 |  Loss: (0.0815) |  Loss2: (0.0000) | Acc: (97.00%) (43726/44928)
Epoch: 193 | Batch_idx: 360 |  Loss: (0.0812) |  Loss2: (0.0000) | Acc: (97.00%) (44976/46208)
Epoch: 193 | Batch_idx: 370 |  Loss: (0.0813) |  Loss2: (0.0000) | Acc: (97.00%) (46213/47488)
Epoch: 193 | Batch_idx: 380 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (47453/48768)
Epoch: 193 | Batch_idx: 390 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (48650/50000)
# TEST : Loss: (0.4744) | Acc: (88.00%) (8813/10000)
percent tensor([0.5853, 0.4147], device='cuda:0')
percent tensor([0.5963, 0.4037], device='cuda:0')
percent tensor([0.5803, 0.4197], device='cuda:0')
percent tensor([0.6685, 0.3315], device='cuda:0')
percent tensor([0.6291, 0.3709], device='cuda:0')
percent tensor([0.6737, 0.3263], device='cuda:0')
percent tensor([0.7408, 0.2592], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 194 | Batch_idx: 0 |  Loss: (0.1077) |  Loss2: (0.0000) | Acc: (94.00%) (121/128)
Epoch: 194 | Batch_idx: 10 |  Loss: (0.0892) |  Loss2: (0.0000) | Acc: (97.00%) (1366/1408)
Epoch: 194 | Batch_idx: 20 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (2619/2688)
Epoch: 194 | Batch_idx: 30 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (3862/3968)
Epoch: 194 | Batch_idx: 40 |  Loss: (0.0833) |  Loss2: (0.0000) | Acc: (97.00%) (5103/5248)
Epoch: 194 | Batch_idx: 50 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (6357/6528)
Epoch: 194 | Batch_idx: 60 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (7618/7808)
Epoch: 194 | Batch_idx: 70 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (8869/9088)
Epoch: 194 | Batch_idx: 80 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (10128/10368)
Epoch: 194 | Batch_idx: 90 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (11372/11648)
Epoch: 194 | Batch_idx: 100 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (12610/12928)
Epoch: 194 | Batch_idx: 110 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (13858/14208)
Epoch: 194 | Batch_idx: 120 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (15110/15488)
Epoch: 194 | Batch_idx: 130 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (16356/16768)
Epoch: 194 | Batch_idx: 140 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (17602/18048)
Epoch: 194 | Batch_idx: 150 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (18845/19328)
Epoch: 194 | Batch_idx: 160 |  Loss: (0.0769) |  Loss2: (0.0000) | Acc: (97.00%) (20094/20608)
Epoch: 194 | Batch_idx: 170 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (21344/21888)
Epoch: 194 | Batch_idx: 180 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (22591/23168)
Epoch: 194 | Batch_idx: 190 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (23841/24448)
Epoch: 194 | Batch_idx: 200 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (25087/25728)
Epoch: 194 | Batch_idx: 210 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (26338/27008)
Epoch: 194 | Batch_idx: 220 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (27579/28288)
Epoch: 194 | Batch_idx: 230 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (28836/29568)
Epoch: 194 | Batch_idx: 240 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (30089/30848)
Epoch: 194 | Batch_idx: 250 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (31338/32128)
Epoch: 194 | Batch_idx: 260 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (32585/33408)
Epoch: 194 | Batch_idx: 270 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (33830/34688)
Epoch: 194 | Batch_idx: 280 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (35076/35968)
Epoch: 194 | Batch_idx: 290 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (36334/37248)
Epoch: 194 | Batch_idx: 300 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (37577/38528)
Epoch: 194 | Batch_idx: 310 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (38824/39808)
Epoch: 194 | Batch_idx: 320 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (40073/41088)
Epoch: 194 | Batch_idx: 330 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (41317/42368)
Epoch: 194 | Batch_idx: 340 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (42562/43648)
Epoch: 194 | Batch_idx: 350 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (43799/44928)
Epoch: 194 | Batch_idx: 360 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (45052/46208)
Epoch: 194 | Batch_idx: 370 |  Loss: (0.0768) |  Loss2: (0.0000) | Acc: (97.00%) (46296/47488)
Epoch: 194 | Batch_idx: 380 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (47534/48768)
Epoch: 194 | Batch_idx: 390 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (48738/50000)
# TEST : Loss: (0.4540) | Acc: (87.00%) (8793/10000)
percent tensor([0.5854, 0.4146], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5800, 0.4200], device='cuda:0')
percent tensor([0.6684, 0.3316], device='cuda:0')
percent tensor([0.6265, 0.3735], device='cuda:0')
percent tensor([0.6735, 0.3265], device='cuda:0')
percent tensor([0.7403, 0.2597], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
False Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=128, out_features=16, bias=False)
True Linear(in_features=16, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=256, out_features=32, bias=False)
True Linear(in_features=32, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=512, out_features=64, bias=False)
True Linear(in_features=64, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
False BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Linear(in_features=1024, out_features=128, bias=False)
True Linear(in_features=128, out_features=2, bias=False)
False Linear(in_features=512, out_features=10, bias=True)
Epoch: 195 | Batch_idx: 0 |  Loss: (0.0932) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 195 | Batch_idx: 10 |  Loss: (0.0752) |  Loss2: (0.0000) | Acc: (97.00%) (1374/1408)
Epoch: 195 | Batch_idx: 20 |  Loss: (0.0779) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 195 | Batch_idx: 30 |  Loss: (0.0871) |  Loss2: (0.0000) | Acc: (96.00%) (3846/3968)
Epoch: 195 | Batch_idx: 40 |  Loss: (0.0884) |  Loss2: (0.0000) | Acc: (96.00%) (5086/5248)
Epoch: 195 | Batch_idx: 50 |  Loss: (0.0919) |  Loss2: (0.0000) | Acc: (96.00%) (6325/6528)
Epoch: 195 | Batch_idx: 60 |  Loss: (0.0977) |  Loss2: (0.0000) | Acc: (96.00%) (7549/7808)
Epoch: 195 | Batch_idx: 70 |  Loss: (0.1001) |  Loss2: (0.0000) | Acc: (96.00%) (8778/9088)
Epoch: 195 | Batch_idx: 80 |  Loss: (0.0988) |  Loss2: (0.0000) | Acc: (96.00%) (10014/10368)
Epoch: 195 | Batch_idx: 90 |  Loss: (0.0981) |  Loss2: (0.0000) | Acc: (96.00%) (11255/11648)
Epoch: 195 | Batch_idx: 100 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (12485/12928)
Epoch: 195 | Batch_idx: 110 |  Loss: (0.0987) |  Loss2: (0.0000) | Acc: (96.00%) (13721/14208)
Epoch: 195 | Batch_idx: 120 |  Loss: (0.0984) |  Loss2: (0.0000) | Acc: (96.00%) (14956/15488)
Epoch: 195 | Batch_idx: 130 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (16195/16768)
Epoch: 195 | Batch_idx: 140 |  Loss: (0.0975) |  Loss2: (0.0000) | Acc: (96.00%) (17433/18048)
Epoch: 195 | Batch_idx: 150 |  Loss: (0.0970) |  Loss2: (0.0000) | Acc: (96.00%) (18674/19328)
Epoch: 195 | Batch_idx: 160 |  Loss: (0.0974) |  Loss2: (0.0000) | Acc: (96.00%) (19903/20608)
Epoch: 195 | Batch_idx: 170 |  Loss: (0.0965) |  Loss2: (0.0000) | Acc: (96.00%) (21145/21888)
Epoch: 195 | Batch_idx: 180 |  Loss: (0.0966) |  Loss2: (0.0000) | Acc: (96.00%) (22389/23168)
Epoch: 195 | Batch_idx: 190 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (23629/24448)
Epoch: 195 | Batch_idx: 200 |  Loss: (0.0972) |  Loss2: (0.0000) | Acc: (96.00%) (24858/25728)
Epoch: 195 | Batch_idx: 210 |  Loss: (0.0976) |  Loss2: (0.0000) | Acc: (96.00%) (26085/27008)
Epoch: 195 | Batch_idx: 220 |  Loss: (0.0968) |  Loss2: (0.0000) | Acc: (96.00%) (27335/28288)
Epoch: 195 | Batch_idx: 230 |  Loss: (0.0964) |  Loss2: (0.0000) | Acc: (96.00%) (28575/29568)
Epoch: 195 | Batch_idx: 240 |  Loss: (0.0960) |  Loss2: (0.0000) | Acc: (96.00%) (29815/30848)
Epoch: 195 | Batch_idx: 250 |  Loss: (0.0958) |  Loss2: (0.0000) | Acc: (96.00%) (31057/32128)
Epoch: 195 | Batch_idx: 260 |  Loss: (0.0953) |  Loss2: (0.0000) | Acc: (96.00%) (32301/33408)
Epoch: 195 | Batch_idx: 270 |  Loss: (0.0957) |  Loss2: (0.0000) | Acc: (96.00%) (33536/34688)
Epoch: 195 | Batch_idx: 280 |  Loss: (0.0950) |  Loss2: (0.0000) | Acc: (96.00%) (34782/35968)
Epoch: 195 | Batch_idx: 290 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (36027/37248)
Epoch: 195 | Batch_idx: 300 |  Loss: (0.0946) |  Loss2: (0.0000) | Acc: (96.00%) (37262/38528)
Epoch: 195 | Batch_idx: 310 |  Loss: (0.0945) |  Loss2: (0.0000) | Acc: (96.00%) (38507/39808)
Epoch: 195 | Batch_idx: 320 |  Loss: (0.0944) |  Loss2: (0.0000) | Acc: (96.00%) (39746/41088)
Epoch: 195 | Batch_idx: 330 |  Loss: (0.0942) |  Loss2: (0.0000) | Acc: (96.00%) (40990/42368)
Epoch: 195 | Batch_idx: 340 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (42238/43648)
Epoch: 195 | Batch_idx: 350 |  Loss: (0.0936) |  Loss2: (0.0000) | Acc: (96.00%) (43472/44928)
Epoch: 195 | Batch_idx: 360 |  Loss: (0.0934) |  Loss2: (0.0000) | Acc: (96.00%) (44712/46208)
Epoch: 195 | Batch_idx: 370 |  Loss: (0.0933) |  Loss2: (0.0000) | Acc: (96.00%) (45958/47488)
Epoch: 195 | Batch_idx: 380 |  Loss: (0.0931) |  Loss2: (0.0000) | Acc: (96.00%) (47203/48768)
Epoch: 195 | Batch_idx: 390 |  Loss: (0.0930) |  Loss2: (0.0000) | Acc: (96.00%) (48393/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_195.pth.tar'
# TEST : Loss: (0.4334) | Acc: (88.00%) (8819/10000)
percent tensor([0.5865, 0.4135], device='cuda:0')
percent tensor([0.5849, 0.4151], device='cuda:0')
percent tensor([0.5842, 0.4158], device='cuda:0')
percent tensor([0.6661, 0.3339], device='cuda:0')
percent tensor([0.6293, 0.3707], device='cuda:0')
percent tensor([0.6811, 0.3189], device='cuda:0')
percent tensor([0.7164, 0.2836], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 196 | Batch_idx: 0 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 196 | Batch_idx: 10 |  Loss: (0.0896) |  Loss2: (0.0000) | Acc: (96.00%) (1365/1408)
Epoch: 196 | Batch_idx: 20 |  Loss: (0.0861) |  Loss2: (0.0000) | Acc: (97.00%) (2610/2688)
Epoch: 196 | Batch_idx: 30 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (3860/3968)
Epoch: 196 | Batch_idx: 40 |  Loss: (0.0831) |  Loss2: (0.0000) | Acc: (97.00%) (5107/5248)
Epoch: 196 | Batch_idx: 50 |  Loss: (0.0844) |  Loss2: (0.0000) | Acc: (97.00%) (6348/6528)
Epoch: 196 | Batch_idx: 60 |  Loss: (0.0816) |  Loss2: (0.0000) | Acc: (97.00%) (7597/7808)
Epoch: 196 | Batch_idx: 70 |  Loss: (0.0809) |  Loss2: (0.0000) | Acc: (97.00%) (8844/9088)
Epoch: 196 | Batch_idx: 80 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (10094/10368)
Epoch: 196 | Batch_idx: 90 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (11336/11648)
Epoch: 196 | Batch_idx: 100 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (12585/12928)
Epoch: 196 | Batch_idx: 110 |  Loss: (0.0806) |  Loss2: (0.0000) | Acc: (97.00%) (13829/14208)
Epoch: 196 | Batch_idx: 120 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (15074/15488)
Epoch: 196 | Batch_idx: 130 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (16306/16768)
Epoch: 196 | Batch_idx: 140 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (17554/18048)
Epoch: 196 | Batch_idx: 150 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (18809/19328)
Epoch: 196 | Batch_idx: 160 |  Loss: (0.0811) |  Loss2: (0.0000) | Acc: (97.00%) (20053/20608)
Epoch: 196 | Batch_idx: 170 |  Loss: (0.0808) |  Loss2: (0.0000) | Acc: (97.00%) (21300/21888)
Epoch: 196 | Batch_idx: 180 |  Loss: (0.0802) |  Loss2: (0.0000) | Acc: (97.00%) (22551/23168)
Epoch: 196 | Batch_idx: 190 |  Loss: (0.0807) |  Loss2: (0.0000) | Acc: (97.00%) (23787/24448)
Epoch: 196 | Batch_idx: 200 |  Loss: (0.0814) |  Loss2: (0.0000) | Acc: (97.00%) (25023/25728)
Epoch: 196 | Batch_idx: 210 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (26274/27008)
Epoch: 196 | Batch_idx: 220 |  Loss: (0.0824) |  Loss2: (0.0000) | Acc: (97.00%) (27519/28288)
Epoch: 196 | Batch_idx: 230 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (28773/29568)
Epoch: 196 | Batch_idx: 240 |  Loss: (0.0826) |  Loss2: (0.0000) | Acc: (97.00%) (30008/30848)
Epoch: 196 | Batch_idx: 250 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (31257/32128)
Epoch: 196 | Batch_idx: 260 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (32506/33408)
Epoch: 196 | Batch_idx: 270 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (33753/34688)
Epoch: 196 | Batch_idx: 280 |  Loss: (0.0817) |  Loss2: (0.0000) | Acc: (97.00%) (34995/35968)
Epoch: 196 | Batch_idx: 290 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (36236/37248)
Epoch: 196 | Batch_idx: 300 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (37475/38528)
Epoch: 196 | Batch_idx: 310 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (38722/39808)
Epoch: 196 | Batch_idx: 320 |  Loss: (0.0821) |  Loss2: (0.0000) | Acc: (97.00%) (39969/41088)
Epoch: 196 | Batch_idx: 330 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (41215/42368)
Epoch: 196 | Batch_idx: 340 |  Loss: (0.0822) |  Loss2: (0.0000) | Acc: (97.00%) (42456/43648)
Epoch: 196 | Batch_idx: 350 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (43704/44928)
Epoch: 196 | Batch_idx: 360 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (44938/46208)
Epoch: 196 | Batch_idx: 370 |  Loss: (0.0823) |  Loss2: (0.0000) | Acc: (97.00%) (46184/47488)
Epoch: 196 | Batch_idx: 380 |  Loss: (0.0819) |  Loss2: (0.0000) | Acc: (97.00%) (47433/48768)
Epoch: 196 | Batch_idx: 390 |  Loss: (0.0820) |  Loss2: (0.0000) | Acc: (97.00%) (48633/50000)
# TEST : Loss: (0.4176) | Acc: (88.00%) (8875/10000)
percent tensor([0.5863, 0.4137], device='cuda:0')
percent tensor([0.5853, 0.4147], device='cuda:0')
percent tensor([0.5832, 0.4168], device='cuda:0')
percent tensor([0.6622, 0.3378], device='cuda:0')
percent tensor([0.6357, 0.3643], device='cuda:0')
percent tensor([0.6853, 0.3147], device='cuda:0')
percent tensor([0.7194, 0.2806], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 197 | Batch_idx: 0 |  Loss: (0.0840) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 197 | Batch_idx: 10 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 197 | Batch_idx: 20 |  Loss: (0.0781) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 197 | Batch_idx: 30 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (3867/3968)
Epoch: 197 | Batch_idx: 40 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (5118/5248)
Epoch: 197 | Batch_idx: 50 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (6364/6528)
Epoch: 197 | Batch_idx: 60 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (7618/7808)
Epoch: 197 | Batch_idx: 70 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (8856/9088)
Epoch: 197 | Batch_idx: 80 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (10109/10368)
Epoch: 197 | Batch_idx: 90 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (11351/11648)
Epoch: 197 | Batch_idx: 100 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (12602/12928)
Epoch: 197 | Batch_idx: 110 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (13850/14208)
Epoch: 197 | Batch_idx: 120 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (15094/15488)
Epoch: 197 | Batch_idx: 130 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (16338/16768)
Epoch: 197 | Batch_idx: 140 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (17590/18048)
Epoch: 197 | Batch_idx: 150 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (18839/19328)
Epoch: 197 | Batch_idx: 160 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (20089/20608)
Epoch: 197 | Batch_idx: 170 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (21328/21888)
Epoch: 197 | Batch_idx: 180 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (22578/23168)
Epoch: 197 | Batch_idx: 190 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (23832/24448)
Epoch: 197 | Batch_idx: 200 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (25081/25728)
Epoch: 197 | Batch_idx: 210 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (26329/27008)
Epoch: 197 | Batch_idx: 220 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (27573/28288)
Epoch: 197 | Batch_idx: 230 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (28816/29568)
Epoch: 197 | Batch_idx: 240 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (30059/30848)
Epoch: 197 | Batch_idx: 250 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (31311/32128)
Epoch: 197 | Batch_idx: 260 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (32560/33408)
Epoch: 197 | Batch_idx: 270 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (33811/34688)
Epoch: 197 | Batch_idx: 280 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (35056/35968)
Epoch: 197 | Batch_idx: 290 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (36309/37248)
Epoch: 197 | Batch_idx: 300 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (37554/38528)
Epoch: 197 | Batch_idx: 310 |  Loss: (0.0744) |  Loss2: (0.0000) | Acc: (97.00%) (38812/39808)
Epoch: 197 | Batch_idx: 320 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (40057/41088)
Epoch: 197 | Batch_idx: 330 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (41302/42368)
Epoch: 197 | Batch_idx: 340 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (42555/43648)
Epoch: 197 | Batch_idx: 350 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (43805/44928)
Epoch: 197 | Batch_idx: 360 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (45051/46208)
Epoch: 197 | Batch_idx: 370 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (46295/47488)
Epoch: 197 | Batch_idx: 380 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (47539/48768)
Epoch: 197 | Batch_idx: 390 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (48738/50000)
# TEST : Loss: (0.4176) | Acc: (88.00%) (8876/10000)
percent tensor([0.5888, 0.4112], device='cuda:0')
percent tensor([0.5849, 0.4151], device='cuda:0')
percent tensor([0.5822, 0.4178], device='cuda:0')
percent tensor([0.6673, 0.3327], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.6843, 0.3157], device='cuda:0')
percent tensor([0.7174, 0.2826], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 198 | Batch_idx: 0 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 198 | Batch_idx: 10 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (1383/1408)
Epoch: 198 | Batch_idx: 20 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (2628/2688)
Epoch: 198 | Batch_idx: 30 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (3876/3968)
Epoch: 198 | Batch_idx: 40 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 198 | Batch_idx: 50 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (6370/6528)
Epoch: 198 | Batch_idx: 60 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (7618/7808)
Epoch: 198 | Batch_idx: 70 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (8866/9088)
Epoch: 198 | Batch_idx: 80 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (10112/10368)
Epoch: 198 | Batch_idx: 90 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (11364/11648)
Epoch: 198 | Batch_idx: 100 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (12611/12928)
Epoch: 198 | Batch_idx: 110 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (13867/14208)
Epoch: 198 | Batch_idx: 120 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (15119/15488)
Epoch: 198 | Batch_idx: 130 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (16373/16768)
Epoch: 198 | Batch_idx: 140 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (17616/18048)
Epoch: 198 | Batch_idx: 150 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (18852/19328)
Epoch: 198 | Batch_idx: 160 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (20100/20608)
Epoch: 198 | Batch_idx: 170 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (21349/21888)
Epoch: 198 | Batch_idx: 180 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (22601/23168)
Epoch: 198 | Batch_idx: 190 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (23849/24448)
Epoch: 198 | Batch_idx: 200 |  Loss: (0.0762) |  Loss2: (0.0000) | Acc: (97.00%) (25091/25728)
Epoch: 198 | Batch_idx: 210 |  Loss: (0.0756) |  Loss2: (0.0000) | Acc: (97.00%) (26342/27008)
Epoch: 198 | Batch_idx: 220 |  Loss: (0.0755) |  Loss2: (0.0000) | Acc: (97.00%) (27590/28288)
Epoch: 198 | Batch_idx: 230 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (28843/29568)
Epoch: 198 | Batch_idx: 240 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (30093/30848)
Epoch: 198 | Batch_idx: 250 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (31355/32128)
Epoch: 198 | Batch_idx: 260 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (32609/33408)
Epoch: 198 | Batch_idx: 270 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (33864/34688)
Epoch: 198 | Batch_idx: 280 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (35113/35968)
Epoch: 198 | Batch_idx: 290 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (36368/37248)
Epoch: 198 | Batch_idx: 300 |  Loss: (0.0734) |  Loss2: (0.0000) | Acc: (97.00%) (37617/38528)
Epoch: 198 | Batch_idx: 310 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (38877/39808)
Epoch: 198 | Batch_idx: 320 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (40134/41088)
Epoch: 198 | Batch_idx: 330 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (41389/42368)
Epoch: 198 | Batch_idx: 340 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (42628/43648)
Epoch: 198 | Batch_idx: 350 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (43880/44928)
Epoch: 198 | Batch_idx: 360 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (45136/46208)
Epoch: 198 | Batch_idx: 370 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (46385/47488)
Epoch: 198 | Batch_idx: 380 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (47637/48768)
Epoch: 198 | Batch_idx: 390 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (48842/50000)
# TEST : Loss: (0.4125) | Acc: (88.00%) (8877/10000)
percent tensor([0.5881, 0.4119], device='cuda:0')
percent tensor([0.5851, 0.4149], device='cuda:0')
percent tensor([0.5828, 0.4172], device='cuda:0')
percent tensor([0.6660, 0.3340], device='cuda:0')
percent tensor([0.6300, 0.3700], device='cuda:0')
percent tensor([0.6804, 0.3196], device='cuda:0')
percent tensor([0.7283, 0.2717], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 199 | Batch_idx: 0 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 199 | Batch_idx: 10 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 199 | Batch_idx: 20 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (2627/2688)
Epoch: 199 | Batch_idx: 30 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 199 | Batch_idx: 40 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (98.00%) (5148/5248)
Epoch: 199 | Batch_idx: 50 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (6397/6528)
Epoch: 199 | Batch_idx: 60 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (7649/7808)
Epoch: 199 | Batch_idx: 70 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (8900/9088)
Epoch: 199 | Batch_idx: 80 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (10148/10368)
Epoch: 199 | Batch_idx: 90 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (11405/11648)
Epoch: 199 | Batch_idx: 100 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (12644/12928)
Epoch: 199 | Batch_idx: 110 |  Loss: (0.0676) |  Loss2: (0.0000) | Acc: (97.00%) (13895/14208)
Epoch: 199 | Batch_idx: 120 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (15143/15488)
Epoch: 199 | Batch_idx: 130 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (16397/16768)
Epoch: 199 | Batch_idx: 140 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (17646/18048)
Epoch: 199 | Batch_idx: 150 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (18891/19328)
Epoch: 199 | Batch_idx: 160 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (20143/20608)
Epoch: 199 | Batch_idx: 170 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (21389/21888)
Epoch: 199 | Batch_idx: 180 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (22643/23168)
Epoch: 199 | Batch_idx: 190 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (23890/24448)
Epoch: 199 | Batch_idx: 200 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (25139/25728)
Epoch: 199 | Batch_idx: 210 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (26392/27008)
Epoch: 199 | Batch_idx: 220 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (27638/28288)
Epoch: 199 | Batch_idx: 230 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (28884/29568)
Epoch: 199 | Batch_idx: 240 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (30128/30848)
Epoch: 199 | Batch_idx: 250 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (31392/32128)
Epoch: 199 | Batch_idx: 260 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (32644/33408)
Epoch: 199 | Batch_idx: 270 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (33900/34688)
Epoch: 199 | Batch_idx: 280 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (35155/35968)
Epoch: 199 | Batch_idx: 290 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (36408/37248)
Epoch: 199 | Batch_idx: 300 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (37653/38528)
Epoch: 199 | Batch_idx: 310 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (38894/39808)
Epoch: 199 | Batch_idx: 320 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (40146/41088)
Epoch: 199 | Batch_idx: 330 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (41397/42368)
Epoch: 199 | Batch_idx: 340 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (42653/43648)
Epoch: 199 | Batch_idx: 350 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (43887/44928)
Epoch: 199 | Batch_idx: 360 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (45148/46208)
Epoch: 199 | Batch_idx: 370 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (46404/47488)
Epoch: 199 | Batch_idx: 380 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (47651/48768)
Epoch: 199 | Batch_idx: 390 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (48847/50000)
# TEST : Loss: (0.4086) | Acc: (89.00%) (8909/10000)
percent tensor([0.5886, 0.4114], device='cuda:0')
percent tensor([0.5841, 0.4159], device='cuda:0')
percent tensor([0.5833, 0.4167], device='cuda:0')
percent tensor([0.6697, 0.3303], device='cuda:0')
percent tensor([0.6339, 0.3661], device='cuda:0')
percent tensor([0.6842, 0.3158], device='cuda:0')
percent tensor([0.7305, 0.2695], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
True Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=128, out_features=16, bias=False)
False Linear(in_features=16, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=256, out_features=32, bias=False)
False Linear(in_features=32, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=512, out_features=64, bias=False)
False Linear(in_features=64, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
True Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
True BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
False Linear(in_features=1024, out_features=128, bias=False)
False Linear(in_features=128, out_features=2, bias=False)
True Linear(in_features=512, out_features=10, bias=True)
Epoch: 200 | Batch_idx: 0 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 200 | Batch_idx: 10 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 200 | Batch_idx: 20 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (98.00%) (2635/2688)
Epoch: 200 | Batch_idx: 30 |  Loss: (0.0645) |  Loss2: (0.0000) | Acc: (97.00%) (3888/3968)
Epoch: 200 | Batch_idx: 40 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (5133/5248)
Epoch: 200 | Batch_idx: 50 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (6386/6528)
Epoch: 200 | Batch_idx: 60 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (7640/7808)
Epoch: 200 | Batch_idx: 70 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (8892/9088)
Epoch: 200 | Batch_idx: 80 |  Loss: (0.0675) |  Loss2: (0.0000) | Acc: (97.00%) (10145/10368)
Epoch: 200 | Batch_idx: 90 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (11394/11648)
Epoch: 200 | Batch_idx: 100 |  Loss: (0.0694) |  Loss2: (0.0000) | Acc: (97.00%) (12638/12928)
Epoch: 200 | Batch_idx: 110 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (13890/14208)
Epoch: 200 | Batch_idx: 120 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (15132/15488)
Epoch: 200 | Batch_idx: 130 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (16378/16768)
Epoch: 200 | Batch_idx: 140 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (17633/18048)
Epoch: 200 | Batch_idx: 150 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (18889/19328)
Epoch: 200 | Batch_idx: 160 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (20138/20608)
Epoch: 200 | Batch_idx: 170 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (21381/21888)
Epoch: 200 | Batch_idx: 180 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (22629/23168)
Epoch: 200 | Batch_idx: 190 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (23873/24448)
Epoch: 200 | Batch_idx: 200 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (25129/25728)
Epoch: 200 | Batch_idx: 210 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (26379/27008)
Epoch: 200 | Batch_idx: 220 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (27632/28288)
Epoch: 200 | Batch_idx: 230 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (28882/29568)
Epoch: 200 | Batch_idx: 240 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (30126/30848)
Epoch: 200 | Batch_idx: 250 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (31373/32128)
Epoch: 200 | Batch_idx: 260 |  Loss: (0.0736) |  Loss2: (0.0000) | Acc: (97.00%) (32609/33408)
Epoch: 200 | Batch_idx: 270 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (33847/34688)
Epoch: 200 | Batch_idx: 280 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (35083/35968)
Epoch: 200 | Batch_idx: 290 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (36333/37248)
Epoch: 200 | Batch_idx: 300 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (37589/38528)
Epoch: 200 | Batch_idx: 310 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (38836/39808)
Epoch: 200 | Batch_idx: 320 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (40081/41088)
Epoch: 200 | Batch_idx: 330 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (41323/42368)
Epoch: 200 | Batch_idx: 340 |  Loss: (0.0758) |  Loss2: (0.0000) | Acc: (97.00%) (42565/43648)
Epoch: 200 | Batch_idx: 350 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (43819/44928)
Epoch: 200 | Batch_idx: 360 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (45055/46208)
Epoch: 200 | Batch_idx: 370 |  Loss: (0.0764) |  Loss2: (0.0000) | Acc: (97.00%) (46300/47488)
Epoch: 200 | Batch_idx: 380 |  Loss: (0.0766) |  Loss2: (0.0000) | Acc: (97.00%) (47545/48768)
Epoch: 200 | Batch_idx: 390 |  Loss: (0.0765) |  Loss2: (0.0000) | Acc: (97.00%) (48747/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_200.pth.tar'
# TEST : Loss: (0.4785) | Acc: (87.00%) (8778/10000)
percent tensor([0.5895, 0.4105], device='cuda:0')
percent tensor([0.5858, 0.4142], device='cuda:0')
percent tensor([0.5829, 0.4171], device='cuda:0')
percent tensor([0.6736, 0.3264], device='cuda:0')
percent tensor([0.6342, 0.3658], device='cuda:0')
percent tensor([0.6881, 0.3119], device='cuda:0')
percent tensor([0.7339, 0.2661], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.0417, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(828.3171, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(831.4186, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1512.1074, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(476.9928, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2301.2979, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4247.3857, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1345.1907, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6263.6299, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11532.1895, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3782.1453, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15984.6445, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 201 | Batch_idx: 0 |  Loss: (0.0962) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 201 | Batch_idx: 10 |  Loss: (0.0917) |  Loss2: (0.0000) | Acc: (96.00%) (1362/1408)
Epoch: 201 | Batch_idx: 20 |  Loss: (0.0818) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 201 | Batch_idx: 30 |  Loss: (0.0825) |  Loss2: (0.0000) | Acc: (97.00%) (3861/3968)
Epoch: 201 | Batch_idx: 40 |  Loss: (0.0751) |  Loss2: (0.0000) | Acc: (97.00%) (5122/5248)
Epoch: 201 | Batch_idx: 50 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (6375/6528)
Epoch: 201 | Batch_idx: 60 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (7621/7808)
Epoch: 201 | Batch_idx: 70 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (8870/9088)
Epoch: 201 | Batch_idx: 80 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (10117/10368)
Epoch: 201 | Batch_idx: 90 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (11367/11648)
Epoch: 201 | Batch_idx: 100 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (12621/12928)
Epoch: 201 | Batch_idx: 110 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (13861/14208)
Epoch: 201 | Batch_idx: 120 |  Loss: (0.0749) |  Loss2: (0.0000) | Acc: (97.00%) (15100/15488)
Epoch: 201 | Batch_idx: 130 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (16357/16768)
Epoch: 201 | Batch_idx: 140 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (17601/18048)
Epoch: 201 | Batch_idx: 150 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (18846/19328)
Epoch: 201 | Batch_idx: 160 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (20087/20608)
Epoch: 201 | Batch_idx: 170 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (97.00%) (21339/21888)
Epoch: 201 | Batch_idx: 180 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (22592/23168)
Epoch: 201 | Batch_idx: 190 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (23824/24448)
Epoch: 201 | Batch_idx: 200 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (25070/25728)
Epoch: 201 | Batch_idx: 210 |  Loss: (0.0761) |  Loss2: (0.0000) | Acc: (97.00%) (26312/27008)
Epoch: 201 | Batch_idx: 220 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (27565/28288)
Epoch: 201 | Batch_idx: 230 |  Loss: (0.0753) |  Loss2: (0.0000) | Acc: (97.00%) (28815/29568)
Epoch: 201 | Batch_idx: 240 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (97.00%) (30059/30848)
Epoch: 201 | Batch_idx: 250 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (31313/32128)
Epoch: 201 | Batch_idx: 260 |  Loss: (0.0767) |  Loss2: (0.0000) | Acc: (97.00%) (32548/33408)
Epoch: 201 | Batch_idx: 270 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (33795/34688)
Epoch: 201 | Batch_idx: 280 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (35036/35968)
Epoch: 201 | Batch_idx: 290 |  Loss: (0.0771) |  Loss2: (0.0000) | Acc: (97.00%) (36285/37248)
Epoch: 201 | Batch_idx: 300 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (37524/38528)
Epoch: 201 | Batch_idx: 310 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (97.00%) (38758/39808)
Epoch: 201 | Batch_idx: 320 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (40004/41088)
Epoch: 201 | Batch_idx: 330 |  Loss: (0.0783) |  Loss2: (0.0000) | Acc: (97.00%) (41249/42368)
Epoch: 201 | Batch_idx: 340 |  Loss: (0.0782) |  Loss2: (0.0000) | Acc: (97.00%) (42498/43648)
Epoch: 201 | Batch_idx: 350 |  Loss: (0.0777) |  Loss2: (0.0000) | Acc: (97.00%) (43749/44928)
Epoch: 201 | Batch_idx: 360 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (45006/46208)
Epoch: 201 | Batch_idx: 370 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (46257/47488)
Epoch: 201 | Batch_idx: 380 |  Loss: (0.0772) |  Loss2: (0.0000) | Acc: (97.00%) (47512/48768)
Epoch: 201 | Batch_idx: 390 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (48708/50000)
# TEST : Loss: (0.4732) | Acc: (88.00%) (8824/10000)
percent tensor([0.5894, 0.4106], device='cuda:0')
percent tensor([0.5859, 0.4141], device='cuda:0')
percent tensor([0.5853, 0.4147], device='cuda:0')
percent tensor([0.6764, 0.3236], device='cuda:0')
percent tensor([0.6388, 0.3612], device='cuda:0')
percent tensor([0.6917, 0.3083], device='cuda:0')
percent tensor([0.7468, 0.2532], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 202 | Batch_idx: 0 |  Loss: (0.0747) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 202 | Batch_idx: 10 |  Loss: (0.0641) |  Loss2: (0.0000) | Acc: (97.00%) (1375/1408)
Epoch: 202 | Batch_idx: 20 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (2619/2688)
Epoch: 202 | Batch_idx: 30 |  Loss: (0.0750) |  Loss2: (0.0000) | Acc: (97.00%) (3856/3968)
Epoch: 202 | Batch_idx: 40 |  Loss: (0.0742) |  Loss2: (0.0000) | Acc: (97.00%) (5099/5248)
Epoch: 202 | Batch_idx: 50 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (6348/6528)
Epoch: 202 | Batch_idx: 60 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (7599/7808)
Epoch: 202 | Batch_idx: 70 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (8855/9088)
Epoch: 202 | Batch_idx: 80 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (10095/10368)
Epoch: 202 | Batch_idx: 90 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (11340/11648)
Epoch: 202 | Batch_idx: 100 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (12600/12928)
Epoch: 202 | Batch_idx: 110 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (13848/14208)
Epoch: 202 | Batch_idx: 120 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (15101/15488)
Epoch: 202 | Batch_idx: 130 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (16359/16768)
Epoch: 202 | Batch_idx: 140 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (17612/18048)
Epoch: 202 | Batch_idx: 150 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (18861/19328)
Epoch: 202 | Batch_idx: 160 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (20107/20608)
Epoch: 202 | Batch_idx: 170 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (21358/21888)
Epoch: 202 | Batch_idx: 180 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (22617/23168)
Epoch: 202 | Batch_idx: 190 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (23856/24448)
Epoch: 202 | Batch_idx: 200 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (25113/25728)
Epoch: 202 | Batch_idx: 210 |  Loss: (0.0703) |  Loss2: (0.0000) | Acc: (97.00%) (26357/27008)
Epoch: 202 | Batch_idx: 220 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (27592/28288)
Epoch: 202 | Batch_idx: 230 |  Loss: (0.0714) |  Loss2: (0.0000) | Acc: (97.00%) (28839/29568)
Epoch: 202 | Batch_idx: 240 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (30095/30848)
Epoch: 202 | Batch_idx: 250 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (31341/32128)
Epoch: 202 | Batch_idx: 260 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (32584/33408)
Epoch: 202 | Batch_idx: 270 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (33841/34688)
Epoch: 202 | Batch_idx: 280 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (35087/35968)
Epoch: 202 | Batch_idx: 290 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (36335/37248)
Epoch: 202 | Batch_idx: 300 |  Loss: (0.0719) |  Loss2: (0.0000) | Acc: (97.00%) (37581/38528)
Epoch: 202 | Batch_idx: 310 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (38832/39808)
Epoch: 202 | Batch_idx: 320 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (40085/41088)
Epoch: 202 | Batch_idx: 330 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (41333/42368)
Epoch: 202 | Batch_idx: 340 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (42573/43648)
Epoch: 202 | Batch_idx: 350 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (43822/44928)
Epoch: 202 | Batch_idx: 360 |  Loss: (0.0731) |  Loss2: (0.0000) | Acc: (97.00%) (45069/46208)
Epoch: 202 | Batch_idx: 370 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (46302/47488)
Epoch: 202 | Batch_idx: 380 |  Loss: (0.0737) |  Loss2: (0.0000) | Acc: (97.00%) (47549/48768)
Epoch: 202 | Batch_idx: 390 |  Loss: (0.0741) |  Loss2: (0.0000) | Acc: (97.00%) (48741/50000)
# TEST : Loss: (0.4365) | Acc: (88.00%) (8857/10000)
percent tensor([0.5897, 0.4103], device='cuda:0')
percent tensor([0.5871, 0.4129], device='cuda:0')
percent tensor([0.5894, 0.4106], device='cuda:0')
percent tensor([0.6788, 0.3212], device='cuda:0')
percent tensor([0.6389, 0.3611], device='cuda:0')
percent tensor([0.6996, 0.3004], device='cuda:0')
percent tensor([0.7386, 0.2614], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 203 | Batch_idx: 0 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 203 | Batch_idx: 10 |  Loss: (0.0792) |  Loss2: (0.0000) | Acc: (97.00%) (1369/1408)
Epoch: 203 | Batch_idx: 20 |  Loss: (0.0760) |  Loss2: (0.0000) | Acc: (97.00%) (2617/2688)
Epoch: 203 | Batch_idx: 30 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (3871/3968)
Epoch: 203 | Batch_idx: 40 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (5124/5248)
Epoch: 203 | Batch_idx: 50 |  Loss: (0.0710) |  Loss2: (0.0000) | Acc: (97.00%) (6378/6528)
Epoch: 203 | Batch_idx: 60 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (7642/7808)
Epoch: 203 | Batch_idx: 70 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (8888/9088)
Epoch: 203 | Batch_idx: 80 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (10132/10368)
Epoch: 203 | Batch_idx: 90 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (11365/11648)
Epoch: 203 | Batch_idx: 100 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (12613/12928)
Epoch: 203 | Batch_idx: 110 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (13860/14208)
Epoch: 203 | Batch_idx: 120 |  Loss: (0.0735) |  Loss2: (0.0000) | Acc: (97.00%) (15115/15488)
Epoch: 203 | Batch_idx: 130 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (97.00%) (16358/16768)
Epoch: 203 | Batch_idx: 140 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (17614/18048)
Epoch: 203 | Batch_idx: 150 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (18864/19328)
Epoch: 203 | Batch_idx: 160 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (20117/20608)
Epoch: 203 | Batch_idx: 170 |  Loss: (0.0715) |  Loss2: (0.0000) | Acc: (97.00%) (21373/21888)
Epoch: 203 | Batch_idx: 180 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (22616/23168)
Epoch: 203 | Batch_idx: 190 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (23858/24448)
Epoch: 203 | Batch_idx: 200 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (25104/25728)
Epoch: 203 | Batch_idx: 210 |  Loss: (0.0739) |  Loss2: (0.0000) | Acc: (97.00%) (26345/27008)
Epoch: 203 | Batch_idx: 220 |  Loss: (0.0748) |  Loss2: (0.0000) | Acc: (97.00%) (27584/28288)
Epoch: 203 | Batch_idx: 230 |  Loss: (0.0743) |  Loss2: (0.0000) | Acc: (97.00%) (28839/29568)
Epoch: 203 | Batch_idx: 240 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (30096/30848)
Epoch: 203 | Batch_idx: 250 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (31333/32128)
Epoch: 203 | Batch_idx: 260 |  Loss: (0.0746) |  Loss2: (0.0000) | Acc: (97.00%) (32577/33408)
Epoch: 203 | Batch_idx: 270 |  Loss: (0.0740) |  Loss2: (0.0000) | Acc: (97.00%) (33837/34688)
Epoch: 203 | Batch_idx: 280 |  Loss: (0.0738) |  Loss2: (0.0000) | Acc: (97.00%) (35092/35968)
Epoch: 203 | Batch_idx: 290 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (36340/37248)
Epoch: 203 | Batch_idx: 300 |  Loss: (0.0745) |  Loss2: (0.0000) | Acc: (97.00%) (37583/38528)
Epoch: 203 | Batch_idx: 310 |  Loss: (0.0754) |  Loss2: (0.0000) | Acc: (97.00%) (38816/39808)
Epoch: 203 | Batch_idx: 320 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (40063/41088)
Epoch: 203 | Batch_idx: 330 |  Loss: (0.0763) |  Loss2: (0.0000) | Acc: (97.00%) (41305/42368)
Epoch: 203 | Batch_idx: 340 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (42543/43648)
Epoch: 203 | Batch_idx: 350 |  Loss: (0.0773) |  Loss2: (0.0000) | Acc: (97.00%) (43776/44928)
Epoch: 203 | Batch_idx: 360 |  Loss: (0.0770) |  Loss2: (0.0000) | Acc: (97.00%) (45029/46208)
Epoch: 203 | Batch_idx: 370 |  Loss: (0.0774) |  Loss2: (0.0000) | Acc: (97.00%) (46268/47488)
Epoch: 203 | Batch_idx: 380 |  Loss: (0.0775) |  Loss2: (0.0000) | Acc: (97.00%) (47511/48768)
Epoch: 203 | Batch_idx: 390 |  Loss: (0.0776) |  Loss2: (0.0000) | Acc: (97.00%) (48708/50000)
# TEST : Loss: (0.4516) | Acc: (88.00%) (8837/10000)
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.5868, 0.4132], device='cuda:0')
percent tensor([0.5911, 0.4089], device='cuda:0')
percent tensor([0.6837, 0.3163], device='cuda:0')
percent tensor([0.6448, 0.3552], device='cuda:0')
percent tensor([0.7000, 0.3000], device='cuda:0')
percent tensor([0.7473, 0.2527], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 204 | Batch_idx: 0 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 204 | Batch_idx: 10 |  Loss: (0.0757) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 204 | Batch_idx: 20 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (2620/2688)
Epoch: 204 | Batch_idx: 30 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (3874/3968)
Epoch: 204 | Batch_idx: 40 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (5123/5248)
Epoch: 204 | Batch_idx: 50 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (6367/6528)
Epoch: 204 | Batch_idx: 60 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (7620/7808)
Epoch: 204 | Batch_idx: 70 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (8867/9088)
Epoch: 204 | Batch_idx: 80 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (10124/10368)
Epoch: 204 | Batch_idx: 90 |  Loss: (0.0698) |  Loss2: (0.0000) | Acc: (97.00%) (11373/11648)
Epoch: 204 | Batch_idx: 100 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (12627/12928)
Epoch: 204 | Batch_idx: 110 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (13880/14208)
Epoch: 204 | Batch_idx: 120 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (15120/15488)
Epoch: 204 | Batch_idx: 130 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (16372/16768)
Epoch: 204 | Batch_idx: 140 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (17621/18048)
Epoch: 204 | Batch_idx: 150 |  Loss: (0.0720) |  Loss2: (0.0000) | Acc: (97.00%) (18863/19328)
Epoch: 204 | Batch_idx: 160 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (20113/20608)
Epoch: 204 | Batch_idx: 170 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (21358/21888)
Epoch: 204 | Batch_idx: 180 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (22609/23168)
Epoch: 204 | Batch_idx: 190 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (23860/24448)
Epoch: 204 | Batch_idx: 200 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (25104/25728)
Epoch: 204 | Batch_idx: 210 |  Loss: (0.0733) |  Loss2: (0.0000) | Acc: (97.00%) (26355/27008)
Epoch: 204 | Batch_idx: 220 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (27606/28288)
Epoch: 204 | Batch_idx: 230 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (28850/29568)
Epoch: 204 | Batch_idx: 240 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (30105/30848)
Epoch: 204 | Batch_idx: 250 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (31355/32128)
Epoch: 204 | Batch_idx: 260 |  Loss: (0.0723) |  Loss2: (0.0000) | Acc: (97.00%) (32609/33408)
Epoch: 204 | Batch_idx: 270 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (33855/34688)
Epoch: 204 | Batch_idx: 280 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (35100/35968)
Epoch: 204 | Batch_idx: 290 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (36353/37248)
Epoch: 204 | Batch_idx: 300 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (37598/38528)
Epoch: 204 | Batch_idx: 310 |  Loss: (0.0729) |  Loss2: (0.0000) | Acc: (97.00%) (38846/39808)
Epoch: 204 | Batch_idx: 320 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (40097/41088)
Epoch: 204 | Batch_idx: 330 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (41350/42368)
Epoch: 204 | Batch_idx: 340 |  Loss: (0.0724) |  Loss2: (0.0000) | Acc: (97.00%) (42596/43648)
Epoch: 204 | Batch_idx: 350 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (43845/44928)
Epoch: 204 | Batch_idx: 360 |  Loss: (0.0726) |  Loss2: (0.0000) | Acc: (97.00%) (45097/46208)
Epoch: 204 | Batch_idx: 370 |  Loss: (0.0728) |  Loss2: (0.0000) | Acc: (97.00%) (46344/47488)
Epoch: 204 | Batch_idx: 380 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (47590/48768)
Epoch: 204 | Batch_idx: 390 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (48798/50000)
# TEST : Loss: (0.4950) | Acc: (87.00%) (8784/10000)
percent tensor([0.5952, 0.4048], device='cuda:0')
percent tensor([0.5879, 0.4121], device='cuda:0')
percent tensor([0.5886, 0.4114], device='cuda:0')
percent tensor([0.6820, 0.3180], device='cuda:0')
percent tensor([0.6425, 0.3575], device='cuda:0')
percent tensor([0.7046, 0.2954], device='cuda:0')
percent tensor([0.7509, 0.2491], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 205 | Batch_idx: 0 |  Loss: (0.0784) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 205 | Batch_idx: 10 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (1371/1408)
Epoch: 205 | Batch_idx: 20 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (2618/2688)
Epoch: 205 | Batch_idx: 30 |  Loss: (0.0717) |  Loss2: (0.0000) | Acc: (97.00%) (3864/3968)
Epoch: 205 | Batch_idx: 40 |  Loss: (0.0722) |  Loss2: (0.0000) | Acc: (97.00%) (5112/5248)
Epoch: 205 | Batch_idx: 50 |  Loss: (0.0725) |  Loss2: (0.0000) | Acc: (97.00%) (6358/6528)
Epoch: 205 | Batch_idx: 60 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (7615/7808)
Epoch: 205 | Batch_idx: 70 |  Loss: (0.0712) |  Loss2: (0.0000) | Acc: (97.00%) (8857/9088)
Epoch: 205 | Batch_idx: 80 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (10105/10368)
Epoch: 205 | Batch_idx: 90 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (11361/11648)
Epoch: 205 | Batch_idx: 100 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (12609/12928)
Epoch: 205 | Batch_idx: 110 |  Loss: (0.0704) |  Loss2: (0.0000) | Acc: (97.00%) (13853/14208)
Epoch: 205 | Batch_idx: 120 |  Loss: (0.0700) |  Loss2: (0.0000) | Acc: (97.00%) (15108/15488)
Epoch: 205 | Batch_idx: 130 |  Loss: (0.0701) |  Loss2: (0.0000) | Acc: (97.00%) (16354/16768)
Epoch: 205 | Batch_idx: 140 |  Loss: (0.0699) |  Loss2: (0.0000) | Acc: (97.00%) (17607/18048)
Epoch: 205 | Batch_idx: 150 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (18865/19328)
Epoch: 205 | Batch_idx: 160 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (20114/20608)
Epoch: 205 | Batch_idx: 170 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (21363/21888)
Epoch: 205 | Batch_idx: 180 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (22620/23168)
Epoch: 205 | Batch_idx: 190 |  Loss: (0.0692) |  Loss2: (0.0000) | Acc: (97.00%) (23869/24448)
Epoch: 205 | Batch_idx: 200 |  Loss: (0.0691) |  Loss2: (0.0000) | Acc: (97.00%) (25121/25728)
Epoch: 205 | Batch_idx: 210 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (26374/27008)
Epoch: 205 | Batch_idx: 220 |  Loss: (0.0685) |  Loss2: (0.0000) | Acc: (97.00%) (27623/28288)
Epoch: 205 | Batch_idx: 230 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (28869/29568)
Epoch: 205 | Batch_idx: 240 |  Loss: (0.0688) |  Loss2: (0.0000) | Acc: (97.00%) (30120/30848)
Epoch: 205 | Batch_idx: 250 |  Loss: (0.0686) |  Loss2: (0.0000) | Acc: (97.00%) (31368/32128)
Epoch: 205 | Batch_idx: 260 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (32608/33408)
Epoch: 205 | Batch_idx: 270 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (33853/34688)
Epoch: 205 | Batch_idx: 280 |  Loss: (0.0702) |  Loss2: (0.0000) | Acc: (97.00%) (35102/35968)
Epoch: 205 | Batch_idx: 290 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (36344/37248)
Epoch: 205 | Batch_idx: 300 |  Loss: (0.0709) |  Loss2: (0.0000) | Acc: (97.00%) (37590/38528)
Epoch: 205 | Batch_idx: 310 |  Loss: (0.0706) |  Loss2: (0.0000) | Acc: (97.00%) (38845/39808)
Epoch: 205 | Batch_idx: 320 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (40098/41088)
Epoch: 205 | Batch_idx: 330 |  Loss: (0.0707) |  Loss2: (0.0000) | Acc: (97.00%) (41344/42368)
Epoch: 205 | Batch_idx: 340 |  Loss: (0.0705) |  Loss2: (0.0000) | Acc: (97.00%) (42597/43648)
Epoch: 205 | Batch_idx: 350 |  Loss: (0.0708) |  Loss2: (0.0000) | Acc: (97.00%) (43840/44928)
Epoch: 205 | Batch_idx: 360 |  Loss: (0.0711) |  Loss2: (0.0000) | Acc: (97.00%) (45083/46208)
Epoch: 205 | Batch_idx: 370 |  Loss: (0.0713) |  Loss2: (0.0000) | Acc: (97.00%) (46337/47488)
Epoch: 205 | Batch_idx: 380 |  Loss: (0.0716) |  Loss2: (0.0000) | Acc: (97.00%) (47579/48768)
Epoch: 205 | Batch_idx: 390 |  Loss: (0.0718) |  Loss2: (0.0000) | Acc: (97.00%) (48777/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_205.pth.tar'
# TEST : Loss: (0.4605) | Acc: (88.00%) (8875/10000)
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.5899, 0.4101], device='cuda:0')
percent tensor([0.5919, 0.4081], device='cuda:0')
percent tensor([0.6810, 0.3190], device='cuda:0')
percent tensor([0.6469, 0.3531], device='cuda:0')
percent tensor([0.7090, 0.2910], device='cuda:0')
percent tensor([0.7514, 0.2486], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 206 | Batch_idx: 0 |  Loss: (0.0759) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 206 | Batch_idx: 10 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (1372/1408)
Epoch: 206 | Batch_idx: 20 |  Loss: (0.0696) |  Loss2: (0.0000) | Acc: (97.00%) (2621/2688)
Epoch: 206 | Batch_idx: 30 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (3876/3968)
Epoch: 206 | Batch_idx: 40 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (5138/5248)
Epoch: 206 | Batch_idx: 50 |  Loss: (0.0643) |  Loss2: (0.0000) | Acc: (97.00%) (6388/6528)
Epoch: 206 | Batch_idx: 60 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (7636/7808)
Epoch: 206 | Batch_idx: 70 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (8890/9088)
Epoch: 206 | Batch_idx: 80 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (10145/10368)
Epoch: 206 | Batch_idx: 90 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (11394/11648)
Epoch: 206 | Batch_idx: 100 |  Loss: (0.0672) |  Loss2: (0.0000) | Acc: (97.00%) (12644/12928)
Epoch: 206 | Batch_idx: 110 |  Loss: (0.0671) |  Loss2: (0.0000) | Acc: (97.00%) (13896/14208)
Epoch: 206 | Batch_idx: 120 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (15148/15488)
Epoch: 206 | Batch_idx: 130 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (16406/16768)
Epoch: 206 | Batch_idx: 140 |  Loss: (0.0661) |  Loss2: (0.0000) | Acc: (97.00%) (17654/18048)
Epoch: 206 | Batch_idx: 150 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (18903/19328)
Epoch: 206 | Batch_idx: 160 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (20154/20608)
Epoch: 206 | Batch_idx: 170 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (21405/21888)
Epoch: 206 | Batch_idx: 180 |  Loss: (0.0662) |  Loss2: (0.0000) | Acc: (97.00%) (22657/23168)
Epoch: 206 | Batch_idx: 190 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (23911/24448)
Epoch: 206 | Batch_idx: 200 |  Loss: (0.0679) |  Loss2: (0.0000) | Acc: (97.00%) (25148/25728)
Epoch: 206 | Batch_idx: 210 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (26397/27008)
Epoch: 206 | Batch_idx: 220 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (27642/28288)
Epoch: 206 | Batch_idx: 230 |  Loss: (0.0690) |  Loss2: (0.0000) | Acc: (97.00%) (28892/29568)
Epoch: 206 | Batch_idx: 240 |  Loss: (0.0695) |  Loss2: (0.0000) | Acc: (97.00%) (30139/30848)
Epoch: 206 | Batch_idx: 250 |  Loss: (0.0697) |  Loss2: (0.0000) | Acc: (97.00%) (31392/32128)
Epoch: 206 | Batch_idx: 260 |  Loss: (0.0693) |  Loss2: (0.0000) | Acc: (97.00%) (32648/33408)
Epoch: 206 | Batch_idx: 270 |  Loss: (0.0689) |  Loss2: (0.0000) | Acc: (97.00%) (33903/34688)
Epoch: 206 | Batch_idx: 280 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (35163/35968)
Epoch: 206 | Batch_idx: 290 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (36415/37248)
Epoch: 206 | Batch_idx: 300 |  Loss: (0.0684) |  Loss2: (0.0000) | Acc: (97.00%) (37665/38528)
Epoch: 206 | Batch_idx: 310 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (38920/39808)
Epoch: 206 | Batch_idx: 320 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (40173/41088)
Epoch: 206 | Batch_idx: 330 |  Loss: (0.0681) |  Loss2: (0.0000) | Acc: (97.00%) (41422/42368)
Epoch: 206 | Batch_idx: 340 |  Loss: (0.0680) |  Loss2: (0.0000) | Acc: (97.00%) (42675/43648)
Epoch: 206 | Batch_idx: 350 |  Loss: (0.0683) |  Loss2: (0.0000) | Acc: (97.00%) (43926/44928)
Epoch: 206 | Batch_idx: 360 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (45179/46208)
Epoch: 206 | Batch_idx: 370 |  Loss: (0.0682) |  Loss2: (0.0000) | Acc: (97.00%) (46429/47488)
Epoch: 206 | Batch_idx: 380 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (47665/48768)
Epoch: 206 | Batch_idx: 390 |  Loss: (0.0687) |  Loss2: (0.0000) | Acc: (97.00%) (48870/50000)
# TEST : Loss: (0.4611) | Acc: (88.00%) (8815/10000)
percent tensor([0.5938, 0.4062], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.5912, 0.4088], device='cuda:0')
percent tensor([0.6806, 0.3194], device='cuda:0')
percent tensor([0.6451, 0.3549], device='cuda:0')
percent tensor([0.7132, 0.2868], device='cuda:0')
percent tensor([0.7585, 0.2415], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 207 | Batch_idx: 0 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 207 | Batch_idx: 10 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 207 | Batch_idx: 20 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 207 | Batch_idx: 30 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (3893/3968)
Epoch: 207 | Batch_idx: 40 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (5148/5248)
Epoch: 207 | Batch_idx: 50 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 207 | Batch_idx: 60 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (7662/7808)
Epoch: 207 | Batch_idx: 70 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (8916/9088)
Epoch: 207 | Batch_idx: 80 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (98.00%) (10174/10368)
Epoch: 207 | Batch_idx: 90 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (11424/11648)
Epoch: 207 | Batch_idx: 100 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (98.00%) (12676/12928)
Epoch: 207 | Batch_idx: 110 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (98.00%) (13929/14208)
Epoch: 207 | Batch_idx: 120 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (98.00%) (15179/15488)
Epoch: 207 | Batch_idx: 130 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (98.00%) (16433/16768)
Epoch: 207 | Batch_idx: 140 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (17686/18048)
Epoch: 207 | Batch_idx: 150 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (18936/19328)
Epoch: 207 | Batch_idx: 160 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (20191/20608)
Epoch: 207 | Batch_idx: 170 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (21446/21888)
Epoch: 207 | Batch_idx: 180 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (22704/23168)
Epoch: 207 | Batch_idx: 190 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (98.00%) (23961/24448)
Epoch: 207 | Batch_idx: 200 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (25223/25728)
Epoch: 207 | Batch_idx: 210 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (26480/27008)
Epoch: 207 | Batch_idx: 220 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (27733/28288)
Epoch: 207 | Batch_idx: 230 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (28993/29568)
Epoch: 207 | Batch_idx: 240 |  Loss: (0.0617) |  Loss2: (0.0000) | Acc: (98.00%) (30245/30848)
Epoch: 207 | Batch_idx: 250 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (31505/32128)
Epoch: 207 | Batch_idx: 260 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (32754/33408)
Epoch: 207 | Batch_idx: 270 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (34002/34688)
Epoch: 207 | Batch_idx: 280 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (35256/35968)
Epoch: 207 | Batch_idx: 290 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (98.00%) (36506/37248)
Epoch: 207 | Batch_idx: 300 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (37756/38528)
Epoch: 207 | Batch_idx: 310 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (39003/39808)
Epoch: 207 | Batch_idx: 320 |  Loss: (0.0631) |  Loss2: (0.0000) | Acc: (97.00%) (40257/41088)
Epoch: 207 | Batch_idx: 330 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (41503/42368)
Epoch: 207 | Batch_idx: 340 |  Loss: (0.0635) |  Loss2: (0.0000) | Acc: (97.00%) (42752/43648)
Epoch: 207 | Batch_idx: 350 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (44000/44928)
Epoch: 207 | Batch_idx: 360 |  Loss: (0.0637) |  Loss2: (0.0000) | Acc: (97.00%) (45255/46208)
Epoch: 207 | Batch_idx: 370 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (46511/47488)
Epoch: 207 | Batch_idx: 380 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (47768/48768)
Epoch: 207 | Batch_idx: 390 |  Loss: (0.0638) |  Loss2: (0.0000) | Acc: (97.00%) (48968/50000)
# TEST : Loss: (0.4467) | Acc: (88.00%) (8893/10000)
percent tensor([0.5933, 0.4067], device='cuda:0')
percent tensor([0.5902, 0.4098], device='cuda:0')
percent tensor([0.5890, 0.4110], device='cuda:0')
percent tensor([0.6874, 0.3126], device='cuda:0')
percent tensor([0.6416, 0.3584], device='cuda:0')
percent tensor([0.7184, 0.2816], device='cuda:0')
percent tensor([0.7530, 0.2470], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 208 | Batch_idx: 0 |  Loss: (0.0873) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 208 | Batch_idx: 10 |  Loss: (0.0727) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 208 | Batch_idx: 20 |  Loss: (0.0674) |  Loss2: (0.0000) | Acc: (97.00%) (2629/2688)
Epoch: 208 | Batch_idx: 30 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (3886/3968)
Epoch: 208 | Batch_idx: 40 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (5140/5248)
Epoch: 208 | Batch_idx: 50 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (6384/6528)
Epoch: 208 | Batch_idx: 60 |  Loss: (0.0677) |  Loss2: (0.0000) | Acc: (97.00%) (7632/7808)
Epoch: 208 | Batch_idx: 70 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (8886/9088)
Epoch: 208 | Batch_idx: 80 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (10139/10368)
Epoch: 208 | Batch_idx: 90 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (11392/11648)
Epoch: 208 | Batch_idx: 100 |  Loss: (0.0668) |  Loss2: (0.0000) | Acc: (97.00%) (12644/12928)
Epoch: 208 | Batch_idx: 110 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (13890/14208)
Epoch: 208 | Batch_idx: 120 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (15139/15488)
Epoch: 208 | Batch_idx: 130 |  Loss: (0.0669) |  Loss2: (0.0000) | Acc: (97.00%) (16390/16768)
Epoch: 208 | Batch_idx: 140 |  Loss: (0.0665) |  Loss2: (0.0000) | Acc: (97.00%) (17645/18048)
Epoch: 208 | Batch_idx: 150 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (18897/19328)
Epoch: 208 | Batch_idx: 160 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (20148/20608)
Epoch: 208 | Batch_idx: 170 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (21393/21888)
Epoch: 208 | Batch_idx: 180 |  Loss: (0.0659) |  Loss2: (0.0000) | Acc: (97.00%) (22654/23168)
Epoch: 208 | Batch_idx: 190 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (23909/24448)
Epoch: 208 | Batch_idx: 200 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (25160/25728)
Epoch: 208 | Batch_idx: 210 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (26415/27008)
Epoch: 208 | Batch_idx: 220 |  Loss: (0.0652) |  Loss2: (0.0000) | Acc: (97.00%) (27673/28288)
Epoch: 208 | Batch_idx: 230 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (28918/29568)
Epoch: 208 | Batch_idx: 240 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (30168/30848)
Epoch: 208 | Batch_idx: 250 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (31426/32128)
Epoch: 208 | Batch_idx: 260 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (32682/33408)
Epoch: 208 | Batch_idx: 270 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (33937/34688)
Epoch: 208 | Batch_idx: 280 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (35186/35968)
Epoch: 208 | Batch_idx: 290 |  Loss: (0.0657) |  Loss2: (0.0000) | Acc: (97.00%) (36434/37248)
Epoch: 208 | Batch_idx: 300 |  Loss: (0.0656) |  Loss2: (0.0000) | Acc: (97.00%) (37685/38528)
Epoch: 208 | Batch_idx: 310 |  Loss: (0.0655) |  Loss2: (0.0000) | Acc: (97.00%) (38938/39808)
Epoch: 208 | Batch_idx: 320 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (40183/41088)
Epoch: 208 | Batch_idx: 330 |  Loss: (0.0660) |  Loss2: (0.0000) | Acc: (97.00%) (41437/42368)
Epoch: 208 | Batch_idx: 340 |  Loss: (0.0658) |  Loss2: (0.0000) | Acc: (97.00%) (42694/43648)
Epoch: 208 | Batch_idx: 350 |  Loss: (0.0663) |  Loss2: (0.0000) | Acc: (97.00%) (43937/44928)
Epoch: 208 | Batch_idx: 360 |  Loss: (0.0666) |  Loss2: (0.0000) | Acc: (97.00%) (45188/46208)
Epoch: 208 | Batch_idx: 370 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (46430/47488)
Epoch: 208 | Batch_idx: 380 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (47693/48768)
Epoch: 208 | Batch_idx: 390 |  Loss: (0.0670) |  Loss2: (0.0000) | Acc: (97.00%) (48882/50000)
# TEST : Loss: (0.4533) | Acc: (88.00%) (8861/10000)
percent tensor([0.5957, 0.4043], device='cuda:0')
percent tensor([0.5928, 0.4072], device='cuda:0')
percent tensor([0.5917, 0.4083], device='cuda:0')
percent tensor([0.6867, 0.3133], device='cuda:0')
percent tensor([0.6430, 0.3570], device='cuda:0')
percent tensor([0.7151, 0.2849], device='cuda:0')
percent tensor([0.7577, 0.2423], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 209 | Batch_idx: 0 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 209 | Batch_idx: 10 |  Loss: (0.0780) |  Loss2: (0.0000) | Acc: (97.00%) (1373/1408)
Epoch: 209 | Batch_idx: 20 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 209 | Batch_idx: 30 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (98.00%) (3890/3968)
Epoch: 209 | Batch_idx: 40 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (5148/5248)
Epoch: 209 | Batch_idx: 50 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (6409/6528)
Epoch: 209 | Batch_idx: 60 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (7674/7808)
Epoch: 209 | Batch_idx: 70 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (8927/9088)
Epoch: 209 | Batch_idx: 80 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (10183/10368)
Epoch: 209 | Batch_idx: 90 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (11435/11648)
Epoch: 209 | Batch_idx: 100 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (12686/12928)
Epoch: 209 | Batch_idx: 110 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (13947/14208)
Epoch: 209 | Batch_idx: 120 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (15201/15488)
Epoch: 209 | Batch_idx: 130 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (16464/16768)
Epoch: 209 | Batch_idx: 140 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (17719/18048)
Epoch: 209 | Batch_idx: 150 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (18971/19328)
Epoch: 209 | Batch_idx: 160 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (20226/20608)
Epoch: 209 | Batch_idx: 170 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (21483/21888)
Epoch: 209 | Batch_idx: 180 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (22741/23168)
Epoch: 209 | Batch_idx: 190 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (98.00%) (23999/24448)
Epoch: 209 | Batch_idx: 200 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (25254/25728)
Epoch: 209 | Batch_idx: 210 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (26503/27008)
Epoch: 209 | Batch_idx: 220 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (27755/28288)
Epoch: 209 | Batch_idx: 230 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (29012/29568)
Epoch: 209 | Batch_idx: 240 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (30261/30848)
Epoch: 209 | Batch_idx: 250 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (98.00%) (31509/32128)
Epoch: 209 | Batch_idx: 260 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (32758/33408)
Epoch: 209 | Batch_idx: 270 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (34013/34688)
Epoch: 209 | Batch_idx: 280 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (35254/35968)
Epoch: 209 | Batch_idx: 290 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (36503/37248)
Epoch: 209 | Batch_idx: 300 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (37756/38528)
Epoch: 209 | Batch_idx: 310 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (98.00%) (39014/39808)
Epoch: 209 | Batch_idx: 320 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (98.00%) (40268/41088)
Epoch: 209 | Batch_idx: 330 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (41514/42368)
Epoch: 209 | Batch_idx: 340 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (42760/43648)
Epoch: 209 | Batch_idx: 350 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (44015/44928)
Epoch: 209 | Batch_idx: 360 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (45277/46208)
Epoch: 209 | Batch_idx: 370 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (46527/47488)
Epoch: 209 | Batch_idx: 380 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (47781/48768)
Epoch: 209 | Batch_idx: 390 |  Loss: (0.0626) |  Loss2: (0.0000) | Acc: (97.00%) (48992/50000)
# TEST : Loss: (0.5027) | Acc: (87.00%) (8773/10000)
percent tensor([0.5949, 0.4051], device='cuda:0')
percent tensor([0.5901, 0.4099], device='cuda:0')
percent tensor([0.5922, 0.4078], device='cuda:0')
percent tensor([0.6869, 0.3131], device='cuda:0')
percent tensor([0.6409, 0.3591], device='cuda:0')
percent tensor([0.7189, 0.2811], device='cuda:0')
percent tensor([0.7560, 0.2440], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 210 | Batch_idx: 0 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 210 | Batch_idx: 10 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 210 | Batch_idx: 20 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (2645/2688)
Epoch: 210 | Batch_idx: 30 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (3889/3968)
Epoch: 210 | Batch_idx: 40 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (5146/5248)
Epoch: 210 | Batch_idx: 50 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (6407/6528)
Epoch: 210 | Batch_idx: 60 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (7662/7808)
Epoch: 210 | Batch_idx: 70 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (8917/9088)
Epoch: 210 | Batch_idx: 80 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (10165/10368)
Epoch: 210 | Batch_idx: 90 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (11427/11648)
Epoch: 210 | Batch_idx: 100 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (12686/12928)
Epoch: 210 | Batch_idx: 110 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (13934/14208)
Epoch: 210 | Batch_idx: 120 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (15181/15488)
Epoch: 210 | Batch_idx: 130 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (16436/16768)
Epoch: 210 | Batch_idx: 140 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (17692/18048)
Epoch: 210 | Batch_idx: 150 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (98.00%) (18945/19328)
Epoch: 210 | Batch_idx: 160 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (20198/20608)
Epoch: 210 | Batch_idx: 170 |  Loss: (0.0609) |  Loss2: (0.0000) | Acc: (98.00%) (21454/21888)
Epoch: 210 | Batch_idx: 180 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (22709/23168)
Epoch: 210 | Batch_idx: 190 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (23960/24448)
Epoch: 210 | Batch_idx: 200 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (25218/25728)
Epoch: 210 | Batch_idx: 210 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (98.00%) (26478/27008)
Epoch: 210 | Batch_idx: 220 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (98.00%) (27725/28288)
Epoch: 210 | Batch_idx: 230 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (28983/29568)
Epoch: 210 | Batch_idx: 240 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (30221/30848)
Epoch: 210 | Batch_idx: 250 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (31481/32128)
Epoch: 210 | Batch_idx: 260 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (32722/33408)
Epoch: 210 | Batch_idx: 270 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (33973/34688)
Epoch: 210 | Batch_idx: 280 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (35225/35968)
Epoch: 210 | Batch_idx: 290 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (97.00%) (36483/37248)
Epoch: 210 | Batch_idx: 300 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (37729/38528)
Epoch: 210 | Batch_idx: 310 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (38986/39808)
Epoch: 210 | Batch_idx: 320 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (40242/41088)
Epoch: 210 | Batch_idx: 330 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (41498/42368)
Epoch: 210 | Batch_idx: 340 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (42750/43648)
Epoch: 210 | Batch_idx: 350 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (44005/44928)
Epoch: 210 | Batch_idx: 360 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (45258/46208)
Epoch: 210 | Batch_idx: 370 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (46510/47488)
Epoch: 210 | Batch_idx: 380 |  Loss: (0.0630) |  Loss2: (0.0000) | Acc: (97.00%) (47756/48768)
Epoch: 210 | Batch_idx: 390 |  Loss: (0.0633) |  Loss2: (0.0000) | Acc: (97.00%) (48957/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_210.pth.tar'
# TEST : Loss: (0.4606) | Acc: (88.00%) (8841/10000)
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.5910, 0.4090], device='cuda:0')
percent tensor([0.5926, 0.4074], device='cuda:0')
percent tensor([0.6892, 0.3108], device='cuda:0')
percent tensor([0.6464, 0.3536], device='cuda:0')
percent tensor([0.7206, 0.2794], device='cuda:0')
percent tensor([0.7517, 0.2483], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(183.9798, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(832.3492, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(836.2307, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1514.5625, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(475.5151, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2316.0005, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4253.7686, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1340.7950, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6299.4678, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11507.2139, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3767.7246, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15919.8008, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 211 | Batch_idx: 0 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 211 | Batch_idx: 10 |  Loss: (0.0664) |  Loss2: (0.0000) | Acc: (97.00%) (1377/1408)
Epoch: 211 | Batch_idx: 20 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (2637/2688)
Epoch: 211 | Batch_idx: 30 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (3893/3968)
Epoch: 211 | Batch_idx: 40 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (98.00%) (5147/5248)
Epoch: 211 | Batch_idx: 50 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (6405/6528)
Epoch: 211 | Batch_idx: 60 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (7644/7808)
Epoch: 211 | Batch_idx: 70 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (8900/9088)
Epoch: 211 | Batch_idx: 80 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (10162/10368)
Epoch: 211 | Batch_idx: 90 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (97.00%) (11411/11648)
Epoch: 211 | Batch_idx: 100 |  Loss: (0.0620) |  Loss2: (0.0000) | Acc: (97.00%) (12661/12928)
Epoch: 211 | Batch_idx: 110 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (13918/14208)
Epoch: 211 | Batch_idx: 120 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (15177/15488)
Epoch: 211 | Batch_idx: 130 |  Loss: (0.0616) |  Loss2: (0.0000) | Acc: (98.00%) (16434/16768)
Epoch: 211 | Batch_idx: 140 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (17678/18048)
Epoch: 211 | Batch_idx: 150 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (18931/19328)
Epoch: 211 | Batch_idx: 160 |  Loss: (0.0622) |  Loss2: (0.0000) | Acc: (97.00%) (20192/20608)
Epoch: 211 | Batch_idx: 170 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (21449/21888)
Epoch: 211 | Batch_idx: 180 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (22694/23168)
Epoch: 211 | Batch_idx: 190 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (23946/24448)
Epoch: 211 | Batch_idx: 200 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (25208/25728)
Epoch: 211 | Batch_idx: 210 |  Loss: (0.0615) |  Loss2: (0.0000) | Acc: (97.00%) (26467/27008)
Epoch: 211 | Batch_idx: 220 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (27718/28288)
Epoch: 211 | Batch_idx: 230 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (97.00%) (28976/29568)
Epoch: 211 | Batch_idx: 240 |  Loss: (0.0614) |  Loss2: (0.0000) | Acc: (98.00%) (30237/30848)
Epoch: 211 | Batch_idx: 250 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (31496/32128)
Epoch: 211 | Batch_idx: 260 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (98.00%) (32753/33408)
Epoch: 211 | Batch_idx: 270 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (34004/34688)
Epoch: 211 | Batch_idx: 280 |  Loss: (0.0619) |  Loss2: (0.0000) | Acc: (98.00%) (35250/35968)
Epoch: 211 | Batch_idx: 290 |  Loss: (0.0625) |  Loss2: (0.0000) | Acc: (97.00%) (36496/37248)
Epoch: 211 | Batch_idx: 300 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (37748/38528)
Epoch: 211 | Batch_idx: 310 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (39003/39808)
Epoch: 211 | Batch_idx: 320 |  Loss: (0.0629) |  Loss2: (0.0000) | Acc: (97.00%) (40255/41088)
Epoch: 211 | Batch_idx: 330 |  Loss: (0.0627) |  Loss2: (0.0000) | Acc: (97.00%) (41506/42368)
Epoch: 211 | Batch_idx: 340 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (42757/43648)
Epoch: 211 | Batch_idx: 350 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (44008/44928)
Epoch: 211 | Batch_idx: 360 |  Loss: (0.0636) |  Loss2: (0.0000) | Acc: (97.00%) (45261/46208)
Epoch: 211 | Batch_idx: 370 |  Loss: (0.0634) |  Loss2: (0.0000) | Acc: (97.00%) (46519/47488)
Epoch: 211 | Batch_idx: 380 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (47774/48768)
Epoch: 211 | Batch_idx: 390 |  Loss: (0.0628) |  Loss2: (0.0000) | Acc: (97.00%) (48986/50000)
# TEST : Loss: (0.4337) | Acc: (89.00%) (8913/10000)
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.5918, 0.4082], device='cuda:0')
percent tensor([0.5964, 0.4036], device='cuda:0')
percent tensor([0.6915, 0.3085], device='cuda:0')
percent tensor([0.6444, 0.3556], device='cuda:0')
percent tensor([0.7265, 0.2735], device='cuda:0')
percent tensor([0.7527, 0.2473], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 212 | Batch_idx: 0 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 212 | Batch_idx: 10 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (97.00%) (1379/1408)
Epoch: 212 | Batch_idx: 20 |  Loss: (0.0524) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 212 | Batch_idx: 30 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 212 | Batch_idx: 40 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (5156/5248)
Epoch: 212 | Batch_idx: 50 |  Loss: (0.0555) |  Loss2: (0.0000) | Acc: (98.00%) (6411/6528)
Epoch: 212 | Batch_idx: 60 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (7669/7808)
Epoch: 212 | Batch_idx: 70 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (8922/9088)
Epoch: 212 | Batch_idx: 80 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (10180/10368)
Epoch: 212 | Batch_idx: 90 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (11432/11648)
Epoch: 212 | Batch_idx: 100 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (12695/12928)
Epoch: 212 | Batch_idx: 110 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (13954/14208)
Epoch: 212 | Batch_idx: 120 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (15211/15488)
Epoch: 212 | Batch_idx: 130 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (16461/16768)
Epoch: 212 | Batch_idx: 140 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (17719/18048)
Epoch: 212 | Batch_idx: 150 |  Loss: (0.0579) |  Loss2: (0.0000) | Acc: (98.00%) (18973/19328)
Epoch: 212 | Batch_idx: 160 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (20232/20608)
Epoch: 212 | Batch_idx: 170 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (21488/21888)
Epoch: 212 | Batch_idx: 180 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (22754/23168)
Epoch: 212 | Batch_idx: 190 |  Loss: (0.0574) |  Loss2: (0.0000) | Acc: (98.00%) (24006/24448)
Epoch: 212 | Batch_idx: 200 |  Loss: (0.0582) |  Loss2: (0.0000) | Acc: (98.00%) (25255/25728)
Epoch: 212 | Batch_idx: 210 |  Loss: (0.0585) |  Loss2: (0.0000) | Acc: (98.00%) (26511/27008)
Epoch: 212 | Batch_idx: 220 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (27773/28288)
Epoch: 212 | Batch_idx: 230 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (29026/29568)
Epoch: 212 | Batch_idx: 240 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (30270/30848)
Epoch: 212 | Batch_idx: 250 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (31513/32128)
Epoch: 212 | Batch_idx: 260 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (32764/33408)
Epoch: 212 | Batch_idx: 270 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (34020/34688)
Epoch: 212 | Batch_idx: 280 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (98.00%) (35275/35968)
Epoch: 212 | Batch_idx: 290 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (98.00%) (36524/37248)
Epoch: 212 | Batch_idx: 300 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (37779/38528)
Epoch: 212 | Batch_idx: 310 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (98.00%) (39035/39808)
Epoch: 212 | Batch_idx: 320 |  Loss: (0.0607) |  Loss2: (0.0000) | Acc: (98.00%) (40286/41088)
Epoch: 212 | Batch_idx: 330 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (41534/42368)
Epoch: 212 | Batch_idx: 340 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (98.00%) (42782/43648)
Epoch: 212 | Batch_idx: 350 |  Loss: (0.0611) |  Loss2: (0.0000) | Acc: (98.00%) (44035/44928)
Epoch: 212 | Batch_idx: 360 |  Loss: (0.0613) |  Loss2: (0.0000) | Acc: (97.00%) (45281/46208)
Epoch: 212 | Batch_idx: 370 |  Loss: (0.0621) |  Loss2: (0.0000) | Acc: (97.00%) (46522/47488)
Epoch: 212 | Batch_idx: 380 |  Loss: (0.0623) |  Loss2: (0.0000) | Acc: (97.00%) (47773/48768)
Epoch: 212 | Batch_idx: 390 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (48973/50000)
# TEST : Loss: (0.4575) | Acc: (88.00%) (8880/10000)
percent tensor([0.5945, 0.4055], device='cuda:0')
percent tensor([0.5916, 0.4084], device='cuda:0')
percent tensor([0.5980, 0.4020], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.6402, 0.3598], device='cuda:0')
percent tensor([0.7295, 0.2705], device='cuda:0')
percent tensor([0.7584, 0.2416], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 213 | Batch_idx: 0 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 213 | Batch_idx: 10 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 213 | Batch_idx: 20 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (2641/2688)
Epoch: 213 | Batch_idx: 30 |  Loss: (0.0632) |  Loss2: (0.0000) | Acc: (97.00%) (3880/3968)
Epoch: 213 | Batch_idx: 40 |  Loss: (0.0624) |  Loss2: (0.0000) | Acc: (97.00%) (5136/5248)
Epoch: 213 | Batch_idx: 50 |  Loss: (0.0612) |  Loss2: (0.0000) | Acc: (97.00%) (6391/6528)
Epoch: 213 | Batch_idx: 60 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (97.00%) (7651/7808)
Epoch: 213 | Batch_idx: 70 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (8913/9088)
Epoch: 213 | Batch_idx: 80 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (10177/10368)
Epoch: 213 | Batch_idx: 90 |  Loss: (0.0565) |  Loss2: (0.0000) | Acc: (98.00%) (11430/11648)
Epoch: 213 | Batch_idx: 100 |  Loss: (0.0581) |  Loss2: (0.0000) | Acc: (98.00%) (12678/12928)
Epoch: 213 | Batch_idx: 110 |  Loss: (0.0580) |  Loss2: (0.0000) | Acc: (98.00%) (13935/14208)
Epoch: 213 | Batch_idx: 120 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (15197/15488)
Epoch: 213 | Batch_idx: 130 |  Loss: (0.0568) |  Loss2: (0.0000) | Acc: (98.00%) (16457/16768)
Epoch: 213 | Batch_idx: 140 |  Loss: (0.0573) |  Loss2: (0.0000) | Acc: (98.00%) (17711/18048)
Epoch: 213 | Batch_idx: 150 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (18956/19328)
Epoch: 213 | Batch_idx: 160 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (20209/20608)
Epoch: 213 | Batch_idx: 170 |  Loss: (0.0592) |  Loss2: (0.0000) | Acc: (98.00%) (21458/21888)
Epoch: 213 | Batch_idx: 180 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (22715/23168)
Epoch: 213 | Batch_idx: 190 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (23973/24448)
Epoch: 213 | Batch_idx: 200 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (25233/25728)
Epoch: 213 | Batch_idx: 210 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (26483/27008)
Epoch: 213 | Batch_idx: 220 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (27742/28288)
Epoch: 213 | Batch_idx: 230 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (28993/29568)
Epoch: 213 | Batch_idx: 240 |  Loss: (0.0593) |  Loss2: (0.0000) | Acc: (98.00%) (30248/30848)
Epoch: 213 | Batch_idx: 250 |  Loss: (0.0591) |  Loss2: (0.0000) | Acc: (98.00%) (31509/32128)
Epoch: 213 | Batch_idx: 260 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (32767/33408)
Epoch: 213 | Batch_idx: 270 |  Loss: (0.0588) |  Loss2: (0.0000) | Acc: (98.00%) (34026/34688)
Epoch: 213 | Batch_idx: 280 |  Loss: (0.0587) |  Loss2: (0.0000) | Acc: (98.00%) (35283/35968)
Epoch: 213 | Batch_idx: 290 |  Loss: (0.0583) |  Loss2: (0.0000) | Acc: (98.00%) (36548/37248)
Epoch: 213 | Batch_idx: 300 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (37807/38528)
Epoch: 213 | Batch_idx: 310 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (39052/39808)
Epoch: 213 | Batch_idx: 320 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (40295/41088)
Epoch: 213 | Batch_idx: 330 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (41546/42368)
Epoch: 213 | Batch_idx: 340 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (42797/43648)
Epoch: 213 | Batch_idx: 350 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (44052/44928)
Epoch: 213 | Batch_idx: 360 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (98.00%) (45308/46208)
Epoch: 213 | Batch_idx: 370 |  Loss: (0.0595) |  Loss2: (0.0000) | Acc: (98.00%) (46567/47488)
Epoch: 213 | Batch_idx: 380 |  Loss: (0.0597) |  Loss2: (0.0000) | Acc: (98.00%) (47814/48768)
Epoch: 213 | Batch_idx: 390 |  Loss: (0.0598) |  Loss2: (0.0000) | Acc: (98.00%) (49018/50000)
# TEST : Loss: (0.4672) | Acc: (88.00%) (8865/10000)
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.5920, 0.4080], device='cuda:0')
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.6904, 0.3096], device='cuda:0')
percent tensor([0.6449, 0.3551], device='cuda:0')
percent tensor([0.7304, 0.2696], device='cuda:0')
percent tensor([0.7596, 0.2404], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 214 | Batch_idx: 0 |  Loss: (0.0594) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 214 | Batch_idx: 10 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 214 | Batch_idx: 20 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (2638/2688)
Epoch: 214 | Batch_idx: 30 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 214 | Batch_idx: 40 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (5151/5248)
Epoch: 214 | Batch_idx: 50 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (6406/6528)
Epoch: 214 | Batch_idx: 60 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (7666/7808)
Epoch: 214 | Batch_idx: 70 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (8921/9088)
Epoch: 214 | Batch_idx: 80 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (10171/10368)
Epoch: 214 | Batch_idx: 90 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (11426/11648)
Epoch: 214 | Batch_idx: 100 |  Loss: (0.0559) |  Loss2: (0.0000) | Acc: (98.00%) (12680/12928)
Epoch: 214 | Batch_idx: 110 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (13932/14208)
Epoch: 214 | Batch_idx: 120 |  Loss: (0.0571) |  Loss2: (0.0000) | Acc: (98.00%) (15184/15488)
Epoch: 214 | Batch_idx: 130 |  Loss: (0.0578) |  Loss2: (0.0000) | Acc: (98.00%) (16435/16768)
Epoch: 214 | Batch_idx: 140 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (17685/18048)
Epoch: 214 | Batch_idx: 150 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (18938/19328)
Epoch: 214 | Batch_idx: 160 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (97.00%) (20184/20608)
Epoch: 214 | Batch_idx: 170 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (97.00%) (21432/21888)
Epoch: 214 | Batch_idx: 180 |  Loss: (0.0610) |  Loss2: (0.0000) | Acc: (97.00%) (22681/23168)
Epoch: 214 | Batch_idx: 190 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (23935/24448)
Epoch: 214 | Batch_idx: 200 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (25189/25728)
Epoch: 214 | Batch_idx: 210 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (97.00%) (26451/27008)
Epoch: 214 | Batch_idx: 220 |  Loss: (0.0600) |  Loss2: (0.0000) | Acc: (97.00%) (27700/28288)
Epoch: 214 | Batch_idx: 230 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (97.00%) (28957/29568)
Epoch: 214 | Batch_idx: 240 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (30208/30848)
Epoch: 214 | Batch_idx: 250 |  Loss: (0.0606) |  Loss2: (0.0000) | Acc: (97.00%) (31455/32128)
Epoch: 214 | Batch_idx: 260 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (32717/33408)
Epoch: 214 | Batch_idx: 270 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (33964/34688)
Epoch: 214 | Batch_idx: 280 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (97.00%) (35221/35968)
Epoch: 214 | Batch_idx: 290 |  Loss: (0.0602) |  Loss2: (0.0000) | Acc: (97.00%) (36476/37248)
Epoch: 214 | Batch_idx: 300 |  Loss: (0.0603) |  Loss2: (0.0000) | Acc: (97.00%) (37731/38528)
Epoch: 214 | Batch_idx: 310 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (97.00%) (38988/39808)
Epoch: 214 | Batch_idx: 320 |  Loss: (0.0596) |  Loss2: (0.0000) | Acc: (97.00%) (40255/41088)
Epoch: 214 | Batch_idx: 330 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (97.00%) (41503/42368)
Epoch: 214 | Batch_idx: 340 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (97.00%) (42754/43648)
Epoch: 214 | Batch_idx: 350 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (97.00%) (44008/44928)
Epoch: 214 | Batch_idx: 360 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (97.00%) (45263/46208)
Epoch: 214 | Batch_idx: 370 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (97.00%) (46516/47488)
Epoch: 214 | Batch_idx: 380 |  Loss: (0.0608) |  Loss2: (0.0000) | Acc: (97.00%) (47768/48768)
Epoch: 214 | Batch_idx: 390 |  Loss: (0.0605) |  Loss2: (0.0000) | Acc: (97.00%) (48982/50000)
# TEST : Loss: (0.4821) | Acc: (88.00%) (8831/10000)
percent tensor([0.5967, 0.4033], device='cuda:0')
percent tensor([0.5934, 0.4066], device='cuda:0')
percent tensor([0.5986, 0.4014], device='cuda:0')
percent tensor([0.6917, 0.3083], device='cuda:0')
percent tensor([0.6441, 0.3559], device='cuda:0')
percent tensor([0.7316, 0.2684], device='cuda:0')
percent tensor([0.7544, 0.2456], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 215 | Batch_idx: 0 |  Loss: (0.0786) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 215 | Batch_idx: 10 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 215 | Batch_idx: 20 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 215 | Batch_idx: 30 |  Loss: (0.0529) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 215 | Batch_idx: 40 |  Loss: (0.0533) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 215 | Batch_idx: 50 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (6431/6528)
Epoch: 215 | Batch_idx: 60 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (7686/7808)
Epoch: 215 | Batch_idx: 70 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (8940/9088)
Epoch: 215 | Batch_idx: 80 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (10204/10368)
Epoch: 215 | Batch_idx: 90 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (11469/11648)
Epoch: 215 | Batch_idx: 100 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (12726/12928)
Epoch: 215 | Batch_idx: 110 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (13987/14208)
Epoch: 215 | Batch_idx: 120 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (15242/15488)
Epoch: 215 | Batch_idx: 130 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (16500/16768)
Epoch: 215 | Batch_idx: 140 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (17760/18048)
Epoch: 215 | Batch_idx: 150 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (19023/19328)
Epoch: 215 | Batch_idx: 160 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (20291/20608)
Epoch: 215 | Batch_idx: 170 |  Loss: (0.0506) |  Loss2: (0.0000) | Acc: (98.00%) (21552/21888)
Epoch: 215 | Batch_idx: 180 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (22796/23168)
Epoch: 215 | Batch_idx: 190 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (24053/24448)
Epoch: 215 | Batch_idx: 200 |  Loss: (0.0526) |  Loss2: (0.0000) | Acc: (98.00%) (25307/25728)
Epoch: 215 | Batch_idx: 210 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (26565/27008)
Epoch: 215 | Batch_idx: 220 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (27820/28288)
Epoch: 215 | Batch_idx: 230 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (29077/29568)
Epoch: 215 | Batch_idx: 240 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (30329/30848)
Epoch: 215 | Batch_idx: 250 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (31583/32128)
Epoch: 215 | Batch_idx: 260 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (32842/33408)
Epoch: 215 | Batch_idx: 270 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (34095/34688)
Epoch: 215 | Batch_idx: 280 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (35355/35968)
Epoch: 215 | Batch_idx: 290 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (36607/37248)
Epoch: 215 | Batch_idx: 300 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (37869/38528)
Epoch: 215 | Batch_idx: 310 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (39119/39808)
Epoch: 215 | Batch_idx: 320 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (40371/41088)
Epoch: 215 | Batch_idx: 330 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (41626/42368)
Epoch: 215 | Batch_idx: 340 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (42893/43648)
Epoch: 215 | Batch_idx: 350 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (44148/44928)
Epoch: 215 | Batch_idx: 360 |  Loss: (0.0547) |  Loss2: (0.0000) | Acc: (98.00%) (45400/46208)
Epoch: 215 | Batch_idx: 370 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (46654/47488)
Epoch: 215 | Batch_idx: 380 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (47912/48768)
Epoch: 215 | Batch_idx: 390 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (49122/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_215.pth.tar'
# TEST : Loss: (0.4553) | Acc: (89.00%) (8906/10000)
percent tensor([0.5972, 0.4028], device='cuda:0')
percent tensor([0.5924, 0.4076], device='cuda:0')
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.6898, 0.3102], device='cuda:0')
percent tensor([0.6511, 0.3489], device='cuda:0')
percent tensor([0.7382, 0.2618], device='cuda:0')
percent tensor([0.7642, 0.2358], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 216 | Batch_idx: 0 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 216 | Batch_idx: 10 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (99.00%) (1394/1408)
Epoch: 216 | Batch_idx: 20 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 216 | Batch_idx: 30 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (3911/3968)
Epoch: 216 | Batch_idx: 40 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (5170/5248)
Epoch: 216 | Batch_idx: 50 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (6425/6528)
Epoch: 216 | Batch_idx: 60 |  Loss: (0.0521) |  Loss2: (0.0000) | Acc: (98.00%) (7676/7808)
Epoch: 216 | Batch_idx: 70 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (8929/9088)
Epoch: 216 | Batch_idx: 80 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (10177/10368)
Epoch: 216 | Batch_idx: 90 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (11434/11648)
Epoch: 216 | Batch_idx: 100 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (12696/12928)
Epoch: 216 | Batch_idx: 110 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (13954/14208)
Epoch: 216 | Batch_idx: 120 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (15211/15488)
Epoch: 216 | Batch_idx: 130 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (16471/16768)
Epoch: 216 | Batch_idx: 140 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (17727/18048)
Epoch: 216 | Batch_idx: 150 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (18987/19328)
Epoch: 216 | Batch_idx: 160 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (20243/20608)
Epoch: 216 | Batch_idx: 170 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (21493/21888)
Epoch: 216 | Batch_idx: 180 |  Loss: (0.0532) |  Loss2: (0.0000) | Acc: (98.00%) (22749/23168)
Epoch: 216 | Batch_idx: 190 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (23997/24448)
Epoch: 216 | Batch_idx: 200 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (25249/25728)
Epoch: 216 | Batch_idx: 210 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (26504/27008)
Epoch: 216 | Batch_idx: 220 |  Loss: (0.0548) |  Loss2: (0.0000) | Acc: (98.00%) (27755/28288)
Epoch: 216 | Batch_idx: 230 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (29018/29568)
Epoch: 216 | Batch_idx: 240 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (30288/30848)
Epoch: 216 | Batch_idx: 250 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (31548/32128)
Epoch: 216 | Batch_idx: 260 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (32804/33408)
Epoch: 216 | Batch_idx: 270 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (34060/34688)
Epoch: 216 | Batch_idx: 280 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (35321/35968)
Epoch: 216 | Batch_idx: 290 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (36578/37248)
Epoch: 216 | Batch_idx: 300 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (37826/38528)
Epoch: 216 | Batch_idx: 310 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (39082/39808)
Epoch: 216 | Batch_idx: 320 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (40346/41088)
Epoch: 216 | Batch_idx: 330 |  Loss: (0.0540) |  Loss2: (0.0000) | Acc: (98.00%) (41599/42368)
Epoch: 216 | Batch_idx: 340 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (42861/43648)
Epoch: 216 | Batch_idx: 350 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (44111/44928)
Epoch: 216 | Batch_idx: 360 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (45365/46208)
Epoch: 216 | Batch_idx: 370 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (46619/47488)
Epoch: 216 | Batch_idx: 380 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (47878/48768)
Epoch: 216 | Batch_idx: 390 |  Loss: (0.0546) |  Loss2: (0.0000) | Acc: (98.00%) (49086/50000)
# TEST : Loss: (0.4472) | Acc: (88.00%) (8869/10000)
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.5936, 0.4064], device='cuda:0')
percent tensor([0.5998, 0.4002], device='cuda:0')
percent tensor([0.6933, 0.3067], device='cuda:0')
percent tensor([0.6468, 0.3532], device='cuda:0')
percent tensor([0.7420, 0.2580], device='cuda:0')
percent tensor([0.7616, 0.2384], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 217 | Batch_idx: 0 |  Loss: (0.0843) |  Loss2: (0.0000) | Acc: (95.00%) (122/128)
Epoch: 217 | Batch_idx: 10 |  Loss: (0.0599) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 217 | Batch_idx: 20 |  Loss: (0.0618) |  Loss2: (0.0000) | Acc: (98.00%) (2635/2688)
Epoch: 217 | Batch_idx: 30 |  Loss: (0.0642) |  Loss2: (0.0000) | Acc: (97.00%) (3885/3968)
Epoch: 217 | Batch_idx: 40 |  Loss: (0.0589) |  Loss2: (0.0000) | Acc: (98.00%) (5147/5248)
Epoch: 217 | Batch_idx: 50 |  Loss: (0.0577) |  Loss2: (0.0000) | Acc: (98.00%) (6407/6528)
Epoch: 217 | Batch_idx: 60 |  Loss: (0.0584) |  Loss2: (0.0000) | Acc: (98.00%) (7659/7808)
Epoch: 217 | Batch_idx: 70 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (8914/9088)
Epoch: 217 | Batch_idx: 80 |  Loss: (0.0562) |  Loss2: (0.0000) | Acc: (98.00%) (10179/10368)
Epoch: 217 | Batch_idx: 90 |  Loss: (0.0561) |  Loss2: (0.0000) | Acc: (98.00%) (11434/11648)
Epoch: 217 | Batch_idx: 100 |  Loss: (0.0556) |  Loss2: (0.0000) | Acc: (98.00%) (12691/12928)
Epoch: 217 | Batch_idx: 110 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (13948/14208)
Epoch: 217 | Batch_idx: 120 |  Loss: (0.0558) |  Loss2: (0.0000) | Acc: (98.00%) (15206/15488)
Epoch: 217 | Batch_idx: 130 |  Loss: (0.0564) |  Loss2: (0.0000) | Acc: (98.00%) (16461/16768)
Epoch: 217 | Batch_idx: 140 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (17712/18048)
Epoch: 217 | Batch_idx: 150 |  Loss: (0.0576) |  Loss2: (0.0000) | Acc: (98.00%) (18966/19328)
Epoch: 217 | Batch_idx: 160 |  Loss: (0.0572) |  Loss2: (0.0000) | Acc: (98.00%) (20227/20608)
Epoch: 217 | Batch_idx: 170 |  Loss: (0.0569) |  Loss2: (0.0000) | Acc: (98.00%) (21486/21888)
Epoch: 217 | Batch_idx: 180 |  Loss: (0.0567) |  Loss2: (0.0000) | Acc: (98.00%) (22743/23168)
Epoch: 217 | Batch_idx: 190 |  Loss: (0.0566) |  Loss2: (0.0000) | Acc: (98.00%) (24000/24448)
Epoch: 217 | Batch_idx: 200 |  Loss: (0.0560) |  Loss2: (0.0000) | Acc: (98.00%) (25261/25728)
Epoch: 217 | Batch_idx: 210 |  Loss: (0.0557) |  Loss2: (0.0000) | Acc: (98.00%) (26519/27008)
Epoch: 217 | Batch_idx: 220 |  Loss: (0.0554) |  Loss2: (0.0000) | Acc: (98.00%) (27782/28288)
Epoch: 217 | Batch_idx: 230 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (29038/29568)
Epoch: 217 | Batch_idx: 240 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (30298/30848)
Epoch: 217 | Batch_idx: 250 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (31552/32128)
Epoch: 217 | Batch_idx: 260 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (32808/33408)
Epoch: 217 | Batch_idx: 270 |  Loss: (0.0550) |  Loss2: (0.0000) | Acc: (98.00%) (34061/34688)
Epoch: 217 | Batch_idx: 280 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (35313/35968)
Epoch: 217 | Batch_idx: 290 |  Loss: (0.0553) |  Loss2: (0.0000) | Acc: (98.00%) (36574/37248)
Epoch: 217 | Batch_idx: 300 |  Loss: (0.0551) |  Loss2: (0.0000) | Acc: (98.00%) (37832/38528)
Epoch: 217 | Batch_idx: 310 |  Loss: (0.0552) |  Loss2: (0.0000) | Acc: (98.00%) (39087/39808)
Epoch: 217 | Batch_idx: 320 |  Loss: (0.0549) |  Loss2: (0.0000) | Acc: (98.00%) (40347/41088)
Epoch: 217 | Batch_idx: 330 |  Loss: (0.0545) |  Loss2: (0.0000) | Acc: (98.00%) (41611/42368)
Epoch: 217 | Batch_idx: 340 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (42873/43648)
Epoch: 217 | Batch_idx: 350 |  Loss: (0.0542) |  Loss2: (0.0000) | Acc: (98.00%) (44132/44928)
Epoch: 217 | Batch_idx: 360 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (45389/46208)
Epoch: 217 | Batch_idx: 370 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (46648/47488)
Epoch: 217 | Batch_idx: 380 |  Loss: (0.0535) |  Loss2: (0.0000) | Acc: (98.00%) (47916/48768)
Epoch: 217 | Batch_idx: 390 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (49128/50000)
# TEST : Loss: (0.4509) | Acc: (88.00%) (8894/10000)
percent tensor([0.5947, 0.4053], device='cuda:0')
percent tensor([0.5943, 0.4057], device='cuda:0')
percent tensor([0.5981, 0.4019], device='cuda:0')
percent tensor([0.6865, 0.3135], device='cuda:0')
percent tensor([0.6491, 0.3509], device='cuda:0')
percent tensor([0.7431, 0.2569], device='cuda:0')
percent tensor([0.7637, 0.2363], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 218 | Batch_idx: 0 |  Loss: (0.1060) |  Loss2: (0.0000) | Acc: (96.00%) (123/128)
Epoch: 218 | Batch_idx: 10 |  Loss: (0.0601) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 218 | Batch_idx: 20 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (2642/2688)
Epoch: 218 | Batch_idx: 30 |  Loss: (0.0530) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 218 | Batch_idx: 40 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (5157/5248)
Epoch: 218 | Batch_idx: 50 |  Loss: (0.0543) |  Loss2: (0.0000) | Acc: (98.00%) (6407/6528)
Epoch: 218 | Batch_idx: 60 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (7671/7808)
Epoch: 218 | Batch_idx: 70 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (8929/9088)
Epoch: 218 | Batch_idx: 80 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (10188/10368)
Epoch: 218 | Batch_idx: 90 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (11449/11648)
Epoch: 218 | Batch_idx: 100 |  Loss: (0.0514) |  Loss2: (0.0000) | Acc: (98.00%) (12701/12928)
Epoch: 218 | Batch_idx: 110 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (13959/14208)
Epoch: 218 | Batch_idx: 120 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (15216/15488)
Epoch: 218 | Batch_idx: 130 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (16478/16768)
Epoch: 218 | Batch_idx: 140 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (17739/18048)
Epoch: 218 | Batch_idx: 150 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (18989/19328)
Epoch: 218 | Batch_idx: 160 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (20250/20608)
Epoch: 218 | Batch_idx: 170 |  Loss: (0.0516) |  Loss2: (0.0000) | Acc: (98.00%) (21508/21888)
Epoch: 218 | Batch_idx: 180 |  Loss: (0.0520) |  Loss2: (0.0000) | Acc: (98.00%) (22762/23168)
Epoch: 218 | Batch_idx: 190 |  Loss: (0.0518) |  Loss2: (0.0000) | Acc: (98.00%) (24019/24448)
Epoch: 218 | Batch_idx: 200 |  Loss: (0.0527) |  Loss2: (0.0000) | Acc: (98.00%) (25263/25728)
Epoch: 218 | Batch_idx: 210 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (26524/27008)
Epoch: 218 | Batch_idx: 220 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (27781/28288)
Epoch: 218 | Batch_idx: 230 |  Loss: (0.0528) |  Loss2: (0.0000) | Acc: (98.00%) (29039/29568)
Epoch: 218 | Batch_idx: 240 |  Loss: (0.0531) |  Loss2: (0.0000) | Acc: (98.00%) (30293/30848)
Epoch: 218 | Batch_idx: 250 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (31542/32128)
Epoch: 218 | Batch_idx: 260 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (32802/33408)
Epoch: 218 | Batch_idx: 270 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (34055/34688)
Epoch: 218 | Batch_idx: 280 |  Loss: (0.0534) |  Loss2: (0.0000) | Acc: (98.00%) (35321/35968)
Epoch: 218 | Batch_idx: 290 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (36571/37248)
Epoch: 218 | Batch_idx: 300 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (37826/38528)
Epoch: 218 | Batch_idx: 310 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (39088/39808)
Epoch: 218 | Batch_idx: 320 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (40344/41088)
Epoch: 218 | Batch_idx: 330 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (41607/42368)
Epoch: 218 | Batch_idx: 340 |  Loss: (0.0541) |  Loss2: (0.0000) | Acc: (98.00%) (42867/43648)
Epoch: 218 | Batch_idx: 350 |  Loss: (0.0536) |  Loss2: (0.0000) | Acc: (98.00%) (44130/44928)
Epoch: 218 | Batch_idx: 360 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (45381/46208)
Epoch: 218 | Batch_idx: 370 |  Loss: (0.0538) |  Loss2: (0.0000) | Acc: (98.00%) (46640/47488)
Epoch: 218 | Batch_idx: 380 |  Loss: (0.0539) |  Loss2: (0.0000) | Acc: (98.00%) (47897/48768)
Epoch: 218 | Batch_idx: 390 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (49111/50000)
# TEST : Loss: (0.4548) | Acc: (89.00%) (8915/10000)
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.5945, 0.4055], device='cuda:0')
percent tensor([0.6028, 0.3972], device='cuda:0')
percent tensor([0.6926, 0.3074], device='cuda:0')
percent tensor([0.6479, 0.3521], device='cuda:0')
percent tensor([0.7439, 0.2561], device='cuda:0')
percent tensor([0.7634, 0.2366], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 219 | Batch_idx: 0 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 219 | Batch_idx: 10 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (1381/1408)
Epoch: 219 | Batch_idx: 20 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 219 | Batch_idx: 30 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 219 | Batch_idx: 40 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (5176/5248)
Epoch: 219 | Batch_idx: 50 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (6427/6528)
Epoch: 219 | Batch_idx: 60 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (7686/7808)
Epoch: 219 | Batch_idx: 70 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (8951/9088)
Epoch: 219 | Batch_idx: 80 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (10206/10368)
Epoch: 219 | Batch_idx: 90 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (11466/11648)
Epoch: 219 | Batch_idx: 100 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (12735/12928)
Epoch: 219 | Batch_idx: 110 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (13996/14208)
Epoch: 219 | Batch_idx: 120 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (15261/15488)
Epoch: 219 | Batch_idx: 130 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (16520/16768)
Epoch: 219 | Batch_idx: 140 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (17782/18048)
Epoch: 219 | Batch_idx: 150 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (19043/19328)
Epoch: 219 | Batch_idx: 160 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (20302/20608)
Epoch: 219 | Batch_idx: 170 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (21565/21888)
Epoch: 219 | Batch_idx: 180 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (22821/23168)
Epoch: 219 | Batch_idx: 190 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (24088/24448)
Epoch: 219 | Batch_idx: 200 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (25353/25728)
Epoch: 219 | Batch_idx: 210 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (26606/27008)
Epoch: 219 | Batch_idx: 220 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (27867/28288)
Epoch: 219 | Batch_idx: 230 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (29118/29568)
Epoch: 219 | Batch_idx: 240 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (30380/30848)
Epoch: 219 | Batch_idx: 250 |  Loss: (0.0500) |  Loss2: (0.0000) | Acc: (98.00%) (31633/32128)
Epoch: 219 | Batch_idx: 260 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (32896/33408)
Epoch: 219 | Batch_idx: 270 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (34146/34688)
Epoch: 219 | Batch_idx: 280 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (35394/35968)
Epoch: 219 | Batch_idx: 290 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (36652/37248)
Epoch: 219 | Batch_idx: 300 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (37910/38528)
Epoch: 219 | Batch_idx: 310 |  Loss: (0.0511) |  Loss2: (0.0000) | Acc: (98.00%) (39175/39808)
Epoch: 219 | Batch_idx: 320 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (40427/41088)
Epoch: 219 | Batch_idx: 330 |  Loss: (0.0517) |  Loss2: (0.0000) | Acc: (98.00%) (41679/42368)
Epoch: 219 | Batch_idx: 340 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (42935/43648)
Epoch: 219 | Batch_idx: 350 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (44193/44928)
Epoch: 219 | Batch_idx: 360 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (45450/46208)
Epoch: 219 | Batch_idx: 370 |  Loss: (0.0522) |  Loss2: (0.0000) | Acc: (98.00%) (46710/47488)
Epoch: 219 | Batch_idx: 380 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (47970/48768)
Epoch: 219 | Batch_idx: 390 |  Loss: (0.0525) |  Loss2: (0.0000) | Acc: (98.00%) (49177/50000)
# TEST : Loss: (0.4582) | Acc: (88.00%) (8896/10000)
percent tensor([0.5983, 0.4017], device='cuda:0')
percent tensor([0.5968, 0.4032], device='cuda:0')
percent tensor([0.6034, 0.3966], device='cuda:0')
percent tensor([0.6925, 0.3075], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.7453, 0.2547], device='cuda:0')
percent tensor([0.7615, 0.2385], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 220 | Batch_idx: 0 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 220 | Batch_idx: 10 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (1391/1408)
Epoch: 220 | Batch_idx: 20 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 220 | Batch_idx: 30 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 220 | Batch_idx: 40 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (5181/5248)
Epoch: 220 | Batch_idx: 50 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (6446/6528)
Epoch: 220 | Batch_idx: 60 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (7709/7808)
Epoch: 220 | Batch_idx: 70 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (8972/9088)
Epoch: 220 | Batch_idx: 80 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 220 | Batch_idx: 90 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (11492/11648)
Epoch: 220 | Batch_idx: 100 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (12751/12928)
Epoch: 220 | Batch_idx: 110 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 220 | Batch_idx: 120 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (15270/15488)
Epoch: 220 | Batch_idx: 130 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (16536/16768)
Epoch: 220 | Batch_idx: 140 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (17796/18048)
Epoch: 220 | Batch_idx: 150 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (19053/19328)
Epoch: 220 | Batch_idx: 160 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (20311/20608)
Epoch: 220 | Batch_idx: 170 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (21574/21888)
Epoch: 220 | Batch_idx: 180 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (22837/23168)
Epoch: 220 | Batch_idx: 190 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (24097/24448)
Epoch: 220 | Batch_idx: 200 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (25352/25728)
Epoch: 220 | Batch_idx: 210 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (26614/27008)
Epoch: 220 | Batch_idx: 220 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (27878/28288)
Epoch: 220 | Batch_idx: 230 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (29133/29568)
Epoch: 220 | Batch_idx: 240 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (30394/30848)
Epoch: 220 | Batch_idx: 250 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (31654/32128)
Epoch: 220 | Batch_idx: 260 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (32911/33408)
Epoch: 220 | Batch_idx: 270 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (34168/34688)
Epoch: 220 | Batch_idx: 280 |  Loss: (0.0490) |  Loss2: (0.0000) | Acc: (98.00%) (35418/35968)
Epoch: 220 | Batch_idx: 290 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (36672/37248)
Epoch: 220 | Batch_idx: 300 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (37927/38528)
Epoch: 220 | Batch_idx: 310 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (39178/39808)
Epoch: 220 | Batch_idx: 320 |  Loss: (0.0505) |  Loss2: (0.0000) | Acc: (98.00%) (40440/41088)
Epoch: 220 | Batch_idx: 330 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (41694/42368)
Epoch: 220 | Batch_idx: 340 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (42955/43648)
Epoch: 220 | Batch_idx: 350 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (44222/44928)
Epoch: 220 | Batch_idx: 360 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (45473/46208)
Epoch: 220 | Batch_idx: 370 |  Loss: (0.0510) |  Loss2: (0.0000) | Acc: (98.00%) (46730/47488)
Epoch: 220 | Batch_idx: 380 |  Loss: (0.0512) |  Loss2: (0.0000) | Acc: (98.00%) (47988/48768)
Epoch: 220 | Batch_idx: 390 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (49199/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_220.pth.tar'
# TEST : Loss: (0.4601) | Acc: (89.00%) (8910/10000)
percent tensor([0.5984, 0.4016], device='cuda:0')
percent tensor([0.5951, 0.4049], device='cuda:0')
percent tensor([0.6020, 0.3980], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.6551, 0.3449], device='cuda:0')
percent tensor([0.7447, 0.2553], device='cuda:0')
percent tensor([0.7633, 0.2367], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(184.4155, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(835.3308, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(839.6172, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.4095, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(473.9271, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2327.3760, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4257.7256, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1336.1738, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6330.8511, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11480.8428, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3753.1980, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15855.3867, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 221 | Batch_idx: 0 |  Loss: (0.0586) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 221 | Batch_idx: 10 |  Loss: (0.0590) |  Loss2: (0.0000) | Acc: (98.00%) (1380/1408)
Epoch: 221 | Batch_idx: 20 |  Loss: (0.0544) |  Loss2: (0.0000) | Acc: (98.00%) (2639/2688)
Epoch: 221 | Batch_idx: 30 |  Loss: (0.0537) |  Loss2: (0.0000) | Acc: (98.00%) (3896/3968)
Epoch: 221 | Batch_idx: 40 |  Loss: (0.0519) |  Loss2: (0.0000) | Acc: (98.00%) (5158/5248)
Epoch: 221 | Batch_idx: 50 |  Loss: (0.0501) |  Loss2: (0.0000) | Acc: (98.00%) (6421/6528)
Epoch: 221 | Batch_idx: 60 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (7678/7808)
Epoch: 221 | Batch_idx: 70 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (8934/9088)
Epoch: 221 | Batch_idx: 80 |  Loss: (0.0508) |  Loss2: (0.0000) | Acc: (98.00%) (10192/10368)
Epoch: 221 | Batch_idx: 90 |  Loss: (0.0503) |  Loss2: (0.0000) | Acc: (98.00%) (11453/11648)
Epoch: 221 | Batch_idx: 100 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (12712/12928)
Epoch: 221 | Batch_idx: 110 |  Loss: (0.0507) |  Loss2: (0.0000) | Acc: (98.00%) (13965/14208)
Epoch: 221 | Batch_idx: 120 |  Loss: (0.0499) |  Loss2: (0.0000) | Acc: (98.00%) (15229/15488)
Epoch: 221 | Batch_idx: 130 |  Loss: (0.0494) |  Loss2: (0.0000) | Acc: (98.00%) (16490/16768)
Epoch: 221 | Batch_idx: 140 |  Loss: (0.0492) |  Loss2: (0.0000) | Acc: (98.00%) (17753/18048)
Epoch: 221 | Batch_idx: 150 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (19016/19328)
Epoch: 221 | Batch_idx: 160 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (20271/20608)
Epoch: 221 | Batch_idx: 170 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (21527/21888)
Epoch: 221 | Batch_idx: 180 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (22785/23168)
Epoch: 221 | Batch_idx: 190 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (24053/24448)
Epoch: 221 | Batch_idx: 200 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (25308/25728)
Epoch: 221 | Batch_idx: 210 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (26570/27008)
Epoch: 221 | Batch_idx: 220 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (27825/28288)
Epoch: 221 | Batch_idx: 230 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (29086/29568)
Epoch: 221 | Batch_idx: 240 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (30348/30848)
Epoch: 221 | Batch_idx: 250 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (31607/32128)
Epoch: 221 | Batch_idx: 260 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (32869/33408)
Epoch: 221 | Batch_idx: 270 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (34133/34688)
Epoch: 221 | Batch_idx: 280 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (35385/35968)
Epoch: 221 | Batch_idx: 290 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (36645/37248)
Epoch: 221 | Batch_idx: 300 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (37908/38528)
Epoch: 221 | Batch_idx: 310 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (39172/39808)
Epoch: 221 | Batch_idx: 320 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (40431/41088)
Epoch: 221 | Batch_idx: 330 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (41692/42368)
Epoch: 221 | Batch_idx: 340 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (42943/43648)
Epoch: 221 | Batch_idx: 350 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (44203/44928)
Epoch: 221 | Batch_idx: 360 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (45466/46208)
Epoch: 221 | Batch_idx: 370 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (46726/47488)
Epoch: 221 | Batch_idx: 380 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (47987/48768)
Epoch: 221 | Batch_idx: 390 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (49204/50000)
# TEST : Loss: (0.4573) | Acc: (88.00%) (8887/10000)
percent tensor([0.5976, 0.4024], device='cuda:0')
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.5973, 0.4027], device='cuda:0')
percent tensor([0.6941, 0.3059], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.7466, 0.2534], device='cuda:0')
percent tensor([0.7697, 0.2303], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 222 | Batch_idx: 0 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 222 | Batch_idx: 10 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 222 | Batch_idx: 20 |  Loss: (0.0515) |  Loss2: (0.0000) | Acc: (98.00%) (2640/2688)
Epoch: 222 | Batch_idx: 30 |  Loss: (0.0523) |  Loss2: (0.0000) | Acc: (98.00%) (3900/3968)
Epoch: 222 | Batch_idx: 40 |  Loss: (0.0489) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 222 | Batch_idx: 50 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (6429/6528)
Epoch: 222 | Batch_idx: 60 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (7685/7808)
Epoch: 222 | Batch_idx: 70 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (8940/9088)
Epoch: 222 | Batch_idx: 80 |  Loss: (0.0498) |  Loss2: (0.0000) | Acc: (98.00%) (10201/10368)
Epoch: 222 | Batch_idx: 90 |  Loss: (0.0504) |  Loss2: (0.0000) | Acc: (98.00%) (11457/11648)
Epoch: 222 | Batch_idx: 100 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (12718/12928)
Epoch: 222 | Batch_idx: 110 |  Loss: (0.0495) |  Loss2: (0.0000) | Acc: (98.00%) (13982/14208)
Epoch: 222 | Batch_idx: 120 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (15248/15488)
Epoch: 222 | Batch_idx: 130 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (16513/16768)
Epoch: 222 | Batch_idx: 140 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (17780/18048)
Epoch: 222 | Batch_idx: 150 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (19045/19328)
Epoch: 222 | Batch_idx: 160 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (20304/20608)
Epoch: 222 | Batch_idx: 170 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (21560/21888)
Epoch: 222 | Batch_idx: 180 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (22816/23168)
Epoch: 222 | Batch_idx: 190 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (24083/24448)
Epoch: 222 | Batch_idx: 200 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (25340/25728)
Epoch: 222 | Batch_idx: 210 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (26595/27008)
Epoch: 222 | Batch_idx: 220 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (27854/28288)
Epoch: 222 | Batch_idx: 230 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (29114/29568)
Epoch: 222 | Batch_idx: 240 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (30381/30848)
Epoch: 222 | Batch_idx: 250 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (31642/32128)
Epoch: 222 | Batch_idx: 260 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (32893/33408)
Epoch: 222 | Batch_idx: 270 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (34147/34688)
Epoch: 222 | Batch_idx: 280 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (35404/35968)
Epoch: 222 | Batch_idx: 290 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (36664/37248)
Epoch: 222 | Batch_idx: 300 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (37930/38528)
Epoch: 222 | Batch_idx: 310 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (39192/39808)
Epoch: 222 | Batch_idx: 320 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (40449/41088)
Epoch: 222 | Batch_idx: 330 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (41706/42368)
Epoch: 222 | Batch_idx: 340 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (42967/43648)
Epoch: 222 | Batch_idx: 350 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (44224/44928)
Epoch: 222 | Batch_idx: 360 |  Loss: (0.0486) |  Loss2: (0.0000) | Acc: (98.00%) (45476/46208)
Epoch: 222 | Batch_idx: 370 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (46737/47488)
Epoch: 222 | Batch_idx: 380 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (47997/48768)
Epoch: 222 | Batch_idx: 390 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (49217/50000)
# TEST : Loss: (0.4523) | Acc: (89.00%) (8924/10000)
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.5931, 0.4069], device='cuda:0')
percent tensor([0.5995, 0.4005], device='cuda:0')
percent tensor([0.6944, 0.3056], device='cuda:0')
percent tensor([0.6525, 0.3475], device='cuda:0')
percent tensor([0.7491, 0.2509], device='cuda:0')
percent tensor([0.7684, 0.2316], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 223 | Batch_idx: 0 |  Loss: (0.0233) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 223 | Batch_idx: 10 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (1392/1408)
Epoch: 223 | Batch_idx: 20 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (2656/2688)
Epoch: 223 | Batch_idx: 30 |  Loss: (0.0433) |  Loss2: (0.0000) | Acc: (98.00%) (3918/3968)
Epoch: 223 | Batch_idx: 40 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (5181/5248)
Epoch: 223 | Batch_idx: 50 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (6444/6528)
Epoch: 223 | Batch_idx: 60 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (7710/7808)
Epoch: 223 | Batch_idx: 70 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (8970/9088)
Epoch: 223 | Batch_idx: 80 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (10235/10368)
Epoch: 223 | Batch_idx: 90 |  Loss: (0.0426) |  Loss2: (0.0000) | Acc: (98.00%) (11503/11648)
Epoch: 223 | Batch_idx: 100 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (12762/12928)
Epoch: 223 | Batch_idx: 110 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (14020/14208)
Epoch: 223 | Batch_idx: 120 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (15285/15488)
Epoch: 223 | Batch_idx: 130 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (16542/16768)
Epoch: 223 | Batch_idx: 140 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (17801/18048)
Epoch: 223 | Batch_idx: 150 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (19064/19328)
Epoch: 223 | Batch_idx: 160 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (20327/20608)
Epoch: 223 | Batch_idx: 170 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (21593/21888)
Epoch: 223 | Batch_idx: 180 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (22859/23168)
Epoch: 223 | Batch_idx: 190 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (24120/24448)
Epoch: 223 | Batch_idx: 200 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (25375/25728)
Epoch: 223 | Batch_idx: 210 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (26631/27008)
Epoch: 223 | Batch_idx: 220 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (27896/28288)
Epoch: 223 | Batch_idx: 230 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (29162/29568)
Epoch: 223 | Batch_idx: 240 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (30428/30848)
Epoch: 223 | Batch_idx: 250 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (31689/32128)
Epoch: 223 | Batch_idx: 260 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (32945/33408)
Epoch: 223 | Batch_idx: 270 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (34203/34688)
Epoch: 223 | Batch_idx: 280 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (35468/35968)
Epoch: 223 | Batch_idx: 290 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (36732/37248)
Epoch: 223 | Batch_idx: 300 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (37988/38528)
Epoch: 223 | Batch_idx: 310 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (39247/39808)
Epoch: 223 | Batch_idx: 320 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (40509/41088)
Epoch: 223 | Batch_idx: 330 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (41769/42368)
Epoch: 223 | Batch_idx: 340 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (43030/43648)
Epoch: 223 | Batch_idx: 350 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (44288/44928)
Epoch: 223 | Batch_idx: 360 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (45551/46208)
Epoch: 223 | Batch_idx: 370 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (46811/47488)
Epoch: 223 | Batch_idx: 380 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (48073/48768)
Epoch: 223 | Batch_idx: 390 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (49289/50000)
# TEST : Loss: (0.4683) | Acc: (88.00%) (8891/10000)
percent tensor([0.5971, 0.4029], device='cuda:0')
percent tensor([0.5942, 0.4058], device='cuda:0')
percent tensor([0.6020, 0.3980], device='cuda:0')
percent tensor([0.6928, 0.3072], device='cuda:0')
percent tensor([0.6529, 0.3471], device='cuda:0')
percent tensor([0.7499, 0.2501], device='cuda:0')
percent tensor([0.7686, 0.2314], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 224 | Batch_idx: 0 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 224 | Batch_idx: 10 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (1384/1408)
Epoch: 224 | Batch_idx: 20 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 224 | Batch_idx: 30 |  Loss: (0.0496) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 224 | Batch_idx: 40 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 224 | Batch_idx: 50 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (6424/6528)
Epoch: 224 | Batch_idx: 60 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (7688/7808)
Epoch: 224 | Batch_idx: 70 |  Loss: (0.0488) |  Loss2: (0.0000) | Acc: (98.00%) (8943/9088)
Epoch: 224 | Batch_idx: 80 |  Loss: (0.0493) |  Loss2: (0.0000) | Acc: (98.00%) (10197/10368)
Epoch: 224 | Batch_idx: 90 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (11459/11648)
Epoch: 224 | Batch_idx: 100 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (12719/12928)
Epoch: 224 | Batch_idx: 110 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (13982/14208)
Epoch: 224 | Batch_idx: 120 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (15250/15488)
Epoch: 224 | Batch_idx: 130 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (16512/16768)
Epoch: 224 | Batch_idx: 140 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (17778/18048)
Epoch: 224 | Batch_idx: 150 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (19036/19328)
Epoch: 224 | Batch_idx: 160 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (20298/20608)
Epoch: 224 | Batch_idx: 170 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (21556/21888)
Epoch: 224 | Batch_idx: 180 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (22819/23168)
Epoch: 224 | Batch_idx: 190 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (24073/24448)
Epoch: 224 | Batch_idx: 200 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (25333/25728)
Epoch: 224 | Batch_idx: 210 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (26588/27008)
Epoch: 224 | Batch_idx: 220 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (27849/28288)
Epoch: 224 | Batch_idx: 230 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (29105/29568)
Epoch: 224 | Batch_idx: 240 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (30366/30848)
Epoch: 224 | Batch_idx: 250 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (31633/32128)
Epoch: 224 | Batch_idx: 260 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (32898/33408)
Epoch: 224 | Batch_idx: 270 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (34158/34688)
Epoch: 224 | Batch_idx: 280 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (35420/35968)
Epoch: 224 | Batch_idx: 290 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (36674/37248)
Epoch: 224 | Batch_idx: 300 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (37931/38528)
Epoch: 224 | Batch_idx: 310 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (39192/39808)
Epoch: 224 | Batch_idx: 320 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (40453/41088)
Epoch: 224 | Batch_idx: 330 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (41709/42368)
Epoch: 224 | Batch_idx: 340 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (42971/43648)
Epoch: 224 | Batch_idx: 350 |  Loss: (0.0481) |  Loss2: (0.0000) | Acc: (98.00%) (44226/44928)
Epoch: 224 | Batch_idx: 360 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (45478/46208)
Epoch: 224 | Batch_idx: 370 |  Loss: (0.0483) |  Loss2: (0.0000) | Acc: (98.00%) (46739/47488)
Epoch: 224 | Batch_idx: 380 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (47997/48768)
Epoch: 224 | Batch_idx: 390 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (49220/50000)
# TEST : Loss: (0.4487) | Acc: (89.00%) (8915/10000)
percent tensor([0.5994, 0.4006], device='cuda:0')
percent tensor([0.5945, 0.4055], device='cuda:0')
percent tensor([0.6032, 0.3968], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.6528, 0.3472], device='cuda:0')
percent tensor([0.7483, 0.2517], device='cuda:0')
percent tensor([0.7664, 0.2336], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 225 | Batch_idx: 0 |  Loss: (0.0654) |  Loss2: (0.0000) | Acc: (97.00%) (125/128)
Epoch: 225 | Batch_idx: 10 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (1390/1408)
Epoch: 225 | Batch_idx: 20 |  Loss: (0.0421) |  Loss2: (0.0000) | Acc: (98.00%) (2647/2688)
Epoch: 225 | Batch_idx: 30 |  Loss: (0.0473) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 225 | Batch_idx: 40 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (5164/5248)
Epoch: 225 | Batch_idx: 50 |  Loss: (0.0471) |  Loss2: (0.0000) | Acc: (98.00%) (6420/6528)
Epoch: 225 | Batch_idx: 60 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (7680/7808)
Epoch: 225 | Batch_idx: 70 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (8941/9088)
Epoch: 225 | Batch_idx: 80 |  Loss: (0.0466) |  Loss2: (0.0000) | Acc: (98.00%) (10198/10368)
Epoch: 225 | Batch_idx: 90 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (11461/11648)
Epoch: 225 | Batch_idx: 100 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (12722/12928)
Epoch: 225 | Batch_idx: 110 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (13988/14208)
Epoch: 225 | Batch_idx: 120 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (15251/15488)
Epoch: 225 | Batch_idx: 130 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (16511/16768)
Epoch: 225 | Batch_idx: 140 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (17767/18048)
Epoch: 225 | Batch_idx: 150 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (19035/19328)
Epoch: 225 | Batch_idx: 160 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (20298/20608)
Epoch: 225 | Batch_idx: 170 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (21560/21888)
Epoch: 225 | Batch_idx: 180 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (22822/23168)
Epoch: 225 | Batch_idx: 190 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (24081/24448)
Epoch: 225 | Batch_idx: 200 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (25340/25728)
Epoch: 225 | Batch_idx: 210 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (26605/27008)
Epoch: 225 | Batch_idx: 220 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (27860/28288)
Epoch: 225 | Batch_idx: 230 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (29125/29568)
Epoch: 225 | Batch_idx: 240 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (30383/30848)
Epoch: 225 | Batch_idx: 250 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (31642/32128)
Epoch: 225 | Batch_idx: 260 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (32904/33408)
Epoch: 225 | Batch_idx: 270 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (34167/34688)
Epoch: 225 | Batch_idx: 280 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (35422/35968)
Epoch: 225 | Batch_idx: 290 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (36678/37248)
Epoch: 225 | Batch_idx: 300 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (37931/38528)
Epoch: 225 | Batch_idx: 310 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (39195/39808)
Epoch: 225 | Batch_idx: 320 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (40453/41088)
Epoch: 225 | Batch_idx: 330 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (41714/42368)
Epoch: 225 | Batch_idx: 340 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (42970/43648)
Epoch: 225 | Batch_idx: 350 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (44229/44928)
Epoch: 225 | Batch_idx: 360 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (45493/46208)
Epoch: 225 | Batch_idx: 370 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (46755/47488)
Epoch: 225 | Batch_idx: 380 |  Loss: (0.0477) |  Loss2: (0.0000) | Acc: (98.00%) (48012/48768)
Epoch: 225 | Batch_idx: 390 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (49227/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_225.pth.tar'
# TEST : Loss: (0.4701) | Acc: (89.00%) (8919/10000)
percent tensor([0.5989, 0.4011], device='cuda:0')
percent tensor([0.5949, 0.4051], device='cuda:0')
percent tensor([0.6041, 0.3959], device='cuda:0')
percent tensor([0.6969, 0.3031], device='cuda:0')
percent tensor([0.6571, 0.3429], device='cuda:0')
percent tensor([0.7524, 0.2476], device='cuda:0')
percent tensor([0.7662, 0.2338], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 226 | Batch_idx: 0 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 226 | Batch_idx: 10 |  Loss: (0.0425) |  Loss2: (0.0000) | Acc: (98.00%) (1385/1408)
Epoch: 226 | Batch_idx: 20 |  Loss: (0.0452) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 226 | Batch_idx: 30 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 226 | Batch_idx: 40 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 226 | Batch_idx: 50 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (6433/6528)
Epoch: 226 | Batch_idx: 60 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (7690/7808)
Epoch: 226 | Batch_idx: 70 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (8962/9088)
Epoch: 226 | Batch_idx: 80 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (10229/10368)
Epoch: 226 | Batch_idx: 90 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (11486/11648)
Epoch: 226 | Batch_idx: 100 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (12737/12928)
Epoch: 226 | Batch_idx: 110 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (14001/14208)
Epoch: 226 | Batch_idx: 120 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (15259/15488)
Epoch: 226 | Batch_idx: 130 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (16523/16768)
Epoch: 226 | Batch_idx: 140 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (17787/18048)
Epoch: 226 | Batch_idx: 150 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (19051/19328)
Epoch: 226 | Batch_idx: 160 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (20308/20608)
Epoch: 226 | Batch_idx: 170 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (21576/21888)
Epoch: 226 | Batch_idx: 180 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (22837/23168)
Epoch: 226 | Batch_idx: 190 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (24095/24448)
Epoch: 226 | Batch_idx: 200 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (25360/25728)
Epoch: 226 | Batch_idx: 210 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (26620/27008)
Epoch: 226 | Batch_idx: 220 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (27886/28288)
Epoch: 226 | Batch_idx: 230 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (29145/29568)
Epoch: 226 | Batch_idx: 240 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (30411/30848)
Epoch: 226 | Batch_idx: 250 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (31671/32128)
Epoch: 226 | Batch_idx: 260 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (32931/33408)
Epoch: 226 | Batch_idx: 270 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (34193/34688)
Epoch: 226 | Batch_idx: 280 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (35459/35968)
Epoch: 226 | Batch_idx: 290 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (36715/37248)
Epoch: 226 | Batch_idx: 300 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (37971/38528)
Epoch: 226 | Batch_idx: 310 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (39234/39808)
Epoch: 226 | Batch_idx: 320 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (40491/41088)
Epoch: 226 | Batch_idx: 330 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (41742/42368)
Epoch: 226 | Batch_idx: 340 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (43005/43648)
Epoch: 226 | Batch_idx: 350 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (44262/44928)
Epoch: 226 | Batch_idx: 360 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (45522/46208)
Epoch: 226 | Batch_idx: 370 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (46786/47488)
Epoch: 226 | Batch_idx: 380 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (48048/48768)
Epoch: 226 | Batch_idx: 390 |  Loss: (0.0458) |  Loss2: (0.0000) | Acc: (98.00%) (49264/50000)
# TEST : Loss: (0.4629) | Acc: (88.00%) (8899/10000)
percent tensor([0.5990, 0.4010], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6037, 0.3963], device='cuda:0')
percent tensor([0.6980, 0.3020], device='cuda:0')
percent tensor([0.6547, 0.3453], device='cuda:0')
percent tensor([0.7498, 0.2502], device='cuda:0')
percent tensor([0.7628, 0.2372], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 227 | Batch_idx: 0 |  Loss: (0.0430) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 227 | Batch_idx: 10 |  Loss: (0.0575) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 227 | Batch_idx: 20 |  Loss: (0.0513) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 227 | Batch_idx: 30 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 227 | Batch_idx: 40 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (5169/5248)
Epoch: 227 | Batch_idx: 50 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (6433/6528)
Epoch: 227 | Batch_idx: 60 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (7700/7808)
Epoch: 227 | Batch_idx: 70 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (8964/9088)
Epoch: 227 | Batch_idx: 80 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (10231/10368)
Epoch: 227 | Batch_idx: 90 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (11495/11648)
Epoch: 227 | Batch_idx: 100 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (12755/12928)
Epoch: 227 | Batch_idx: 110 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (14025/14208)
Epoch: 227 | Batch_idx: 120 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (15290/15488)
Epoch: 227 | Batch_idx: 130 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (16550/16768)
Epoch: 227 | Batch_idx: 140 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (17813/18048)
Epoch: 227 | Batch_idx: 150 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (19082/19328)
Epoch: 227 | Batch_idx: 160 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (20347/20608)
Epoch: 227 | Batch_idx: 170 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (21607/21888)
Epoch: 227 | Batch_idx: 180 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (22874/23168)
Epoch: 227 | Batch_idx: 190 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (24130/24448)
Epoch: 227 | Batch_idx: 200 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (25392/25728)
Epoch: 227 | Batch_idx: 210 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (26659/27008)
Epoch: 227 | Batch_idx: 220 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (27924/28288)
Epoch: 227 | Batch_idx: 230 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (29187/29568)
Epoch: 227 | Batch_idx: 240 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (30446/30848)
Epoch: 227 | Batch_idx: 250 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (31713/32128)
Epoch: 227 | Batch_idx: 260 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (32977/33408)
Epoch: 227 | Batch_idx: 270 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (34235/34688)
Epoch: 227 | Batch_idx: 280 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (35494/35968)
Epoch: 227 | Batch_idx: 290 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (36763/37248)
Epoch: 227 | Batch_idx: 300 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (38027/38528)
Epoch: 227 | Batch_idx: 310 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (39280/39808)
Epoch: 227 | Batch_idx: 320 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (40547/41088)
Epoch: 227 | Batch_idx: 330 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (41806/42368)
Epoch: 227 | Batch_idx: 340 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (43064/43648)
Epoch: 227 | Batch_idx: 350 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (44325/44928)
Epoch: 227 | Batch_idx: 360 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (45584/46208)
Epoch: 227 | Batch_idx: 370 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (46847/47488)
Epoch: 227 | Batch_idx: 380 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (48102/48768)
Epoch: 227 | Batch_idx: 390 |  Loss: (0.0423) |  Loss2: (0.0000) | Acc: (98.00%) (49324/50000)
# TEST : Loss: (0.4556) | Acc: (89.00%) (8946/10000)
percent tensor([0.6003, 0.3997], device='cuda:0')
percent tensor([0.5942, 0.4058], device='cuda:0')
percent tensor([0.6053, 0.3947], device='cuda:0')
percent tensor([0.6956, 0.3044], device='cuda:0')
percent tensor([0.6506, 0.3494], device='cuda:0')
percent tensor([0.7550, 0.2450], device='cuda:0')
percent tensor([0.7651, 0.2349], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 228 | Batch_idx: 0 |  Loss: (0.0309) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 228 | Batch_idx: 10 |  Loss: (0.0363) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 228 | Batch_idx: 20 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (2653/2688)
Epoch: 228 | Batch_idx: 30 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (3916/3968)
Epoch: 228 | Batch_idx: 40 |  Loss: (0.0438) |  Loss2: (0.0000) | Acc: (98.00%) (5174/5248)
Epoch: 228 | Batch_idx: 50 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (6435/6528)
Epoch: 228 | Batch_idx: 60 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (7703/7808)
Epoch: 228 | Batch_idx: 70 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (8974/9088)
Epoch: 228 | Batch_idx: 80 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (10237/10368)
Epoch: 228 | Batch_idx: 90 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (11491/11648)
Epoch: 228 | Batch_idx: 100 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (12747/12928)
Epoch: 228 | Batch_idx: 110 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (14011/14208)
Epoch: 228 | Batch_idx: 120 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (15264/15488)
Epoch: 228 | Batch_idx: 130 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (16527/16768)
Epoch: 228 | Batch_idx: 140 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (17784/18048)
Epoch: 228 | Batch_idx: 150 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (19045/19328)
Epoch: 228 | Batch_idx: 160 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (20302/20608)
Epoch: 228 | Batch_idx: 170 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (21563/21888)
Epoch: 228 | Batch_idx: 180 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (22819/23168)
Epoch: 228 | Batch_idx: 190 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (24082/24448)
Epoch: 228 | Batch_idx: 200 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (25347/25728)
Epoch: 228 | Batch_idx: 210 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (26615/27008)
Epoch: 228 | Batch_idx: 220 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (27869/28288)
Epoch: 228 | Batch_idx: 230 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (29126/29568)
Epoch: 228 | Batch_idx: 240 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (30387/30848)
Epoch: 228 | Batch_idx: 250 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (31648/32128)
Epoch: 228 | Batch_idx: 260 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (32909/33408)
Epoch: 228 | Batch_idx: 270 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (34169/34688)
Epoch: 228 | Batch_idx: 280 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (35430/35968)
Epoch: 228 | Batch_idx: 290 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (36689/37248)
Epoch: 228 | Batch_idx: 300 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (37953/38528)
Epoch: 228 | Batch_idx: 310 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (39219/39808)
Epoch: 228 | Batch_idx: 320 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (40483/41088)
Epoch: 228 | Batch_idx: 330 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (41744/42368)
Epoch: 228 | Batch_idx: 340 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (43003/43648)
Epoch: 228 | Batch_idx: 350 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (44262/44928)
Epoch: 228 | Batch_idx: 360 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (45523/46208)
Epoch: 228 | Batch_idx: 370 |  Loss: (0.0470) |  Loss2: (0.0000) | Acc: (98.00%) (46774/47488)
Epoch: 228 | Batch_idx: 380 |  Loss: (0.0472) |  Loss2: (0.0000) | Acc: (98.00%) (48033/48768)
Epoch: 228 | Batch_idx: 390 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (49244/50000)
# TEST : Loss: (0.4969) | Acc: (88.00%) (8829/10000)
percent tensor([0.6032, 0.3968], device='cuda:0')
percent tensor([0.5965, 0.4035], device='cuda:0')
percent tensor([0.6055, 0.3945], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.6527, 0.3473], device='cuda:0')
percent tensor([0.7501, 0.2499], device='cuda:0')
percent tensor([0.7576, 0.2424], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 229 | Batch_idx: 0 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (126/128)
Epoch: 229 | Batch_idx: 10 |  Loss: (0.0485) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 229 | Batch_idx: 20 |  Loss: (0.0497) |  Loss2: (0.0000) | Acc: (98.00%) (2645/2688)
Epoch: 229 | Batch_idx: 30 |  Loss: (0.0484) |  Loss2: (0.0000) | Acc: (98.00%) (3907/3968)
Epoch: 229 | Batch_idx: 40 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (5175/5248)
Epoch: 229 | Batch_idx: 50 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (6431/6528)
Epoch: 229 | Batch_idx: 60 |  Loss: (0.0475) |  Loss2: (0.0000) | Acc: (98.00%) (7689/7808)
Epoch: 229 | Batch_idx: 70 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (8946/9088)
Epoch: 229 | Batch_idx: 80 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (10206/10368)
Epoch: 229 | Batch_idx: 90 |  Loss: (0.0478) |  Loss2: (0.0000) | Acc: (98.00%) (11462/11648)
Epoch: 229 | Batch_idx: 100 |  Loss: (0.0482) |  Loss2: (0.0000) | Acc: (98.00%) (12716/12928)
Epoch: 229 | Batch_idx: 110 |  Loss: (0.0480) |  Loss2: (0.0000) | Acc: (98.00%) (13976/14208)
Epoch: 229 | Batch_idx: 120 |  Loss: (0.0479) |  Loss2: (0.0000) | Acc: (98.00%) (15237/15488)
Epoch: 229 | Batch_idx: 130 |  Loss: (0.0476) |  Loss2: (0.0000) | Acc: (98.00%) (16498/16768)
Epoch: 229 | Batch_idx: 140 |  Loss: (0.0474) |  Loss2: (0.0000) | Acc: (98.00%) (17758/18048)
Epoch: 229 | Batch_idx: 150 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (19024/19328)
Epoch: 229 | Batch_idx: 160 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (20294/20608)
Epoch: 229 | Batch_idx: 170 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (21560/21888)
Epoch: 229 | Batch_idx: 180 |  Loss: (0.0468) |  Loss2: (0.0000) | Acc: (98.00%) (22817/23168)
Epoch: 229 | Batch_idx: 190 |  Loss: (0.0469) |  Loss2: (0.0000) | Acc: (98.00%) (24072/24448)
Epoch: 229 | Batch_idx: 200 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (25336/25728)
Epoch: 229 | Batch_idx: 210 |  Loss: (0.0467) |  Loss2: (0.0000) | Acc: (98.00%) (26594/27008)
Epoch: 229 | Batch_idx: 220 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (27856/28288)
Epoch: 229 | Batch_idx: 230 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (29119/29568)
Epoch: 229 | Batch_idx: 240 |  Loss: (0.0463) |  Loss2: (0.0000) | Acc: (98.00%) (30384/30848)
Epoch: 229 | Batch_idx: 250 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (31650/32128)
Epoch: 229 | Batch_idx: 260 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (32914/33408)
Epoch: 229 | Batch_idx: 270 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (34171/34688)
Epoch: 229 | Batch_idx: 280 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (35432/35968)
Epoch: 229 | Batch_idx: 290 |  Loss: (0.0460) |  Loss2: (0.0000) | Acc: (98.00%) (36694/37248)
Epoch: 229 | Batch_idx: 300 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (37954/38528)
Epoch: 229 | Batch_idx: 310 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (39214/39808)
Epoch: 229 | Batch_idx: 320 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (40473/41088)
Epoch: 229 | Batch_idx: 330 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (41731/42368)
Epoch: 229 | Batch_idx: 340 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (42991/43648)
Epoch: 229 | Batch_idx: 350 |  Loss: (0.0462) |  Loss2: (0.0000) | Acc: (98.00%) (44255/44928)
Epoch: 229 | Batch_idx: 360 |  Loss: (0.0459) |  Loss2: (0.0000) | Acc: (98.00%) (45520/46208)
Epoch: 229 | Batch_idx: 370 |  Loss: (0.0461) |  Loss2: (0.0000) | Acc: (98.00%) (46776/47488)
Epoch: 229 | Batch_idx: 380 |  Loss: (0.0464) |  Loss2: (0.0000) | Acc: (98.00%) (48025/48768)
Epoch: 229 | Batch_idx: 390 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (49236/50000)
# TEST : Loss: (0.5089) | Acc: (87.00%) (8787/10000)
percent tensor([0.6009, 0.3991], device='cuda:0')
percent tensor([0.5969, 0.4031], device='cuda:0')
percent tensor([0.6059, 0.3941], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.6545, 0.3455], device='cuda:0')
percent tensor([0.7517, 0.2483], device='cuda:0')
percent tensor([0.7604, 0.2396], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 230 | Batch_idx: 0 |  Loss: (0.0263) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 230 | Batch_idx: 10 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 230 | Batch_idx: 20 |  Loss: (0.0509) |  Loss2: (0.0000) | Acc: (98.00%) (2644/2688)
Epoch: 230 | Batch_idx: 30 |  Loss: (0.0487) |  Loss2: (0.0000) | Acc: (98.00%) (3904/3968)
Epoch: 230 | Batch_idx: 40 |  Loss: (0.0454) |  Loss2: (0.0000) | Acc: (98.00%) (5168/5248)
Epoch: 230 | Batch_idx: 50 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (6438/6528)
Epoch: 230 | Batch_idx: 60 |  Loss: (0.0431) |  Loss2: (0.0000) | Acc: (98.00%) (7705/7808)
Epoch: 230 | Batch_idx: 70 |  Loss: (0.0434) |  Loss2: (0.0000) | Acc: (98.00%) (8964/9088)
Epoch: 230 | Batch_idx: 80 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (10236/10368)
Epoch: 230 | Batch_idx: 90 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (11502/11648)
Epoch: 230 | Batch_idx: 100 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (12760/12928)
Epoch: 230 | Batch_idx: 110 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (14006/14208)
Epoch: 230 | Batch_idx: 120 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (15272/15488)
Epoch: 230 | Batch_idx: 130 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (16529/16768)
Epoch: 230 | Batch_idx: 140 |  Loss: (0.0429) |  Loss2: (0.0000) | Acc: (98.00%) (17795/18048)
Epoch: 230 | Batch_idx: 150 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (19054/19328)
Epoch: 230 | Batch_idx: 160 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (20321/20608)
Epoch: 230 | Batch_idx: 170 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (21586/21888)
Epoch: 230 | Batch_idx: 180 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (22853/23168)
Epoch: 230 | Batch_idx: 190 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (24120/24448)
Epoch: 230 | Batch_idx: 200 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (25390/25728)
Epoch: 230 | Batch_idx: 210 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (26655/27008)
Epoch: 230 | Batch_idx: 220 |  Loss: (0.0419) |  Loss2: (0.0000) | Acc: (98.00%) (27916/28288)
Epoch: 230 | Batch_idx: 230 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (29177/29568)
Epoch: 230 | Batch_idx: 240 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (30447/30848)
Epoch: 230 | Batch_idx: 250 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (31716/32128)
Epoch: 230 | Batch_idx: 260 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (32975/33408)
Epoch: 230 | Batch_idx: 270 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (34237/34688)
Epoch: 230 | Batch_idx: 280 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (35500/35968)
Epoch: 230 | Batch_idx: 290 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (36757/37248)
Epoch: 230 | Batch_idx: 300 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (38019/38528)
Epoch: 230 | Batch_idx: 310 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (39284/39808)
Epoch: 230 | Batch_idx: 320 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (40543/41088)
Epoch: 230 | Batch_idx: 330 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (41803/42368)
Epoch: 230 | Batch_idx: 340 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (43070/43648)
Epoch: 230 | Batch_idx: 350 |  Loss: (0.0422) |  Loss2: (0.0000) | Acc: (98.00%) (44330/44928)
Epoch: 230 | Batch_idx: 360 |  Loss: (0.0420) |  Loss2: (0.0000) | Acc: (98.00%) (45599/46208)
Epoch: 230 | Batch_idx: 370 |  Loss: (0.0424) |  Loss2: (0.0000) | Acc: (98.00%) (46854/47488)
Epoch: 230 | Batch_idx: 380 |  Loss: (0.0428) |  Loss2: (0.0000) | Acc: (98.00%) (48108/48768)
Epoch: 230 | Batch_idx: 390 |  Loss: (0.0435) |  Loss2: (0.0000) | Acc: (98.00%) (49308/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_230.pth.tar'
# TEST : Loss: (0.4833) | Acc: (88.00%) (8868/10000)
percent tensor([0.5992, 0.4008], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6042, 0.3958], device='cuda:0')
percent tensor([0.6936, 0.3064], device='cuda:0')
percent tensor([0.6500, 0.3500], device='cuda:0')
percent tensor([0.7542, 0.2458], device='cuda:0')
percent tensor([0.7640, 0.2360], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.0036, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(837.7780, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(842.1896, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.7166, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(472.3576, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2336.6162, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4259.3608, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1331.4941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6355.9507, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11452.4600, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3738.8018, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15791.1738, device='cuda:0', grad_fn=<NormBackward0>)
Epoch: 231 | Batch_idx: 0 |  Loss: (0.0732) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 231 | Batch_idx: 10 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (1382/1408)
Epoch: 231 | Batch_idx: 20 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (2651/2688)
Epoch: 231 | Batch_idx: 30 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (3917/3968)
Epoch: 231 | Batch_idx: 40 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (5184/5248)
Epoch: 231 | Batch_idx: 50 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (6449/6528)
Epoch: 231 | Batch_idx: 60 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (7711/7808)
Epoch: 231 | Batch_idx: 70 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (8976/9088)
Epoch: 231 | Batch_idx: 80 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (10240/10368)
Epoch: 231 | Batch_idx: 90 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (11507/11648)
Epoch: 231 | Batch_idx: 100 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (12766/12928)
Epoch: 231 | Batch_idx: 110 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (14032/14208)
Epoch: 231 | Batch_idx: 120 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (15298/15488)
Epoch: 231 | Batch_idx: 130 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (16565/16768)
Epoch: 231 | Batch_idx: 140 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (17836/18048)
Epoch: 231 | Batch_idx: 150 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (19098/19328)
Epoch: 231 | Batch_idx: 160 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (20361/20608)
Epoch: 231 | Batch_idx: 170 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (21628/21888)
Epoch: 231 | Batch_idx: 180 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (22891/23168)
Epoch: 231 | Batch_idx: 190 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (24152/24448)
Epoch: 231 | Batch_idx: 200 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (25412/25728)
Epoch: 231 | Batch_idx: 210 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (26673/27008)
Epoch: 231 | Batch_idx: 220 |  Loss: (0.0408) |  Loss2: (0.0000) | Acc: (98.00%) (27934/28288)
Epoch: 231 | Batch_idx: 230 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (29191/29568)
Epoch: 231 | Batch_idx: 240 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (30456/30848)
Epoch: 231 | Batch_idx: 250 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (31719/32128)
Epoch: 231 | Batch_idx: 260 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (32975/33408)
Epoch: 231 | Batch_idx: 270 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (34238/34688)
Epoch: 231 | Batch_idx: 280 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (35506/35968)
Epoch: 231 | Batch_idx: 290 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (36766/37248)
Epoch: 231 | Batch_idx: 300 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (38027/38528)
Epoch: 231 | Batch_idx: 310 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (39290/39808)
Epoch: 231 | Batch_idx: 320 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (40553/41088)
Epoch: 231 | Batch_idx: 330 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (41818/42368)
Epoch: 231 | Batch_idx: 340 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (43084/43648)
Epoch: 231 | Batch_idx: 350 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (44351/44928)
Epoch: 231 | Batch_idx: 360 |  Loss: (0.0409) |  Loss2: (0.0000) | Acc: (98.00%) (45615/46208)
Epoch: 231 | Batch_idx: 370 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (46884/47488)
Epoch: 231 | Batch_idx: 380 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (48148/48768)
Epoch: 231 | Batch_idx: 390 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (49366/50000)
# TEST : Loss: (0.4496) | Acc: (89.00%) (8936/10000)
percent tensor([0.6012, 0.3988], device='cuda:0')
percent tensor([0.5950, 0.4050], device='cuda:0')
percent tensor([0.6063, 0.3937], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.6540, 0.3460], device='cuda:0')
percent tensor([0.7573, 0.2427], device='cuda:0')
percent tensor([0.7650, 0.2350], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 232 | Batch_idx: 0 |  Loss: (0.0177) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 232 | Batch_idx: 10 |  Loss: (0.0279) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 232 | Batch_idx: 20 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (2663/2688)
Epoch: 232 | Batch_idx: 30 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (98.00%) (3927/3968)
Epoch: 232 | Batch_idx: 40 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (5194/5248)
Epoch: 232 | Batch_idx: 50 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (6462/6528)
Epoch: 232 | Batch_idx: 60 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (98.00%) (7729/7808)
Epoch: 232 | Batch_idx: 70 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (8987/9088)
Epoch: 232 | Batch_idx: 80 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 232 | Batch_idx: 90 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (11513/11648)
Epoch: 232 | Batch_idx: 100 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (12775/12928)
Epoch: 232 | Batch_idx: 110 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (14036/14208)
Epoch: 232 | Batch_idx: 120 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (15295/15488)
Epoch: 232 | Batch_idx: 130 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (16562/16768)
Epoch: 232 | Batch_idx: 140 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (17824/18048)
Epoch: 232 | Batch_idx: 150 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (19093/19328)
Epoch: 232 | Batch_idx: 160 |  Loss: (0.0402) |  Loss2: (0.0000) | Acc: (98.00%) (20353/20608)
Epoch: 232 | Batch_idx: 170 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (21614/21888)
Epoch: 232 | Batch_idx: 180 |  Loss: (0.0401) |  Loss2: (0.0000) | Acc: (98.00%) (22884/23168)
Epoch: 232 | Batch_idx: 190 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (24144/24448)
Epoch: 232 | Batch_idx: 200 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (25406/25728)
Epoch: 232 | Batch_idx: 210 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (26665/27008)
Epoch: 232 | Batch_idx: 220 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (27934/28288)
Epoch: 232 | Batch_idx: 230 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (29202/29568)
Epoch: 232 | Batch_idx: 240 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (30464/30848)
Epoch: 232 | Batch_idx: 250 |  Loss: (0.0405) |  Loss2: (0.0000) | Acc: (98.00%) (31722/32128)
Epoch: 232 | Batch_idx: 260 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (32990/33408)
Epoch: 232 | Batch_idx: 270 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (34255/34688)
Epoch: 232 | Batch_idx: 280 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (35512/35968)
Epoch: 232 | Batch_idx: 290 |  Loss: (0.0410) |  Loss2: (0.0000) | Acc: (98.00%) (36768/37248)
Epoch: 232 | Batch_idx: 300 |  Loss: (0.0412) |  Loss2: (0.0000) | Acc: (98.00%) (38035/38528)
Epoch: 232 | Batch_idx: 310 |  Loss: (0.0411) |  Loss2: (0.0000) | Acc: (98.00%) (39298/39808)
Epoch: 232 | Batch_idx: 320 |  Loss: (0.0407) |  Loss2: (0.0000) | Acc: (98.00%) (40569/41088)
Epoch: 232 | Batch_idx: 330 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (41825/42368)
Epoch: 232 | Batch_idx: 340 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (43085/43648)
Epoch: 232 | Batch_idx: 350 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (44347/44928)
Epoch: 232 | Batch_idx: 360 |  Loss: (0.0414) |  Loss2: (0.0000) | Acc: (98.00%) (45608/46208)
Epoch: 232 | Batch_idx: 370 |  Loss: (0.0416) |  Loss2: (0.0000) | Acc: (98.00%) (46864/47488)
Epoch: 232 | Batch_idx: 380 |  Loss: (0.0417) |  Loss2: (0.0000) | Acc: (98.00%) (48127/48768)
Epoch: 232 | Batch_idx: 390 |  Loss: (0.0418) |  Loss2: (0.0000) | Acc: (98.00%) (49341/50000)
# TEST : Loss: (0.4666) | Acc: (89.00%) (8922/10000)
percent tensor([0.6000, 0.4000], device='cuda:0')
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.6056, 0.3944], device='cuda:0')
percent tensor([0.6986, 0.3014], device='cuda:0')
percent tensor([0.6551, 0.3449], device='cuda:0')
percent tensor([0.7544, 0.2456], device='cuda:0')
percent tensor([0.7670, 0.2330], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 233 | Batch_idx: 0 |  Loss: (0.0285) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 233 | Batch_idx: 10 |  Loss: (0.0273) |  Loss2: (0.0000) | Acc: (99.00%) (1401/1408)
Epoch: 233 | Batch_idx: 20 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (99.00%) (2665/2688)
Epoch: 233 | Batch_idx: 30 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (99.00%) (3935/3968)
Epoch: 233 | Batch_idx: 40 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (99.00%) (5199/5248)
Epoch: 233 | Batch_idx: 50 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (99.00%) (6468/6528)
Epoch: 233 | Batch_idx: 60 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (99.00%) (7731/7808)
Epoch: 233 | Batch_idx: 70 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (99.00%) (8998/9088)
Epoch: 233 | Batch_idx: 80 |  Loss: (0.0364) |  Loss2: (0.0000) | Acc: (98.00%) (10254/10368)
Epoch: 233 | Batch_idx: 90 |  Loss: (0.0361) |  Loss2: (0.0000) | Acc: (98.00%) (11523/11648)
Epoch: 233 | Batch_idx: 100 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (12785/12928)
Epoch: 233 | Batch_idx: 110 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (14051/14208)
Epoch: 233 | Batch_idx: 120 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (15309/15488)
Epoch: 233 | Batch_idx: 130 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (16579/16768)
Epoch: 233 | Batch_idx: 140 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (17842/18048)
Epoch: 233 | Batch_idx: 150 |  Loss: (0.0366) |  Loss2: (0.0000) | Acc: (98.00%) (19113/19328)
Epoch: 233 | Batch_idx: 160 |  Loss: (0.0365) |  Loss2: (0.0000) | Acc: (98.00%) (20378/20608)
Epoch: 233 | Batch_idx: 170 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (21638/21888)
Epoch: 233 | Batch_idx: 180 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (22904/23168)
Epoch: 233 | Batch_idx: 190 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (24162/24448)
Epoch: 233 | Batch_idx: 200 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (25430/25728)
Epoch: 233 | Batch_idx: 210 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (26682/27008)
Epoch: 233 | Batch_idx: 220 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (27941/28288)
Epoch: 233 | Batch_idx: 230 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (29200/29568)
Epoch: 233 | Batch_idx: 240 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (30464/30848)
Epoch: 233 | Batch_idx: 250 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (31728/32128)
Epoch: 233 | Batch_idx: 260 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (32994/33408)
Epoch: 233 | Batch_idx: 270 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (34261/34688)
Epoch: 233 | Batch_idx: 280 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (35524/35968)
Epoch: 233 | Batch_idx: 290 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (36789/37248)
Epoch: 233 | Batch_idx: 300 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (38047/38528)
Epoch: 233 | Batch_idx: 310 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (39307/39808)
Epoch: 233 | Batch_idx: 320 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (40569/41088)
Epoch: 233 | Batch_idx: 330 |  Loss: (0.0399) |  Loss2: (0.0000) | Acc: (98.00%) (41830/42368)
Epoch: 233 | Batch_idx: 340 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (43103/43648)
Epoch: 233 | Batch_idx: 350 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (44370/44928)
Epoch: 233 | Batch_idx: 360 |  Loss: (0.0398) |  Loss2: (0.0000) | Acc: (98.00%) (45624/46208)
Epoch: 233 | Batch_idx: 370 |  Loss: (0.0400) |  Loss2: (0.0000) | Acc: (98.00%) (46885/47488)
Epoch: 233 | Batch_idx: 380 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (48139/48768)
Epoch: 233 | Batch_idx: 390 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (49353/50000)
# TEST : Loss: (0.5365) | Acc: (88.00%) (8819/10000)
percent tensor([0.6015, 0.3985], device='cuda:0')
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.6080, 0.3920], device='cuda:0')
percent tensor([0.6965, 0.3035], device='cuda:0')
percent tensor([0.6585, 0.3415], device='cuda:0')
percent tensor([0.7570, 0.2430], device='cuda:0')
percent tensor([0.7645, 0.2355], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 234 | Batch_idx: 0 |  Loss: (0.0604) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 234 | Batch_idx: 10 |  Loss: (0.0415) |  Loss2: (0.0000) | Acc: (98.00%) (1387/1408)
Epoch: 234 | Batch_idx: 20 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (2646/2688)
Epoch: 234 | Batch_idx: 30 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (3903/3968)
Epoch: 234 | Batch_idx: 40 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (5167/5248)
Epoch: 234 | Batch_idx: 50 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (6430/6528)
Epoch: 234 | Batch_idx: 60 |  Loss: (0.0455) |  Loss2: (0.0000) | Acc: (98.00%) (7688/7808)
Epoch: 234 | Batch_idx: 70 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (8954/9088)
Epoch: 234 | Batch_idx: 80 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (10211/10368)
Epoch: 234 | Batch_idx: 90 |  Loss: (0.0456) |  Loss2: (0.0000) | Acc: (98.00%) (11467/11648)
Epoch: 234 | Batch_idx: 100 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (12735/12928)
Epoch: 234 | Batch_idx: 110 |  Loss: (0.0439) |  Loss2: (0.0000) | Acc: (98.00%) (14000/14208)
Epoch: 234 | Batch_idx: 120 |  Loss: (0.0436) |  Loss2: (0.0000) | Acc: (98.00%) (15264/15488)
Epoch: 234 | Batch_idx: 130 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (16521/16768)
Epoch: 234 | Batch_idx: 140 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (17783/18048)
Epoch: 234 | Batch_idx: 150 |  Loss: (0.0441) |  Loss2: (0.0000) | Acc: (98.00%) (19049/19328)
Epoch: 234 | Batch_idx: 160 |  Loss: (0.0437) |  Loss2: (0.0000) | Acc: (98.00%) (20316/20608)
Epoch: 234 | Batch_idx: 170 |  Loss: (0.0442) |  Loss2: (0.0000) | Acc: (98.00%) (21572/21888)
Epoch: 234 | Batch_idx: 180 |  Loss: (0.0443) |  Loss2: (0.0000) | Acc: (98.00%) (22829/23168)
Epoch: 234 | Batch_idx: 190 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (24095/24448)
Epoch: 234 | Batch_idx: 200 |  Loss: (0.0440) |  Loss2: (0.0000) | Acc: (98.00%) (25353/25728)
Epoch: 234 | Batch_idx: 210 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (26598/27008)
Epoch: 234 | Batch_idx: 220 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (27859/28288)
Epoch: 234 | Batch_idx: 230 |  Loss: (0.0457) |  Loss2: (0.0000) | Acc: (98.00%) (29117/29568)
Epoch: 234 | Batch_idx: 240 |  Loss: (0.0453) |  Loss2: (0.0000) | Acc: (98.00%) (30387/30848)
Epoch: 234 | Batch_idx: 250 |  Loss: (0.0451) |  Loss2: (0.0000) | Acc: (98.00%) (31648/32128)
Epoch: 234 | Batch_idx: 260 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (32912/33408)
Epoch: 234 | Batch_idx: 270 |  Loss: (0.0450) |  Loss2: (0.0000) | Acc: (98.00%) (34177/34688)
Epoch: 234 | Batch_idx: 280 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (35438/35968)
Epoch: 234 | Batch_idx: 290 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (36704/37248)
Epoch: 234 | Batch_idx: 300 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (37962/38528)
Epoch: 234 | Batch_idx: 310 |  Loss: (0.0448) |  Loss2: (0.0000) | Acc: (98.00%) (39220/39808)
Epoch: 234 | Batch_idx: 320 |  Loss: (0.0446) |  Loss2: (0.0000) | Acc: (98.00%) (40484/41088)
Epoch: 234 | Batch_idx: 330 |  Loss: (0.0447) |  Loss2: (0.0000) | Acc: (98.00%) (41743/42368)
Epoch: 234 | Batch_idx: 340 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (43010/43648)
Epoch: 234 | Batch_idx: 350 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (44274/44928)
Epoch: 234 | Batch_idx: 360 |  Loss: (0.0444) |  Loss2: (0.0000) | Acc: (98.00%) (45534/46208)
Epoch: 234 | Batch_idx: 370 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (46791/47488)
Epoch: 234 | Batch_idx: 380 |  Loss: (0.0445) |  Loss2: (0.0000) | Acc: (98.00%) (48050/48768)
Epoch: 234 | Batch_idx: 390 |  Loss: (0.0449) |  Loss2: (0.0000) | Acc: (98.00%) (49264/50000)
# TEST : Loss: (0.5480) | Acc: (87.00%) (8753/10000)
percent tensor([0.6032, 0.3968], device='cuda:0')
percent tensor([0.5954, 0.4046], device='cuda:0')
percent tensor([0.6070, 0.3930], device='cuda:0')
percent tensor([0.6935, 0.3065], device='cuda:0')
percent tensor([0.6578, 0.3422], device='cuda:0')
percent tensor([0.7600, 0.2400], device='cuda:0')
percent tensor([0.7564, 0.2436], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 235 | Batch_idx: 0 |  Loss: (0.0270) |  Loss2: (0.0000) | Acc: (100.00%) (128/128)
Epoch: 235 | Batch_idx: 10 |  Loss: (0.0404) |  Loss2: (0.0000) | Acc: (98.00%) (1393/1408)
Epoch: 235 | Batch_idx: 20 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (2659/2688)
Epoch: 235 | Batch_idx: 30 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (3919/3968)
Epoch: 235 | Batch_idx: 40 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (5186/5248)
Epoch: 235 | Batch_idx: 50 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (98.00%) (6451/6528)
Epoch: 235 | Batch_idx: 60 |  Loss: (0.0393) |  Loss2: (0.0000) | Acc: (98.00%) (7720/7808)
Epoch: 235 | Batch_idx: 70 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (8989/9088)
Epoch: 235 | Batch_idx: 80 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (10256/10368)
Epoch: 235 | Batch_idx: 90 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (11521/11648)
Epoch: 235 | Batch_idx: 100 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (12786/12928)
Epoch: 235 | Batch_idx: 110 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (14051/14208)
Epoch: 235 | Batch_idx: 120 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (15317/15488)
Epoch: 235 | Batch_idx: 130 |  Loss: (0.0378) |  Loss2: (0.0000) | Acc: (98.00%) (16581/16768)
Epoch: 235 | Batch_idx: 140 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (17847/18048)
Epoch: 235 | Batch_idx: 150 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (19106/19328)
Epoch: 235 | Batch_idx: 160 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (20371/20608)
Epoch: 235 | Batch_idx: 170 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (21639/21888)
Epoch: 235 | Batch_idx: 180 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (22904/23168)
Epoch: 235 | Batch_idx: 190 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (24166/24448)
Epoch: 235 | Batch_idx: 200 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (25430/25728)
Epoch: 235 | Batch_idx: 210 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (26691/27008)
Epoch: 235 | Batch_idx: 220 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (27958/28288)
Epoch: 235 | Batch_idx: 230 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (29220/29568)
Epoch: 235 | Batch_idx: 240 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (30492/30848)
Epoch: 235 | Batch_idx: 250 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (31756/32128)
Epoch: 235 | Batch_idx: 260 |  Loss: (0.0380) |  Loss2: (0.0000) | Acc: (98.00%) (33025/33408)
Epoch: 235 | Batch_idx: 270 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (34288/34688)
Epoch: 235 | Batch_idx: 280 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (35551/35968)
Epoch: 235 | Batch_idx: 290 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (36811/37248)
Epoch: 235 | Batch_idx: 300 |  Loss: (0.0386) |  Loss2: (0.0000) | Acc: (98.00%) (38075/38528)
Epoch: 235 | Batch_idx: 310 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (39342/39808)
Epoch: 235 | Batch_idx: 320 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (40609/41088)
Epoch: 235 | Batch_idx: 330 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (41875/42368)
Epoch: 235 | Batch_idx: 340 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (43139/43648)
Epoch: 235 | Batch_idx: 350 |  Loss: (0.0387) |  Loss2: (0.0000) | Acc: (98.00%) (44399/44928)
Epoch: 235 | Batch_idx: 360 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (45653/46208)
Epoch: 235 | Batch_idx: 370 |  Loss: (0.0392) |  Loss2: (0.0000) | Acc: (98.00%) (46914/47488)
Epoch: 235 | Batch_idx: 380 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (48174/48768)
Epoch: 235 | Batch_idx: 390 |  Loss: (0.0395) |  Loss2: (0.0000) | Acc: (98.00%) (49389/50000)
=> saving checkpoint 'drive/app/torch/save_Routing_Gate_2/checkpoint_235.pth.tar'
# TEST : Loss: (0.4728) | Acc: (89.00%) (8932/10000)
percent tensor([0.6049, 0.3951], device='cuda:0')
percent tensor([0.5948, 0.4052], device='cuda:0')
percent tensor([0.6087, 0.3913], device='cuda:0')
percent tensor([0.6931, 0.3069], device='cuda:0')
percent tensor([0.6590, 0.3410], device='cuda:0')
percent tensor([0.7609, 0.2391], device='cuda:0')
percent tensor([0.7607, 0.2393], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 236 | Batch_idx: 0 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 236 | Batch_idx: 10 |  Loss: (0.0330) |  Loss2: (0.0000) | Acc: (99.00%) (1395/1408)
Epoch: 236 | Batch_idx: 20 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (2652/2688)
Epoch: 236 | Batch_idx: 30 |  Loss: (0.0413) |  Loss2: (0.0000) | Acc: (98.00%) (3915/3968)
Epoch: 236 | Batch_idx: 40 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (5180/5248)
Epoch: 236 | Batch_idx: 50 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (6448/6528)
Epoch: 236 | Batch_idx: 60 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (7721/7808)
Epoch: 236 | Batch_idx: 70 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (8987/9088)
Epoch: 236 | Batch_idx: 80 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (10253/10368)
Epoch: 236 | Batch_idx: 90 |  Loss: (0.0382) |  Loss2: (0.0000) | Acc: (98.00%) (11512/11648)
Epoch: 236 | Batch_idx: 100 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (12781/12928)
Epoch: 236 | Batch_idx: 110 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (14049/14208)
Epoch: 236 | Batch_idx: 120 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (15310/15488)
Epoch: 236 | Batch_idx: 130 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (16575/16768)
Epoch: 236 | Batch_idx: 140 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (17839/18048)
Epoch: 236 | Batch_idx: 150 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (19104/19328)
Epoch: 236 | Batch_idx: 160 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (20375/20608)
Epoch: 236 | Batch_idx: 170 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (21637/21888)
Epoch: 236 | Batch_idx: 180 |  Loss: (0.0373) |  Loss2: (0.0000) | Acc: (98.00%) (22906/23168)
Epoch: 236 | Batch_idx: 190 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (24178/24448)
Epoch: 236 | Batch_idx: 200 |  Loss: (0.0367) |  Loss2: (0.0000) | Acc: (98.00%) (25445/25728)
Epoch: 236 | Batch_idx: 210 |  Loss: (0.0370) |  Loss2: (0.0000) | Acc: (98.00%) (26709/27008)
Epoch: 236 | Batch_idx: 220 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (27971/28288)
Epoch: 236 | Batch_idx: 230 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (29238/29568)
Epoch: 236 | Batch_idx: 240 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (30505/30848)
Epoch: 236 | Batch_idx: 250 |  Loss: (0.0369) |  Loss2: (0.0000) | Acc: (98.00%) (31773/32128)
Epoch: 236 | Batch_idx: 260 |  Loss: (0.0372) |  Loss2: (0.0000) | Acc: (98.00%) (33033/33408)
Epoch: 236 | Batch_idx: 270 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (34289/34688)
Epoch: 236 | Batch_idx: 280 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (35561/35968)
Epoch: 236 | Batch_idx: 290 |  Loss: (0.0371) |  Loss2: (0.0000) | Acc: (98.00%) (36828/37248)
Epoch: 236 | Batch_idx: 300 |  Loss: (0.0374) |  Loss2: (0.0000) | Acc: (98.00%) (38087/38528)
Epoch: 236 | Batch_idx: 310 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (39349/39808)
Epoch: 236 | Batch_idx: 320 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (40608/41088)
Epoch: 236 | Batch_idx: 330 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (41865/42368)
Epoch: 236 | Batch_idx: 340 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (43131/43648)
Epoch: 236 | Batch_idx: 350 |  Loss: (0.0379) |  Loss2: (0.0000) | Acc: (98.00%) (44397/44928)
Epoch: 236 | Batch_idx: 360 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (45657/46208)
Epoch: 236 | Batch_idx: 370 |  Loss: (0.0383) |  Loss2: (0.0000) | Acc: (98.00%) (46919/47488)
Epoch: 236 | Batch_idx: 380 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (48179/48768)
Epoch: 236 | Batch_idx: 390 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (49399/50000)
# TEST : Loss: (0.4822) | Acc: (89.00%) (8919/10000)
percent tensor([0.6046, 0.3954], device='cuda:0')
percent tensor([0.5930, 0.4070], device='cuda:0')
percent tensor([0.6064, 0.3936], device='cuda:0')
percent tensor([0.6964, 0.3036], device='cuda:0')
percent tensor([0.6537, 0.3463], device='cuda:0')
percent tensor([0.7609, 0.2391], device='cuda:0')
percent tensor([0.7680, 0.2320], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 237 | Batch_idx: 0 |  Loss: (0.0278) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 237 | Batch_idx: 10 |  Loss: (0.0296) |  Loss2: (0.0000) | Acc: (99.00%) (1397/1408)
Epoch: 237 | Batch_idx: 20 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (2659/2688)
Epoch: 237 | Batch_idx: 30 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (3925/3968)
Epoch: 237 | Batch_idx: 40 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (5194/5248)
Epoch: 237 | Batch_idx: 50 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (6462/6528)
Epoch: 237 | Batch_idx: 60 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (7732/7808)
Epoch: 237 | Batch_idx: 70 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (9001/9088)
Epoch: 237 | Batch_idx: 80 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (10276/10368)
Epoch: 237 | Batch_idx: 90 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (99.00%) (11542/11648)
Epoch: 237 | Batch_idx: 100 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (99.00%) (12806/12928)
Epoch: 237 | Batch_idx: 110 |  Loss: (0.0340) |  Loss2: (0.0000) | Acc: (99.00%) (14074/14208)
Epoch: 237 | Batch_idx: 120 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (99.00%) (15339/15488)
Epoch: 237 | Batch_idx: 130 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (99.00%) (16608/16768)
Epoch: 237 | Batch_idx: 140 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (99.00%) (17876/18048)
Epoch: 237 | Batch_idx: 150 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (99.00%) (19144/19328)
Epoch: 237 | Batch_idx: 160 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (99.00%) (20408/20608)
Epoch: 237 | Batch_idx: 170 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (21669/21888)
Epoch: 237 | Batch_idx: 180 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (22930/23168)
Epoch: 237 | Batch_idx: 190 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (24198/24448)
Epoch: 237 | Batch_idx: 200 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (25463/25728)
Epoch: 237 | Batch_idx: 210 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (26730/27008)
Epoch: 237 | Batch_idx: 220 |  Loss: (0.0352) |  Loss2: (0.0000) | Acc: (98.00%) (27987/28288)
Epoch: 237 | Batch_idx: 230 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (29253/29568)
Epoch: 237 | Batch_idx: 240 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (98.00%) (30515/30848)
Epoch: 237 | Batch_idx: 250 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (31781/32128)
Epoch: 237 | Batch_idx: 260 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (33045/33408)
Epoch: 237 | Batch_idx: 270 |  Loss: (0.0362) |  Loss2: (0.0000) | Acc: (98.00%) (34306/34688)
Epoch: 237 | Batch_idx: 280 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (35562/35968)
Epoch: 237 | Batch_idx: 290 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (36817/37248)
Epoch: 237 | Batch_idx: 300 |  Loss: (0.0377) |  Loss2: (0.0000) | Acc: (98.00%) (38079/38528)
Epoch: 237 | Batch_idx: 310 |  Loss: (0.0381) |  Loss2: (0.0000) | Acc: (98.00%) (39335/39808)
Epoch: 237 | Batch_idx: 320 |  Loss: (0.0384) |  Loss2: (0.0000) | Acc: (98.00%) (40593/41088)
Epoch: 237 | Batch_idx: 330 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (41858/42368)
Epoch: 237 | Batch_idx: 340 |  Loss: (0.0385) |  Loss2: (0.0000) | Acc: (98.00%) (43121/43648)
Epoch: 237 | Batch_idx: 350 |  Loss: (0.0389) |  Loss2: (0.0000) | Acc: (98.00%) (44375/44928)
Epoch: 237 | Batch_idx: 360 |  Loss: (0.0391) |  Loss2: (0.0000) | Acc: (98.00%) (45633/46208)
Epoch: 237 | Batch_idx: 370 |  Loss: (0.0394) |  Loss2: (0.0000) | Acc: (98.00%) (46890/47488)
Epoch: 237 | Batch_idx: 380 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (48151/48768)
Epoch: 237 | Batch_idx: 390 |  Loss: (0.0397) |  Loss2: (0.0000) | Acc: (98.00%) (49368/50000)
# TEST : Loss: (0.5283) | Acc: (88.00%) (8838/10000)
percent tensor([0.6016, 0.3984], device='cuda:0')
percent tensor([0.5924, 0.4076], device='cuda:0')
percent tensor([0.6061, 0.3939], device='cuda:0')
percent tensor([0.6949, 0.3051], device='cuda:0')
percent tensor([0.6599, 0.3401], device='cuda:0')
percent tensor([0.7588, 0.2412], device='cuda:0')
percent tensor([0.7666, 0.2334], device='cuda:0')
percent tensor([0.9998, 0.0002], device='cuda:0')
Epoch: 238 | Batch_idx: 0 |  Loss: (0.0667) |  Loss2: (0.0000) | Acc: (96.00%) (124/128)
Epoch: 238 | Batch_idx: 10 |  Loss: (0.0465) |  Loss2: (0.0000) | Acc: (98.00%) (1388/1408)
Epoch: 238 | Batch_idx: 20 |  Loss: (0.0403) |  Loss2: (0.0000) | Acc: (98.00%) (2657/2688)
Epoch: 238 | Batch_idx: 30 |  Loss: (0.0376) |  Loss2: (0.0000) | Acc: (98.00%) (3925/3968)
Epoch: 238 | Batch_idx: 40 |  Loss: (0.0388) |  Loss2: (0.0000) | Acc: (98.00%) (5190/5248)
Epoch: 238 | Batch_idx: 50 |  Loss: (0.0375) |  Loss2: (0.0000) | Acc: (98.00%) (6458/6528)
Epoch: 238 | Batch_idx: 60 |  Loss: (0.0368) |  Loss2: (0.0000) | Acc: (98.00%) (7728/7808)
Epoch: 238 | Batch_idx: 70 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (99.00%) (9001/9088)
Epoch: 238 | Batch_idx: 80 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (99.00%) (10270/10368)
Epoch: 238 | Batch_idx: 90 |  Loss: (0.0353) |  Loss2: (0.0000) | Acc: (99.00%) (11536/11648)
Epoch: 238 | Batch_idx: 100 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (99.00%) (12808/12928)
Epoch: 238 | Batch_idx: 110 |  Loss: (0.0342) |  Loss2: (0.0000) | Acc: (99.00%) (14077/14208)
Epoch: 238 | Batch_idx: 120 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (15350/15488)
Epoch: 238 | Batch_idx: 130 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (99.00%) (16614/16768)
Epoch: 238 | Batch_idx: 140 |  Loss: (0.0332) |  Loss2: (0.0000) | Acc: (99.00%) (17887/18048)
Epoch: 238 | Batch_idx: 150 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (99.00%) (19155/19328)
Epoch: 238 | Batch_idx: 160 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (20428/20608)
Epoch: 238 | Batch_idx: 170 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (21699/21888)
Epoch: 238 | Batch_idx: 180 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (22968/23168)
Epoch: 238 | Batch_idx: 190 |  Loss: (0.0323) |  Loss2: (0.0000) | Acc: (99.00%) (24233/24448)
Epoch: 238 | Batch_idx: 200 |  Loss: (0.0324) |  Loss2: (0.0000) | Acc: (99.00%) (25498/25728)
Epoch: 238 | Batch_idx: 210 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (26768/27008)
Epoch: 238 | Batch_idx: 220 |  Loss: (0.0325) |  Loss2: (0.0000) | Acc: (99.00%) (28037/28288)
Epoch: 238 | Batch_idx: 230 |  Loss: (0.0326) |  Loss2: (0.0000) | Acc: (99.00%) (29306/29568)
Epoch: 238 | Batch_idx: 240 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (99.00%) (30571/30848)
Epoch: 238 | Batch_idx: 250 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (99.00%) (31837/32128)
Epoch: 238 | Batch_idx: 260 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (99.00%) (33100/33408)
Epoch: 238 | Batch_idx: 270 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (99.00%) (34367/34688)
Epoch: 238 | Batch_idx: 280 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (99.00%) (35632/35968)
Epoch: 238 | Batch_idx: 290 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (99.00%) (36897/37248)
Epoch: 238 | Batch_idx: 300 |  Loss: (0.0343) |  Loss2: (0.0000) | Acc: (99.00%) (38159/38528)
Epoch: 238 | Batch_idx: 310 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (99.00%) (39424/39808)
Epoch: 238 | Batch_idx: 320 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (99.00%) (40688/41088)
Epoch: 238 | Batch_idx: 330 |  Loss: (0.0346) |  Loss2: (0.0000) | Acc: (99.00%) (41950/42368)
Epoch: 238 | Batch_idx: 340 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (99.00%) (43216/43648)
Epoch: 238 | Batch_idx: 350 |  Loss: (0.0347) |  Loss2: (0.0000) | Acc: (99.00%) (44482/44928)
Epoch: 238 | Batch_idx: 360 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (45743/46208)
Epoch: 238 | Batch_idx: 370 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (47007/47488)
Epoch: 238 | Batch_idx: 380 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (48271/48768)
Epoch: 238 | Batch_idx: 390 |  Loss: (0.0350) |  Loss2: (0.0000) | Acc: (98.00%) (49496/50000)
# TEST : Loss: (0.4782) | Acc: (89.00%) (8907/10000)
percent tensor([0.6046, 0.3954], device='cuda:0')
percent tensor([0.5925, 0.4075], device='cuda:0')
percent tensor([0.6085, 0.3915], device='cuda:0')
percent tensor([0.6974, 0.3026], device='cuda:0')
percent tensor([0.6621, 0.3379], device='cuda:0')
percent tensor([0.7595, 0.2405], device='cuda:0')
percent tensor([0.7639, 0.2361], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Epoch: 239 | Batch_idx: 0 |  Loss: (0.0406) |  Loss2: (0.0000) | Acc: (99.00%) (127/128)
Epoch: 239 | Batch_idx: 10 |  Loss: (0.0396) |  Loss2: (0.0000) | Acc: (98.00%) (1389/1408)
Epoch: 239 | Batch_idx: 20 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (2656/2688)
Epoch: 239 | Batch_idx: 30 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (3921/3968)
Epoch: 239 | Batch_idx: 40 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (5190/5248)
Epoch: 239 | Batch_idx: 50 |  Loss: (0.0328) |  Loss2: (0.0000) | Acc: (98.00%) (6460/6528)
Epoch: 239 | Batch_idx: 60 |  Loss: (0.0344) |  Loss2: (0.0000) | Acc: (98.00%) (7724/7808)
Epoch: 239 | Batch_idx: 70 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (8982/9088)
Epoch: 239 | Batch_idx: 80 |  Loss: (0.0349) |  Loss2: (0.0000) | Acc: (98.00%) (10250/10368)
Epoch: 239 | Batch_idx: 90 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (11516/11648)
Epoch: 239 | Batch_idx: 100 |  Loss: (0.0341) |  Loss2: (0.0000) | Acc: (98.00%) (12789/12928)
Epoch: 239 | Batch_idx: 110 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (14061/14208)
Epoch: 239 | Batch_idx: 120 |  Loss: (0.0334) |  Loss2: (0.0000) | Acc: (98.00%) (15329/15488)
Epoch: 239 | Batch_idx: 130 |  Loss: (0.0333) |  Loss2: (0.0000) | Acc: (98.00%) (16595/16768)
Epoch: 239 | Batch_idx: 140 |  Loss: (0.0331) |  Loss2: (0.0000) | Acc: (98.00%) (17863/18048)
Epoch: 239 | Batch_idx: 150 |  Loss: (0.0338) |  Loss2: (0.0000) | Acc: (98.00%) (19124/19328)
Epoch: 239 | Batch_idx: 160 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (20391/20608)
Epoch: 239 | Batch_idx: 170 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (21660/21888)
Epoch: 239 | Batch_idx: 180 |  Loss: (0.0336) |  Loss2: (0.0000) | Acc: (98.00%) (22926/23168)
Epoch: 239 | Batch_idx: 190 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (24193/24448)
Epoch: 239 | Batch_idx: 200 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (25460/25728)
Epoch: 239 | Batch_idx: 210 |  Loss: (0.0335) |  Loss2: (0.0000) | Acc: (98.00%) (26730/27008)
Epoch: 239 | Batch_idx: 220 |  Loss: (0.0337) |  Loss2: (0.0000) | Acc: (98.00%) (27992/28288)
Epoch: 239 | Batch_idx: 230 |  Loss: (0.0339) |  Loss2: (0.0000) | Acc: (98.00%) (29256/29568)
Epoch: 239 | Batch_idx: 240 |  Loss: (0.0345) |  Loss2: (0.0000) | Acc: (98.00%) (30516/30848)
Epoch: 239 | Batch_idx: 250 |  Loss: (0.0348) |  Loss2: (0.0000) | Acc: (98.00%) (31779/32128)
Epoch: 239 | Batch_idx: 260 |  Loss: (0.0351) |  Loss2: (0.0000) | Acc: (98.00%) (33041/33408)
Epoch: 239 | Batch_idx: 270 |  Loss: (0.0354) |  Loss2: (0.0000) | Acc: (98.00%) (34303/34688)
Epoch: 239 | Batch_idx: 280 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (35566/35968)
Epoch: 239 | Batch_idx: 290 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (36828/37248)
Epoch: 239 | Batch_idx: 300 |  Loss: (0.0360) |  Loss2: (0.0000) | Acc: (98.00%) (38095/38528)
Epoch: 239 | Batch_idx: 310 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (39370/39808)
Epoch: 239 | Batch_idx: 320 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (40639/41088)
Epoch: 239 | Batch_idx: 330 |  Loss: (0.0357) |  Loss2: (0.0000) | Acc: (98.00%) (41903/42368)
Epoch: 239 | Batch_idx: 340 |  Loss: (0.0355) |  Loss2: (0.0000) | Acc: (98.00%) (43172/43648)
Epoch: 239 | Batch_idx: 350 |  Loss: (0.0356) |  Loss2: (0.0000) | Acc: (98.00%) (44439/44928)
Epoch: 239 | Batch_idx: 360 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (45704/46208)
Epoch: 239 | Batch_idx: 370 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (46968/47488)
Epoch: 239 | Batch_idx: 380 |  Loss: (0.0358) |  Loss2: (0.0000) | Acc: (98.00%) (48236/48768)
Epoch: 239 | Batch_idx: 390 |  Loss: (0.0359) |  Loss2: (0.0000) | Acc: (98.00%) (49451/50000)
# TEST : Loss: (0.4848) | Acc: (89.00%) (8900/10000)
percent tensor([0.6031, 0.3969], device='cuda:0')
percent tensor([0.5962, 0.4038], device='cuda:0')
percent tensor([0.6066, 0.3934], device='cuda:0')
percent tensor([0.6977, 0.3023], device='cuda:0')
percent tensor([0.6644, 0.3356], device='cuda:0')
percent tensor([0.7618, 0.2382], device='cuda:0')
percent tensor([0.7696, 0.2304], device='cuda:0')
percent tensor([0.9999, 0.0001], device='cuda:0')
Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 3, 3, 3]) tensor(185.1941, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(839.3493, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([64, 64, 3, 3]) tensor(844.2593, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([128, 64, 3, 3]) tensor(1515.5647, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([128, 64, 1, 1]) tensor(470.9799, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([128, 128, 3, 3]) tensor(2343.2397, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([256, 128, 3, 3]) tensor(4259.6582, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([256, 128, 1, 1]) tensor(1327.3346, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([256, 256, 3, 3]) tensor(6375.0479, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) torch.Size([512, 256, 3, 3]) tensor(11425.7852, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) torch.Size([512, 256, 1, 1]) tensor(3725.8076, device='cuda:0', grad_fn=<NormBackward0>)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) torch.Size([512, 512, 3, 3]) tensor(15733.9062, device='cuda:0', grad_fn=<NormBackward0>)
2 hours 27 mins 30 secs for training